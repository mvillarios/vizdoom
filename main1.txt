/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 16.5     |
|    ep_rew_mean      | -0.065   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 4        |
|    fps              | 499      |
|    time_elapsed     | 0        |
|    total_timesteps  | 66       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 16.9     |
|    ep_rew_mean      | 0.0586   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 8        |
|    fps              | 864      |
|    time_elapsed     | 0        |
|    total_timesteps  | 135      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0951   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1192     |
|    time_elapsed     | 0        |
|    total_timesteps  | 218      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0551   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1381     |
|    time_elapsed     | 0        |
|    total_timesteps  | 284      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0794   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1552     |
|    time_elapsed     | 0        |
|    total_timesteps  | 358      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0525   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1710     |
|    time_elapsed     | 0        |
|    total_timesteps  | 441      |
----------------------------------
Eval num_timesteps=500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 500      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.034    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 28       |
|    fps              | 68       |
|    time_elapsed     | 7        |
|    total_timesteps  | 519      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0195   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 32       |
|    fps              | 78       |
|    time_elapsed     | 7        |
|    total_timesteps  | 602      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00947  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 36       |
|    fps              | 87       |
|    time_elapsed     | 7        |
|    total_timesteps  | 674      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0268   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 40       |
|    fps              | 96       |
|    time_elapsed     | 7        |
|    total_timesteps  | 743      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0184   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 44       |
|    fps              | 104      |
|    time_elapsed     | 7        |
|    total_timesteps  | 809      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0111   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 48       |
|    fps              | 113      |
|    time_elapsed     | 7        |
|    total_timesteps  | 879      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0241   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 52       |
|    fps              | 122      |
|    time_elapsed     | 7        |
|    total_timesteps  | 950      |
----------------------------------
Eval num_timesteps=1000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 56       |
|    fps              | 99       |
|    time_elapsed     | 10       |
|    total_timesteps  | 1025     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0111   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 60       |
|    fps              | 106      |
|    time_elapsed     | 10       |
|    total_timesteps  | 1099     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00561  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 64       |
|    fps              | 113      |
|    time_elapsed     | 10       |
|    total_timesteps  | 1177     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00134  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 68       |
|    fps              | 120      |
|    time_elapsed     | 10       |
|    total_timesteps  | 1245     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00279 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 72       |
|    fps              | 127      |
|    time_elapsed     | 10       |
|    total_timesteps  | 1319     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00754 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 76       |
|    fps              | 135      |
|    time_elapsed     | 10       |
|    total_timesteps  | 1413     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0111  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 80       |
|    fps              | 143      |
|    time_elapsed     | 10       |
|    total_timesteps  | 1492     |
----------------------------------
Eval num_timesteps=1500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0138  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 84       |
|    fps              | 121      |
|    time_elapsed     | 12       |
|    total_timesteps  | 1562     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.00547 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 88       |
|    fps              | 127      |
|    time_elapsed     | 12       |
|    total_timesteps  | 1643     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00253  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 92       |
|    fps              | 132      |
|    time_elapsed     | 12       |
|    total_timesteps  | 1716     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00049 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 96       |
|    fps              | 137      |
|    time_elapsed     | 12       |
|    total_timesteps  | 1787     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00311 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 100      |
|    fps              | 142      |
|    time_elapsed     | 12       |
|    total_timesteps  | 1854     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00319 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 104      |
|    fps              | 147      |
|    time_elapsed     | 13       |
|    total_timesteps  | 1922     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0132  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 108      |
|    fps              | 152      |
|    time_elapsed     | 13       |
|    total_timesteps  | 1990     |
----------------------------------
Eval num_timesteps=2000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 112      |
|    fps              | 133      |
|    time_elapsed     | 15       |
|    total_timesteps  | 2062     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0126  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 116      |
|    fps              | 137      |
|    time_elapsed     | 15       |
|    total_timesteps  | 2126     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0223  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 120      |
|    fps              | 141      |
|    time_elapsed     | 15       |
|    total_timesteps  | 2192     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0221  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 124      |
|    fps              | 145      |
|    time_elapsed     | 15       |
|    total_timesteps  | 2271     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0229  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 128      |
|    fps              | 151      |
|    time_elapsed     | 15       |
|    total_timesteps  | 2369     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0228  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 132      |
|    fps              | 156      |
|    time_elapsed     | 15       |
|    total_timesteps  | 2448     |
----------------------------------
Eval num_timesteps=2500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0129  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 136      |
|    fps              | 139      |
|    time_elapsed     | 18       |
|    total_timesteps  | 2522     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 140      |
|    fps              | 142      |
|    time_elapsed     | 18       |
|    total_timesteps  | 2585     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0227  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 144      |
|    fps              | 146      |
|    time_elapsed     | 18       |
|    total_timesteps  | 2654     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0131  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 148      |
|    fps              | 150      |
|    time_elapsed     | 18       |
|    total_timesteps  | 2733     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0232  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 152      |
|    fps              | 154      |
|    time_elapsed     | 18       |
|    total_timesteps  | 2805     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.023   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 156      |
|    fps              | 158      |
|    time_elapsed     | 18       |
|    total_timesteps  | 2876     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0233  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 160      |
|    fps              | 162      |
|    time_elapsed     | 18       |
|    total_timesteps  | 2958     |
----------------------------------
Eval num_timesteps=3000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0129  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 164      |
|    fps              | 146      |
|    time_elapsed     | 20       |
|    total_timesteps  | 3026     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0134  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 168      |
|    fps              | 150      |
|    time_elapsed     | 20       |
|    total_timesteps  | 3107     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0137  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 172      |
|    fps              | 153      |
|    time_elapsed     | 20       |
|    total_timesteps  | 3187     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0129  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 176      |
|    fps              | 157      |
|    time_elapsed     | 20       |
|    total_timesteps  | 3262     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0135  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 180      |
|    fps              | 161      |
|    time_elapsed     | 20       |
|    total_timesteps  | 3356     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0135  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 184      |
|    fps              | 164      |
|    time_elapsed     | 20       |
|    total_timesteps  | 3427     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0131  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 188      |
|    fps              | 168      |
|    time_elapsed     | 20       |
|    total_timesteps  | 3497     |
----------------------------------
Eval num_timesteps=3500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0229  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 192      |
|    fps              | 153      |
|    time_elapsed     | 23       |
|    total_timesteps  | 3565     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0234  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 196      |
|    fps              | 156      |
|    time_elapsed     | 23       |
|    total_timesteps  | 3649     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0141  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 200      |
|    fps              | 159      |
|    time_elapsed     | 23       |
|    total_timesteps  | 3732     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.014   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 204      |
|    fps              | 162      |
|    time_elapsed     | 23       |
|    total_timesteps  | 3797     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0143  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 208      |
|    fps              | 165      |
|    time_elapsed     | 23       |
|    total_timesteps  | 3874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0243  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 212      |
|    fps              | 168      |
|    time_elapsed     | 23       |
|    total_timesteps  | 3945     |
----------------------------------
Eval num_timesteps=4000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0145  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 216      |
|    fps              | 154      |
|    time_elapsed     | 25       |
|    total_timesteps  | 4014     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0146  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 220      |
|    fps              | 157      |
|    time_elapsed     | 25       |
|    total_timesteps  | 4082     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.00405 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 224      |
|    fps              | 159      |
|    time_elapsed     | 26       |
|    total_timesteps  | 4148     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00293 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 228      |
|    fps              | 162      |
|    time_elapsed     | 26       |
|    total_timesteps  | 4218     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00321 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 232      |
|    fps              | 165      |
|    time_elapsed     | 26       |
|    total_timesteps  | 4304     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.00718  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 236      |
|    fps              | 167      |
|    time_elapsed     | 26       |
|    total_timesteps  | 4369     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0163   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 240      |
|    fps              | 170      |
|    time_elapsed     | 26       |
|    total_timesteps  | 4455     |
----------------------------------
Eval num_timesteps=4500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0161   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 244      |
|    fps              | 158      |
|    time_elapsed     | 28       |
|    total_timesteps  | 4528     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00643  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 248      |
|    fps              | 160      |
|    time_elapsed     | 28       |
|    total_timesteps  | 4599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00647  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 252      |
|    fps              | 163      |
|    time_elapsed     | 28       |
|    total_timesteps  | 4670     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00631  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 256      |
|    fps              | 165      |
|    time_elapsed     | 28       |
|    total_timesteps  | 4745     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00675  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 260      |
|    fps              | 168      |
|    time_elapsed     | 28       |
|    total_timesteps  | 4816     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00647  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 264      |
|    fps              | 170      |
|    time_elapsed     | 28       |
|    total_timesteps  | 4891     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00679  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 268      |
|    fps              | 172      |
|    time_elapsed     | 28       |
|    total_timesteps  | 4964     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.017    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 272      |
|    fps              | 161      |
|    time_elapsed     | 31       |
|    total_timesteps  | 5040     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0173   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 276      |
|    fps              | 163      |
|    time_elapsed     | 31       |
|    total_timesteps  | 5107     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.018    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 280      |
|    fps              | 166      |
|    time_elapsed     | 31       |
|    total_timesteps  | 5184     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0179   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 284      |
|    fps              | 168      |
|    time_elapsed     | 31       |
|    total_timesteps  | 5256     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00792  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 288      |
|    fps              | 170      |
|    time_elapsed     | 31       |
|    total_timesteps  | 5326     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00772  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 292      |
|    fps              | 172      |
|    time_elapsed     | 31       |
|    total_timesteps  | 5399     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00848  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 296      |
|    fps              | 174      |
|    time_elapsed     | 31       |
|    total_timesteps  | 5464     |
----------------------------------
Eval num_timesteps=5500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00087 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 300      |
|    fps              | 163      |
|    time_elapsed     | 33       |
|    total_timesteps  | 5531     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00896  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 304      |
|    fps              | 165      |
|    time_elapsed     | 33       |
|    total_timesteps  | 5600     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00904  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 308      |
|    fps              | 167      |
|    time_elapsed     | 33       |
|    total_timesteps  | 5675     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0194   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 312      |
|    fps              | 169      |
|    time_elapsed     | 33       |
|    total_timesteps  | 5736     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00942  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 316      |
|    fps              | 171      |
|    time_elapsed     | 33       |
|    total_timesteps  | 5805     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00906  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 320      |
|    fps              | 173      |
|    time_elapsed     | 33       |
|    total_timesteps  | 5882     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00125 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 324      |
|    fps              | 175      |
|    time_elapsed     | 33       |
|    total_timesteps  | 5956     |
----------------------------------
Eval num_timesteps=6000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00161 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 328      |
|    fps              | 166      |
|    time_elapsed     | 36       |
|    total_timesteps  | 6035     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00129 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 332      |
|    fps              | 168      |
|    time_elapsed     | 36       |
|    total_timesteps  | 6113     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0223  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 336      |
|    fps              | 170      |
|    time_elapsed     | 36       |
|    total_timesteps  | 6202     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0316  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 340      |
|    fps              | 172      |
|    time_elapsed     | 36       |
|    total_timesteps  | 6272     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.032   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 344      |
|    fps              | 174      |
|    time_elapsed     | 36       |
|    total_timesteps  | 6353     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0115  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 348      |
|    fps              | 175      |
|    time_elapsed     | 36       |
|    total_timesteps  | 6412     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0113  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 352      |
|    fps              | 177      |
|    time_elapsed     | 36       |
|    total_timesteps  | 6478     |
----------------------------------
Eval num_timesteps=6500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.011   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 356      |
|    fps              | 168      |
|    time_elapsed     | 38       |
|    total_timesteps  | 6545     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0114  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 360      |
|    fps              | 170      |
|    time_elapsed     | 38       |
|    total_timesteps  | 6628     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0121  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 364      |
|    fps              | 172      |
|    time_elapsed     | 38       |
|    total_timesteps  | 6718     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0119  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 368      |
|    fps              | 173      |
|    time_elapsed     | 39       |
|    total_timesteps  | 6788     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0215  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 372      |
|    fps              | 175      |
|    time_elapsed     | 39       |
|    total_timesteps  | 6853     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0217  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 376      |
|    fps              | 177      |
|    time_elapsed     | 39       |
|    total_timesteps  | 6925     |
----------------------------------
Eval num_timesteps=7000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0122  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 380      |
|    fps              | 168      |
|    time_elapsed     | 41       |
|    total_timesteps  | 7013     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.012   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 384      |
|    fps              | 170      |
|    time_elapsed     | 41       |
|    total_timesteps  | 7082     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0122  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 388      |
|    fps              | 172      |
|    time_elapsed     | 41       |
|    total_timesteps  | 7157     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00197 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 392      |
|    fps              | 173      |
|    time_elapsed     | 41       |
|    total_timesteps  | 7223     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00189 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 396      |
|    fps              | 175      |
|    time_elapsed     | 41       |
|    total_timesteps  | 7286     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00245 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 400      |
|    fps              | 176      |
|    time_elapsed     | 41       |
|    total_timesteps  | 7367     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00263 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 404      |
|    fps              | 178      |
|    time_elapsed     | 41       |
|    total_timesteps  | 7441     |
----------------------------------
Eval num_timesteps=7500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00291 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 408      |
|    fps              | 170      |
|    time_elapsed     | 44       |
|    total_timesteps  | 7523     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0131  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 412      |
|    fps              | 171      |
|    time_elapsed     | 44       |
|    total_timesteps  | 7588     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0135  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 416      |
|    fps              | 173      |
|    time_elapsed     | 44       |
|    total_timesteps  | 7667     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0131  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 420      |
|    fps              | 175      |
|    time_elapsed     | 44       |
|    total_timesteps  | 7736     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0135  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 424      |
|    fps              | 176      |
|    time_elapsed     | 44       |
|    total_timesteps  | 7818     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00293 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 428      |
|    fps              | 178      |
|    time_elapsed     | 44       |
|    total_timesteps  | 7884     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00297 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 432      |
|    fps              | 179      |
|    time_elapsed     | 44       |
|    total_timesteps  | 7963     |
----------------------------------
Eval num_timesteps=8000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00225 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 436      |
|    fps              | 171      |
|    time_elapsed     | 46       |
|    total_timesteps  | 8034     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00221 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 440      |
|    fps              | 173      |
|    time_elapsed     | 46       |
|    total_timesteps  | 8103     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00213 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 444      |
|    fps              | 174      |
|    time_elapsed     | 46       |
|    total_timesteps  | 8182     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0125  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 448      |
|    fps              | 176      |
|    time_elapsed     | 46       |
|    total_timesteps  | 8250     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 452      |
|    fps              | 177      |
|    time_elapsed     | 46       |
|    total_timesteps  | 8322     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0126  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 456      |
|    fps              | 178      |
|    time_elapsed     | 46       |
|    total_timesteps  | 8386     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00228 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 460      |
|    fps              | 180      |
|    time_elapsed     | 46       |
|    total_timesteps  | 8460     |
----------------------------------
Eval num_timesteps=8500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0124  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 464      |
|    fps              | 173      |
|    time_elapsed     | 49       |
|    total_timesteps  | 8554     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0125  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 468      |
|    fps              | 174      |
|    time_elapsed     | 49       |
|    total_timesteps  | 8626     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0129  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 472      |
|    fps              | 176      |
|    time_elapsed     | 49       |
|    total_timesteps  | 8702     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0132  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 476      |
|    fps              | 177      |
|    time_elapsed     | 49       |
|    total_timesteps  | 8780     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0229  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 480      |
|    fps              | 179      |
|    time_elapsed     | 49       |
|    total_timesteps  | 8861     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00227 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 484      |
|    fps              | 180      |
|    time_elapsed     | 49       |
|    total_timesteps  | 8914     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00788  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 488      |
|    fps              | 181      |
|    time_elapsed     | 49       |
|    total_timesteps  | 8985     |
----------------------------------
Eval num_timesteps=9000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00219 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 492      |
|    fps              | 174      |
|    time_elapsed     | 51       |
|    total_timesteps  | 9053     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00757  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 496      |
|    fps              | 175      |
|    time_elapsed     | 52       |
|    total_timesteps  | 9122     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00801  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 500      |
|    fps              | 176      |
|    time_elapsed     | 52       |
|    total_timesteps  | 9192     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0018  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 504      |
|    fps              | 177      |
|    time_elapsed     | 52       |
|    total_timesteps  | 9261     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00224 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 508      |
|    fps              | 179      |
|    time_elapsed     | 52       |
|    total_timesteps  | 9354     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00769  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 512      |
|    fps              | 180      |
|    time_elapsed     | 52       |
|    total_timesteps  | 9421     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00821  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 516      |
|    fps              | 181      |
|    time_elapsed     | 52       |
|    total_timesteps  | 9487     |
----------------------------------
Eval num_timesteps=9500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00781  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 520      |
|    fps              | 175      |
|    time_elapsed     | 54       |
|    total_timesteps  | 9566     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0183   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 524      |
|    fps              | 176      |
|    time_elapsed     | 54       |
|    total_timesteps  | 9637     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0082   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 528      |
|    fps              | 177      |
|    time_elapsed     | 54       |
|    total_timesteps  | 9704     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0183   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 532      |
|    fps              | 178      |
|    time_elapsed     | 54       |
|    total_timesteps  | 9780     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0185   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 536      |
|    fps              | 180      |
|    time_elapsed     | 54       |
|    total_timesteps  | 9848     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0183   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 540      |
|    fps              | 181      |
|    time_elapsed     | 54       |
|    total_timesteps  | 9920     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0187   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 544      |
|    fps              | 182      |
|    time_elapsed     | 54       |
|    total_timesteps  | 9991     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00846  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 548      |
|    fps              | 175      |
|    time_elapsed     | 57       |
|    total_timesteps  | 10064    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0087   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 552      |
|    fps              | 176      |
|    time_elapsed     | 57       |
|    total_timesteps  | 10130    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00846  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 556      |
|    fps              | 178      |
|    time_elapsed     | 57       |
|    total_timesteps  | 10200    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00864  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 560      |
|    fps              | 179      |
|    time_elapsed     | 57       |
|    total_timesteps  | 10270    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0189   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 564      |
|    fps              | 180      |
|    time_elapsed     | 57       |
|    total_timesteps  | 10358    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0193   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 568      |
|    fps              | 181      |
|    time_elapsed     | 57       |
|    total_timesteps  | 10420    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0299   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 572      |
|    fps              | 182      |
|    time_elapsed     | 57       |
|    total_timesteps  | 10482    |
----------------------------------
Eval num_timesteps=10500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0302   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 576      |
|    fps              | 176      |
|    time_elapsed     | 59       |
|    total_timesteps  | 10551    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0509   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 580      |
|    fps              | 177      |
|    time_elapsed     | 59       |
|    total_timesteps  | 10615    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0304   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 584      |
|    fps              | 178      |
|    time_elapsed     | 59       |
|    total_timesteps  | 10679    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0204   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 588      |
|    fps              | 179      |
|    time_elapsed     | 59       |
|    total_timesteps  | 10752    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0202   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 592      |
|    fps              | 180      |
|    time_elapsed     | 59       |
|    total_timesteps  | 10825    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0103   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 596      |
|    fps              | 181      |
|    time_elapsed     | 59       |
|    total_timesteps  | 10891    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0103   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 600      |
|    fps              | 182      |
|    time_elapsed     | 59       |
|    total_timesteps  | 10961    |
----------------------------------
Eval num_timesteps=11000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00965  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 604      |
|    fps              | 176      |
|    time_elapsed     | 62       |
|    total_timesteps  | 11046    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0207   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 608      |
|    fps              | 177      |
|    time_elapsed     | 62       |
|    total_timesteps  | 11112    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0106   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 612      |
|    fps              | 178      |
|    time_elapsed     | 62       |
|    total_timesteps  | 11182    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0312   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 616      |
|    fps              | 179      |
|    time_elapsed     | 62       |
|    total_timesteps  | 11233    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0316   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 620      |
|    fps              | 180      |
|    time_elapsed     | 62       |
|    total_timesteps  | 11302    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0216   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 624      |
|    fps              | 181      |
|    time_elapsed     | 62       |
|    total_timesteps  | 11373    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0213   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 628      |
|    fps              | 182      |
|    time_elapsed     | 62       |
|    total_timesteps  | 11446    |
----------------------------------
Eval num_timesteps=11500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.0117   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 632      |
|    fps              | 177      |
|    time_elapsed     | 65       |
|    total_timesteps  | 11513    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0115   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 636      |
|    fps              | 178      |
|    time_elapsed     | 65       |
|    total_timesteps  | 11585    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0107   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 640      |
|    fps              | 179      |
|    time_elapsed     | 65       |
|    total_timesteps  | 11678    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0111   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 644      |
|    fps              | 180      |
|    time_elapsed     | 65       |
|    total_timesteps  | 11739    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0112   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 648      |
|    fps              | 181      |
|    time_elapsed     | 65       |
|    total_timesteps  | 11809    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0214   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 652      |
|    fps              | 182      |
|    time_elapsed     | 65       |
|    total_timesteps  | 11871    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0208   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 656      |
|    fps              | 183      |
|    time_elapsed     | 65       |
|    total_timesteps  | 11955    |
----------------------------------
Eval num_timesteps=12000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.011    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 660      |
|    fps              | 177      |
|    time_elapsed     | 67       |
|    total_timesteps  | 12021    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.00147  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 664      |
|    fps              | 178      |
|    time_elapsed     | 67       |
|    total_timesteps  | 12097    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0112   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 668      |
|    fps              | 179      |
|    time_elapsed     | 67       |
|    total_timesteps  | 12166    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00077  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 672      |
|    fps              | 180      |
|    time_elapsed     | 67       |
|    total_timesteps  | 12238    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00077  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 676      |
|    fps              | 181      |
|    time_elapsed     | 67       |
|    total_timesteps  | 12307    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.02    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 680      |
|    fps              | 182      |
|    time_elapsed     | 67       |
|    total_timesteps  | 12390    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0203  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 684      |
|    fps              | 183      |
|    time_elapsed     | 67       |
|    total_timesteps  | 12463    |
----------------------------------
Eval num_timesteps=12500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0105  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 688      |
|    fps              | 178      |
|    time_elapsed     | 70       |
|    total_timesteps  | 12539    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00058 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 692      |
|    fps              | 179      |
|    time_elapsed     | 70       |
|    total_timesteps  | 12615    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0007  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 696      |
|    fps              | 180      |
|    time_elapsed     | 70       |
|    total_timesteps  | 12684    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0093   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 700      |
|    fps              | 181      |
|    time_elapsed     | 70       |
|    total_timesteps  | 12754    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0199   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 704      |
|    fps              | 182      |
|    time_elapsed     | 70       |
|    total_timesteps  | 12824    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00984  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 708      |
|    fps              | 183      |
|    time_elapsed     | 70       |
|    total_timesteps  | 12892    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.01     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 712      |
|    fps              | 184      |
|    time_elapsed     | 70       |
|    total_timesteps  | 12957    |
----------------------------------
Eval num_timesteps=13000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00173 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 716      |
|    fps              | 179      |
|    time_elapsed     | 72       |
|    total_timesteps  | 13052    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00189 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 720      |
|    fps              | 180      |
|    time_elapsed     | 72       |
|    total_timesteps  | 13125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00193 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 724      |
|    fps              | 181      |
|    time_elapsed     | 72       |
|    total_timesteps  | 13197    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00803  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 728      |
|    fps              | 182      |
|    time_elapsed     | 72       |
|    total_timesteps  | 13271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0277   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 732      |
|    fps              | 183      |
|    time_elapsed     | 72       |
|    total_timesteps  | 13347    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0275   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 736      |
|    fps              | 184      |
|    time_elapsed     | 72       |
|    total_timesteps  | 13424    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0284   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 740      |
|    fps              | 184      |
|    time_elapsed     | 72       |
|    total_timesteps  | 13495    |
----------------------------------
Eval num_timesteps=13500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0277   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 744      |
|    fps              | 180      |
|    time_elapsed     | 75       |
|    total_timesteps  | 13574    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0377   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 748      |
|    fps              | 180      |
|    time_elapsed     | 75       |
|    total_timesteps  | 13642    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0373   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 752      |
|    fps              | 181      |
|    time_elapsed     | 75       |
|    total_timesteps  | 13716    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0379   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 756      |
|    fps              | 182      |
|    time_elapsed     | 75       |
|    total_timesteps  | 13783    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0378   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 760      |
|    fps              | 183      |
|    time_elapsed     | 75       |
|    total_timesteps  | 13853    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0484   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 764      |
|    fps              | 184      |
|    time_elapsed     | 75       |
|    total_timesteps  | 13913    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0386   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 768      |
|    fps              | 185      |
|    time_elapsed     | 75       |
|    total_timesteps  | 13978    |
----------------------------------
Eval num_timesteps=14000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0488   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 772      |
|    fps              | 180      |
|    time_elapsed     | 77       |
|    total_timesteps  | 14046    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0579   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 776      |
|    fps              | 181      |
|    time_elapsed     | 78       |
|    total_timesteps  | 14136    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0685   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 780      |
|    fps              | 182      |
|    time_elapsed     | 78       |
|    total_timesteps  | 14203    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0684   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 784      |
|    fps              | 182      |
|    time_elapsed     | 78       |
|    total_timesteps  | 14280    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0586   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 788      |
|    fps              | 183      |
|    time_elapsed     | 78       |
|    total_timesteps  | 14351    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0589   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 792      |
|    fps              | 184      |
|    time_elapsed     | 78       |
|    total_timesteps  | 14420    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0586   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 796      |
|    fps              | 185      |
|    time_elapsed     | 78       |
|    total_timesteps  | 14495    |
----------------------------------
Eval num_timesteps=14500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0584   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 800      |
|    fps              | 180      |
|    time_elapsed     | 80       |
|    total_timesteps  | 14571    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0483   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 804      |
|    fps              | 181      |
|    time_elapsed     | 80       |
|    total_timesteps  | 14644    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0479   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 808      |
|    fps              | 182      |
|    time_elapsed     | 80       |
|    total_timesteps  | 14720    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0475   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 812      |
|    fps              | 183      |
|    time_elapsed     | 80       |
|    total_timesteps  | 14795    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0387   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 816      |
|    fps              | 184      |
|    time_elapsed     | 80       |
|    total_timesteps  | 14862    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0389   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 820      |
|    fps              | 184      |
|    time_elapsed     | 80       |
|    total_timesteps  | 14929    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0492   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 824      |
|    fps              | 185      |
|    time_elapsed     | 80       |
|    total_timesteps  | 14994    |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0491   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 828      |
|    fps              | 181      |
|    time_elapsed     | 83       |
|    total_timesteps  | 15070    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0287   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 832      |
|    fps              | 182      |
|    time_elapsed     | 83       |
|    total_timesteps  | 15155    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0394   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 836      |
|    fps              | 182      |
|    time_elapsed     | 83       |
|    total_timesteps  | 15215    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0393   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 840      |
|    fps              | 183      |
|    time_elapsed     | 83       |
|    total_timesteps  | 15289    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0394   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 844      |
|    fps              | 184      |
|    time_elapsed     | 83       |
|    total_timesteps  | 15366    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0292   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 848      |
|    fps              | 185      |
|    time_elapsed     | 83       |
|    total_timesteps  | 15438    |
----------------------------------
Eval num_timesteps=15500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0294   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 852      |
|    fps              | 180      |
|    time_elapsed     | 85       |
|    total_timesteps  | 15508    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0286   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 856      |
|    fps              | 181      |
|    time_elapsed     | 85       |
|    total_timesteps  | 15594    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0289   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 860      |
|    fps              | 182      |
|    time_elapsed     | 85       |
|    total_timesteps  | 15658    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0185   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 864      |
|    fps              | 183      |
|    time_elapsed     | 85       |
|    total_timesteps  | 15728    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0183   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 868      |
|    fps              | 184      |
|    time_elapsed     | 85       |
|    total_timesteps  | 15797    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0081   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 872      |
|    fps              | 184      |
|    time_elapsed     | 85       |
|    total_timesteps  | 15870    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00107 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 876      |
|    fps              | 185      |
|    time_elapsed     | 85       |
|    total_timesteps  | 15939    |
----------------------------------
Eval num_timesteps=16000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00086 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 880      |
|    fps              | 181      |
|    time_elapsed     | 88       |
|    total_timesteps  | 16001    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00074 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 884      |
|    fps              | 181      |
|    time_elapsed     | 88       |
|    total_timesteps  | 16075    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00866  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 888      |
|    fps              | 182      |
|    time_elapsed     | 88       |
|    total_timesteps  | 16161    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00887  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 892      |
|    fps              | 183      |
|    time_elapsed     | 88       |
|    total_timesteps  | 16225    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0194   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 896      |
|    fps              | 184      |
|    time_elapsed     | 88       |
|    total_timesteps  | 16288    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00947  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 900      |
|    fps              | 184      |
|    time_elapsed     | 88       |
|    total_timesteps  | 16361    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.02     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 904      |
|    fps              | 185      |
|    time_elapsed     | 88       |
|    total_timesteps  | 16422    |
----------------------------------
Eval num_timesteps=16500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0296   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 908      |
|    fps              | 181      |
|    time_elapsed     | 90       |
|    total_timesteps  | 16507    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0295   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 912      |
|    fps              | 182      |
|    time_elapsed     | 90       |
|    total_timesteps  | 16586    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0391   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 916      |
|    fps              | 183      |
|    time_elapsed     | 91       |
|    total_timesteps  | 16663    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.049    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 920      |
|    fps              | 183      |
|    time_elapsed     | 91       |
|    total_timesteps  | 16731    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0387   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 924      |
|    fps              | 184      |
|    time_elapsed     | 91       |
|    total_timesteps  | 16805    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0389   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 928      |
|    fps              | 185      |
|    time_elapsed     | 91       |
|    total_timesteps  | 16875    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0392   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 932      |
|    fps              | 186      |
|    time_elapsed     | 91       |
|    total_timesteps  | 16952    |
----------------------------------
Eval num_timesteps=17000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0288   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 936      |
|    fps              | 181      |
|    time_elapsed     | 93       |
|    total_timesteps  | 17022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0288   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 940      |
|    fps              | 182      |
|    time_elapsed     | 93       |
|    total_timesteps  | 17096    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.029    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 944      |
|    fps              | 183      |
|    time_elapsed     | 93       |
|    total_timesteps  | 17167    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0392   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 948      |
|    fps              | 184      |
|    time_elapsed     | 93       |
|    total_timesteps  | 17236    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.049    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 952      |
|    fps              | 184      |
|    time_elapsed     | 93       |
|    total_timesteps  | 17309    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0498   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 956      |
|    fps              | 185      |
|    time_elapsed     | 93       |
|    total_timesteps  | 17376    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0491   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 960      |
|    fps              | 186      |
|    time_elapsed     | 93       |
|    total_timesteps  | 17457    |
----------------------------------
Eval num_timesteps=17500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0493   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 964      |
|    fps              | 182      |
|    time_elapsed     | 96       |
|    total_timesteps  | 17523    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0494   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 968      |
|    fps              | 182      |
|    time_elapsed     | 96       |
|    total_timesteps  | 17590    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0586   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 972      |
|    fps              | 183      |
|    time_elapsed     | 96       |
|    total_timesteps  | 17683    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0684   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 976      |
|    fps              | 184      |
|    time_elapsed     | 96       |
|    total_timesteps  | 17757    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0579   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 980      |
|    fps              | 185      |
|    time_elapsed     | 96       |
|    total_timesteps  | 17831    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0578   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 984      |
|    fps              | 186      |
|    time_elapsed     | 96       |
|    total_timesteps  | 17907    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0482   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 988      |
|    fps              | 186      |
|    time_elapsed     | 96       |
|    total_timesteps  | 17984    |
----------------------------------
Eval num_timesteps=18000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0482   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 992      |
|    fps              | 182      |
|    time_elapsed     | 98       |
|    total_timesteps  | 18047    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0376   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 996      |
|    fps              | 183      |
|    time_elapsed     | 98       |
|    total_timesteps  | 18126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0374   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 184      |
|    time_elapsed     | 98       |
|    total_timesteps  | 18203    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0264   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 185      |
|    time_elapsed     | 98       |
|    total_timesteps  | 18289    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 185      |
|    time_elapsed     | 98       |
|    total_timesteps  | 18355    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0273   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 186      |
|    time_elapsed     | 98       |
|    total_timesteps  | 18429    |
----------------------------------
Eval num_timesteps=18500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0173   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 182      |
|    time_elapsed     | 101      |
|    total_timesteps  | 18506    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00741  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 183      |
|    time_elapsed     | 101      |
|    total_timesteps  | 18572    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00749  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 183      |
|    time_elapsed     | 101      |
|    total_timesteps  | 18644    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00757  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 184      |
|    time_elapsed     | 101      |
|    total_timesteps  | 18712    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00785  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 185      |
|    time_elapsed     | 101      |
|    total_timesteps  | 18782    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00785  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 185      |
|    time_elapsed     | 101      |
|    total_timesteps  | 18852    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00773  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 186      |
|    time_elapsed     | 101      |
|    total_timesteps  | 18929    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00793  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 187      |
|    time_elapsed     | 101      |
|    total_timesteps  | 18995    |
----------------------------------
Eval num_timesteps=19000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00213 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 183      |
|    time_elapsed     | 103      |
|    total_timesteps  | 19065    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0223  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 184      |
|    time_elapsed     | 103      |
|    total_timesteps  | 19142    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0224  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 184      |
|    time_elapsed     | 103      |
|    total_timesteps  | 19213    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0223  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 185      |
|    time_elapsed     | 103      |
|    total_timesteps  | 19290    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0222  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 186      |
|    time_elapsed     | 104      |
|    total_timesteps  | 19355    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0124  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 186      |
|    time_elapsed     | 104      |
|    total_timesteps  | 19427    |
----------------------------------
Eval num_timesteps=19500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0217  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 183      |
|    time_elapsed     | 106      |
|    total_timesteps  | 19500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0319  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 183      |
|    time_elapsed     | 106      |
|    total_timesteps  | 19580    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.022   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 184      |
|    time_elapsed     | 106      |
|    total_timesteps  | 19658    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0117  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 185      |
|    time_elapsed     | 106      |
|    total_timesteps  | 19726    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00162 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 185      |
|    time_elapsed     | 106      |
|    total_timesteps  | 19801    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00178 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 186      |
|    time_elapsed     | 106      |
|    total_timesteps  | 19868    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00817  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 187      |
|    time_elapsed     | 106      |
|    total_timesteps  | 19948    |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00785  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 183      |
|    time_elapsed     | 109      |
|    total_timesteps  | 20033    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00841  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 184      |
|    time_elapsed     | 109      |
|    total_timesteps  | 20105    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00829  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 184      |
|    time_elapsed     | 109      |
|    total_timesteps  | 20174    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00816  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 185      |
|    time_elapsed     | 109      |
|    total_timesteps  | 20251    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00864  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 186      |
|    time_elapsed     | 109      |
|    total_timesteps  | 20316    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00852  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 186      |
|    time_elapsed     | 109      |
|    total_timesteps  | 20385    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0084   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 187      |
|    time_elapsed     | 109      |
|    total_timesteps  | 20460    |
----------------------------------
Eval num_timesteps=20500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00167 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 183      |
|    time_elapsed     | 111      |
|    total_timesteps  | 20530    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00155 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 184      |
|    time_elapsed     | 111      |
|    total_timesteps  | 20597    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00167 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 184      |
|    time_elapsed     | 111      |
|    total_timesteps  | 20670    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00147 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 185      |
|    time_elapsed     | 111      |
|    total_timesteps  | 20742    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00187 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 186      |
|    time_elapsed     | 111      |
|    total_timesteps  | 20818    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00167 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 186      |
|    time_elapsed     | 111      |
|    total_timesteps  | 20883    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00147 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 187      |
|    time_elapsed     | 111      |
|    total_timesteps  | 20955    |
----------------------------------
Eval num_timesteps=21000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00195 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 184      |
|    time_elapsed     | 114      |
|    total_timesteps  | 21038    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00199 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 184      |
|    time_elapsed     | 114      |
|    total_timesteps  | 21116    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00239 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 185      |
|    time_elapsed     | 114      |
|    total_timesteps  | 21191    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 186      |
|    time_elapsed     | 114      |
|    total_timesteps  | 21271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0126  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 186      |
|    time_elapsed     | 114      |
|    total_timesteps  | 21341    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.012   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 187      |
|    time_elapsed     | 114      |
|    total_timesteps  | 21406    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0116  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 187      |
|    time_elapsed     | 114      |
|    total_timesteps  | 21474    |
----------------------------------
Eval num_timesteps=21500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0119  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 184      |
|    time_elapsed     | 116      |
|    total_timesteps  | 21550    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00157 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 184      |
|    time_elapsed     | 116      |
|    total_timesteps  | 21615    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00188 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 185      |
|    time_elapsed     | 116      |
|    total_timesteps  | 21690    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0113  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 186      |
|    time_elapsed     | 116      |
|    total_timesteps  | 21755    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0107  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 186      |
|    time_elapsed     | 116      |
|    total_timesteps  | 21826    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0111  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 187      |
|    time_elapsed     | 116      |
|    total_timesteps  | 21909    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00088 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 187      |
|    time_elapsed     | 117      |
|    total_timesteps  | 21971    |
----------------------------------
Eval num_timesteps=22000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0109  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 184      |
|    time_elapsed     | 119      |
|    total_timesteps  | 22048    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0112  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 185      |
|    time_elapsed     | 119      |
|    total_timesteps  | 22121    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00932  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 185      |
|    time_elapsed     | 119      |
|    total_timesteps  | 22178    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00968  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 186      |
|    time_elapsed     | 119      |
|    total_timesteps  | 22244    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00988  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 186      |
|    time_elapsed     | 119      |
|    total_timesteps  | 22309    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00896  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 187      |
|    time_elapsed     | 119      |
|    total_timesteps  | 22399    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0088   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 187      |
|    time_elapsed     | 119      |
|    total_timesteps  | 22476    |
----------------------------------
Eval num_timesteps=22500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00856  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 184      |
|    time_elapsed     | 122      |
|    total_timesteps  | 22554    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0191   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 185      |
|    time_elapsed     | 122      |
|    total_timesteps  | 22617    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0185   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 185      |
|    time_elapsed     | 122      |
|    total_timesteps  | 22698    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0289   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 186      |
|    time_elapsed     | 122      |
|    total_timesteps  | 22759    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0289   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 187      |
|    time_elapsed     | 122      |
|    total_timesteps  | 22841    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0292   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 187      |
|    time_elapsed     | 122      |
|    total_timesteps  | 22912    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0295   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 188      |
|    time_elapsed     | 122      |
|    total_timesteps  | 22979    |
----------------------------------
Eval num_timesteps=23000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0294   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 185      |
|    time_elapsed     | 124      |
|    total_timesteps  | 23062    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0293   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 185      |
|    time_elapsed     | 124      |
|    total_timesteps  | 23135    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0394   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 186      |
|    time_elapsed     | 124      |
|    total_timesteps  | 23199    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0293   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 186      |
|    time_elapsed     | 124      |
|    total_timesteps  | 23268    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0299   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1284     |
|    fps              | 187      |
|    time_elapsed     | 124      |
|    total_timesteps  | 23330    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0198   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1288     |
|    fps              | 187      |
|    time_elapsed     | 124      |
|    total_timesteps  | 23398    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0101   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1292     |
|    fps              | 188      |
|    time_elapsed     | 124      |
|    total_timesteps  | 23465    |
----------------------------------
Eval num_timesteps=23500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.02     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1296     |
|    fps              | 185      |
|    time_elapsed     | 127      |
|    total_timesteps  | 23532    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0202   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1300     |
|    fps              | 185      |
|    time_elapsed     | 127      |
|    total_timesteps  | 23598    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0208   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1304     |
|    fps              | 186      |
|    time_elapsed     | 127      |
|    total_timesteps  | 23666    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.02     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 186      |
|    time_elapsed     | 127      |
|    total_timesteps  | 23748    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0404   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 187      |
|    time_elapsed     | 127      |
|    total_timesteps  | 23817    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0399   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1316     |
|    fps              | 187      |
|    time_elapsed     | 127      |
|    total_timesteps  | 23902    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0292   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1320     |
|    fps              | 188      |
|    time_elapsed     | 127      |
|    total_timesteps  | 23975    |
----------------------------------
Eval num_timesteps=24000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0394   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1324     |
|    fps              | 185      |
|    time_elapsed     | 129      |
|    total_timesteps  | 24036    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0391   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1328     |
|    fps              | 186      |
|    time_elapsed     | 129      |
|    total_timesteps  | 24108    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0396   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1332     |
|    fps              | 186      |
|    time_elapsed     | 129      |
|    total_timesteps  | 24187    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.04     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1336     |
|    fps              | 187      |
|    time_elapsed     | 129      |
|    total_timesteps  | 24253    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0395   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1340     |
|    fps              | 187      |
|    time_elapsed     | 129      |
|    total_timesteps  | 24344    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0288   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1344     |
|    fps              | 188      |
|    time_elapsed     | 129      |
|    total_timesteps  | 24424    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0291   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 188      |
|    time_elapsed     | 129      |
|    total_timesteps  | 24497    |
----------------------------------
Eval num_timesteps=24500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0189   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1352     |
|    fps              | 186      |
|    time_elapsed     | 131      |
|    total_timesteps  | 24562    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0197   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1356     |
|    fps              | 186      |
|    time_elapsed     | 131      |
|    total_timesteps  | 24626    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0191   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 187      |
|    time_elapsed     | 131      |
|    total_timesteps  | 24710    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0187   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1364     |
|    fps              | 187      |
|    time_elapsed     | 131      |
|    total_timesteps  | 24789    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0192   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1368     |
|    fps              | 188      |
|    time_elapsed     | 131      |
|    total_timesteps  | 24859    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0189   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1372     |
|    fps              | 188      |
|    time_elapsed     | 132      |
|    total_timesteps  | 24939    |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00888  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1376     |
|    fps              | 185      |
|    time_elapsed     | 134      |
|    total_timesteps  | 25003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0092   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1380     |
|    fps              | 186      |
|    time_elapsed     | 134      |
|    total_timesteps  | 25064    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00123 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1384     |
|    fps              | 186      |
|    time_elapsed     | 134      |
|    total_timesteps  | 25137    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0113  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1388     |
|    fps              | 187      |
|    time_elapsed     | 134      |
|    total_timesteps  | 25206    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.012   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1392     |
|    fps              | 187      |
|    time_elapsed     | 134      |
|    total_timesteps  | 25291    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0222  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1396     |
|    fps              | 188      |
|    time_elapsed     | 134      |
|    total_timesteps  | 25364    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0224  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1400     |
|    fps              | 188      |
|    time_elapsed     | 134      |
|    total_timesteps  | 25434    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0123  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1404     |
|    fps              | 189      |
|    time_elapsed     | 134      |
|    total_timesteps  | 25499    |
----------------------------------
Eval num_timesteps=25500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0115  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1408     |
|    fps              | 186      |
|    time_elapsed     | 137      |
|    total_timesteps  | 25561    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0317  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1412     |
|    fps              | 186      |
|    time_elapsed     | 137      |
|    total_timesteps  | 25634    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0316  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1416     |
|    fps              | 187      |
|    time_elapsed     | 137      |
|    total_timesteps  | 25717    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0413  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1420     |
|    fps              | 187      |
|    time_elapsed     | 137      |
|    total_timesteps  | 25782    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0414  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1424     |
|    fps              | 188      |
|    time_elapsed     | 137      |
|    total_timesteps  | 25847    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0416  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1428     |
|    fps              | 188      |
|    time_elapsed     | 137      |
|    total_timesteps  | 25924    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.041   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1432     |
|    fps              | 189      |
|    time_elapsed     | 137      |
|    total_timesteps  | 25987    |
----------------------------------
Eval num_timesteps=26000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0316  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1436     |
|    fps              | 186      |
|    time_elapsed     | 139      |
|    total_timesteps  | 26069    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0305  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1440     |
|    fps              | 186      |
|    time_elapsed     | 139      |
|    total_timesteps  | 26132    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0197  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1444     |
|    fps              | 187      |
|    time_elapsed     | 139      |
|    total_timesteps  | 26192    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0196  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1448     |
|    fps              | 187      |
|    time_elapsed     | 139      |
|    total_timesteps  | 26262    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.00943 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1452     |
|    fps              | 188      |
|    time_elapsed     | 139      |
|    total_timesteps  | 26323    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0102   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1456     |
|    fps              | 188      |
|    time_elapsed     | 139      |
|    total_timesteps  | 26396    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0104   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 189      |
|    time_elapsed     | 139      |
|    total_timesteps  | 26475    |
----------------------------------
Eval num_timesteps=26500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0208   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1464     |
|    fps              | 186      |
|    time_elapsed     | 142      |
|    total_timesteps  | 26546    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0203   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1468     |
|    fps              | 186      |
|    time_elapsed     | 142      |
|    total_timesteps  | 26626    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0207   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 187      |
|    time_elapsed     | 142      |
|    total_timesteps  | 26696    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0208   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1476     |
|    fps              | 187      |
|    time_elapsed     | 142      |
|    total_timesteps  | 26759    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0206   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1480     |
|    fps              | 188      |
|    time_elapsed     | 142      |
|    total_timesteps  | 26826    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0205   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1484     |
|    fps              | 188      |
|    time_elapsed     | 142      |
|    total_timesteps  | 26901    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0305   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1488     |
|    fps              | 189      |
|    time_elapsed     | 142      |
|    total_timesteps  | 26968    |
----------------------------------
Eval num_timesteps=27000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0414   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1492     |
|    fps              | 186      |
|    time_elapsed     | 144      |
|    total_timesteps  | 27031    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0415   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1496     |
|    fps              | 186      |
|    time_elapsed     | 144      |
|    total_timesteps  | 27102    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.0518   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1500     |
|    fps              | 187      |
|    time_elapsed     | 145      |
|    total_timesteps  | 27166    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.0618   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1504     |
|    fps              | 187      |
|    time_elapsed     | 145      |
|    total_timesteps  | 27231    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0512   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1508     |
|    fps              | 188      |
|    time_elapsed     | 145      |
|    total_timesteps  | 27307    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0611   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1512     |
|    fps              | 188      |
|    time_elapsed     | 145      |
|    total_timesteps  | 27382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0615   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1516     |
|    fps              | 189      |
|    time_elapsed     | 145      |
|    total_timesteps  | 27455    |
----------------------------------
Eval num_timesteps=27500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.0718   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1520     |
|    fps              | 186      |
|    time_elapsed     | 147      |
|    total_timesteps  | 27512    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0611   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 186      |
|    time_elapsed     | 147      |
|    total_timesteps  | 27595    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0714   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1528     |
|    fps              | 187      |
|    time_elapsed     | 147      |
|    total_timesteps  | 27664    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0713   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1532     |
|    fps              | 187      |
|    time_elapsed     | 147      |
|    total_timesteps  | 27732    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0613   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1536     |
|    fps              | 188      |
|    time_elapsed     | 147      |
|    total_timesteps  | 27813    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0608   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1540     |
|    fps              | 188      |
|    time_elapsed     | 147      |
|    total_timesteps  | 27889    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0504   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1544     |
|    fps              | 189      |
|    time_elapsed     | 147      |
|    total_timesteps  | 27957    |
----------------------------------
Eval num_timesteps=28000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0604   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1548     |
|    fps              | 186      |
|    time_elapsed     | 150      |
|    total_timesteps  | 28029    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0491   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1552     |
|    fps              | 187      |
|    time_elapsed     | 150      |
|    total_timesteps  | 28122    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0397   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 187      |
|    time_elapsed     | 150      |
|    total_timesteps  | 28180    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0494   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1560     |
|    fps              | 188      |
|    time_elapsed     | 150      |
|    total_timesteps  | 28265    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0396   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1564     |
|    fps              | 188      |
|    time_elapsed     | 150      |
|    total_timesteps  | 28331    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0397   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1568     |
|    fps              | 188      |
|    time_elapsed     | 150      |
|    total_timesteps  | 28409    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0592   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1572     |
|    fps              | 189      |
|    time_elapsed     | 150      |
|    total_timesteps  | 28491    |
----------------------------------
Eval num_timesteps=28500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0689   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1576     |
|    fps              | 186      |
|    time_elapsed     | 152      |
|    total_timesteps  | 28563    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0784   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1580     |
|    fps              | 187      |
|    time_elapsed     | 152      |
|    total_timesteps  | 28642    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0784   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1584     |
|    fps              | 187      |
|    time_elapsed     | 152      |
|    total_timesteps  | 28718    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0684   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1588     |
|    fps              | 188      |
|    time_elapsed     | 152      |
|    total_timesteps  | 28785    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0583   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1592     |
|    fps              | 188      |
|    time_elapsed     | 152      |
|    total_timesteps  | 28851    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0583   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1596     |
|    fps              | 189      |
|    time_elapsed     | 152      |
|    total_timesteps  | 28921    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.058    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1600     |
|    fps              | 189      |
|    time_elapsed     | 152      |
|    total_timesteps  | 28992    |
----------------------------------
Eval num_timesteps=29000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0377   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1604     |
|    fps              | 186      |
|    time_elapsed     | 155      |
|    total_timesteps  | 29065    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0377   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1608     |
|    fps              | 187      |
|    time_elapsed     | 155      |
|    total_timesteps  | 29140    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0376   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1612     |
|    fps              | 187      |
|    time_elapsed     | 155      |
|    total_timesteps  | 29219    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0376   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1616     |
|    fps              | 188      |
|    time_elapsed     | 155      |
|    total_timesteps  | 29292    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0369   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1620     |
|    fps              | 188      |
|    time_elapsed     | 155      |
|    total_timesteps  | 29366    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0579   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1624     |
|    fps              | 189      |
|    time_elapsed     | 155      |
|    total_timesteps  | 29425    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0477   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1628     |
|    fps              | 189      |
|    time_elapsed     | 155      |
|    total_timesteps  | 29499    |
----------------------------------
Eval num_timesteps=29500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0475   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1632     |
|    fps              | 187      |
|    time_elapsed     | 158      |
|    total_timesteps  | 29570    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.048    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1636     |
|    fps              | 187      |
|    time_elapsed     | 158      |
|    total_timesteps  | 29640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0482   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1640     |
|    fps              | 187      |
|    time_elapsed     | 158      |
|    total_timesteps  | 29710    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0479   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1644     |
|    fps              | 188      |
|    time_elapsed     | 158      |
|    total_timesteps  | 29785    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0375   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1648     |
|    fps              | 188      |
|    time_elapsed     | 158      |
|    total_timesteps  | 29869    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0385   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1652     |
|    fps              | 189      |
|    time_elapsed     | 158      |
|    total_timesteps  | 29936    |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0279   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1656     |
|    fps              | 186      |
|    time_elapsed     | 160      |
|    total_timesteps  | 30008    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0187   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1660     |
|    fps              | 187      |
|    time_elapsed     | 160      |
|    total_timesteps  | 30073    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0186   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1664     |
|    fps              | 187      |
|    time_elapsed     | 160      |
|    total_timesteps  | 30143    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0285   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1668     |
|    fps              | 188      |
|    time_elapsed     | 160      |
|    total_timesteps  | 30224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0193   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1672     |
|    fps              | 188      |
|    time_elapsed     | 160      |
|    total_timesteps  | 30286    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00867  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1676     |
|    fps              | 188      |
|    time_elapsed     | 160      |
|    total_timesteps  | 30373    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00093 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1680     |
|    fps              | 189      |
|    time_elapsed     | 160      |
|    total_timesteps  | 30442    |
----------------------------------
Eval num_timesteps=30500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00049 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1684     |
|    fps              | 186      |
|    time_elapsed     | 163      |
|    total_timesteps  | 30507    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00061 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1688     |
|    fps              | 187      |
|    time_elapsed     | 163      |
|    total_timesteps  | 30577    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00061 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1692     |
|    fps              | 187      |
|    time_elapsed     | 163      |
|    total_timesteps  | 30643    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00045 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1696     |
|    fps              | 187      |
|    time_elapsed     | 163      |
|    total_timesteps  | 30709    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0105  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1700     |
|    fps              | 188      |
|    time_elapsed     | 163      |
|    total_timesteps  | 30781    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.00015 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1704     |
|    fps              | 188      |
|    time_elapsed     | 163      |
|    total_timesteps  | 30845    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00997  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1708     |
|    fps              | 189      |
|    time_elapsed     | 163      |
|    total_timesteps  | 30917    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0105   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1712     |
|    fps              | 189      |
|    time_elapsed     | 163      |
|    total_timesteps  | 30984    |
----------------------------------
Eval num_timesteps=31000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0108   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1716     |
|    fps              | 187      |
|    time_elapsed     | 165      |
|    total_timesteps  | 31048    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.00094  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1720     |
|    fps              | 187      |
|    time_elapsed     | 165      |
|    total_timesteps  | 31119    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.00943 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1724     |
|    fps              | 187      |
|    time_elapsed     | 165      |
|    total_timesteps  | 31187    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.00028  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1728     |
|    fps              | 188      |
|    time_elapsed     | 166      |
|    total_timesteps  | 31268    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0101   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1732     |
|    fps              | 188      |
|    time_elapsed     | 166      |
|    total_timesteps  | 31344    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0101   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1736     |
|    fps              | 189      |
|    time_elapsed     | 166      |
|    total_timesteps  | 31413    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.02     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1740     |
|    fps              | 189      |
|    time_elapsed     | 166      |
|    total_timesteps  | 31486    |
----------------------------------
Eval num_timesteps=31500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0301   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1744     |
|    fps              | 187      |
|    time_elapsed     | 168      |
|    total_timesteps  | 31559    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0307   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1748     |
|    fps              | 187      |
|    time_elapsed     | 168      |
|    total_timesteps  | 31627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0305   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1752     |
|    fps              | 187      |
|    time_elapsed     | 168      |
|    total_timesteps  | 31699    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0307   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1756     |
|    fps              | 188      |
|    time_elapsed     | 168      |
|    total_timesteps  | 31766    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0303   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1760     |
|    fps              | 188      |
|    time_elapsed     | 168      |
|    total_timesteps  | 31843    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0297   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1764     |
|    fps              | 189      |
|    time_elapsed     | 168      |
|    total_timesteps  | 31927    |
----------------------------------
Eval num_timesteps=32000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0196   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1768     |
|    fps              | 187      |
|    time_elapsed     | 171      |
|    total_timesteps  | 32009    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00924  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1772     |
|    fps              | 187      |
|    time_elapsed     | 171      |
|    total_timesteps  | 32081    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0101   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1776     |
|    fps              | 187      |
|    time_elapsed     | 171      |
|    total_timesteps  | 32147    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.01     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1780     |
|    fps              | 188      |
|    time_elapsed     | 171      |
|    total_timesteps  | 32218    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00964  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1784     |
|    fps              | 188      |
|    time_elapsed     | 171      |
|    total_timesteps  | 32292    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0199   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1788     |
|    fps              | 188      |
|    time_elapsed     | 171      |
|    total_timesteps  | 32357    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0298   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1792     |
|    fps              | 189      |
|    time_elapsed     | 171      |
|    total_timesteps  | 32423    |
----------------------------------
Eval num_timesteps=32500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0292   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1796     |
|    fps              | 187      |
|    time_elapsed     | 173      |
|    total_timesteps  | 32505    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0293   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1800     |
|    fps              | 187      |
|    time_elapsed     | 173      |
|    total_timesteps  | 32574    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0187   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1804     |
|    fps              | 187      |
|    time_elapsed     | 173      |
|    total_timesteps  | 32654    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00842  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1808     |
|    fps              | 188      |
|    time_elapsed     | 173      |
|    total_timesteps  | 32733    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.002   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1812     |
|    fps              | 188      |
|    time_elapsed     | 173      |
|    total_timesteps  | 32810    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00248 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1816     |
|    fps              | 189      |
|    time_elapsed     | 173      |
|    total_timesteps  | 32886    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00232 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1820     |
|    fps              | 189      |
|    time_elapsed     | 173      |
|    total_timesteps  | 32953    |
----------------------------------
Eval num_timesteps=33000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1824     |
|    fps              | 187      |
|    time_elapsed     | 176      |
|    total_timesteps  | 33030    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0225  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1828     |
|    fps              | 187      |
|    time_elapsed     | 176      |
|    total_timesteps  | 33106    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0322  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1832     |
|    fps              | 188      |
|    time_elapsed     | 176      |
|    total_timesteps  | 33175    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0325  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1836     |
|    fps              | 188      |
|    time_elapsed     | 176      |
|    total_timesteps  | 33251    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0426  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1840     |
|    fps              | 188      |
|    time_elapsed     | 176      |
|    total_timesteps  | 33326    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0321  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1844     |
|    fps              | 189      |
|    time_elapsed     | 176      |
|    total_timesteps  | 33388    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0224  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1848     |
|    fps              | 189      |
|    time_elapsed     | 176      |
|    total_timesteps  | 33463    |
----------------------------------
Eval num_timesteps=33500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1852     |
|    fps              | 187      |
|    time_elapsed     | 179      |
|    total_timesteps  | 33540    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0227  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1856     |
|    fps              | 187      |
|    time_elapsed     | 179      |
|    total_timesteps  | 33609    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0223  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1860     |
|    fps              | 188      |
|    time_elapsed     | 179      |
|    total_timesteps  | 33677    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0217  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1864     |
|    fps              | 188      |
|    time_elapsed     | 179      |
|    total_timesteps  | 33746    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0211  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1868     |
|    fps              | 188      |
|    time_elapsed     | 179      |
|    total_timesteps  | 33812    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.021   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1872     |
|    fps              | 189      |
|    time_elapsed     | 179      |
|    total_timesteps  | 33881    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0109  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1876     |
|    fps              | 189      |
|    time_elapsed     | 179      |
|    total_timesteps  | 33945    |
----------------------------------
Eval num_timesteps=34000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0106  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1880     |
|    fps              | 187      |
|    time_elapsed     | 181      |
|    total_timesteps  | 34009    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0105  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1884     |
|    fps              | 187      |
|    time_elapsed     | 181      |
|    total_timesteps  | 34081    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0206  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1888     |
|    fps              | 187      |
|    time_elapsed     | 181      |
|    total_timesteps  | 34148    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0208  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1892     |
|    fps              | 188      |
|    time_elapsed     | 181      |
|    total_timesteps  | 34219    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0202  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1896     |
|    fps              | 188      |
|    time_elapsed     | 181      |
|    total_timesteps  | 34285    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0201  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1900     |
|    fps              | 189      |
|    time_elapsed     | 181      |
|    total_timesteps  | 34352    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.00967 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1904     |
|    fps              | 189      |
|    time_elapsed     | 181      |
|    total_timesteps  | 34421    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.00915 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1908     |
|    fps              | 189      |
|    time_elapsed     | 181      |
|    total_timesteps  | 34487    |
----------------------------------
Eval num_timesteps=34500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.00112  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1912     |
|    fps              | 187      |
|    time_elapsed     | 184      |
|    total_timesteps  | 34557    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.00116  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1916     |
|    fps              | 187      |
|    time_elapsed     | 184      |
|    total_timesteps  | 34632    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.00096  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1920     |
|    fps              | 188      |
|    time_elapsed     | 184      |
|    total_timesteps  | 34704    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0109   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1924     |
|    fps              | 188      |
|    time_elapsed     | 184      |
|    total_timesteps  | 34782    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0107   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1928     |
|    fps              | 189      |
|    time_elapsed     | 184      |
|    total_timesteps  | 34865    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0105   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1932     |
|    fps              | 189      |
|    time_elapsed     | 184      |
|    total_timesteps  | 34938    |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0207   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1936     |
|    fps              | 187      |
|    time_elapsed     | 186      |
|    total_timesteps  | 35010    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0308   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1940     |
|    fps              | 187      |
|    time_elapsed     | 186      |
|    total_timesteps  | 35081    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0308   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1944     |
|    fps              | 188      |
|    time_elapsed     | 186      |
|    total_timesteps  | 35144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0213   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1948     |
|    fps              | 188      |
|    time_elapsed     | 186      |
|    total_timesteps  | 35208    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0216   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1952     |
|    fps              | 188      |
|    time_elapsed     | 186      |
|    total_timesteps  | 35277    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0314   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1956     |
|    fps              | 189      |
|    time_elapsed     | 186      |
|    total_timesteps  | 35349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0313   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1960     |
|    fps              | 189      |
|    time_elapsed     | 187      |
|    total_timesteps  | 35421    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0411   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1964     |
|    fps              | 189      |
|    time_elapsed     | 187      |
|    total_timesteps  | 35494    |
----------------------------------
Eval num_timesteps=35500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0409   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1968     |
|    fps              | 187      |
|    time_elapsed     | 189      |
|    total_timesteps  | 35566    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0508   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1972     |
|    fps              | 188      |
|    time_elapsed     | 189      |
|    total_timesteps  | 35637    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0406   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1976     |
|    fps              | 188      |
|    time_elapsed     | 189      |
|    total_timesteps  | 35706    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0402   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1980     |
|    fps              | 188      |
|    time_elapsed     | 189      |
|    total_timesteps  | 35781    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0505   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1984     |
|    fps              | 189      |
|    time_elapsed     | 189      |
|    total_timesteps  | 35845    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0505   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1988     |
|    fps              | 189      |
|    time_elapsed     | 189      |
|    total_timesteps  | 35911    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0406   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1992     |
|    fps              | 189      |
|    time_elapsed     | 189      |
|    total_timesteps  | 35979    |
----------------------------------
Eval num_timesteps=36000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0403   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1996     |
|    fps              | 187      |
|    time_elapsed     | 192      |
|    total_timesteps  | 36054    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0404   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2000     |
|    fps              | 187      |
|    time_elapsed     | 192      |
|    total_timesteps  | 36117    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0303   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2004     |
|    fps              | 188      |
|    time_elapsed     | 192      |
|    total_timesteps  | 36190    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0301   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2008     |
|    fps              | 188      |
|    time_elapsed     | 192      |
|    total_timesteps  | 36261    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0303   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2012     |
|    fps              | 188      |
|    time_elapsed     | 192      |
|    total_timesteps  | 36326    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0304   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2016     |
|    fps              | 189      |
|    time_elapsed     | 192      |
|    total_timesteps  | 36398    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0306   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2020     |
|    fps              | 189      |
|    time_elapsed     | 192      |
|    total_timesteps  | 36466    |
----------------------------------
Eval num_timesteps=36500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0209   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2024     |
|    fps              | 187      |
|    time_elapsed     | 194      |
|    total_timesteps  | 36536    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.021    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2028     |
|    fps              | 187      |
|    time_elapsed     | 194      |
|    total_timesteps  | 36616    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0215   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2032     |
|    fps              | 188      |
|    time_elapsed     | 194      |
|    total_timesteps  | 36677    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0216   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2036     |
|    fps              | 188      |
|    time_elapsed     | 194      |
|    total_timesteps  | 36746    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0116   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2040     |
|    fps              | 188      |
|    time_elapsed     | 194      |
|    total_timesteps  | 36816    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | -0.00865 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2044     |
|    fps              | 189      |
|    time_elapsed     | 194      |
|    total_timesteps  | 36884    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.00893 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2048     |
|    fps              | 189      |
|    time_elapsed     | 194      |
|    total_timesteps  | 36955    |
----------------------------------
Eval num_timesteps=37000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.00905 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2052     |
|    fps              | 187      |
|    time_elapsed     | 197      |
|    total_timesteps  | 37027    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.0191  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2056     |
|    fps              | 187      |
|    time_elapsed     | 197      |
|    total_timesteps  | 37101    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.00931 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2060     |
|    fps              | 188      |
|    time_elapsed     | 197      |
|    total_timesteps  | 37178    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.0191  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2064     |
|    fps              | 188      |
|    time_elapsed     | 197      |
|    total_timesteps  | 37246    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.00927 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2068     |
|    fps              | 189      |
|    time_elapsed     | 197      |
|    total_timesteps  | 37322    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0192  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2072     |
|    fps              | 189      |
|    time_elapsed     | 197      |
|    total_timesteps  | 37392    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0195  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2076     |
|    fps              | 189      |
|    time_elapsed     | 197      |
|    total_timesteps  | 37469    |
----------------------------------
Eval num_timesteps=37500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.0089  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2080     |
|    fps              | 187      |
|    time_elapsed     | 199      |
|    total_timesteps  | 37528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0194  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2084     |
|    fps              | 188      |
|    time_elapsed     | 200      |
|    total_timesteps  | 37604    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0096  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2088     |
|    fps              | 188      |
|    time_elapsed     | 200      |
|    total_timesteps  | 37676    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00061  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2092     |
|    fps              | 188      |
|    time_elapsed     | 200      |
|    total_timesteps  | 37739    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00049  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2096     |
|    fps              | 189      |
|    time_elapsed     | 200      |
|    total_timesteps  | 37817    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0104   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2100     |
|    fps              | 189      |
|    time_elapsed     | 200      |
|    total_timesteps  | 37883    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0103   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2104     |
|    fps              | 189      |
|    time_elapsed     | 200      |
|    total_timesteps  | 37958    |
----------------------------------
Eval num_timesteps=38000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0101   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2108     |
|    fps              | 187      |
|    time_elapsed     | 202      |
|    total_timesteps  | 38034    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -3e-05   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2112     |
|    fps              | 188      |
|    time_elapsed     | 202      |
|    total_timesteps  | 38102    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.00013  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2116     |
|    fps              | 188      |
|    time_elapsed     | 202      |
|    total_timesteps  | 38170    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0102   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2120     |
|    fps              | 188      |
|    time_elapsed     | 202      |
|    total_timesteps  | 38235    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0204   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2124     |
|    fps              | 188      |
|    time_elapsed     | 202      |
|    total_timesteps  | 38300    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.031    |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2128     |
|    fps              | 189      |
|    time_elapsed     | 202      |
|    total_timesteps  | 38367    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0308   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2132     |
|    fps              | 189      |
|    time_elapsed     | 202      |
|    total_timesteps  | 38433    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0311   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2136     |
|    fps              | 189      |
|    time_elapsed     | 202      |
|    total_timesteps  | 38493    |
----------------------------------
Eval num_timesteps=38500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0307   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2140     |
|    fps              | 187      |
|    time_elapsed     | 205      |
|    total_timesteps  | 38573    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0401   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2144     |
|    fps              | 188      |
|    time_elapsed     | 205      |
|    total_timesteps  | 38657    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0397   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2148     |
|    fps              | 188      |
|    time_elapsed     | 205      |
|    total_timesteps  | 38738    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0499   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2152     |
|    fps              | 188      |
|    time_elapsed     | 205      |
|    total_timesteps  | 38805    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0502   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2156     |
|    fps              | 189      |
|    time_elapsed     | 205      |
|    total_timesteps  | 38871    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0405   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2160     |
|    fps              | 189      |
|    time_elapsed     | 205      |
|    total_timesteps  | 38942    |
----------------------------------
Eval num_timesteps=39000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.05     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2164     |
|    fps              | 187      |
|    time_elapsed     | 207      |
|    total_timesteps  | 39023    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0399   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2168     |
|    fps              | 188      |
|    time_elapsed     | 207      |
|    total_timesteps  | 39100    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0399   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2172     |
|    fps              | 188      |
|    time_elapsed     | 207      |
|    total_timesteps  | 39172    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0403   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2176     |
|    fps              | 188      |
|    time_elapsed     | 207      |
|    total_timesteps  | 39239    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0301   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2180     |
|    fps              | 189      |
|    time_elapsed     | 207      |
|    total_timesteps  | 39301    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0304   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2184     |
|    fps              | 189      |
|    time_elapsed     | 207      |
|    total_timesteps  | 39370    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0202   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2188     |
|    fps              | 189      |
|    time_elapsed     | 207      |
|    total_timesteps  | 39448    |
----------------------------------
Eval num_timesteps=39500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00991  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2192     |
|    fps              | 187      |
|    time_elapsed     | 210      |
|    total_timesteps  | 39517    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0101   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2196     |
|    fps              | 188      |
|    time_elapsed     | 210      |
|    total_timesteps  | 39591    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00053 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2200     |
|    fps              | 188      |
|    time_elapsed     | 210      |
|    total_timesteps  | 39672    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.00025 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2204     |
|    fps              | 188      |
|    time_elapsed     | 210      |
|    total_timesteps  | 39740    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.00017 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2208     |
|    fps              | 189      |
|    time_elapsed     | 210      |
|    total_timesteps  | 39814    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.00029 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2212     |
|    fps              | 189      |
|    time_elapsed     | 210      |
|    total_timesteps  | 39885    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.00033 |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2216     |
|    fps              | 189      |
|    time_elapsed     | 210      |
|    total_timesteps  | 39954    |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0105  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2220     |
|    fps              | 187      |
|    time_elapsed     | 213      |
|    total_timesteps  | 40024    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000468 |
|    n_updates        | 5        |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0207  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2224     |
|    fps              | 187      |
|    time_elapsed     | 213      |
|    total_timesteps  | 40094    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00786  |
|    n_updates        | 23       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.031   |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2228     |
|    fps              | 188      |
|    time_elapsed     | 213      |
|    total_timesteps  | 40169    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.07e-05 |
|    n_updates        | 42       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0311  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2232     |
|    fps              | 188      |
|    time_elapsed     | 213      |
|    total_timesteps  | 40236    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.99e-05 |
|    n_updates        | 58       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0422  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2236     |
|    fps              | 188      |
|    time_elapsed     | 213      |
|    total_timesteps  | 40323    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.49e-05 |
|    n_updates        | 80       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0317  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2240     |
|    fps              | 189      |
|    time_elapsed     | 213      |
|    total_timesteps  | 40392    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.17e-05 |
|    n_updates        | 97       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0409  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2244     |
|    fps              | 189      |
|    time_elapsed     | 213      |
|    total_timesteps  | 40456    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.8e-05  |
|    n_updates        | 113      |
----------------------------------
Eval num_timesteps=40500, episode_reward=0.06 +/- 0.33
Episode length: 14.64 +/- 1.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 14.6     |
|    mean_reward      | 0.0624   |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0145   |
|    n_updates        | 124      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0303  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2248     |
|    fps              | 189      |
|    time_elapsed     | 214      |
|    total_timesteps  | 40521    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00727  |
|    n_updates        | 130      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0405  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2252     |
|    fps              | 189      |
|    time_elapsed     | 214      |
|    total_timesteps  | 40593    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.53e-05 |
|    n_updates        | 148      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0404  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2256     |
|    fps              | 189      |
|    time_elapsed     | 214      |
|    total_timesteps  | 40657    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00723  |
|    n_updates        | 164      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0405  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2260     |
|    fps              | 189      |
|    time_elapsed     | 214      |
|    total_timesteps  | 40731    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00786  |
|    n_updates        | 182      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0298  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2264     |
|    fps              | 190      |
|    time_elapsed     | 214      |
|    total_timesteps  | 40794    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.4e-05  |
|    n_updates        | 198      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0196  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2268     |
|    fps              | 190      |
|    time_elapsed     | 214      |
|    total_timesteps  | 40867    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.49e-05 |
|    n_updates        | 216      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0194  |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 2272     |
|    fps              | 190      |
|    time_elapsed     | 214      |
|    total_timesteps  | 40934    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00728  |
|    n_updates        | 233      |
----------------------------------
Eval num_timesteps=41000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.92e-05 |
|    n_updates        | 249      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0203  |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2276     |
|    fps              | 188      |
|    time_elapsed     | 217      |
|    total_timesteps  | 41024    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.5e-05  |
|    n_updates        | 255      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0212  |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2280     |
|    fps              | 189      |
|    time_elapsed     | 217      |
|    total_timesteps  | 41108    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.88e-05 |
|    n_updates        | 276      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0219  |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2284     |
|    fps              | 189      |
|    time_elapsed     | 217      |
|    total_timesteps  | 41194    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.75e-05 |
|    n_updates        | 298      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2288     |
|    fps              | 189      |
|    time_elapsed     | 217      |
|    total_timesteps  | 41289    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.78e-05 |
|    n_updates        | 322      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2292     |
|    fps              | 190      |
|    time_elapsed     | 217      |
|    total_timesteps  | 41361    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.21e-05 |
|    n_updates        | 340      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0132  |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2296     |
|    fps              | 190      |
|    time_elapsed     | 217      |
|    total_timesteps  | 41448    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00725  |
|    n_updates        | 361      |
----------------------------------
Eval num_timesteps=41500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.64e-05 |
|    n_updates        | 374      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.013   |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2300     |
|    fps              | 188      |
|    time_elapsed     | 220      |
|    total_timesteps  | 41524    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00783  |
|    n_updates        | 380      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.013   |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2304     |
|    fps              | 189      |
|    time_elapsed     | 220      |
|    total_timesteps  | 41592    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0073   |
|    n_updates        | 397      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0132  |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2308     |
|    fps              | 189      |
|    time_elapsed     | 220      |
|    total_timesteps  | 41671    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.37e-05 |
|    n_updates        | 417      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0134  |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2312     |
|    fps              | 189      |
|    time_elapsed     | 220      |
|    total_timesteps  | 41747    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.56e-05 |
|    n_updates        | 436      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0134  |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2316     |
|    fps              | 189      |
|    time_elapsed     | 220      |
|    total_timesteps  | 41816    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00724  |
|    n_updates        | 453      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.013   |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2320     |
|    fps              | 190      |
|    time_elapsed     | 220      |
|    total_timesteps  | 41877    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.18e-05 |
|    n_updates        | 469      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.00356 |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2324     |
|    fps              | 190      |
|    time_elapsed     | 220      |
|    total_timesteps  | 41960    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00727  |
|    n_updates        | 489      |
----------------------------------
Eval num_timesteps=42000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.93e-05 |
|    n_updates        | 499      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00328 |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 2328     |
|    fps              | 188      |
|    time_elapsed     | 222      |
|    total_timesteps  | 42028    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.78e-05 |
|    n_updates        | 506      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00352 |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2332     |
|    fps              | 188      |
|    time_elapsed     | 222      |
|    total_timesteps  | 42101    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.72e-05 |
|    n_updates        | 525      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00653  |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2336     |
|    fps              | 189      |
|    time_elapsed     | 223      |
|    total_timesteps  | 42187    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.42e-05 |
|    n_updates        | 546      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00578  |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2340     |
|    fps              | 189      |
|    time_elapsed     | 223      |
|    total_timesteps  | 42275    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.98e-05 |
|    n_updates        | 568      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0055   |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2344     |
|    fps              | 189      |
|    time_elapsed     | 223      |
|    total_timesteps  | 42346    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.015    |
|    n_updates        | 586      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.00506 |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2348     |
|    fps              | 190      |
|    time_elapsed     | 223      |
|    total_timesteps  | 42425    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00725  |
|    n_updates        | 606      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.00533  |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2352     |
|    fps              | 190      |
|    time_elapsed     | 223      |
|    total_timesteps  | 42487    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00784  |
|    n_updates        | 621      |
----------------------------------
Eval num_timesteps=42500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.87e-05 |
|    n_updates        | 624      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.00453  |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2356     |
|    fps              | 188      |
|    time_elapsed     | 225      |
|    total_timesteps  | 42571    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.66e-05 |
|    n_updates        | 642      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.00437  |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2360     |
|    fps              | 188      |
|    time_elapsed     | 225      |
|    total_timesteps  | 42649    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.8e-05  |
|    n_updates        | 662      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.00463  |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2364     |
|    fps              | 189      |
|    time_elapsed     | 225      |
|    total_timesteps  | 42705    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.99e-05 |
|    n_updates        | 676      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.00502  |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2368     |
|    fps              | 189      |
|    time_elapsed     | 225      |
|    total_timesteps  | 42768    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00729  |
|    n_updates        | 691      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.00486  |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2372     |
|    fps              | 189      |
|    time_elapsed     | 225      |
|    total_timesteps  | 42839    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00719  |
|    n_updates        | 709      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0055   |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 2376     |
|    fps              | 189      |
|    time_elapsed     | 226      |
|    total_timesteps  | 42913    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 728      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0262   |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 2380     |
|    fps              | 190      |
|    time_elapsed     | 226      |
|    total_timesteps  | 42979    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.78e-05 |
|    n_updates        | 744      |
----------------------------------
Eval num_timesteps=43000, episode_reward=-0.12 +/- 0.18
Episode length: 34.66 +/- 26.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.7     |
|    mean_reward      | -0.118   |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.24e-05 |
|    n_updates        | 749      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0265   |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 2384     |
|    fps              | 189      |
|    time_elapsed     | 227      |
|    total_timesteps  | 43057    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.43e-05 |
|    n_updates        | 764      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0377   |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 2388     |
|    fps              | 189      |
|    time_elapsed     | 227      |
|    total_timesteps  | 43122    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.6e-05  |
|    n_updates        | 780      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.038    |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 2392     |
|    fps              | 189      |
|    time_elapsed     | 227      |
|    total_timesteps  | 43187    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.64e-05 |
|    n_updates        | 796      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0387   |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 2396     |
|    fps              | 190      |
|    time_elapsed     | 227      |
|    total_timesteps  | 43255    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.62e-05 |
|    n_updates        | 813      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.059    |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 2400     |
|    fps              | 190      |
|    time_elapsed     | 227      |
|    total_timesteps  | 43324    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.51e-05 |
|    n_updates        | 830      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0792   |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 2404     |
|    fps              | 190      |
|    time_elapsed     | 227      |
|    total_timesteps  | 43388    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.06e-05 |
|    n_updates        | 846      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0893   |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 2408     |
|    fps              | 190      |
|    time_elapsed     | 227      |
|    total_timesteps  | 43465    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00716  |
|    n_updates        | 866      |
----------------------------------
Eval num_timesteps=43500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 14.6     |
|    mean_reward      | 0.0425   |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.42e-05 |
|    n_updates        | 874      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0895   |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 2412     |
|    fps              | 190      |
|    time_elapsed     | 228      |
|    total_timesteps  | 43536    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.1e-05  |
|    n_updates        | 883      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0892   |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 2416     |
|    fps              | 191      |
|    time_elapsed     | 228      |
|    total_timesteps  | 43613    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.44e-05 |
|    n_updates        | 903      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0984   |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 2420     |
|    fps              | 191      |
|    time_elapsed     | 228      |
|    total_timesteps  | 43692    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00709  |
|    n_updates        | 922      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0988   |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 2424     |
|    fps              | 191      |
|    time_elapsed     | 228      |
|    total_timesteps  | 43766    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.02e-05 |
|    n_updates        | 941      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0981   |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 2428     |
|    fps              | 191      |
|    time_elapsed     | 228      |
|    total_timesteps  | 43851    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.24e-05 |
|    n_updates        | 962      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0976   |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 2432     |
|    fps              | 192      |
|    time_elapsed     | 228      |
|    total_timesteps  | 43936    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.37e-05 |
|    n_updates        | 983      |
----------------------------------
Eval num_timesteps=44000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.43e-05 |
|    n_updates        | 999      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0881   |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 2436     |
|    fps              | 190      |
|    time_elapsed     | 231      |
|    total_timesteps  | 44009    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2e-05    |
|    n_updates        | 1002     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0891   |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 2440     |
|    fps              | 190      |
|    time_elapsed     | 231      |
|    total_timesteps  | 44072    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.33e-05 |
|    n_updates        | 1017     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.089    |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 2444     |
|    fps              | 190      |
|    time_elapsed     | 231      |
|    total_timesteps  | 44145    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.33e-05 |
|    n_updates        | 1036     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0888   |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 2448     |
|    fps              | 191      |
|    time_elapsed     | 231      |
|    total_timesteps  | 44229    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00722  |
|    n_updates        | 1057     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0786   |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 2452     |
|    fps              | 191      |
|    time_elapsed     | 231      |
|    total_timesteps  | 44298    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.55e-05 |
|    n_updates        | 1074     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0788   |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 2456     |
|    fps              | 191      |
|    time_elapsed     | 231      |
|    total_timesteps  | 44377    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00726  |
|    n_updates        | 1094     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.079    |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 2460     |
|    fps              | 192      |
|    time_elapsed     | 231      |
|    total_timesteps  | 44449    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.87e-05 |
|    n_updates        | 1112     |
----------------------------------
Eval num_timesteps=44500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.35e-05 |
|    n_updates        | 1124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0583   |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 2464     |
|    fps              | 190      |
|    time_elapsed     | 233      |
|    total_timesteps  | 44523    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.07e-05 |
|    n_updates        | 1130     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0477   |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 2468     |
|    fps              | 190      |
|    time_elapsed     | 233      |
|    total_timesteps  | 44599    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.11e-05 |
|    n_updates        | 1149     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.048    |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 2472     |
|    fps              | 190      |
|    time_elapsed     | 234      |
|    total_timesteps  | 44664    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.92e-05 |
|    n_updates        | 1165     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0483   |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 2476     |
|    fps              | 191      |
|    time_elapsed     | 234      |
|    total_timesteps  | 44731    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.52e-05 |
|    n_updates        | 1182     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0274   |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 2480     |
|    fps              | 191      |
|    time_elapsed     | 234      |
|    total_timesteps  | 44818    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00727  |
|    n_updates        | 1204     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0275   |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 2484     |
|    fps              | 191      |
|    time_elapsed     | 234      |
|    total_timesteps  | 44895    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.96e-05 |
|    n_updates        | 1223     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0175   |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 2488     |
|    fps              | 191      |
|    time_elapsed     | 234      |
|    total_timesteps  | 44959    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.47e-05 |
|    n_updates        | 1239     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.28 +/- 0.05
Episode length: 70.76 +/- 12.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.8     |
|    mean_reward      | -0.283   |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.29e-05 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0173   |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 2492     |
|    fps              | 190      |
|    time_elapsed     | 236      |
|    total_timesteps  | 45029    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.72e-05 |
|    n_updates        | 1257     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0174   |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 2496     |
|    fps              | 190      |
|    time_elapsed     | 236      |
|    total_timesteps  | 45096    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.21e-05 |
|    n_updates        | 1273     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00262 |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 2500     |
|    fps              | 190      |
|    time_elapsed     | 236      |
|    total_timesteps  | 45165    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.9e-05  |
|    n_updates        | 1291     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 2504     |
|    fps              | 190      |
|    time_elapsed     | 236      |
|    total_timesteps  | 45230    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.09e-05 |
|    n_updates        | 1307     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.023   |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 2508     |
|    fps              | 191      |
|    time_elapsed     | 236      |
|    total_timesteps  | 45315    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.66e-05 |
|    n_updates        | 1328     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0132  |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 2512     |
|    fps              | 191      |
|    time_elapsed     | 236      |
|    total_timesteps  | 45390    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.16e-05 |
|    n_updates        | 1347     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0133  |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 2516     |
|    fps              | 191      |
|    time_elapsed     | 237      |
|    total_timesteps  | 45470    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.75e-05 |
|    n_updates        | 1367     |
----------------------------------
Eval num_timesteps=45500, episode_reward=-0.17 +/- 0.16
Episode length: 48.88 +/- 19.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.9     |
|    mean_reward      | -0.175   |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.8e-05  |
|    n_updates        | 1374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00281 |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 2520     |
|    fps              | 190      |
|    time_elapsed     | 238      |
|    total_timesteps  | 45538    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00723  |
|    n_updates        | 1384     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 2524     |
|    fps              | 191      |
|    time_elapsed     | 238      |
|    total_timesteps  | 45609    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.48e-05 |
|    n_updates        | 1402     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00784  |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 2528     |
|    fps              | 191      |
|    time_elapsed     | 238      |
|    total_timesteps  | 45681    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.02e-05 |
|    n_updates        | 1420     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00816  |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 2532     |
|    fps              | 191      |
|    time_elapsed     | 238      |
|    total_timesteps  | 45758    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.38e-05 |
|    n_updates        | 1439     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00768  |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 2536     |
|    fps              | 191      |
|    time_elapsed     | 238      |
|    total_timesteps  | 45843    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00717  |
|    n_updates        | 1460     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00267 |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 2540     |
|    fps              | 192      |
|    time_elapsed     | 238      |
|    total_timesteps  | 45915    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.5e-05  |
|    n_updates        | 1478     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.00702  |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 2544     |
|    fps              | 192      |
|    time_elapsed     | 239      |
|    total_timesteps  | 45996    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00717  |
|    n_updates        | 1498     |
----------------------------------
Eval num_timesteps=46000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.69e-05 |
|    n_updates        | 1499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00758  |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 2548     |
|    fps              | 190      |
|    time_elapsed     | 241      |
|    total_timesteps  | 46066    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.84e-05 |
|    n_updates        | 1516     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0177   |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 2552     |
|    fps              | 190      |
|    time_elapsed     | 241      |
|    total_timesteps  | 46133    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 1533     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 2556     |
|    fps              | 191      |
|    time_elapsed     | 241      |
|    total_timesteps  | 46226    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.59e-05 |
|    n_updates        | 1556     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0173   |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 2560     |
|    fps              | 191      |
|    time_elapsed     | 241      |
|    total_timesteps  | 46293    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.11e-05 |
|    n_updates        | 1573     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0176   |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 2564     |
|    fps              | 191      |
|    time_elapsed     | 241      |
|    total_timesteps  | 46361    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.99e-05 |
|    n_updates        | 1590     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0177   |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 2568     |
|    fps              | 192      |
|    time_elapsed     | 241      |
|    total_timesteps  | 46433    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.12e-05 |
|    n_updates        | 1608     |
----------------------------------
Eval num_timesteps=46500, episode_reward=-0.28 +/- 0.06
Episode length: 69.68 +/- 14.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.7     |
|    mean_reward      | -0.279   |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00742  |
|    n_updates        | 1624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0176   |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 2572     |
|    fps              | 190      |
|    time_elapsed     | 244      |
|    total_timesteps  | 46502    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.6e-05  |
|    n_updates        | 1625     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0176   |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 2576     |
|    fps              | 190      |
|    time_elapsed     | 244      |
|    total_timesteps  | 46567    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.56e-05 |
|    n_updates        | 1641     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.018    |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 2580     |
|    fps              | 190      |
|    time_elapsed     | 244      |
|    total_timesteps  | 46645    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.58e-05 |
|    n_updates        | 1661     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0182   |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 2584     |
|    fps              | 191      |
|    time_elapsed     | 244      |
|    total_timesteps  | 46717    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.02e-05 |
|    n_updates        | 1679     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0181   |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 2588     |
|    fps              | 191      |
|    time_elapsed     | 244      |
|    total_timesteps  | 46784    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.53e-05 |
|    n_updates        | 1695     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00815  |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 2592     |
|    fps              | 191      |
|    time_elapsed     | 244      |
|    total_timesteps  | 46852    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00722  |
|    n_updates        | 1712     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0176   |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 2596     |
|    fps              | 191      |
|    time_elapsed     | 244      |
|    total_timesteps  | 46934    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.89e-05 |
|    n_updates        | 1733     |
----------------------------------
Eval num_timesteps=47000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 14.8     |
|    mean_reward      | 0.00196  |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.7e-05  |
|    n_updates        | 1749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 2600     |
|    fps              | 191      |
|    time_elapsed     | 245      |
|    total_timesteps  | 47012    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.48e-05 |
|    n_updates        | 1752     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 2604     |
|    fps              | 192      |
|    time_elapsed     | 245      |
|    total_timesteps  | 47078    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.23e-05 |
|    n_updates        | 1769     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0176   |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 2608     |
|    fps              | 192      |
|    time_elapsed     | 245      |
|    total_timesteps  | 47153    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.8e-05  |
|    n_updates        | 1788     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0178   |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 2612     |
|    fps              | 192      |
|    time_elapsed     | 245      |
|    total_timesteps  | 47223    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.94e-05 |
|    n_updates        | 1805     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0182   |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 2616     |
|    fps              | 192      |
|    time_elapsed     | 245      |
|    total_timesteps  | 47293    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00727  |
|    n_updates        | 1823     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0017  |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 2620     |
|    fps              | 192      |
|    time_elapsed     | 245      |
|    total_timesteps  | 47357    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.39e-05 |
|    n_updates        | 1839     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0015  |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 2624     |
|    fps              | 193      |
|    time_elapsed     | 245      |
|    total_timesteps  | 47423    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0073   |
|    n_updates        | 1855     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0214  |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 2628     |
|    fps              | 193      |
|    time_elapsed     | 245      |
|    total_timesteps  | 47492    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.5e-05  |
|    n_updates        | 1872     |
----------------------------------
Eval num_timesteps=47500, episode_reward=-0.12 +/- 0.30
Episode length: 44.78 +/- 30.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | -0.119   |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.79e-05 |
|    n_updates        | 1874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0213  |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 2632     |
|    fps              | 192      |
|    time_elapsed     | 247      |
|    total_timesteps  | 47567    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.26e-05 |
|    n_updates        | 1891     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0206  |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 2636     |
|    fps              | 192      |
|    time_elapsed     | 247      |
|    total_timesteps  | 47634    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.72e-05 |
|    n_updates        | 1908     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0104  |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 2640     |
|    fps              | 193      |
|    time_elapsed     | 247      |
|    total_timesteps  | 47702    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.85e-05 |
|    n_updates        | 1925     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0096  |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 2644     |
|    fps              | 193      |
|    time_elapsed     | 247      |
|    total_timesteps  | 47762    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.11e-05 |
|    n_updates        | 1940     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0105   |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 2648     |
|    fps              | 193      |
|    time_elapsed     | 247      |
|    total_timesteps  | 47830    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.61e-05 |
|    n_updates        | 1957     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00047  |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 2652     |
|    fps              | 193      |
|    time_elapsed     | 247      |
|    total_timesteps  | 47897    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0143   |
|    n_updates        | 1974     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.00131  |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 2656     |
|    fps              | 193      |
|    time_elapsed     | 247      |
|    total_timesteps  | 47969    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00775  |
|    n_updates        | 1992     |
----------------------------------
Eval num_timesteps=48000, episode_reward=-0.19 +/- 0.29
Episode length: 68.46 +/- 10.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.5     |
|    mean_reward      | -0.193   |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.81e-05 |
|    n_updates        | 1999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.00135  |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 2660     |
|    fps              | 192      |
|    time_elapsed     | 249      |
|    total_timesteps  | 48035    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.22e-05 |
|    n_updates        | 2008     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00043  |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 2664     |
|    fps              | 192      |
|    time_elapsed     | 249      |
|    total_timesteps  | 48126    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.98e-05 |
|    n_updates        | 2031     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0206   |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 2668     |
|    fps              | 192      |
|    time_elapsed     | 249      |
|    total_timesteps  | 48193    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.49e-05 |
|    n_updates        | 2048     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0208   |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 2672     |
|    fps              | 193      |
|    time_elapsed     | 249      |
|    total_timesteps  | 48258    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.17e-05 |
|    n_updates        | 2064     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0306   |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 2676     |
|    fps              | 193      |
|    time_elapsed     | 249      |
|    total_timesteps  | 48329    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.75e-05 |
|    n_updates        | 2082     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0304   |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 2680     |
|    fps              | 193      |
|    time_elapsed     | 249      |
|    total_timesteps  | 48410    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.17e-05 |
|    n_updates        | 2102     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0406   |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 2684     |
|    fps              | 193      |
|    time_elapsed     | 250      |
|    total_timesteps  | 48477    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0144   |
|    n_updates        | 2119     |
----------------------------------
Eval num_timesteps=48500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00762  |
|    n_updates        | 2124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0508   |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 2688     |
|    fps              | 192      |
|    time_elapsed     | 252      |
|    total_timesteps  | 48540    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.26e-05 |
|    n_updates        | 2134     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0505   |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 2692     |
|    fps              | 192      |
|    time_elapsed     | 252      |
|    total_timesteps  | 48616    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.29e-05 |
|    n_updates        | 2153     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0513   |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 2696     |
|    fps              | 192      |
|    time_elapsed     | 252      |
|    total_timesteps  | 48677    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00731  |
|    n_updates        | 2169     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.0619   |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 2700     |
|    fps              | 192      |
|    time_elapsed     | 252      |
|    total_timesteps  | 48742    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.89e-05 |
|    n_updates        | 2185     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.0619   |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 2704     |
|    fps              | 193      |
|    time_elapsed     | 252      |
|    total_timesteps  | 48807    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00721  |
|    n_updates        | 2201     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.2     |
|    ep_rew_mean      | 0.0622   |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 2708     |
|    fps              | 193      |
|    time_elapsed     | 252      |
|    total_timesteps  | 48875    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.26e-05 |
|    n_updates        | 2218     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.052    |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 2712     |
|    fps              | 193      |
|    time_elapsed     | 252      |
|    total_timesteps  | 48950    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.25e-05 |
|    n_updates        | 2237     |
----------------------------------
Eval num_timesteps=49000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0071   |
|    n_updates        | 2249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.2     |
|    ep_rew_mean      | 0.0523   |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 2716     |
|    fps              | 191      |
|    time_elapsed     | 255      |
|    total_timesteps  | 49013    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.08e-05 |
|    n_updates        | 2253     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.0619   |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 2720     |
|    fps              | 192      |
|    time_elapsed     | 255      |
|    total_timesteps  | 49086    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00719  |
|    n_updates        | 2271     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.072    |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 2724     |
|    fps              | 192      |
|    time_elapsed     | 255      |
|    total_timesteps  | 49149    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.06e-06 |
|    n_updates        | 2287     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0716   |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 2728     |
|    fps              | 192      |
|    time_elapsed     | 255      |
|    total_timesteps  | 49227    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.57e-05 |
|    n_updates        | 2306     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0708   |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 2732     |
|    fps              | 192      |
|    time_elapsed     | 255      |
|    total_timesteps  | 49323    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.01e-05 |
|    n_updates        | 2330     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0706   |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 2736     |
|    fps              | 193      |
|    time_elapsed     | 255      |
|    total_timesteps  | 49394    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.76e-05 |
|    n_updates        | 2348     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0705   |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 2740     |
|    fps              | 193      |
|    time_elapsed     | 255      |
|    total_timesteps  | 49466    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.2e-05  |
|    n_updates        | 2366     |
----------------------------------
Eval num_timesteps=49500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.94e-05 |
|    n_updates        | 2374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0603   |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 2744     |
|    fps              | 191      |
|    time_elapsed     | 258      |
|    total_timesteps  | 49532    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.13e-05 |
|    n_updates        | 2382     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0399   |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 2748     |
|    fps              | 192      |
|    time_elapsed     | 258      |
|    total_timesteps  | 49609    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00733  |
|    n_updates        | 2402     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0395   |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 2752     |
|    fps              | 192      |
|    time_elapsed     | 258      |
|    total_timesteps  | 49687    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0073   |
|    n_updates        | 2421     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0398   |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 2756     |
|    fps              | 192      |
|    time_elapsed     | 258      |
|    total_timesteps  | 49751    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.89e-05 |
|    n_updates        | 2437     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0494   |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 2760     |
|    fps              | 192      |
|    time_elapsed     | 258      |
|    total_timesteps  | 49827    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00717  |
|    n_updates        | 2456     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0504   |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 2764     |
|    fps              | 193      |
|    time_elapsed     | 258      |
|    total_timesteps  | 49892    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.32e-05 |
|    n_updates        | 2472     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0306   |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 2768     |
|    fps              | 193      |
|    time_elapsed     | 258      |
|    total_timesteps  | 49955    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.51e-05 |
|    n_updates        | 2488     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.984    |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.68e-05 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0408   |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 2772     |
|    fps              | 191      |
|    time_elapsed     | 261      |
|    total_timesteps  | 50016    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00727  |
|    n_updates        | 2503     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.0312   |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 2776     |
|    fps              | 191      |
|    time_elapsed     | 261      |
|    total_timesteps  | 50077    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.12e-05 |
|    n_updates        | 2519     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0316   |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 2780     |
|    fps              | 192      |
|    time_elapsed     | 261      |
|    total_timesteps  | 50146    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.39e-05 |
|    n_updates        | 2536     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0214   |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 2784     |
|    fps              | 192      |
|    time_elapsed     | 261      |
|    total_timesteps  | 50219    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.05e-05 |
|    n_updates        | 2554     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0106   |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 2788     |
|    fps              | 192      |
|    time_elapsed     | 261      |
|    total_timesteps  | 50301    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.68e-05 |
|    n_updates        | 2575     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0102   |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 2792     |
|    fps              | 192      |
|    time_elapsed     | 261      |
|    total_timesteps  | 50387    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.2e-05  |
|    n_updates        | 2596     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0301   |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 2796     |
|    fps              | 193      |
|    time_elapsed     | 261      |
|    total_timesteps  | 50452    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0144   |
|    n_updates        | 2612     |
----------------------------------
Eval num_timesteps=50500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 50500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 2624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0301   |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 2800     |
|    fps              | 191      |
|    time_elapsed     | 263      |
|    total_timesteps  | 50517    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.6e-05  |
|    n_updates        | 2629     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0295   |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 2804     |
|    fps              | 191      |
|    time_elapsed     | 263      |
|    total_timesteps  | 50596    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00777  |
|    n_updates        | 2648     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0393   |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 2808     |
|    fps              | 191      |
|    time_elapsed     | 263      |
|    total_timesteps  | 50671    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00777  |
|    n_updates        | 2667     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0396   |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 2812     |
|    fps              | 192      |
|    time_elapsed     | 264      |
|    total_timesteps  | 50738    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00716  |
|    n_updates        | 2684     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0389   |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 2816     |
|    fps              | 192      |
|    time_elapsed     | 264      |
|    total_timesteps  | 50818    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.56e-05 |
|    n_updates        | 2704     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0292   |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 2820     |
|    fps              | 192      |
|    time_elapsed     | 264      |
|    total_timesteps  | 50884    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.77e-05 |
|    n_updates        | 2720     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.019    |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 2824     |
|    fps              | 192      |
|    time_elapsed     | 264      |
|    total_timesteps  | 50953    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.12e-06 |
|    n_updates        | 2738     |
----------------------------------
Eval num_timesteps=51000, episode_reward=0.05 +/- 0.34
Episode length: 17.10 +/- 11.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.1     |
|    mean_reward      | 0.0526   |
| rollout/            |          |
|    exploration_rate | 0.982    |
| time/               |          |
|    total_timesteps  | 51000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00713  |
|    n_updates        | 2749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.019    |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 2828     |
|    fps              | 192      |
|    time_elapsed     | 264      |
|    total_timesteps  | 51030    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 2757     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0196   |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 2832     |
|    fps              | 192      |
|    time_elapsed     | 264      |
|    total_timesteps  | 51111    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.5e-05  |
|    n_updates        | 2777     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0197   |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 2836     |
|    fps              | 193      |
|    time_elapsed     | 264      |
|    total_timesteps  | 51179    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00772  |
|    n_updates        | 2794     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0101   |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 2840     |
|    fps              | 193      |
|    time_elapsed     | 265      |
|    total_timesteps  | 51240    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.65e-05 |
|    n_updates        | 2809     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00986  |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 2844     |
|    fps              | 193      |
|    time_elapsed     | 265      |
|    total_timesteps  | 51313    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 2828     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0101   |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 2848     |
|    fps              | 193      |
|    time_elapsed     | 265      |
|    total_timesteps  | 51384    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.17e-05 |
|    n_updates        | 2845     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0101   |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 2852     |
|    fps              | 194      |
|    time_elapsed     | 265      |
|    total_timesteps  | 51461    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.99e-05 |
|    n_updates        | 2865     |
----------------------------------
Eval num_timesteps=51500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.981    |
| time/               |          |
|    total_timesteps  | 51500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 2874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0202   |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 2856     |
|    fps              | 192      |
|    time_elapsed     | 267      |
|    total_timesteps  | 51525    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.22e-05 |
|    n_updates        | 2881     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0102   |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 2860     |
|    fps              | 192      |
|    time_elapsed     | 267      |
|    total_timesteps  | 51599    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.66e-05 |
|    n_updates        | 2899     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0194   |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 2864     |
|    fps              | 193      |
|    time_elapsed     | 267      |
|    total_timesteps  | 51686    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.27e-05 |
|    n_updates        | 2921     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0181   |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 2868     |
|    fps              | 193      |
|    time_elapsed     | 267      |
|    total_timesteps  | 51781    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0001   |
|    n_updates        | 2945     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0178   |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 2872     |
|    fps              | 193      |
|    time_elapsed     | 267      |
|    total_timesteps  | 51848    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.59e-05 |
|    n_updates        | 2961     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 2876     |
|    fps              | 193      |
|    time_elapsed     | 267      |
|    total_timesteps  | 51927    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.74e-05 |
|    n_updates        | 2981     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0174   |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 2880     |
|    fps              | 193      |
|    time_elapsed     | 268      |
|    total_timesteps  | 51990    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.5e-05  |
|    n_updates        | 2997     |
----------------------------------
Eval num_timesteps=52000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 52000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 2999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0174   |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 2884     |
|    fps              | 192      |
|    time_elapsed     | 270      |
|    total_timesteps  | 52061    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0078   |
|    n_updates        | 3015     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0278   |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 2888     |
|    fps              | 192      |
|    time_elapsed     | 270      |
|    total_timesteps  | 52135    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0072   |
|    n_updates        | 3033     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0287   |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 2892     |
|    fps              | 192      |
|    time_elapsed     | 270      |
|    total_timesteps  | 52199    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 3049     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00146 |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 2896     |
|    fps              | 193      |
|    time_elapsed     | 270      |
|    total_timesteps  | 52266    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.41e-05 |
|    n_updates        | 3066     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0086   |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 2900     |
|    fps              | 193      |
|    time_elapsed     | 270      |
|    total_timesteps  | 52329    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.21e-05 |
|    n_updates        | 3082     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00089 |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 2904     |
|    fps              | 193      |
|    time_elapsed     | 270      |
|    total_timesteps  | 52395    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.47e-05 |
|    n_updates        | 3098     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0107  |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 2908     |
|    fps              | 193      |
|    time_elapsed     | 270      |
|    total_timesteps  | 52465    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.64e-05 |
|    n_updates        | 3116     |
----------------------------------
Eval num_timesteps=52500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.978    |
| time/               |          |
|    total_timesteps  | 52500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.93e-05 |
|    n_updates        | 3124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00082 |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 2912     |
|    fps              | 192      |
|    time_elapsed     | 273      |
|    total_timesteps  | 52535    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8e-05    |
|    n_updates        | 3133     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00066 |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 2916     |
|    fps              | 192      |
|    time_elapsed     | 273      |
|    total_timesteps  | 52611    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.72e-05 |
|    n_updates        | 3152     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0007  |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 2920     |
|    fps              | 192      |
|    time_elapsed     | 273      |
|    total_timesteps  | 52678    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00722  |
|    n_updates        | 3169     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00937  |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 2924     |
|    fps              | 192      |
|    time_elapsed     | 273      |
|    total_timesteps  | 52745    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.8e-05  |
|    n_updates        | 3186     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00849  |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 2928     |
|    fps              | 193      |
|    time_elapsed     | 273      |
|    total_timesteps  | 52844    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.82e-05 |
|    n_updates        | 3210     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00929  |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 2932     |
|    fps              | 193      |
|    time_elapsed     | 273      |
|    total_timesteps  | 52905    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0072   |
|    n_updates        | 3226     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00941  |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 2936     |
|    fps              | 193      |
|    time_elapsed     | 273      |
|    total_timesteps  | 52970    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.51e-05 |
|    n_updates        | 3242     |
----------------------------------
Eval num_timesteps=53000, episode_reward=-0.08 +/- 0.00
Episode length: 20.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20       |
|    mean_reward      | -0.079   |
| rollout/            |          |
|    exploration_rate | 0.977    |
| time/               |          |
|    total_timesteps  | 53000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.41e-05 |
|    n_updates        | 3249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00917  |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 2940     |
|    fps              | 193      |
|    time_elapsed     | 274      |
|    total_timesteps  | 53037    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00723  |
|    n_updates        | 3259     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0185   |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 2944     |
|    fps              | 193      |
|    time_elapsed     | 274      |
|    total_timesteps  | 53128    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 3281     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.018    |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 2948     |
|    fps              | 193      |
|    time_elapsed     | 274      |
|    total_timesteps  | 53211    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 3302     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0286   |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 2952     |
|    fps              | 193      |
|    time_elapsed     | 274      |
|    total_timesteps  | 53272    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00729  |
|    n_updates        | 3317     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0276   |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 2956     |
|    fps              | 194      |
|    time_elapsed     | 274      |
|    total_timesteps  | 53362    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.99e-05 |
|    n_updates        | 3340     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0274   |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 2960     |
|    fps              | 194      |
|    time_elapsed     | 274      |
|    total_timesteps  | 53440    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 3359     |
----------------------------------
Eval num_timesteps=53500, episode_reward=-0.09 +/- 0.00
Episode length: 22.44 +/- 1.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22.4     |
|    mean_reward      | -0.0888  |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 53500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.74e-05 |
|    n_updates        | 3374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0181   |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 2964     |
|    fps              | 194      |
|    time_elapsed     | 275      |
|    total_timesteps  | 53509    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.64e-05 |
|    n_updates        | 3377     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0187   |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 2968     |
|    fps              | 194      |
|    time_elapsed     | 275      |
|    total_timesteps  | 53589    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.4e-05  |
|    n_updates        | 3397     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0187   |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 2972     |
|    fps              | 194      |
|    time_elapsed     | 275      |
|    total_timesteps  | 53655    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.21e-05 |
|    n_updates        | 3413     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0182   |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 2976     |
|    fps              | 194      |
|    time_elapsed     | 275      |
|    total_timesteps  | 53748    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.42e-05 |
|    n_updates        | 3436     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0179   |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 2980     |
|    fps              | 195      |
|    time_elapsed     | 275      |
|    total_timesteps  | 53817    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00764  |
|    n_updates        | 3454     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0281   |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 2984     |
|    fps              | 195      |
|    time_elapsed     | 275      |
|    total_timesteps  | 53884    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.99e-05 |
|    n_updates        | 3470     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0178   |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 2988     |
|    fps              | 195      |
|    time_elapsed     | 275      |
|    total_timesteps  | 53965    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.7e-05  |
|    n_updates        | 3491     |
----------------------------------
Eval num_timesteps=54000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.974    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.04e-05 |
|    n_updates        | 3499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0267   |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 2992     |
|    fps              | 194      |
|    time_elapsed     | 278      |
|    total_timesteps  | 54055    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.46e-05 |
|    n_updates        | 3513     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0263   |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 2996     |
|    fps              | 194      |
|    time_elapsed     | 278      |
|    total_timesteps  | 54132    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.52e-05 |
|    n_updates        | 3532     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0161   |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 3000     |
|    fps              | 194      |
|    time_elapsed     | 278      |
|    total_timesteps  | 54202    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 3550     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0155   |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 3004     |
|    fps              | 194      |
|    time_elapsed     | 278      |
|    total_timesteps  | 54282    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 3570     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0359   |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 3008     |
|    fps              | 195      |
|    time_elapsed     | 278      |
|    total_timesteps  | 54343    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.04e-05 |
|    n_updates        | 3585     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0254   |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 3012     |
|    fps              | 195      |
|    time_elapsed     | 278      |
|    total_timesteps  | 54425    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.78e-05 |
|    n_updates        | 3606     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0258   |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 3016     |
|    fps              | 195      |
|    time_elapsed     | 278      |
|    total_timesteps  | 54492    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00712  |
|    n_updates        | 3622     |
----------------------------------
Eval num_timesteps=54500, episode_reward=-0.30 +/- 0.03
Episode length: 73.80 +/- 8.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.8     |
|    mean_reward      | -0.295   |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 54500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.72e-05 |
|    n_updates        | 3624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0259   |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 3020     |
|    fps              | 193      |
|    time_elapsed     | 281      |
|    total_timesteps  | 54557    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000118 |
|    n_updates        | 3639     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0259   |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 3024     |
|    fps              | 194      |
|    time_elapsed     | 281      |
|    total_timesteps  | 54624    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.88e-05 |
|    n_updates        | 3655     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0371   |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 3028     |
|    fps              | 194      |
|    time_elapsed     | 281      |
|    total_timesteps  | 54693    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.81e-05 |
|    n_updates        | 3673     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0369   |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 3032     |
|    fps              | 194      |
|    time_elapsed     | 281      |
|    total_timesteps  | 54758    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.75e-05 |
|    n_updates        | 3689     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0368   |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 3036     |
|    fps              | 194      |
|    time_elapsed     | 281      |
|    total_timesteps  | 54826    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.27e-05 |
|    n_updates        | 3706     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0369   |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 3040     |
|    fps              | 194      |
|    time_elapsed     | 281      |
|    total_timesteps  | 54891    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.26e-05 |
|    n_updates        | 3722     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0278   |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 3044     |
|    fps              | 195      |
|    time_elapsed     | 281      |
|    total_timesteps  | 54957    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.14e-05 |
|    n_updates        | 3739     |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.971    |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.81e-05 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0284   |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 3048     |
|    fps              | 193      |
|    time_elapsed     | 284      |
|    total_timesteps  | 55027    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.53e-05 |
|    n_updates        | 3756     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0282   |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 3052     |
|    fps              | 193      |
|    time_elapsed     | 284      |
|    total_timesteps  | 55092    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.57e-05 |
|    n_updates        | 3772     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0189   |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 3056     |
|    fps              | 194      |
|    time_elapsed     | 284      |
|    total_timesteps  | 55165    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.98e-05 |
|    n_updates        | 3791     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0191   |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 3060     |
|    fps              | 194      |
|    time_elapsed     | 284      |
|    total_timesteps  | 55239    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.42e-05 |
|    n_updates        | 3809     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0188   |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 3064     |
|    fps              | 194      |
|    time_elapsed     | 284      |
|    total_timesteps  | 55316    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00709  |
|    n_updates        | 3828     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.019    |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 3068     |
|    fps              | 194      |
|    time_elapsed     | 284      |
|    total_timesteps  | 55389    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.61e-05 |
|    n_updates        | 3847     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00895  |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 3072     |
|    fps              | 194      |
|    time_elapsed     | 284      |
|    total_timesteps  | 55457    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.61e-05 |
|    n_updates        | 3864     |
----------------------------------
Eval num_timesteps=55500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.97     |
| time/               |          |
|    total_timesteps  | 55500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00699  |
|    n_updates        | 3874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00983  |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 3076     |
|    fps              | 193      |
|    time_elapsed     | 286      |
|    total_timesteps  | 55528    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00794  |
|    n_updates        | 3881     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00967  |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 3080     |
|    fps              | 193      |
|    time_elapsed     | 286      |
|    total_timesteps  | 55601    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00715  |
|    n_updates        | 3900     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00037 |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 3084     |
|    fps              | 193      |
|    time_elapsed     | 287      |
|    total_timesteps  | 55669    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.79e-05 |
|    n_updates        | 3917     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.00015  |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 3088     |
|    fps              | 194      |
|    time_elapsed     | 287      |
|    total_timesteps  | 55737    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.94e-05 |
|    n_updates        | 3934     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.009   |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 3092     |
|    fps              | 194      |
|    time_elapsed     | 287      |
|    total_timesteps  | 55806    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.88e-05 |
|    n_updates        | 3951     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.00916 |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 3096     |
|    fps              | 194      |
|    time_elapsed     | 287      |
|    total_timesteps  | 55887    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.11e-05 |
|    n_updates        | 3971     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0193  |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 3100     |
|    fps              | 194      |
|    time_elapsed     | 287      |
|    total_timesteps  | 55961    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00701  |
|    n_updates        | 3990     |
----------------------------------
Eval num_timesteps=56000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.968    |
| time/               |          |
|    total_timesteps  | 56000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.22e-05 |
|    n_updates        | 3999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.0188  |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 3104     |
|    fps              | 193      |
|    time_elapsed     | 289      |
|    total_timesteps  | 56028    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.2e-05  |
|    n_updates        | 4006     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0297  |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 3108     |
|    fps              | 193      |
|    time_elapsed     | 289      |
|    total_timesteps  | 56112    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.49e-05 |
|    n_updates        | 4027     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0292  |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 3112     |
|    fps              | 193      |
|    time_elapsed     | 289      |
|    total_timesteps  | 56180    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 4044     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0292  |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 3116     |
|    fps              | 193      |
|    time_elapsed     | 289      |
|    total_timesteps  | 56248    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.17e-05 |
|    n_updates        | 4061     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0298  |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 3120     |
|    fps              | 194      |
|    time_elapsed     | 290      |
|    total_timesteps  | 56328    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.93e-05 |
|    n_updates        | 4081     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0399  |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 3124     |
|    fps              | 194      |
|    time_elapsed     | 290      |
|    total_timesteps  | 56396    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.98e-05 |
|    n_updates        | 4098     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0295  |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 3128     |
|    fps              | 194      |
|    time_elapsed     | 290      |
|    total_timesteps  | 56455    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.39e-05 |
|    n_updates        | 4113     |
----------------------------------
Eval num_timesteps=56500, episode_reward=-0.04 +/- 0.25
Episode length: 26.18 +/- 9.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 26.2     |
|    mean_reward      | -0.0437  |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 56500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.64e-05 |
|    n_updates        | 4124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0298  |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 3132     |
|    fps              | 194      |
|    time_elapsed     | 291      |
|    total_timesteps  | 56528    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.73e-05 |
|    n_updates        | 4131     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0195  |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 3136     |
|    fps              | 194      |
|    time_elapsed     | 291      |
|    total_timesteps  | 56590    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.02e-05 |
|    n_updates        | 4147     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0196  |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 3140     |
|    fps              | 194      |
|    time_elapsed     | 291      |
|    total_timesteps  | 56656    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.28e-05 |
|    n_updates        | 4163     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0197  |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 3144     |
|    fps              | 194      |
|    time_elapsed     | 291      |
|    total_timesteps  | 56725    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00705  |
|    n_updates        | 4181     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.02    |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 3148     |
|    fps              | 194      |
|    time_elapsed     | 291      |
|    total_timesteps  | 56803    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.98e-05 |
|    n_updates        | 4200     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0304  |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 3152     |
|    fps              | 195      |
|    time_elapsed     | 291      |
|    total_timesteps  | 56878    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.96e-05 |
|    n_updates        | 4219     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0306  |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 3156     |
|    fps              | 195      |
|    time_elapsed     | 291      |
|    total_timesteps  | 56955    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00704  |
|    n_updates        | 4238     |
----------------------------------
Eval num_timesteps=57000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.965    |
| time/               |          |
|    total_timesteps  | 57000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.44e-05 |
|    n_updates        | 4249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0307  |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 3160     |
|    fps              | 194      |
|    time_elapsed     | 293      |
|    total_timesteps  | 57031    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 4257     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0307  |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 3164     |
|    fps              | 194      |
|    time_elapsed     | 294      |
|    total_timesteps  | 57108    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00707  |
|    n_updates        | 4276     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0309  |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 3168     |
|    fps              | 194      |
|    time_elapsed     | 294      |
|    total_timesteps  | 57187    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.31e-05 |
|    n_updates        | 4296     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0314  |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 3172     |
|    fps              | 194      |
|    time_elapsed     | 294      |
|    total_timesteps  | 57267    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.82e-05 |
|    n_updates        | 4316     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0313  |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 3176     |
|    fps              | 194      |
|    time_elapsed     | 294      |
|    total_timesteps  | 57336    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.39e-05 |
|    n_updates        | 4333     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0312  |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 3180     |
|    fps              | 195      |
|    time_elapsed     | 294      |
|    total_timesteps  | 57406    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00743  |
|    n_updates        | 4351     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0213  |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 3184     |
|    fps              | 195      |
|    time_elapsed     | 294      |
|    total_timesteps  | 57475    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00702  |
|    n_updates        | 4368     |
----------------------------------
Eval num_timesteps=57500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.964    |
| time/               |          |
|    total_timesteps  | 57500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.43e-05 |
|    n_updates        | 4374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0214  |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 3188     |
|    fps              | 193      |
|    time_elapsed     | 296      |
|    total_timesteps  | 57547    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.53e-05 |
|    n_updates        | 4386     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0212  |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 3192     |
|    fps              | 194      |
|    time_elapsed     | 296      |
|    total_timesteps  | 57611    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00707  |
|    n_updates        | 4402     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0207  |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 3196     |
|    fps              | 194      |
|    time_elapsed     | 296      |
|    total_timesteps  | 57679    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.92e-05 |
|    n_updates        | 4419     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0204  |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 3200     |
|    fps              | 194      |
|    time_elapsed     | 296      |
|    total_timesteps  | 57746    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.49e-05 |
|    n_updates        | 4436     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0103  |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 3204     |
|    fps              | 194      |
|    time_elapsed     | 297      |
|    total_timesteps  | 57811    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00711  |
|    n_updates        | 4452     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.00965 |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 3208     |
|    fps              | 194      |
|    time_elapsed     | 297      |
|    total_timesteps  | 57878    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.8e-05  |
|    n_updates        | 4469     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00067  |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 3212     |
|    fps              | 194      |
|    time_elapsed     | 297      |
|    total_timesteps  | 57938    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.35e-05 |
|    n_updates        | 4484     |
----------------------------------
Eval num_timesteps=58000, episode_reward=-0.14 +/- 0.35
Episode length: 59.04 +/- 22.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59       |
|    mean_reward      | -0.136   |
| rollout/            |          |
|    exploration_rate | 0.962    |
| time/               |          |
|    total_timesteps  | 58000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.81e-05 |
|    n_updates        | 4499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.00019  |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 3216     |
|    fps              | 193      |
|    time_elapsed     | 299      |
|    total_timesteps  | 58018    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.24e-05 |
|    n_updates        | 4504     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0104   |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 3220     |
|    fps              | 194      |
|    time_elapsed     | 299      |
|    total_timesteps  | 58093    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00686  |
|    n_updates        | 4523     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0104   |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 3224     |
|    fps              | 194      |
|    time_elapsed     | 299      |
|    total_timesteps  | 58161    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 4540     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.00983 |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 3228     |
|    fps              | 194      |
|    time_elapsed     | 299      |
|    total_timesteps  | 58226    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.99e-05 |
|    n_updates        | 4556     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0103  |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 3232     |
|    fps              | 194      |
|    time_elapsed     | 299      |
|    total_timesteps  | 58310    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00722  |
|    n_updates        | 4577     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0206  |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 3236     |
|    fps              | 194      |
|    time_elapsed     | 299      |
|    total_timesteps  | 58379    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.05e-05 |
|    n_updates        | 4594     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0206  |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 3240     |
|    fps              | 195      |
|    time_elapsed     | 299      |
|    total_timesteps  | 58447    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.04e-05 |
|    n_updates        | 4611     |
----------------------------------
Eval num_timesteps=58500, episode_reward=0.00 +/- 0.32
Episode length: 24.62 +/- 20.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 24.6     |
|    mean_reward      | 0.00238  |
| rollout/            |          |
|    exploration_rate | 0.961    |
| time/               |          |
|    total_timesteps  | 58500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.95e-05 |
|    n_updates        | 4624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0104  |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 3244     |
|    fps              | 194      |
|    time_elapsed     | 300      |
|    total_timesteps  | 58511    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.97e-05 |
|    n_updates        | 4627     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0104  |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 3248     |
|    fps              | 194      |
|    time_elapsed     | 300      |
|    total_timesteps  | 58588    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.45e-05 |
|    n_updates        | 4646     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0103  |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 3252     |
|    fps              | 195      |
|    time_elapsed     | 300      |
|    total_timesteps  | 58660    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0148   |
|    n_updates        | 4664     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.00989 |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 3256     |
|    fps              | 195      |
|    time_elapsed     | 300      |
|    total_timesteps  | 58727    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.58e-05 |
|    n_updates        | 4681     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0005   |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 3260     |
|    fps              | 195      |
|    time_elapsed     | 300      |
|    total_timesteps  | 58793    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.8e-05  |
|    n_updates        | 4698     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.00094  |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 3264     |
|    fps              | 195      |
|    time_elapsed     | 300      |
|    total_timesteps  | 58859    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00693  |
|    n_updates        | 4714     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.00146  |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 3268     |
|    fps              | 195      |
|    time_elapsed     | 300      |
|    total_timesteps  | 58925    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00699  |
|    n_updates        | 4731     |
----------------------------------
Eval num_timesteps=59000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.959    |
| time/               |          |
|    total_timesteps  | 59000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00689  |
|    n_updates        | 4749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.00166  |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 3272     |
|    fps              | 194      |
|    time_elapsed     | 303      |
|    total_timesteps  | 59000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.00178  |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 3276     |
|    fps              | 194      |
|    time_elapsed     | 303      |
|    total_timesteps  | 59066    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.29e-05 |
|    n_updates        | 4766     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.3     |
|    ep_rew_mean      | 0.0019   |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 3280     |
|    fps              | 194      |
|    time_elapsed     | 303      |
|    total_timesteps  | 59133    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.89e-05 |
|    n_updates        | 4783     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | -0.0085  |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 3284     |
|    fps              | 195      |
|    time_elapsed     | 303      |
|    total_timesteps  | 59212    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.23e-05 |
|    n_updates        | 4802     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.2     |
|    ep_rew_mean      | 0.00199  |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 3288     |
|    fps              | 195      |
|    time_elapsed     | 303      |
|    total_timesteps  | 59272    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.07e-05 |
|    n_updates        | 4817     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.00131  |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 3292     |
|    fps              | 195      |
|    time_elapsed     | 303      |
|    total_timesteps  | 59353    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00721  |
|    n_updates        | 4838     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.00119  |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 3296     |
|    fps              | 195      |
|    time_elapsed     | 303      |
|    total_timesteps  | 59424    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0071   |
|    n_updates        | 4855     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.0115   |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 3300     |
|    fps              | 195      |
|    time_elapsed     | 303      |
|    total_timesteps  | 59484    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.95e-05 |
|    n_updates        | 4870     |
----------------------------------
Eval num_timesteps=59500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 59500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.7e-05  |
|    n_updates        | 4874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | 0.00134  |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 3304     |
|    fps              | 194      |
|    time_elapsed     | 306      |
|    total_timesteps  | 59552    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00718  |
|    n_updates        | 4887     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.0089  |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 3308     |
|    fps              | 194      |
|    time_elapsed     | 306      |
|    total_timesteps  | 59625    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.89e-05 |
|    n_updates        | 4906     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0194  |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 3312     |
|    fps              | 194      |
|    time_elapsed     | 306      |
|    total_timesteps  | 59698    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00726  |
|    n_updates        | 4924     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.0189  |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 3316     |
|    fps              | 195      |
|    time_elapsed     | 306      |
|    total_timesteps  | 59766    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.72e-05 |
|    n_updates        | 4941     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0292  |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 3320     |
|    fps              | 195      |
|    time_elapsed     | 306      |
|    total_timesteps  | 59848    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.69e-05 |
|    n_updates        | 4961     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.0292  |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 3324     |
|    fps              | 195      |
|    time_elapsed     | 306      |
|    total_timesteps  | 59915    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00697  |
|    n_updates        | 4978     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0292  |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 3328     |
|    fps              | 195      |
|    time_elapsed     | 306      |
|    total_timesteps  | 59981    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.37e-05 |
|    n_updates        | 4995     |
----------------------------------
Eval num_timesteps=60000, episode_reward=0.12 +/- 0.39
Episode length: 15.78 +/- 8.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 15.8     |
|    mean_reward      | 0.118    |
| rollout/            |          |
|    exploration_rate | 0.956    |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00789  |
|    n_updates        | 4999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | -0.0285  |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 3332     |
|    fps              | 195      |
|    time_elapsed     | 307      |
|    total_timesteps  | 60048    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.94e-05 |
|    n_updates        | 5011     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | -0.0287  |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 3336     |
|    fps              | 195      |
|    time_elapsed     | 307      |
|    total_timesteps  | 60120    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.87e-05 |
|    n_updates        | 5029     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.4     |
|    ep_rew_mean      | -0.0186  |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 3340     |
|    fps              | 195      |
|    time_elapsed     | 307      |
|    total_timesteps  | 60187    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.04e-05 |
|    n_updates        | 5046     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.029   |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 3344     |
|    fps              | 195      |
|    time_elapsed     | 307      |
|    total_timesteps  | 60262    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 5065     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.0193  |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 3348     |
|    fps              | 196      |
|    time_elapsed     | 307      |
|    total_timesteps  | 60345    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0157   |
|    n_updates        | 5086     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | -0.00891 |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 3352     |
|    fps              | 196      |
|    time_elapsed     | 307      |
|    total_timesteps  | 60408    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.7e-05  |
|    n_updates        | 5101     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.00971 |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 3356     |
|    fps              | 196      |
|    time_elapsed     | 307      |
|    total_timesteps  | 60495    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00724  |
|    n_updates        | 5123     |
----------------------------------
Eval num_timesteps=60500, episode_reward=0.09 +/- 0.37
Episode length: 16.84 +/- 4.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.8     |
|    mean_reward      | 0.0937   |
| rollout/            |          |
|    exploration_rate | 0.954    |
| time/               |          |
|    total_timesteps  | 60500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.2e-05  |
|    n_updates        | 5124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0199  |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 3360     |
|    fps              | 196      |
|    time_elapsed     | 308      |
|    total_timesteps  | 60567    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0001   |
|    n_updates        | 5141     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.02    |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 3364     |
|    fps              | 196      |
|    time_elapsed     | 308      |
|    total_timesteps  | 60635    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.48e-05 |
|    n_updates        | 5158     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0206  |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 3368     |
|    fps              | 196      |
|    time_elapsed     | 308      |
|    total_timesteps  | 60716    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.95e-05 |
|    n_updates        | 5178     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0205  |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 3372     |
|    fps              | 197      |
|    time_elapsed     | 308      |
|    total_timesteps  | 60788    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00714  |
|    n_updates        | 5196     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0208  |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 3376     |
|    fps              | 197      |
|    time_elapsed     | 308      |
|    total_timesteps  | 60862    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.91e-05 |
|    n_updates        | 5215     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0207  |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 3380     |
|    fps              | 197      |
|    time_elapsed     | 308      |
|    total_timesteps  | 60927    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.45e-05 |
|    n_updates        | 5231     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0101  |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 3384     |
|    fps              | 197      |
|    time_elapsed     | 308      |
|    total_timesteps  | 60989    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.23e-05 |
|    n_updates        | 5247     |
----------------------------------
Eval num_timesteps=61000, episode_reward=-0.21 +/- 0.25
Episode length: 62.80 +/- 21.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.8     |
|    mean_reward      | -0.211   |
| rollout/            |          |
|    exploration_rate | 0.953    |
| time/               |          |
|    total_timesteps  | 61000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000157 |
|    n_updates        | 5249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0208  |
|    exploration_rate | 0.952    |
| time/               |          |
|    episodes         | 3388     |
|    fps              | 196      |
|    time_elapsed     | 310      |
|    total_timesteps  | 61067    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.73e-05 |
|    n_updates        | 5266     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0207  |
|    exploration_rate | 0.952    |
| time/               |          |
|    episodes         | 3392     |
|    fps              | 196      |
|    time_elapsed     | 310      |
|    total_timesteps  | 61147    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.56e-05 |
|    n_updates        | 5286     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0111  |
|    exploration_rate | 0.952    |
| time/               |          |
|    episodes         | 3396     |
|    fps              | 196      |
|    time_elapsed     | 310      |
|    total_timesteps  | 61226    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00693  |
|    n_updates        | 5306     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0215  |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 3400     |
|    fps              | 197      |
|    time_elapsed     | 311      |
|    total_timesteps  | 61298    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000205 |
|    n_updates        | 5324     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0217  |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 3404     |
|    fps              | 197      |
|    time_elapsed     | 311      |
|    total_timesteps  | 61369    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 5342     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0215  |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 3408     |
|    fps              | 197      |
|    time_elapsed     | 311      |
|    total_timesteps  | 61437    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.34e-05 |
|    n_updates        | 5359     |
----------------------------------
Eval num_timesteps=61500, episode_reward=-0.21 +/- 0.19
Episode length: 57.68 +/- 26.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.7     |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.951    |
| time/               |          |
|    total_timesteps  | 61500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.46e-05 |
|    n_updates        | 5374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0217  |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 3412     |
|    fps              | 196      |
|    time_elapsed     | 313      |
|    total_timesteps  | 61515    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 5378     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0216  |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 3416     |
|    fps              | 196      |
|    time_elapsed     | 313      |
|    total_timesteps  | 61582    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.55e-05 |
|    n_updates        | 5395     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0209  |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 3420     |
|    fps              | 196      |
|    time_elapsed     | 313      |
|    total_timesteps  | 61646    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00704  |
|    n_updates        | 5411     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0208  |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 3424     |
|    fps              | 196      |
|    time_elapsed     | 313      |
|    total_timesteps  | 61711    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.09e-05 |
|    n_updates        | 5427     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0215  |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 3428     |
|    fps              | 197      |
|    time_elapsed     | 313      |
|    total_timesteps  | 61793    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 5448     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0219  |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 3432     |
|    fps              | 197      |
|    time_elapsed     | 313      |
|    total_timesteps  | 61870    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.49e-05 |
|    n_updates        | 5467     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.022   |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 3436     |
|    fps              | 197      |
|    time_elapsed     | 313      |
|    total_timesteps  | 61945    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 5486     |
----------------------------------
Eval num_timesteps=62000, episode_reward=-0.08 +/- 0.32
Episode length: 40.64 +/- 28.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.6     |
|    mean_reward      | -0.0819  |
| rollout/            |          |
|    exploration_rate | 0.949    |
| time/               |          |
|    total_timesteps  | 62000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.69e-05 |
|    n_updates        | 5499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0323  |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 3440     |
|    fps              | 196      |
|    time_elapsed     | 314      |
|    total_timesteps  | 62021    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.08e-05 |
|    n_updates        | 5505     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.022   |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 3444     |
|    fps              | 197      |
|    time_elapsed     | 314      |
|    total_timesteps  | 62087    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00715  |
|    n_updates        | 5521     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0313  |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 3448     |
|    fps              | 197      |
|    time_elapsed     | 315      |
|    total_timesteps  | 62153    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 5538     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0416  |
|    exploration_rate | 0.948    |
| time/               |          |
|    episodes         | 3452     |
|    fps              | 197      |
|    time_elapsed     | 315      |
|    total_timesteps  | 62224    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.92e-05 |
|    n_updates        | 5555     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0408  |
|    exploration_rate | 0.948    |
| time/               |          |
|    episodes         | 3456     |
|    fps              | 197      |
|    time_elapsed     | 315      |
|    total_timesteps  | 62290    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.72e-05 |
|    n_updates        | 5572     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0406  |
|    exploration_rate | 0.948    |
| time/               |          |
|    episodes         | 3460     |
|    fps              | 197      |
|    time_elapsed     | 315      |
|    total_timesteps  | 62358    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00698  |
|    n_updates        | 5589     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0306  |
|    exploration_rate | 0.948    |
| time/               |          |
|    episodes         | 3464     |
|    fps              | 198      |
|    time_elapsed     | 315      |
|    total_timesteps  | 62427    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00716  |
|    n_updates        | 5606     |
----------------------------------
Eval num_timesteps=62500, episode_reward=-0.24 +/- 0.22
Episode length: 69.90 +/- 12.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.9     |
|    mean_reward      | -0.239   |
| rollout/            |          |
|    exploration_rate | 0.947    |
| time/               |          |
|    total_timesteps  | 62500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.16e-05 |
|    n_updates        | 5624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0307  |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 3468     |
|    fps              | 196      |
|    time_elapsed     | 317      |
|    total_timesteps  | 62510    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00693  |
|    n_updates        | 5627     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0311  |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 3472     |
|    fps              | 197      |
|    time_elapsed     | 317      |
|    total_timesteps  | 62592    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.15e-05 |
|    n_updates        | 5647     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0208  |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 3476     |
|    fps              | 197      |
|    time_elapsed     | 317      |
|    total_timesteps  | 62658    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000139 |
|    n_updates        | 5664     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0212  |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 3480     |
|    fps              | 197      |
|    time_elapsed     | 317      |
|    total_timesteps  | 62734    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.61e-05 |
|    n_updates        | 5683     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0326  |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 3484     |
|    fps              | 197      |
|    time_elapsed     | 317      |
|    total_timesteps  | 62830    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.8e-05  |
|    n_updates        | 5707     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.032   |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 3488     |
|    fps              | 197      |
|    time_elapsed     | 317      |
|    total_timesteps  | 62893    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 5723     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.022   |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 3492     |
|    fps              | 198      |
|    time_elapsed     | 317      |
|    total_timesteps  | 62974    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.48e-05 |
|    n_updates        | 5743     |
----------------------------------
Eval num_timesteps=63000, episode_reward=0.03 +/- 0.43
Episode length: 38.30 +/- 22.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.3     |
|    mean_reward      | 0.0276   |
| rollout/            |          |
|    exploration_rate | 0.946    |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00717  |
|    n_updates        | 5749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0319  |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 3496     |
|    fps              | 197      |
|    time_elapsed     | 319      |
|    total_timesteps  | 63050    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.92e-05 |
|    n_updates        | 5762     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0317  |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 3500     |
|    fps              | 197      |
|    time_elapsed     | 319      |
|    total_timesteps  | 63117    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.7e-05  |
|    n_updates        | 5779     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.032   |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 3504     |
|    fps              | 197      |
|    time_elapsed     | 319      |
|    total_timesteps  | 63194    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.26e-05 |
|    n_updates        | 5798     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0325  |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 3508     |
|    fps              | 198      |
|    time_elapsed     | 319      |
|    total_timesteps  | 63275    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.29e-05 |
|    n_updates        | 5818     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0319  |
|    exploration_rate | 0.944    |
| time/               |          |
|    episodes         | 3512     |
|    fps              | 198      |
|    time_elapsed     | 319      |
|    total_timesteps  | 63338    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00717  |
|    n_updates        | 5834     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0319  |
|    exploration_rate | 0.944    |
| time/               |          |
|    episodes         | 3516     |
|    fps              | 198      |
|    time_elapsed     | 319      |
|    total_timesteps  | 63405    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 5851     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0218  |
|    exploration_rate | 0.944    |
| time/               |          |
|    episodes         | 3520     |
|    fps              | 198      |
|    time_elapsed     | 319      |
|    total_timesteps  | 63468    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0071   |
|    n_updates        | 5866     |
----------------------------------
Eval num_timesteps=63500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.944    |
| time/               |          |
|    total_timesteps  | 63500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.14e-05 |
|    n_updates        | 5874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0223  |
|    exploration_rate | 0.944    |
| time/               |          |
|    episodes         | 3524     |
|    fps              | 197      |
|    time_elapsed     | 322      |
|    total_timesteps  | 63545    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0001   |
|    n_updates        | 5886     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0219  |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 3528     |
|    fps              | 197      |
|    time_elapsed     | 322      |
|    total_timesteps  | 63618    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 5904     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0216  |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 3532     |
|    fps              | 197      |
|    time_elapsed     | 322      |
|    total_timesteps  | 63686    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.88e-05 |
|    n_updates        | 5921     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0217  |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 3536     |
|    fps              | 197      |
|    time_elapsed     | 322      |
|    total_timesteps  | 63765    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.36e-05 |
|    n_updates        | 5941     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0219  |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 3540     |
|    fps              | 198      |
|    time_elapsed     | 322      |
|    total_timesteps  | 63846    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00778  |
|    n_updates        | 5961     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0323  |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 3544     |
|    fps              | 198      |
|    time_elapsed     | 322      |
|    total_timesteps  | 63919    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.79e-05 |
|    n_updates        | 5979     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.022   |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 3548     |
|    fps              | 198      |
|    time_elapsed     | 322      |
|    total_timesteps  | 63980    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0001   |
|    n_updates        | 5994     |
----------------------------------
Eval num_timesteps=64000, episode_reward=0.07 +/- 0.35
Episode length: 18.62 +/- 8.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18.6     |
|    mean_reward      | 0.0666   |
| rollout/            |          |
|    exploration_rate | 0.942    |
| time/               |          |
|    total_timesteps  | 64000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.44e-05 |
|    n_updates        | 5999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0224  |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 3552     |
|    fps              | 198      |
|    time_elapsed     | 323      |
|    total_timesteps  | 64059    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.89e-05 |
|    n_updates        | 6014     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 0.941    |
| time/               |          |
|    episodes         | 3556     |
|    fps              | 198      |
|    time_elapsed     | 323      |
|    total_timesteps  | 64132    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.69e-05 |
|    n_updates        | 6032     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0228  |
|    exploration_rate | 0.941    |
| time/               |          |
|    episodes         | 3560     |
|    fps              | 198      |
|    time_elapsed     | 323      |
|    total_timesteps  | 64205    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 6051     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0328  |
|    exploration_rate | 0.941    |
| time/               |          |
|    episodes         | 3564     |
|    fps              | 198      |
|    time_elapsed     | 323      |
|    total_timesteps  | 64272    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 6067     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0128  |
|    exploration_rate | 0.941    |
| time/               |          |
|    episodes         | 3568     |
|    fps              | 198      |
|    time_elapsed     | 323      |
|    total_timesteps  | 64355    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00684  |
|    n_updates        | 6088     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0124  |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 3572     |
|    fps              | 199      |
|    time_elapsed     | 323      |
|    total_timesteps  | 64427    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000141 |
|    n_updates        | 6106     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0223  |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 3576     |
|    fps              | 199      |
|    time_elapsed     | 323      |
|    total_timesteps  | 64491    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 6122     |
----------------------------------
Eval num_timesteps=64500, episode_reward=0.03 +/- 0.39
Episode length: 33.66 +/- 25.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.7     |
|    mean_reward      | 0.0261   |
| rollout/            |          |
|    exploration_rate | 0.94     |
| time/               |          |
|    total_timesteps  | 64500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.41e-05 |
|    n_updates        | 6124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0224  |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 3580     |
|    fps              | 198      |
|    time_elapsed     | 324      |
|    total_timesteps  | 64570    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.75e-05 |
|    n_updates        | 6142     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0221  |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 3584     |
|    fps              | 199      |
|    time_elapsed     | 324      |
|    total_timesteps  | 64659    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000178 |
|    n_updates        | 6164     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0228  |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 3588     |
|    fps              | 199      |
|    time_elapsed     | 324      |
|    total_timesteps  | 64739    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.72e-05 |
|    n_updates        | 6184     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0331  |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 3592     |
|    fps              | 199      |
|    time_elapsed     | 324      |
|    total_timesteps  | 64827    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 6206     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0328  |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 3596     |
|    fps              | 199      |
|    time_elapsed     | 325      |
|    total_timesteps  | 64895    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.6e-05  |
|    n_updates        | 6223     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0331  |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 3600     |
|    fps              | 199      |
|    time_elapsed     | 325      |
|    total_timesteps  | 64970    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 6242     |
----------------------------------
Eval num_timesteps=65000, episode_reward=-0.26 +/- 0.18
Episode length: 69.26 +/- 17.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.3     |
|    mean_reward      | -0.257   |
| rollout/            |          |
|    exploration_rate | 0.938    |
| time/               |          |
|    total_timesteps  | 65000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.83e-05 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0327  |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 3604     |
|    fps              | 198      |
|    time_elapsed     | 327      |
|    total_timesteps  | 65036    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.85e-05 |
|    n_updates        | 6258     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0326  |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 3608     |
|    fps              | 198      |
|    time_elapsed     | 327      |
|    total_timesteps  | 65115    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00714  |
|    n_updates        | 6278     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0225  |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 3612     |
|    fps              | 198      |
|    time_elapsed     | 327      |
|    total_timesteps  | 65177    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.46e-05 |
|    n_updates        | 6294     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 3616     |
|    fps              | 199      |
|    time_elapsed     | 327      |
|    total_timesteps  | 65249    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.68e-05 |
|    n_updates        | 6312     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00242 |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 3620     |
|    fps              | 199      |
|    time_elapsed     | 327      |
|    total_timesteps  | 65304    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.62e-05 |
|    n_updates        | 6325     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00238 |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 3624     |
|    fps              | 199      |
|    time_elapsed     | 327      |
|    total_timesteps  | 65380    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.7e-05  |
|    n_updates        | 6344     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00782  |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 3628     |
|    fps              | 199      |
|    time_elapsed     | 327      |
|    total_timesteps  | 65448    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.41e-05 |
|    n_updates        | 6361     |
----------------------------------
Eval num_timesteps=65500, episode_reward=0.01 +/- 0.27
Episode length: 16.90 +/- 0.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | 0.0134   |
| rollout/            |          |
|    exploration_rate | 0.936    |
| time/               |          |
|    total_timesteps  | 65500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00703  |
|    n_updates        | 6374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00766  |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 3632     |
|    fps              | 199      |
|    time_elapsed     | 328      |
|    total_timesteps  | 65520    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.96e-05 |
|    n_updates        | 6379     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00806  |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 3636     |
|    fps              | 199      |
|    time_elapsed     | 328      |
|    total_timesteps  | 65589    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.07e-05 |
|    n_updates        | 6397     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00854  |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 3640     |
|    fps              | 199      |
|    time_elapsed     | 328      |
|    total_timesteps  | 65658    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.58e-05 |
|    n_updates        | 6414     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00858  |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 3644     |
|    fps              | 200      |
|    time_elapsed     | 328      |
|    total_timesteps  | 65730    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.44e-05 |
|    n_updates        | 6432     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00215 |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 3648     |
|    fps              | 200      |
|    time_elapsed     | 328      |
|    total_timesteps  | 65809    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000133 |
|    n_updates        | 6452     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00792  |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 3652     |
|    fps              | 200      |
|    time_elapsed     | 328      |
|    total_timesteps  | 65886    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.22e-05 |
|    n_updates        | 6471     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00772  |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 3656     |
|    fps              | 200      |
|    time_elapsed     | 328      |
|    total_timesteps  | 65964    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 6490     |
----------------------------------
Eval num_timesteps=66000, episode_reward=0.11 +/- 0.39
Episode length: 16.68 +/- 0.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.7     |
|    mean_reward      | 0.114    |
| rollout/            |          |
|    exploration_rate | 0.935    |
| time/               |          |
|    total_timesteps  | 66000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.72e-05 |
|    n_updates        | 6499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00784  |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 3660     |
|    fps              | 200      |
|    time_elapsed     | 329      |
|    total_timesteps  | 66034    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.37e-05 |
|    n_updates        | 6508     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00756  |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 3664     |
|    fps              | 200      |
|    time_elapsed     | 329      |
|    total_timesteps  | 66108    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.82e-05 |
|    n_updates        | 6526     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.012   |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 3668     |
|    fps              | 200      |
|    time_elapsed     | 329      |
|    total_timesteps  | 66180    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.02e-05 |
|    n_updates        | 6544     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0119  |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 3672     |
|    fps              | 200      |
|    time_elapsed     | 329      |
|    total_timesteps  | 66249    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.54e-05 |
|    n_updates        | 6562     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00193 |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 3676     |
|    fps              | 201      |
|    time_elapsed     | 329      |
|    total_timesteps  | 66315    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 6578     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00149 |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 3680     |
|    fps              | 201      |
|    time_elapsed     | 329      |
|    total_timesteps  | 66383    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.72e-05 |
|    n_updates        | 6595     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00085 |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 3684     |
|    fps              | 201      |
|    time_elapsed     | 329      |
|    total_timesteps  | 66456    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.81e-05 |
|    n_updates        | 6613     |
----------------------------------
Eval num_timesteps=66500, episode_reward=-0.24 +/- 0.16
Episode length: 65.90 +/- 15.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.9     |
|    mean_reward      | -0.243   |
| rollout/            |          |
|    exploration_rate | 0.933    |
| time/               |          |
|    total_timesteps  | 66500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.28e-05 |
|    n_updates        | 6624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00037 |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 3688     |
|    fps              | 200      |
|    time_elapsed     | 331      |
|    total_timesteps  | 66524    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000133 |
|    n_updates        | 6630     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 7e-05    |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 3692     |
|    fps              | 200      |
|    time_elapsed     | 332      |
|    total_timesteps  | 66601    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.7e-05  |
|    n_updates        | 6650     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0104   |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 3696     |
|    fps              | 200      |
|    time_elapsed     | 332      |
|    total_timesteps  | 66660    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.8e-05  |
|    n_updates        | 6664     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0106   |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 3700     |
|    fps              | 200      |
|    time_elapsed     | 332      |
|    total_timesteps  | 66731    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00765  |
|    n_updates        | 6682     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.0105   |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 3704     |
|    fps              | 201      |
|    time_elapsed     | 332      |
|    total_timesteps  | 66800    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00707  |
|    n_updates        | 6699     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0103   |
|    exploration_rate | 0.931    |
| time/               |          |
|    episodes         | 3708     |
|    fps              | 201      |
|    time_elapsed     | 332      |
|    total_timesteps  | 66884    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.38e-05 |
|    n_updates        | 6720     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.00025 |
|    exploration_rate | 0.931    |
| time/               |          |
|    episodes         | 3712     |
|    fps              | 201      |
|    time_elapsed     | 332      |
|    total_timesteps  | 66959    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.19e-05 |
|    n_updates        | 6739     |
----------------------------------
Eval num_timesteps=67000, episode_reward=0.06 +/- 0.39
Episode length: 25.62 +/- 19.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 25.6     |
|    mean_reward      | 0.0583   |
| rollout/            |          |
|    exploration_rate | 0.931    |
| time/               |          |
|    total_timesteps  | 67000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.57e-05 |
|    n_updates        | 6749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0101  |
|    exploration_rate | 0.931    |
| time/               |          |
|    episodes         | 3716     |
|    fps              | 201      |
|    time_elapsed     | 333      |
|    total_timesteps  | 67028    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.42e-05 |
|    n_updates        | 6756     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0307  |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 3720     |
|    fps              | 201      |
|    time_elapsed     | 333      |
|    total_timesteps  | 67096    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.02e-05 |
|    n_updates        | 6773     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0201  |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 3724     |
|    fps              | 201      |
|    time_elapsed     | 333      |
|    total_timesteps  | 67159    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.03e-05 |
|    n_updates        | 6789     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0304  |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 3728     |
|    fps              | 201      |
|    time_elapsed     | 333      |
|    total_timesteps  | 67233    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.44e-05 |
|    n_updates        | 6808     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0302  |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 3732     |
|    fps              | 201      |
|    time_elapsed     | 333      |
|    total_timesteps  | 67301    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 6825     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0201  |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 3736     |
|    fps              | 201      |
|    time_elapsed     | 333      |
|    total_timesteps  | 67367    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.19e-05 |
|    n_updates        | 6841     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0199  |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 3740     |
|    fps              | 202      |
|    time_elapsed     | 333      |
|    total_timesteps  | 67431    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 6857     |
----------------------------------
Eval num_timesteps=67500, episode_reward=-0.10 +/- 0.34
Episode length: 51.24 +/- 24.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | -0.104   |
| rollout/            |          |
|    exploration_rate | 0.929    |
| time/               |          |
|    total_timesteps  | 67500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.96e-05 |
|    n_updates        | 6874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0102  |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 3744     |
|    fps              | 201      |
|    time_elapsed     | 335      |
|    total_timesteps  | 67510    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.72e-05 |
|    n_updates        | 6877     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.00991 |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 3748     |
|    fps              | 201      |
|    time_elapsed     | 335      |
|    total_timesteps  | 67582    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.4e-05  |
|    n_updates        | 6895     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.00039  |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 3752     |
|    fps              | 201      |
|    time_elapsed     | 335      |
|    total_timesteps  | 67652    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.59e-05 |
|    n_updates        | 6912     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00067  |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 3756     |
|    fps              | 201      |
|    time_elapsed     | 335      |
|    total_timesteps  | 67723    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.13e-05 |
|    n_updates        | 6930     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00067  |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 3760     |
|    fps              | 202      |
|    time_elapsed     | 335      |
|    total_timesteps  | 67793    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0073   |
|    n_updates        | 6948     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.00035  |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 3764     |
|    fps              | 202      |
|    time_elapsed     | 335      |
|    total_timesteps  | 67875    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.46e-05 |
|    n_updates        | 6968     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00051  |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 3768     |
|    fps              | 202      |
|    time_elapsed     | 335      |
|    total_timesteps  | 67943    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00704  |
|    n_updates        | 6985     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.5     |
|    ep_rew_mean      | 0.011    |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 3772     |
|    fps              | 202      |
|    time_elapsed     | 335      |
|    total_timesteps  | 67999    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.2e-05  |
|    n_updates        | 6999     |
----------------------------------
Eval num_timesteps=68000, episode_reward=0.17 +/- 0.43
Episode length: 16.64 +/- 0.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 0.175    |
| time/              |          |
|    total_timesteps | 68000    |
---------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | 0.00044  |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 3776     |
|    fps              | 202      |
|    time_elapsed     | 336      |
|    total_timesteps  | 68079    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.62e-05 |
|    n_updates        | 7019     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.00016 |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 3780     |
|    fps              | 202      |
|    time_elapsed     | 336      |
|    total_timesteps  | 68162    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.97e-05 |
|    n_updates        | 7040     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0002   |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 3784     |
|    fps              | 202      |
|    time_elapsed     | 336      |
|    total_timesteps  | 68226    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.51e-05 |
|    n_updates        | 7056     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 4e-05    |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 3788     |
|    fps              | 202      |
|    time_elapsed     | 336      |
|    total_timesteps  | 68298    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.35e-05 |
|    n_updates        | 7074     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.00012  |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 3792     |
|    fps              | 203      |
|    time_elapsed     | 336      |
|    total_timesteps  | 68373    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.96e-05 |
|    n_updates        | 7093     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0102   |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 3796     |
|    fps              | 203      |
|    time_elapsed     | 336      |
|    total_timesteps  | 68429    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.08e-05 |
|    n_updates        | 7107     |
----------------------------------
Eval num_timesteps=68500, episode_reward=-0.10 +/- 0.34
Episode length: 50.10 +/- 21.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.1     |
|    mean_reward      | -0.0997  |
| rollout/            |          |
|    exploration_rate | 0.925    |
| time/               |          |
|    total_timesteps  | 68500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.76e-05 |
|    n_updates        | 7124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00982  |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 3800     |
|    fps              | 202      |
|    time_elapsed     | 338      |
|    total_timesteps  | 68510    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00719  |
|    n_updates        | 7127     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00946  |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 3804     |
|    fps              | 202      |
|    time_elapsed     | 338      |
|    total_timesteps  | 68588    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.007    |
|    n_updates        | 7146     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00982  |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 3808     |
|    fps              | 202      |
|    time_elapsed     | 338      |
|    total_timesteps  | 68663    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0141   |
|    n_updates        | 7165     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0101   |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 3812     |
|    fps              | 202      |
|    time_elapsed     | 338      |
|    total_timesteps  | 68731    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00729  |
|    n_updates        | 7182     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0201   |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 3816     |
|    fps              | 203      |
|    time_elapsed     | 338      |
|    total_timesteps  | 68799    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.56e-05 |
|    n_updates        | 7199     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.0303   |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 3820     |
|    fps              | 203      |
|    time_elapsed     | 338      |
|    total_timesteps  | 68863    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000134 |
|    n_updates        | 7215     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0398   |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 3824     |
|    fps              | 203      |
|    time_elapsed     | 338      |
|    total_timesteps  | 68939    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.5e-05  |
|    n_updates        | 7234     |
----------------------------------
Eval num_timesteps=69000, episode_reward=-0.17 +/- 0.29
Episode length: 57.50 +/- 24.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.5     |
|    mean_reward      | -0.17    |
| rollout/            |          |
|    exploration_rate | 0.923    |
| time/               |          |
|    total_timesteps  | 69000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000147 |
|    n_updates        | 7249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0393   |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 3828     |
|    fps              | 202      |
|    time_elapsed     | 340      |
|    total_timesteps  | 69025    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00704  |
|    n_updates        | 7256     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0394   |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 3832     |
|    fps              | 202      |
|    time_elapsed     | 340      |
|    total_timesteps  | 69092    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.24e-05 |
|    n_updates        | 7272     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0394   |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 3836     |
|    fps              | 202      |
|    time_elapsed     | 340      |
|    total_timesteps  | 69158    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 7289     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0387   |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 3840     |
|    fps              | 203      |
|    time_elapsed     | 340      |
|    total_timesteps  | 69239    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.1e-05  |
|    n_updates        | 7309     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.029    |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 3844     |
|    fps              | 203      |
|    time_elapsed     | 340      |
|    total_timesteps  | 69310    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.41e-05 |
|    n_updates        | 7327     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.029    |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 3848     |
|    fps              | 203      |
|    time_elapsed     | 341      |
|    total_timesteps  | 69382    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.56e-05 |
|    n_updates        | 7345     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00896  |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 3852     |
|    fps              | 203      |
|    time_elapsed     | 341      |
|    total_timesteps  | 69453    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00786  |
|    n_updates        | 7363     |
----------------------------------
Eval num_timesteps=69500, episode_reward=-0.04 +/- 0.43
Episode length: 64.80 +/- 4.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.8     |
|    mean_reward      | -0.038   |
| rollout/            |          |
|    exploration_rate | 0.921    |
| time/               |          |
|    total_timesteps  | 69500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.15e-05 |
|    n_updates        | 7374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00864  |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 3856     |
|    fps              | 202      |
|    time_elapsed     | 343      |
|    total_timesteps  | 69532    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.78e-05 |
|    n_updates        | 7382     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00864  |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 3860     |
|    fps              | 202      |
|    time_elapsed     | 343      |
|    total_timesteps  | 69602    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.7e-05  |
|    n_updates        | 7400     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0186   |
|    exploration_rate | 0.92     |
| time/               |          |
|    episodes         | 3864     |
|    fps              | 202      |
|    time_elapsed     | 343      |
|    total_timesteps  | 69686    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.46e-05 |
|    n_updates        | 7421     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0286   |
|    exploration_rate | 0.92     |
| time/               |          |
|    episodes         | 3868     |
|    fps              | 203      |
|    time_elapsed     | 343      |
|    total_timesteps  | 69753    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.8e-05  |
|    n_updates        | 7438     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0179   |
|    exploration_rate | 0.92     |
| time/               |          |
|    episodes         | 3872     |
|    fps              | 203      |
|    time_elapsed     | 343      |
|    total_timesteps  | 69828    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.29e-05 |
|    n_updates        | 7456     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0183   |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 3876     |
|    fps              | 203      |
|    time_elapsed     | 343      |
|    total_timesteps  | 69898    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00709  |
|    n_updates        | 7474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.029    |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 3880     |
|    fps              | 203      |
|    time_elapsed     | 343      |
|    total_timesteps  | 69964    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.84e-05 |
|    n_updates        | 7490     |
----------------------------------
Eval num_timesteps=70000, episode_reward=0.00 +/- 0.28
Episode length: 19.02 +/- 8.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 19       |
|    mean_reward      | 0.0049   |
| rollout/            |          |
|    exploration_rate | 0.919    |
| time/               |          |
|    total_timesteps  | 70000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3e-05    |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0288   |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 3884     |
|    fps              | 203      |
|    time_elapsed     | 344      |
|    total_timesteps  | 70033    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.01e-05 |
|    n_updates        | 7508     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0284   |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 3888     |
|    fps              | 203      |
|    time_elapsed     | 344      |
|    total_timesteps  | 70113    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.25e-05 |
|    n_updates        | 7528     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0286   |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 3892     |
|    fps              | 203      |
|    time_elapsed     | 344      |
|    total_timesteps  | 70184    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.55e-05 |
|    n_updates        | 7545     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0183   |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 3896     |
|    fps              | 203      |
|    time_elapsed     | 344      |
|    total_timesteps  | 70249    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.92e-05 |
|    n_updates        | 7562     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0186   |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 3900     |
|    fps              | 204      |
|    time_elapsed     | 344      |
|    total_timesteps  | 70322    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.57e-05 |
|    n_updates        | 7580     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0182   |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 3904     |
|    fps              | 204      |
|    time_elapsed     | 344      |
|    total_timesteps  | 70410    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.95e-05 |
|    n_updates        | 7602     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0386   |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 3908     |
|    fps              | 204      |
|    time_elapsed     | 344      |
|    total_timesteps  | 70476    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.12e-05 |
|    n_updates        | 7618     |
----------------------------------
Eval num_timesteps=70500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.917    |
| time/               |          |
|    total_timesteps  | 70500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.95e-05 |
|    n_updates        | 7624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0379   |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 3912     |
|    fps              | 203      |
|    time_elapsed     | 347      |
|    total_timesteps  | 70560    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.77e-05 |
|    n_updates        | 7639     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0281   |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 3916     |
|    fps              | 203      |
|    time_elapsed     | 347      |
|    total_timesteps  | 70624    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 7655     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0177   |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 3920     |
|    fps              | 203      |
|    time_elapsed     | 347      |
|    total_timesteps  | 70697    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.54e-05 |
|    n_updates        | 7674     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00224 |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 3924     |
|    fps              | 203      |
|    time_elapsed     | 347      |
|    total_timesteps  | 70771    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.41e-05 |
|    n_updates        | 7692     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00844  |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 3928     |
|    fps              | 203      |
|    time_elapsed     | 347      |
|    total_timesteps  | 70840    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.87e-05 |
|    n_updates        | 7709     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00792  |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 3932     |
|    fps              | 204      |
|    time_elapsed     | 347      |
|    total_timesteps  | 70920    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.52e-05 |
|    n_updates        | 7729     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00223 |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 3936     |
|    fps              | 204      |
|    time_elapsed     | 347      |
|    total_timesteps  | 70990    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000199 |
|    n_updates        | 7747     |
----------------------------------
Eval num_timesteps=71000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.915    |
| time/               |          |
|    total_timesteps  | 71000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.22e-05 |
|    n_updates        | 7749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00794  |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 3940     |
|    fps              | 203      |
|    time_elapsed     | 350      |
|    total_timesteps  | 71067    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00705  |
|    n_updates        | 7766     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0071   |
|    exploration_rate | 0.914    |
| time/               |          |
|    episodes         | 3944     |
|    fps              | 203      |
|    time_elapsed     | 350      |
|    total_timesteps  | 71159    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 7789     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00622  |
|    exploration_rate | 0.914    |
| time/               |          |
|    episodes         | 3948     |
|    fps              | 203      |
|    time_elapsed     | 350      |
|    total_timesteps  | 71253    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 7813     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0059   |
|    exploration_rate | 0.913    |
| time/               |          |
|    episodes         | 3952     |
|    fps              | 203      |
|    time_elapsed     | 350      |
|    total_timesteps  | 71332    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.06e-05 |
|    n_updates        | 7832     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00646  |
|    exploration_rate | 0.913    |
| time/               |          |
|    episodes         | 3956     |
|    fps              | 203      |
|    time_elapsed     | 350      |
|    total_timesteps  | 71397    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 7849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0157   |
|    exploration_rate | 0.913    |
| time/               |          |
|    episodes         | 3960     |
|    fps              | 204      |
|    time_elapsed     | 350      |
|    total_timesteps  | 71487    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00724  |
|    n_updates        | 7871     |
----------------------------------
Eval num_timesteps=71500, episode_reward=-0.29 +/- 0.03
Episode length: 73.62 +/- 7.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.6     |
|    mean_reward      | -0.294   |
| rollout/            |          |
|    exploration_rate | 0.913    |
| time/               |          |
|    total_timesteps  | 71500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.59e-05 |
|    n_updates        | 7874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0162   |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 3964     |
|    fps              | 202      |
|    time_elapsed     | 352      |
|    total_timesteps  | 71557    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.05e-05 |
|    n_updates        | 7889     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00624  |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 3968     |
|    fps              | 202      |
|    time_elapsed     | 352      |
|    total_timesteps  | 71623    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.02e-05 |
|    n_updates        | 7905     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00596  |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 3972     |
|    fps              | 203      |
|    time_elapsed     | 352      |
|    total_timesteps  | 71705    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 7926     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00596  |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 3976     |
|    fps              | 203      |
|    time_elapsed     | 353      |
|    total_timesteps  | 71775    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 7943     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00626  |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 3980     |
|    fps              | 203      |
|    time_elapsed     | 353      |
|    total_timesteps  | 71833    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00701  |
|    n_updates        | 7958     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00594  |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 3984     |
|    fps              | 203      |
|    time_elapsed     | 353      |
|    total_timesteps  | 71910    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.38e-05 |
|    n_updates        | 7977     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00614  |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 3988     |
|    fps              | 203      |
|    time_elapsed     | 353      |
|    total_timesteps  | 71985    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.7e-05  |
|    n_updates        | 7996     |
----------------------------------
Eval num_timesteps=72000, episode_reward=-0.29 +/- 0.06
Episode length: 71.52 +/- 13.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.5     |
|    mean_reward      | -0.286   |
| rollout/            |          |
|    exploration_rate | 0.911    |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000141 |
|    n_updates        | 7999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00602  |
|    exploration_rate | 0.91     |
| time/               |          |
|    episodes         | 3992     |
|    fps              | 202      |
|    time_elapsed     | 355      |
|    total_timesteps  | 72059    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 8014     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0042  |
|    exploration_rate | 0.91     |
| time/               |          |
|    episodes         | 3996     |
|    fps              | 202      |
|    time_elapsed     | 355      |
|    total_timesteps  | 72129    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.26e-05 |
|    n_updates        | 8032     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.00408 |
|    exploration_rate | 0.91     |
| time/               |          |
|    episodes         | 4000     |
|    fps              | 202      |
|    time_elapsed     | 355      |
|    total_timesteps  | 72199    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.67e-05 |
|    n_updates        | 8049     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00332 |
|    exploration_rate | 0.91     |
| time/               |          |
|    episodes         | 4004     |
|    fps              | 203      |
|    time_elapsed     | 355      |
|    total_timesteps  | 72268    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.41e-05 |
|    n_updates        | 8066     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0134  |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 4008     |
|    fps              | 203      |
|    time_elapsed     | 355      |
|    total_timesteps  | 72335    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.38e-05 |
|    n_updates        | 8083     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00294 |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 4012     |
|    fps              | 203      |
|    time_elapsed     | 355      |
|    total_timesteps  | 72409    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.55e-05 |
|    n_updates        | 8102     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00302 |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 4016     |
|    fps              | 203      |
|    time_elapsed     | 355      |
|    total_timesteps  | 72475    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 8118     |
----------------------------------
Eval num_timesteps=72500, episode_reward=0.17 +/- 0.45
Episode length: 27.16 +/- 14.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 27.2     |
|    mean_reward      | 0.173    |
| rollout/            |          |
|    exploration_rate | 0.909    |
| time/               |          |
|    total_timesteps  | 72500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.12e-05 |
|    n_updates        | 8124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00326 |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 4020     |
|    fps              | 203      |
|    time_elapsed     | 356      |
|    total_timesteps  | 72554    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00709  |
|    n_updates        | 8138     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00342 |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 4024     |
|    fps              | 203      |
|    time_elapsed     | 356      |
|    total_timesteps  | 72632    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00708  |
|    n_updates        | 8157     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0135  |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 4028     |
|    fps              | 203      |
|    time_elapsed     | 357      |
|    total_timesteps  | 72704    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.59e-05 |
|    n_updates        | 8175     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0132  |
|    exploration_rate | 0.907    |
| time/               |          |
|    episodes         | 4032     |
|    fps              | 203      |
|    time_elapsed     | 357      |
|    total_timesteps  | 72775    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.5e-05  |
|    n_updates        | 8193     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0142  |
|    exploration_rate | 0.907    |
| time/               |          |
|    episodes         | 4036     |
|    fps              | 204      |
|    time_elapsed     | 357      |
|    total_timesteps  | 72870    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.59e-05 |
|    n_updates        | 8217     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0141  |
|    exploration_rate | 0.907    |
| time/               |          |
|    episodes         | 4040     |
|    fps              | 204      |
|    time_elapsed     | 357      |
|    total_timesteps  | 72946    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.66e-05 |
|    n_updates        | 8236     |
----------------------------------
Eval num_timesteps=73000, episode_reward=-0.06 +/- 0.21
Episode length: 25.40 +/- 14.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 25.4     |
|    mean_reward      | -0.0606  |
| rollout/            |          |
|    exploration_rate | 0.906    |
| time/               |          |
|    total_timesteps  | 73000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.96e-05 |
|    n_updates        | 8249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0134  |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 4044     |
|    fps              | 203      |
|    time_elapsed     | 358      |
|    total_timesteps  | 73019    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00704  |
|    n_updates        | 8254     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.00368 |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 4048     |
|    fps              | 204      |
|    time_elapsed     | 358      |
|    total_timesteps  | 73120    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.83e-05 |
|    n_updates        | 8279     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00348 |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 4052     |
|    fps              | 204      |
|    time_elapsed     | 358      |
|    total_timesteps  | 73194    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.36e-05 |
|    n_updates        | 8298     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00664  |
|    exploration_rate | 0.905    |
| time/               |          |
|    episodes         | 4056     |
|    fps              | 204      |
|    time_elapsed     | 358      |
|    total_timesteps  | 73256    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.23e-05 |
|    n_updates        | 8313     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0078   |
|    exploration_rate | 0.905    |
| time/               |          |
|    episodes         | 4060     |
|    fps              | 204      |
|    time_elapsed     | 358      |
|    total_timesteps  | 73317    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.31e-05 |
|    n_updates        | 8329     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00774  |
|    exploration_rate | 0.905    |
| time/               |          |
|    episodes         | 4064     |
|    fps              | 204      |
|    time_elapsed     | 358      |
|    total_timesteps  | 73389    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 8347     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00774  |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 4068     |
|    fps              | 204      |
|    time_elapsed     | 358      |
|    total_timesteps  | 73455    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.01e-05 |
|    n_updates        | 8363     |
----------------------------------
Eval num_timesteps=73500, episode_reward=0.09 +/- 0.37
Episode length: 16.74 +/- 1.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.7     |
|    mean_reward      | 0.0941   |
| rollout/            |          |
|    exploration_rate | 0.904    |
| time/               |          |
|    total_timesteps  | 73500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.37e-05 |
|    n_updates        | 8374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00666  |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 4072     |
|    fps              | 204      |
|    time_elapsed     | 359      |
|    total_timesteps  | 73564    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000133 |
|    n_updates        | 8390     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00646  |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 4076     |
|    fps              | 204      |
|    time_elapsed     | 359      |
|    total_timesteps  | 73639    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.2e-05  |
|    n_updates        | 8409     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.00369 |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 4080     |
|    fps              | 205      |
|    time_elapsed     | 359      |
|    total_timesteps  | 73701    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000114 |
|    n_updates        | 8425     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00673  |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 4084     |
|    fps              | 205      |
|    time_elapsed     | 359      |
|    total_timesteps  | 73768    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.87e-05 |
|    n_updates        | 8441     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 4088     |
|    fps              | 205      |
|    time_elapsed     | 359      |
|    total_timesteps  | 73833    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.6e-05  |
|    n_updates        | 8458     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 4092     |
|    fps              | 205      |
|    time_elapsed     | 359      |
|    total_timesteps  | 73904    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.98e-05 |
|    n_updates        | 8475     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0169   |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 4096     |
|    fps              | 205      |
|    time_elapsed     | 359      |
|    total_timesteps  | 73982    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.25e-05 |
|    n_updates        | 8495     |
----------------------------------
Eval num_timesteps=74000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.902    |
| time/               |          |
|    total_timesteps  | 74000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00699  |
|    n_updates        | 8499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0164   |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 4100     |
|    fps              | 204      |
|    time_elapsed     | 362      |
|    total_timesteps  | 74065    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0148   |
|    n_updates        | 8516     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0162   |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 4104     |
|    fps              | 204      |
|    time_elapsed     | 362      |
|    total_timesteps  | 74139    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00696  |
|    n_updates        | 8534     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0164   |
|    exploration_rate | 0.901    |
| time/               |          |
|    episodes         | 4108     |
|    fps              | 204      |
|    time_elapsed     | 362      |
|    total_timesteps  | 74200    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.44e-05 |
|    n_updates        | 8549     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00594  |
|    exploration_rate | 0.901    |
| time/               |          |
|    episodes         | 4112     |
|    fps              | 205      |
|    time_elapsed     | 362      |
|    total_timesteps  | 74286    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00707  |
|    n_updates        | 8571     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.00546  |
|    exploration_rate | 0.901    |
| time/               |          |
|    episodes         | 4116     |
|    fps              | 205      |
|    time_elapsed     | 362      |
|    total_timesteps  | 74364    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.51e-05 |
|    n_updates        | 8590     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00582  |
|    exploration_rate | 0.9      |
| time/               |          |
|    episodes         | 4120     |
|    fps              | 205      |
|    time_elapsed     | 362      |
|    total_timesteps  | 74434    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.89e-05 |
|    n_updates        | 8608     |
----------------------------------
Eval num_timesteps=74500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.9      |
| time/               |          |
|    total_timesteps  | 74500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00693  |
|    n_updates        | 8624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0061   |
|    exploration_rate | 0.9      |
| time/               |          |
|    episodes         | 4124     |
|    fps              | 204      |
|    time_elapsed     | 364      |
|    total_timesteps  | 74505    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0142   |
|    n_updates        | 8626     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00626  |
|    exploration_rate | 0.9      |
| time/               |          |
|    episodes         | 4128     |
|    fps              | 204      |
|    time_elapsed     | 364      |
|    total_timesteps  | 74573    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 8643     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00634  |
|    exploration_rate | 0.899    |
| time/               |          |
|    episodes         | 4132     |
|    fps              | 204      |
|    time_elapsed     | 365      |
|    total_timesteps  | 74642    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.66e-05 |
|    n_updates        | 8660     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00738  |
|    exploration_rate | 0.899    |
| time/               |          |
|    episodes         | 4136     |
|    fps              | 204      |
|    time_elapsed     | 365      |
|    total_timesteps  | 74711    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.85e-05 |
|    n_updates        | 8677     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00254 |
|    exploration_rate | 0.899    |
| time/               |          |
|    episodes         | 4140     |
|    fps              | 204      |
|    time_elapsed     | 365      |
|    total_timesteps  | 74785    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.22e-05 |
|    n_updates        | 8696     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00258 |
|    exploration_rate | 0.898    |
| time/               |          |
|    episodes         | 4144     |
|    fps              | 204      |
|    time_elapsed     | 365      |
|    total_timesteps  | 74859    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0139   |
|    n_updates        | 8714     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0115  |
|    exploration_rate | 0.898    |
| time/               |          |
|    episodes         | 4148     |
|    fps              | 205      |
|    time_elapsed     | 365      |
|    total_timesteps  | 74934    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0138   |
|    n_updates        | 8733     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.898    |
| time/               |          |
|    total_timesteps  | 75000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.48e-05 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0113  |
|    exploration_rate | 0.898    |
| time/               |          |
|    episodes         | 4152     |
|    fps              | 203      |
|    time_elapsed     | 367      |
|    total_timesteps  | 75002    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.24e-05 |
|    n_updates        | 8750     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0111  |
|    exploration_rate | 0.898    |
| time/               |          |
|    episodes         | 4156     |
|    fps              | 204      |
|    time_elapsed     | 367      |
|    total_timesteps  | 75060    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.6e-05  |
|    n_updates        | 8764     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0217  |
|    exploration_rate | 0.897    |
| time/               |          |
|    episodes         | 4160     |
|    fps              | 204      |
|    time_elapsed     | 367      |
|    total_timesteps  | 75135    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.24e-05 |
|    n_updates        | 8783     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0314  |
|    exploration_rate | 0.897    |
| time/               |          |
|    episodes         | 4164     |
|    fps              | 204      |
|    time_elapsed     | 367      |
|    total_timesteps  | 75200    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.83e-05 |
|    n_updates        | 8799     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0327  |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 4168     |
|    fps              | 204      |
|    time_elapsed     | 368      |
|    total_timesteps  | 75297    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.46e-05 |
|    n_updates        | 8824     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.031   |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 4172     |
|    fps              | 204      |
|    time_elapsed     | 368      |
|    total_timesteps  | 75364    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.61e-05 |
|    n_updates        | 8840     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0206  |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 4176     |
|    fps              | 204      |
|    time_elapsed     | 368      |
|    total_timesteps  | 75429    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 8857     |
----------------------------------
Eval num_timesteps=75500, episode_reward=-0.07 +/- 0.21
Episode length: 26.70 +/- 15.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 26.7     |
|    mean_reward      | -0.0659  |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 75500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.18e-05 |
|    n_updates        | 8874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0215  |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 4180     |
|    fps              | 204      |
|    time_elapsed     | 369      |
|    total_timesteps  | 75514    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.88e-05 |
|    n_updates        | 8878     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0213  |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 4184     |
|    fps              | 204      |
|    time_elapsed     | 369      |
|    total_timesteps  | 75576    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.11e-05 |
|    n_updates        | 8893     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0315  |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 4188     |
|    fps              | 204      |
|    time_elapsed     | 369      |
|    total_timesteps  | 75646    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0071   |
|    n_updates        | 8911     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0317  |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 4192     |
|    fps              | 205      |
|    time_elapsed     | 369      |
|    total_timesteps  | 75722    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.14e-05 |
|    n_updates        | 8930     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0312  |
|    exploration_rate | 0.894    |
| time/               |          |
|    episodes         | 4196     |
|    fps              | 205      |
|    time_elapsed     | 369      |
|    total_timesteps  | 75788    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00718  |
|    n_updates        | 8946     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0304  |
|    exploration_rate | 0.894    |
| time/               |          |
|    episodes         | 4200     |
|    fps              | 205      |
|    time_elapsed     | 369      |
|    total_timesteps  | 75850    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.16e-05 |
|    n_updates        | 8962     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.0306  |
|    exploration_rate | 0.894    |
| time/               |          |
|    episodes         | 4204     |
|    fps              | 205      |
|    time_elapsed     | 369      |
|    total_timesteps  | 75930    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.18e-05 |
|    n_updates        | 8982     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0408  |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 4208     |
|    fps              | 205      |
|    time_elapsed     | 369      |
|    total_timesteps  | 75996    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 8998     |
----------------------------------
Eval num_timesteps=76000, episode_reward=-0.29 +/- 0.03
Episode length: 73.66 +/- 7.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.7     |
|    mean_reward      | -0.295   |
| rollout/            |          |
|    exploration_rate | 0.893    |
| time/               |          |
|    total_timesteps  | 76000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00708  |
|    n_updates        | 8999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.0299  |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 4212     |
|    fps              | 204      |
|    time_elapsed     | 371      |
|    total_timesteps  | 76058    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 9014     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.03    |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 4216     |
|    fps              | 204      |
|    time_elapsed     | 372      |
|    total_timesteps  | 76139    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.63e-05 |
|    n_updates        | 9034     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0303  |
|    exploration_rate | 0.892    |
| time/               |          |
|    episodes         | 4220     |
|    fps              | 204      |
|    time_elapsed     | 372      |
|    total_timesteps  | 76216    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.45e-05 |
|    n_updates        | 9053     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0301  |
|    exploration_rate | 0.892    |
| time/               |          |
|    episodes         | 4224     |
|    fps              | 204      |
|    time_elapsed     | 372      |
|    total_timesteps  | 76283    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 9070     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0301  |
|    exploration_rate | 0.892    |
| time/               |          |
|    episodes         | 4228     |
|    fps              | 205      |
|    time_elapsed     | 372      |
|    total_timesteps  | 76350    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.79e-05 |
|    n_updates        | 9087     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0308  |
|    exploration_rate | 0.891    |
| time/               |          |
|    episodes         | 4232     |
|    fps              | 205      |
|    time_elapsed     | 372      |
|    total_timesteps  | 76438    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.62e-05 |
|    n_updates        | 9109     |
----------------------------------
Eval num_timesteps=76500, episode_reward=0.10 +/- 0.37
Episode length: 16.40 +/- 1.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.4     |
|    mean_reward      | 0.0955   |
| rollout/            |          |
|    exploration_rate | 0.891    |
| time/               |          |
|    total_timesteps  | 76500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.014    |
|    n_updates        | 9124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0312  |
|    exploration_rate | 0.891    |
| time/               |          |
|    episodes         | 4236     |
|    fps              | 205      |
|    time_elapsed     | 372      |
|    total_timesteps  | 76517    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.37e-05 |
|    n_updates        | 9129     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0312  |
|    exploration_rate | 0.891    |
| time/               |          |
|    episodes         | 4240     |
|    fps              | 205      |
|    time_elapsed     | 373      |
|    total_timesteps  | 76592    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.68e-05 |
|    n_updates        | 9147     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0213  |
|    exploration_rate | 0.89     |
| time/               |          |
|    episodes         | 4244     |
|    fps              | 205      |
|    time_elapsed     | 373      |
|    total_timesteps  | 76667    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 9166     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0212  |
|    exploration_rate | 0.89     |
| time/               |          |
|    episodes         | 4248     |
|    fps              | 205      |
|    time_elapsed     | 373      |
|    total_timesteps  | 76741    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.71e-05 |
|    n_updates        | 9185     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0109  |
|    exploration_rate | 0.89     |
| time/               |          |
|    episodes         | 4252     |
|    fps              | 205      |
|    time_elapsed     | 373      |
|    total_timesteps  | 76800    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 9199     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.022   |
|    exploration_rate | 0.889    |
| time/               |          |
|    episodes         | 4256     |
|    fps              | 205      |
|    time_elapsed     | 373      |
|    total_timesteps  | 76885    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.18e-05 |
|    n_updates        | 9221     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0118  |
|    exploration_rate | 0.889    |
| time/               |          |
|    episodes         | 4260     |
|    fps              | 206      |
|    time_elapsed     | 373      |
|    total_timesteps  | 76955    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 9238     |
----------------------------------
Eval num_timesteps=77000, episode_reward=-0.25 +/- 0.21
Episode length: 71.68 +/- 10.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.7     |
|    mean_reward      | -0.247   |
| rollout/            |          |
|    exploration_rate | 0.889    |
| time/               |          |
|    total_timesteps  | 77000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.37e-05 |
|    n_updates        | 9249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0124  |
|    exploration_rate | 0.889    |
| time/               |          |
|    episodes         | 4264     |
|    fps              | 205      |
|    time_elapsed     | 375      |
|    total_timesteps  | 77036    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 9258     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00839  |
|    exploration_rate | 0.888    |
| time/               |          |
|    episodes         | 4268     |
|    fps              | 205      |
|    time_elapsed     | 375      |
|    total_timesteps  | 77114    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 9278     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00791  |
|    exploration_rate | 0.888    |
| time/               |          |
|    episodes         | 4272     |
|    fps              | 205      |
|    time_elapsed     | 375      |
|    total_timesteps  | 77193    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 9298     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00266 |
|    exploration_rate | 0.888    |
| time/               |          |
|    episodes         | 4276     |
|    fps              | 205      |
|    time_elapsed     | 375      |
|    total_timesteps  | 77272    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.51e-05 |
|    n_updates        | 9317     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0077   |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 4280     |
|    fps              | 205      |
|    time_elapsed     | 375      |
|    total_timesteps  | 77348    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.85e-05 |
|    n_updates        | 9336     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00283 |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 4284     |
|    fps              | 205      |
|    time_elapsed     | 376      |
|    total_timesteps  | 77423    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.65e-05 |
|    n_updates        | 9355     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00271 |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 4288     |
|    fps              | 206      |
|    time_elapsed     | 376      |
|    total_timesteps  | 77490    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.77e-05 |
|    n_updates        | 9372     |
----------------------------------
Eval num_timesteps=77500, episode_reward=-0.03 +/- 0.20
Episode length: 16.88 +/- 0.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | -0.0265  |
| rollout/            |          |
|    exploration_rate | 0.887    |
| time/               |          |
|    total_timesteps  | 77500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00708  |
|    n_updates        | 9374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00776  |
|    exploration_rate | 0.886    |
| time/               |          |
|    episodes         | 4292     |
|    fps              | 205      |
|    time_elapsed     | 376      |
|    total_timesteps  | 77554    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00702  |
|    n_updates        | 9388     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0176   |
|    exploration_rate | 0.886    |
| time/               |          |
|    episodes         | 4296     |
|    fps              | 206      |
|    time_elapsed     | 376      |
|    total_timesteps  | 77624    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00702  |
|    n_updates        | 9405     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 0.886    |
| time/               |          |
|    episodes         | 4300     |
|    fps              | 206      |
|    time_elapsed     | 376      |
|    total_timesteps  | 77698    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.88e-05 |
|    n_updates        | 9424     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0277   |
|    exploration_rate | 0.885    |
| time/               |          |
|    episodes         | 4304     |
|    fps              | 206      |
|    time_elapsed     | 376      |
|    total_timesteps  | 77764    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.72e-05 |
|    n_updates        | 9440     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0268   |
|    exploration_rate | 0.885    |
| time/               |          |
|    episodes         | 4308     |
|    fps              | 206      |
|    time_elapsed     | 376      |
|    total_timesteps  | 77852    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.014    |
|    n_updates        | 9462     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0165   |
|    exploration_rate | 0.885    |
| time/               |          |
|    episodes         | 4312     |
|    fps              | 206      |
|    time_elapsed     | 377      |
|    total_timesteps  | 77923    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.87e-05 |
|    n_updates        | 9480     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0368   |
|    exploration_rate | 0.884    |
| time/               |          |
|    episodes         | 4316     |
|    fps              | 206      |
|    time_elapsed     | 377      |
|    total_timesteps  | 77998    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00718  |
|    n_updates        | 9499     |
----------------------------------
Eval num_timesteps=78000, episode_reward=-0.03 +/- 0.20
Episode length: 16.86 +/- 0.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | -0.0264  |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 78000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0367   |
|    exploration_rate | 0.884    |
| time/               |          |
|    episodes         | 4320     |
|    fps              | 206      |
|    time_elapsed     | 377      |
|    total_timesteps  | 78075    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.5e-05  |
|    n_updates        | 9518     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0364   |
|    exploration_rate | 0.884    |
| time/               |          |
|    episodes         | 4324     |
|    fps              | 206      |
|    time_elapsed     | 377      |
|    total_timesteps  | 78151    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.69e-05 |
|    n_updates        | 9537     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.036    |
|    exploration_rate | 0.883    |
| time/               |          |
|    episodes         | 4328     |
|    fps              | 207      |
|    time_elapsed     | 377      |
|    total_timesteps  | 78228    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.17e-05 |
|    n_updates        | 9556     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0468   |
|    exploration_rate | 0.883    |
| time/               |          |
|    episodes         | 4332     |
|    fps              | 207      |
|    time_elapsed     | 377      |
|    total_timesteps  | 78295    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.18e-05 |
|    n_updates        | 9573     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0472   |
|    exploration_rate | 0.883    |
| time/               |          |
|    episodes         | 4336     |
|    fps              | 207      |
|    time_elapsed     | 378      |
|    total_timesteps  | 78365    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.02e-05 |
|    n_updates        | 9591     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0472   |
|    exploration_rate | 0.882    |
| time/               |          |
|    episodes         | 4340     |
|    fps              | 207      |
|    time_elapsed     | 378      |
|    total_timesteps  | 78441    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00721  |
|    n_updates        | 9610     |
----------------------------------
Eval num_timesteps=78500, episode_reward=-0.28 +/- 0.07
Episode length: 68.82 +/- 16.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.8     |
|    mean_reward      | -0.275   |
| rollout/            |          |
|    exploration_rate | 0.882    |
| time/               |          |
|    total_timesteps  | 78500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.98e-05 |
|    n_updates        | 9624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0376   |
|    exploration_rate | 0.882    |
| time/               |          |
|    episodes         | 4344     |
|    fps              | 206      |
|    time_elapsed     | 380      |
|    total_timesteps  | 78506    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.3e-05  |
|    n_updates        | 9626     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.048    |
|    exploration_rate | 0.882    |
| time/               |          |
|    episodes         | 4348     |
|    fps              | 206      |
|    time_elapsed     | 380      |
|    total_timesteps  | 78569    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00713  |
|    n_updates        | 9642     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0376   |
|    exploration_rate | 0.881    |
| time/               |          |
|    episodes         | 4352     |
|    fps              | 206      |
|    time_elapsed     | 380      |
|    total_timesteps  | 78639    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.15e-05 |
|    n_updates        | 9659     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0371   |
|    exploration_rate | 0.881    |
| time/               |          |
|    episodes         | 4356     |
|    fps              | 206      |
|    time_elapsed     | 380      |
|    total_timesteps  | 78735    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.76e-05 |
|    n_updates        | 9683     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0268   |
|    exploration_rate | 0.881    |
| time/               |          |
|    episodes         | 4360     |
|    fps              | 207      |
|    time_elapsed     | 380      |
|    total_timesteps  | 78812    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.21e-05 |
|    n_updates        | 9702     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0274   |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 4364     |
|    fps              | 207      |
|    time_elapsed     | 380      |
|    total_timesteps  | 78879    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.56e-05 |
|    n_updates        | 9719     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00771  |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 4368     |
|    fps              | 207      |
|    time_elapsed     | 380      |
|    total_timesteps  | 78948    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0139   |
|    n_updates        | 9736     |
----------------------------------
Eval num_timesteps=79000, episode_reward=-0.22 +/- 0.22
Episode length: 65.22 +/- 17.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.2     |
|    mean_reward      | -0.221   |
| rollout/            |          |
|    exploration_rate | 0.88     |
| time/               |          |
|    total_timesteps  | 79000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.41e-05 |
|    n_updates        | 9749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0185   |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 4372     |
|    fps              | 206      |
|    time_elapsed     | 382      |
|    total_timesteps  | 79009    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 9752     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0188   |
|    exploration_rate | 0.879    |
| time/               |          |
|    episodes         | 4376     |
|    fps              | 206      |
|    time_elapsed     | 383      |
|    total_timesteps  | 79080    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00717  |
|    n_updates        | 9769     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0185   |
|    exploration_rate | 0.879    |
| time/               |          |
|    episodes         | 4380     |
|    fps              | 206      |
|    time_elapsed     | 383      |
|    total_timesteps  | 79162    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00705  |
|    n_updates        | 9790     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0291   |
|    exploration_rate | 0.879    |
| time/               |          |
|    episodes         | 4384     |
|    fps              | 206      |
|    time_elapsed     | 383      |
|    total_timesteps  | 79222    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 9805     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0291   |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 4388     |
|    fps              | 206      |
|    time_elapsed     | 383      |
|    total_timesteps  | 79291    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 9822     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0188   |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 4392     |
|    fps              | 207      |
|    time_elapsed     | 383      |
|    total_timesteps  | 79362    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.36e-05 |
|    n_updates        | 9840     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00862  |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 4396     |
|    fps              | 207      |
|    time_elapsed     | 383      |
|    total_timesteps  | 79436    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 9858     |
----------------------------------
Eval num_timesteps=79500, episode_reward=0.04 +/- 0.30
Episode length: 16.42 +/- 1.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.4     |
|    mean_reward      | 0.0354   |
| rollout/            |          |
|    exploration_rate | 0.877    |
| time/               |          |
|    total_timesteps  | 79500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00699  |
|    n_updates        | 9874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0087   |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 4400     |
|    fps              | 207      |
|    time_elapsed     | 383      |
|    total_timesteps  | 79508    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0139   |
|    n_updates        | 9876     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0083   |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 4404     |
|    fps              | 207      |
|    time_elapsed     | 384      |
|    total_timesteps  | 79584    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.45e-05 |
|    n_updates        | 9895     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00866  |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 4408     |
|    fps              | 207      |
|    time_elapsed     | 384      |
|    total_timesteps  | 79663    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.81e-05 |
|    n_updates        | 9915     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00838  |
|    exploration_rate | 0.876    |
| time/               |          |
|    episodes         | 4412     |
|    fps              | 207      |
|    time_elapsed     | 384      |
|    total_timesteps  | 79741    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.29e-05 |
|    n_updates        | 9935     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0115  |
|    exploration_rate | 0.876    |
| time/               |          |
|    episodes         | 4416     |
|    fps              | 207      |
|    time_elapsed     | 384      |
|    total_timesteps  | 79812    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.55e-05 |
|    n_updates        | 9952     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00126 |
|    exploration_rate | 0.876    |
| time/               |          |
|    episodes         | 4420     |
|    fps              | 207      |
|    time_elapsed     | 384      |
|    total_timesteps  | 79883    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 9970     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00106 |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 4424     |
|    fps              | 208      |
|    time_elapsed     | 384      |
|    total_timesteps  | 79954    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.15e-05 |
|    n_updates        | 9988     |
----------------------------------
Eval num_timesteps=80000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.875    |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.27e-05 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00912  |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 4428     |
|    fps              | 206      |
|    time_elapsed     | 386      |
|    total_timesteps  | 80027    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.17e-05 |
|    n_updates        | 10006    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00125 |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 4432     |
|    fps              | 207      |
|    time_elapsed     | 386      |
|    total_timesteps  | 80103    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 10025    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00879  |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 4436     |
|    fps              | 207      |
|    time_elapsed     | 386      |
|    total_timesteps  | 80172    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.9e-05  |
|    n_updates        | 10042    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00847  |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 4440     |
|    fps              | 207      |
|    time_elapsed     | 387      |
|    total_timesteps  | 80256    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.68e-05 |
|    n_updates        | 10063    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00739  |
|    exploration_rate | 0.873    |
| time/               |          |
|    episodes         | 4444     |
|    fps              | 207      |
|    time_elapsed     | 387      |
|    total_timesteps  | 80348    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 10086    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00289 |
|    exploration_rate | 0.873    |
| time/               |          |
|    episodes         | 4448     |
|    fps              | 207      |
|    time_elapsed     | 387      |
|    total_timesteps  | 80418    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.61e-05 |
|    n_updates        | 10104    |
----------------------------------
Eval num_timesteps=80500, episode_reward=-0.26 +/- 0.17
Episode length: 69.50 +/- 15.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.5     |
|    mean_reward      | -0.258   |
| rollout/            |          |
|    exploration_rate | 0.873    |
| time/               |          |
|    total_timesteps  | 80500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.88e-05 |
|    n_updates        | 10124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.00365 |
|    exploration_rate | 0.873    |
| time/               |          |
|    episodes         | 4452     |
|    fps              | 206      |
|    time_elapsed     | 389      |
|    total_timesteps  | 80507    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00793  |
|    n_updates        | 10126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00241 |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 4456     |
|    fps              | 206      |
|    time_elapsed     | 389      |
|    total_timesteps  | 80572    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.93e-05 |
|    n_updates        | 10142    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00809  |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 4460     |
|    fps              | 206      |
|    time_elapsed     | 389      |
|    total_timesteps  | 80637    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000173 |
|    n_updates        | 10159    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00777  |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 4464     |
|    fps              | 207      |
|    time_elapsed     | 389      |
|    total_timesteps  | 80712    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.14e-05 |
|    n_updates        | 10177    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00773  |
|    exploration_rate | 0.871    |
| time/               |          |
|    episodes         | 4468     |
|    fps              | 207      |
|    time_elapsed     | 389      |
|    total_timesteps  | 80782    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00701  |
|    n_updates        | 10195    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00261 |
|    exploration_rate | 0.871    |
| time/               |          |
|    episodes         | 4472     |
|    fps              | 207      |
|    time_elapsed     | 389      |
|    total_timesteps  | 80851    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.73e-05 |
|    n_updates        | 10212    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00237 |
|    exploration_rate | 0.871    |
| time/               |          |
|    episodes         | 4476     |
|    fps              | 207      |
|    time_elapsed     | 389      |
|    total_timesteps  | 80916    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.32e-05 |
|    n_updates        | 10228    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0121  |
|    exploration_rate | 0.87     |
| time/               |          |
|    episodes         | 4480     |
|    fps              | 207      |
|    time_elapsed     | 389      |
|    total_timesteps  | 80992    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 10247    |
----------------------------------
Eval num_timesteps=81000, episode_reward=-0.29 +/- 0.03
Episode length: 73.30 +/- 6.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.3     |
|    mean_reward      | -0.293   |
| rollout/            |          |
|    exploration_rate | 0.87     |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 10249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.023   |
|    exploration_rate | 0.87     |
| time/               |          |
|    episodes         | 4484     |
|    fps              | 206      |
|    time_elapsed     | 392      |
|    total_timesteps  | 81074    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00722  |
|    n_updates        | 10268    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 0.87     |
| time/               |          |
|    episodes         | 4488     |
|    fps              | 206      |
|    time_elapsed     | 392      |
|    total_timesteps  | 81136    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.79e-05 |
|    n_updates        | 10283    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00246 |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 4492     |
|    fps              | 206      |
|    time_elapsed     | 392      |
|    total_timesteps  | 81200    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.14e-05 |
|    n_updates        | 10299    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00222 |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 4496     |
|    fps              | 206      |
|    time_elapsed     | 392      |
|    total_timesteps  | 81268    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 10316    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0027  |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 4500     |
|    fps              | 207      |
|    time_elapsed     | 392      |
|    total_timesteps  | 81352    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00785  |
|    n_updates        | 10337    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 0.868    |
| time/               |          |
|    episodes         | 4504     |
|    fps              | 207      |
|    time_elapsed     | 392      |
|    total_timesteps  | 81428    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 10356    |
----------------------------------
Eval num_timesteps=81500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.868    |
| time/               |          |
|    total_timesteps  | 81500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.53e-05 |
|    n_updates        | 10374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0125  |
|    exploration_rate | 0.868    |
| time/               |          |
|    episodes         | 4508     |
|    fps              | 206      |
|    time_elapsed     | 395      |
|    total_timesteps  | 81502    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.92e-05 |
|    n_updates        | 10375    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0123  |
|    exploration_rate | 0.868    |
| time/               |          |
|    episodes         | 4512     |
|    fps              | 206      |
|    time_elapsed     | 395      |
|    total_timesteps  | 81575    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00778  |
|    n_updates        | 10393    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 4516     |
|    fps              | 206      |
|    time_elapsed     | 395      |
|    total_timesteps  | 81655    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.49e-05 |
|    n_updates        | 10413    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0235  |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 4520     |
|    fps              | 206      |
|    time_elapsed     | 395      |
|    total_timesteps  | 81746    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 10436    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0133  |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 4524     |
|    fps              | 206      |
|    time_elapsed     | 395      |
|    total_timesteps  | 81813    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 10453    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0131  |
|    exploration_rate | 0.866    |
| time/               |          |
|    episodes         | 4528     |
|    fps              | 206      |
|    time_elapsed     | 395      |
|    total_timesteps  | 81882    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.84e-05 |
|    n_updates        | 10470    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 0.866    |
| time/               |          |
|    episodes         | 4532     |
|    fps              | 207      |
|    time_elapsed     | 395      |
|    total_timesteps  | 81948    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.99e-05 |
|    n_updates        | 10486    |
----------------------------------
Eval num_timesteps=82000, episode_reward=-0.13 +/- 0.30
Episode length: 52.48 +/- 22.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.5     |
|    mean_reward      | -0.129   |
| rollout/            |          |
|    exploration_rate | 0.866    |
| time/               |          |
|    total_timesteps  | 82000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.46e-05 |
|    n_updates        | 10499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0232  |
|    exploration_rate | 0.866    |
| time/               |          |
|    episodes         | 4536     |
|    fps              | 206      |
|    time_elapsed     | 397      |
|    total_timesteps  | 82029    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.58e-05 |
|    n_updates        | 10507    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 0.865    |
| time/               |          |
|    episodes         | 4540     |
|    fps              | 206      |
|    time_elapsed     | 397      |
|    total_timesteps  | 82098    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 10524    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0216  |
|    exploration_rate | 0.865    |
| time/               |          |
|    episodes         | 4544     |
|    fps              | 206      |
|    time_elapsed     | 397      |
|    total_timesteps  | 82164    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 10540    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.012   |
|    exploration_rate | 0.864    |
| time/               |          |
|    episodes         | 4548     |
|    fps              | 206      |
|    time_elapsed     | 397      |
|    total_timesteps  | 82244    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.25e-05 |
|    n_updates        | 10560    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0113  |
|    exploration_rate | 0.864    |
| time/               |          |
|    episodes         | 4552     |
|    fps              | 206      |
|    time_elapsed     | 397      |
|    total_timesteps  | 82316    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.93e-05 |
|    n_updates        | 10578    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0118  |
|    exploration_rate | 0.864    |
| time/               |          |
|    episodes         | 4556     |
|    fps              | 207      |
|    time_elapsed     | 397      |
|    total_timesteps  | 82393    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00786  |
|    n_updates        | 10598    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0224  |
|    exploration_rate | 0.863    |
| time/               |          |
|    episodes         | 4560     |
|    fps              | 207      |
|    time_elapsed     | 397      |
|    total_timesteps  | 82474    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.59e-05 |
|    n_updates        | 10618    |
----------------------------------
Eval num_timesteps=82500, episode_reward=-0.30 +/- 0.03
Episode length: 73.88 +/- 7.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.9     |
|    mean_reward      | -0.296   |
| rollout/            |          |
|    exploration_rate | 0.863    |
| time/               |          |
|    total_timesteps  | 82500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.42e-05 |
|    n_updates        | 10624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.022   |
|    exploration_rate | 0.863    |
| time/               |          |
|    episodes         | 4564     |
|    fps              | 206      |
|    time_elapsed     | 400      |
|    total_timesteps  | 82538    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.39e-05 |
|    n_updates        | 10634    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0122  |
|    exploration_rate | 0.863    |
| time/               |          |
|    episodes         | 4568     |
|    fps              | 206      |
|    time_elapsed     | 400      |
|    total_timesteps  | 82614    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 10653    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0124  |
|    exploration_rate | 0.862    |
| time/               |          |
|    episodes         | 4572     |
|    fps              | 206      |
|    time_elapsed     | 400      |
|    total_timesteps  | 82688    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.46e-05 |
|    n_updates        | 10671    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00271 |
|    exploration_rate | 0.862    |
| time/               |          |
|    episodes         | 4576     |
|    fps              | 206      |
|    time_elapsed     | 400      |
|    total_timesteps  | 82761    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 10690    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00291 |
|    exploration_rate | 0.862    |
| time/               |          |
|    episodes         | 4580     |
|    fps              | 206      |
|    time_elapsed     | 400      |
|    total_timesteps  | 82842    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00773  |
|    n_updates        | 10710    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00275 |
|    exploration_rate | 0.861    |
| time/               |          |
|    episodes         | 4584     |
|    fps              | 206      |
|    time_elapsed     | 400      |
|    total_timesteps  | 82920    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.38e-05 |
|    n_updates        | 10729    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0132  |
|    exploration_rate | 0.861    |
| time/               |          |
|    episodes         | 4588     |
|    fps              | 207      |
|    time_elapsed     | 400      |
|    total_timesteps  | 82992    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000206 |
|    n_updates        | 10747    |
----------------------------------
Eval num_timesteps=83000, episode_reward=0.11 +/- 0.42
Episode length: 26.72 +/- 16.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 26.7     |
|    mean_reward      | 0.114    |
| rollout/            |          |
|    exploration_rate | 0.861    |
| time/               |          |
|    total_timesteps  | 83000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.15e-05 |
|    n_updates        | 10749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0235  |
|    exploration_rate | 0.861    |
| time/               |          |
|    episodes         | 4592     |
|    fps              | 206      |
|    time_elapsed     | 401      |
|    total_timesteps  | 83064    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.5e-05  |
|    n_updates        | 10765    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0237  |
|    exploration_rate | 0.86     |
| time/               |          |
|    episodes         | 4596     |
|    fps              | 206      |
|    time_elapsed     | 401      |
|    total_timesteps  | 83138    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.58e-05 |
|    n_updates        | 10784    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00345 |
|    exploration_rate | 0.86     |
| time/               |          |
|    episodes         | 4600     |
|    fps              | 207      |
|    time_elapsed     | 401      |
|    total_timesteps  | 83216    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.31e-05 |
|    n_updates        | 10803    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00341 |
|    exploration_rate | 0.859    |
| time/               |          |
|    episodes         | 4604     |
|    fps              | 207      |
|    time_elapsed     | 401      |
|    total_timesteps  | 83291    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.37e-05 |
|    n_updates        | 10822    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00349 |
|    exploration_rate | 0.859    |
| time/               |          |
|    episodes         | 4608     |
|    fps              | 207      |
|    time_elapsed     | 402      |
|    total_timesteps  | 83367    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.78e-05 |
|    n_updates        | 10841    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.00353 |
|    exploration_rate | 0.859    |
| time/               |          |
|    episodes         | 4612     |
|    fps              | 207      |
|    time_elapsed     | 402      |
|    total_timesteps  | 83441    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00791  |
|    n_updates        | 10860    |
----------------------------------
Eval num_timesteps=83500, episode_reward=-0.07 +/- 0.30
Episode length: 42.90 +/- 20.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | -0.0707  |
| rollout/            |          |
|    exploration_rate | 0.858    |
| time/               |          |
|    total_timesteps  | 83500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.99e-05 |
|    n_updates        | 10874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00337 |
|    exploration_rate | 0.858    |
| time/               |          |
|    episodes         | 4616     |
|    fps              | 206      |
|    time_elapsed     | 403      |
|    total_timesteps  | 83517    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.75e-05 |
|    n_updates        | 10879    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00245 |
|    exploration_rate | 0.858    |
| time/               |          |
|    episodes         | 4620     |
|    fps              | 207      |
|    time_elapsed     | 403      |
|    total_timesteps  | 83585    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.17e-05 |
|    n_updates        | 10896    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0132  |
|    exploration_rate | 0.858    |
| time/               |          |
|    episodes         | 4624     |
|    fps              | 207      |
|    time_elapsed     | 403      |
|    total_timesteps  | 83670    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.39e-06 |
|    n_updates        | 10917    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0133  |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 4628     |
|    fps              | 207      |
|    time_elapsed     | 403      |
|    total_timesteps  | 83742    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.67e-05 |
|    n_updates        | 10935    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0135  |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 4632     |
|    fps              | 207      |
|    time_elapsed     | 403      |
|    total_timesteps  | 83813    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.78e-05 |
|    n_updates        | 10953    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0133  |
|    exploration_rate | 0.856    |
| time/               |          |
|    episodes         | 4636     |
|    fps              | 207      |
|    time_elapsed     | 403      |
|    total_timesteps  | 83887    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00699  |
|    n_updates        | 10971    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00318 |
|    exploration_rate | 0.856    |
| time/               |          |
|    episodes         | 4640     |
|    fps              | 207      |
|    time_elapsed     | 403      |
|    total_timesteps  | 83954    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 10988    |
----------------------------------
Eval num_timesteps=84000, episode_reward=0.01 +/- 0.31
Episode length: 23.46 +/- 11.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23.5     |
|    mean_reward      | 0.00722  |
| rollout/            |          |
|    exploration_rate | 0.856    |
| time/               |          |
|    total_timesteps  | 84000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0148   |
|    n_updates        | 10999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0033  |
|    exploration_rate | 0.856    |
| time/               |          |
|    episodes         | 4644     |
|    fps              | 207      |
|    time_elapsed     | 404      |
|    total_timesteps  | 84023    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00692  |
|    n_updates        | 11005    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0128  |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 4648     |
|    fps              | 207      |
|    time_elapsed     | 404      |
|    total_timesteps  | 84091    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.92e-05 |
|    n_updates        | 11022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0126  |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 4652     |
|    fps              | 207      |
|    time_elapsed     | 404      |
|    total_timesteps  | 84156    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 11038    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0018  |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 4656     |
|    fps              | 207      |
|    time_elapsed     | 404      |
|    total_timesteps  | 84214    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000149 |
|    n_updates        | 11053    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00909  |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 4660     |
|    fps              | 208      |
|    time_elapsed     | 405      |
|    total_timesteps  | 84273    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 11068    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00901  |
|    exploration_rate | 0.854    |
| time/               |          |
|    episodes         | 4664     |
|    fps              | 208      |
|    time_elapsed     | 405      |
|    total_timesteps  | 84339    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.23e-05 |
|    n_updates        | 11084    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00105 |
|    exploration_rate | 0.854    |
| time/               |          |
|    episodes         | 4668     |
|    fps              | 208      |
|    time_elapsed     | 405      |
|    total_timesteps  | 84416    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 11103    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00117 |
|    exploration_rate | 0.854    |
| time/               |          |
|    episodes         | 4672     |
|    fps              | 208      |
|    time_elapsed     | 405      |
|    total_timesteps  | 84493    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.22e-05 |
|    n_updates        | 11123    |
----------------------------------
Eval num_timesteps=84500, episode_reward=-0.09 +/- 0.40
Episode length: 56.46 +/- 22.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.5     |
|    mean_reward      | -0.0853  |
| rollout/            |          |
|    exploration_rate | 0.853    |
| time/               |          |
|    total_timesteps  | 84500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.32e-05 |
|    n_updates        | 11124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0113  |
|    exploration_rate | 0.853    |
| time/               |          |
|    episodes         | 4676     |
|    fps              | 207      |
|    time_elapsed     | 407      |
|    total_timesteps  | 84570    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 11142    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00924  |
|    exploration_rate | 0.853    |
| time/               |          |
|    episodes         | 4680     |
|    fps              | 207      |
|    time_elapsed     | 407      |
|    total_timesteps  | 84637    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.17e-05 |
|    n_updates        | 11159    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00928  |
|    exploration_rate | 0.852    |
| time/               |          |
|    episodes         | 4684     |
|    fps              | 207      |
|    time_elapsed     | 407      |
|    total_timesteps  | 84714    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.84e-05 |
|    n_updates        | 11178    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.00952  |
|    exploration_rate | 0.852    |
| time/               |          |
|    episodes         | 4688     |
|    fps              | 208      |
|    time_elapsed     | 407      |
|    total_timesteps  | 84780    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.77e-05 |
|    n_updates        | 11194    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0098   |
|    exploration_rate | 0.852    |
| time/               |          |
|    episodes         | 4692     |
|    fps              | 208      |
|    time_elapsed     | 407      |
|    total_timesteps  | 84845    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.08e-05 |
|    n_updates        | 11211    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.00992  |
|    exploration_rate | 0.851    |
| time/               |          |
|    episodes         | 4696     |
|    fps              | 208      |
|    time_elapsed     | 407      |
|    total_timesteps  | 84916    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.58e-05 |
|    n_updates        | 11228    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.00974 |
|    exploration_rate | 0.851    |
| time/               |          |
|    episodes         | 4700     |
|    fps              | 208      |
|    time_elapsed     | 407      |
|    total_timesteps  | 84985    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 11246    |
----------------------------------
Eval num_timesteps=85000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.851    |
| time/               |          |
|    total_timesteps  | 85000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.00978 |
|    exploration_rate | 0.851    |
| time/               |          |
|    episodes         | 4704     |
|    fps              | 207      |
|    time_elapsed     | 410      |
|    total_timesteps  | 85061    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.74e-05 |
|    n_updates        | 11265    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.00954 |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 4708     |
|    fps              | 207      |
|    time_elapsed     | 410      |
|    total_timesteps  | 85131    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.15e-05 |
|    n_updates        | 11282    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.6     |
|    ep_rew_mean      | -0.00942 |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 4712     |
|    fps              | 207      |
|    time_elapsed     | 410      |
|    total_timesteps  | 85202    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 11300    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | -0.00962 |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 4716     |
|    fps              | 207      |
|    time_elapsed     | 410      |
|    total_timesteps  | 85283    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.04e-05 |
|    n_updates        | 11320    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.00018 |
|    exploration_rate | 0.849    |
| time/               |          |
|    episodes         | 4720     |
|    fps              | 208      |
|    time_elapsed     | 410      |
|    total_timesteps  | 85365    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000165 |
|    n_updates        | 11341    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 2e-05    |
|    exploration_rate | 0.849    |
| time/               |          |
|    episodes         | 4724     |
|    fps              | 208      |
|    time_elapsed     | 410      |
|    total_timesteps  | 85445    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.03e-05 |
|    n_updates        | 11361    |
----------------------------------
Eval num_timesteps=85500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.849    |
| time/               |          |
|    total_timesteps  | 85500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.29e-05 |
|    n_updates        | 11374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0103  |
|    exploration_rate | 0.848    |
| time/               |          |
|    episodes         | 4728     |
|    fps              | 207      |
|    time_elapsed     | 412      |
|    total_timesteps  | 85525    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00777  |
|    n_updates        | 11381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0103  |
|    exploration_rate | 0.848    |
| time/               |          |
|    episodes         | 4732     |
|    fps              | 207      |
|    time_elapsed     | 412      |
|    total_timesteps  | 85595    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00695  |
|    n_updates        | 11398    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0101  |
|    exploration_rate | 0.848    |
| time/               |          |
|    episodes         | 4736     |
|    fps              | 207      |
|    time_elapsed     | 413      |
|    total_timesteps  | 85666    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.5e-05  |
|    n_updates        | 11416    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0203  |
|    exploration_rate | 0.847    |
| time/               |          |
|    episodes         | 4740     |
|    fps              | 207      |
|    time_elapsed     | 413      |
|    total_timesteps  | 85737    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000162 |
|    n_updates        | 11434    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | -0.0101  |
|    exploration_rate | 0.847    |
| time/               |          |
|    episodes         | 4744     |
|    fps              | 207      |
|    time_elapsed     | 413      |
|    total_timesteps  | 85801    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 11450    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.7     |
|    ep_rew_mean      | 0.00023  |
|    exploration_rate | 0.847    |
| time/               |          |
|    episodes         | 4748     |
|    fps              | 207      |
|    time_elapsed     | 413      |
|    total_timesteps  | 85861    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.86e-05 |
|    n_updates        | 11465    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00065 |
|    exploration_rate | 0.846    |
| time/               |          |
|    episodes         | 4752     |
|    fps              | 207      |
|    time_elapsed     | 413      |
|    total_timesteps  | 85948    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.77e-05 |
|    n_updates        | 11486    |
----------------------------------
Eval num_timesteps=86000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.846    |
| time/               |          |
|    total_timesteps  | 86000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 11499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.011   |
|    exploration_rate | 0.846    |
| time/               |          |
|    episodes         | 4756     |
|    fps              | 206      |
|    time_elapsed     | 415      |
|    total_timesteps  | 86014    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000147 |
|    n_updates        | 11503    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0218  |
|    exploration_rate | 0.846    |
| time/               |          |
|    episodes         | 4760     |
|    fps              | 207      |
|    time_elapsed     | 415      |
|    total_timesteps  | 86094    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000161 |
|    n_updates        | 11523    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0221  |
|    exploration_rate | 0.845    |
| time/               |          |
|    episodes         | 4764     |
|    fps              | 207      |
|    time_elapsed     | 415      |
|    total_timesteps  | 86166    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 11541    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0117  |
|    exploration_rate | 0.845    |
| time/               |          |
|    episodes         | 4768     |
|    fps              | 207      |
|    time_elapsed     | 415      |
|    total_timesteps  | 86233    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00712  |
|    n_updates        | 11558    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0121  |
|    exploration_rate | 0.844    |
| time/               |          |
|    episodes         | 4772     |
|    fps              | 207      |
|    time_elapsed     | 416      |
|    total_timesteps  | 86320    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00783  |
|    n_updates        | 11579    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0121  |
|    exploration_rate | 0.844    |
| time/               |          |
|    episodes         | 4776     |
|    fps              | 207      |
|    time_elapsed     | 416      |
|    total_timesteps  | 86399    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00731  |
|    n_updates        | 11599    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0328  |
|    exploration_rate | 0.844    |
| time/               |          |
|    episodes         | 4780     |
|    fps              | 207      |
|    time_elapsed     | 416      |
|    total_timesteps  | 86481    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 11620    |
----------------------------------
Eval num_timesteps=86500, episode_reward=-0.09 +/- 0.28
Episode length: 37.86 +/- 26.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.9     |
|    mean_reward      | -0.0907  |
| rollout/            |          |
|    exploration_rate | 0.843    |
| time/               |          |
|    total_timesteps  | 86500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.43e-05 |
|    n_updates        | 11624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0324  |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 4784     |
|    fps              | 207      |
|    time_elapsed     | 417      |
|    total_timesteps  | 86549    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.09e-05 |
|    n_updates        | 11637    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0332  |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 4788     |
|    fps              | 207      |
|    time_elapsed     | 417      |
|    total_timesteps  | 86635    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.19e-05 |
|    n_updates        | 11658    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0339  |
|    exploration_rate | 0.842    |
| time/               |          |
|    episodes         | 4792     |
|    fps              | 207      |
|    time_elapsed     | 417      |
|    total_timesteps  | 86717    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.23e-05 |
|    n_updates        | 11679    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.034   |
|    exploration_rate | 0.842    |
| time/               |          |
|    episodes         | 4796     |
|    fps              | 207      |
|    time_elapsed     | 417      |
|    total_timesteps  | 86790    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.16e-05 |
|    n_updates        | 11697    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0237  |
|    exploration_rate | 0.842    |
| time/               |          |
|    episodes         | 4800     |
|    fps              | 207      |
|    time_elapsed     | 417      |
|    total_timesteps  | 86853    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 11713    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0233  |
|    exploration_rate | 0.841    |
| time/               |          |
|    episodes         | 4804     |
|    fps              | 208      |
|    time_elapsed     | 417      |
|    total_timesteps  | 86918    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.69e-05 |
|    n_updates        | 11729    |
----------------------------------
Eval num_timesteps=87000, episode_reward=-0.25 +/- 0.09
Episode length: 62.46 +/- 23.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.5     |
|    mean_reward      | -0.25    |
| rollout/            |          |
|    exploration_rate | 0.841    |
| time/               |          |
|    total_timesteps  | 87000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00789  |
|    n_updates        | 11749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.014   |
|    exploration_rate | 0.841    |
| time/               |          |
|    episodes         | 4808     |
|    fps              | 207      |
|    time_elapsed     | 419      |
|    total_timesteps  | 87006    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00771  |
|    n_updates        | 11751    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.014   |
|    exploration_rate | 0.841    |
| time/               |          |
|    episodes         | 4812     |
|    fps              | 207      |
|    time_elapsed     | 419      |
|    total_timesteps  | 87078    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.77e-05 |
|    n_updates        | 11769    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.014   |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 4816     |
|    fps              | 207      |
|    time_elapsed     | 420      |
|    total_timesteps  | 87159    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.85e-05 |
|    n_updates        | 11789    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0239  |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 4820     |
|    fps              | 207      |
|    time_elapsed     | 420      |
|    total_timesteps  | 87238    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.51e-05 |
|    n_updates        | 11809    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0234  |
|    exploration_rate | 0.839    |
| time/               |          |
|    episodes         | 4824     |
|    fps              | 207      |
|    time_elapsed     | 420      |
|    total_timesteps  | 87306    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00695  |
|    n_updates        | 11826    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.023   |
|    exploration_rate | 0.839    |
| time/               |          |
|    episodes         | 4828     |
|    fps              | 207      |
|    time_elapsed     | 420      |
|    total_timesteps  | 87375    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.84e-05 |
|    n_updates        | 11843    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 0.839    |
| time/               |          |
|    episodes         | 4832     |
|    fps              | 208      |
|    time_elapsed     | 420      |
|    total_timesteps  | 87436    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.55e-05 |
|    n_updates        | 11858    |
----------------------------------
Eval num_timesteps=87500, episode_reward=-0.28 +/- 0.17
Episode length: 74.06 +/- 6.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.1     |
|    mean_reward      | -0.276   |
| rollout/            |          |
|    exploration_rate | 0.838    |
| time/               |          |
|    total_timesteps  | 87500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.28e-05 |
|    n_updates        | 11874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 4836     |
|    fps              | 206      |
|    time_elapsed     | 422      |
|    total_timesteps  | 87507    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 11876    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0126  |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 4840     |
|    fps              | 207      |
|    time_elapsed     | 422      |
|    total_timesteps  | 87577    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.15e-05 |
|    n_updates        | 11894    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 4844     |
|    fps              | 207      |
|    time_elapsed     | 422      |
|    total_timesteps  | 87643    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.16e-05 |
|    n_updates        | 11910    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0332  |
|    exploration_rate | 0.837    |
| time/               |          |
|    episodes         | 4848     |
|    fps              | 207      |
|    time_elapsed     | 422      |
|    total_timesteps  | 87716    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.57e-05 |
|    n_updates        | 11928    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0329  |
|    exploration_rate | 0.837    |
| time/               |          |
|    episodes         | 4852     |
|    fps              | 207      |
|    time_elapsed     | 423      |
|    total_timesteps  | 87797    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.86e-05 |
|    n_updates        | 11949    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0332  |
|    exploration_rate | 0.837    |
| time/               |          |
|    episodes         | 4856     |
|    fps              | 207      |
|    time_elapsed     | 423      |
|    total_timesteps  | 87871    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000114 |
|    n_updates        | 11967    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.033   |
|    exploration_rate | 0.836    |
| time/               |          |
|    episodes         | 4860     |
|    fps              | 207      |
|    time_elapsed     | 423      |
|    total_timesteps  | 87944    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 11985    |
----------------------------------
Eval num_timesteps=88000, episode_reward=0.07 +/- 0.36
Episode length: 17.62 +/- 8.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.6     |
|    mean_reward      | 0.0705   |
| rollout/            |          |
|    exploration_rate | 0.836    |
| time/               |          |
|    total_timesteps  | 88000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00795  |
|    n_updates        | 11999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0329  |
|    exploration_rate | 0.836    |
| time/               |          |
|    episodes         | 4864     |
|    fps              | 207      |
|    time_elapsed     | 423      |
|    total_timesteps  | 88014    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000138 |
|    n_updates        | 12003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0332  |
|    exploration_rate | 0.835    |
| time/               |          |
|    episodes         | 4868     |
|    fps              | 207      |
|    time_elapsed     | 423      |
|    total_timesteps  | 88089    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.49e-05 |
|    n_updates        | 12022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0324  |
|    exploration_rate | 0.835    |
| time/               |          |
|    episodes         | 4872     |
|    fps              | 207      |
|    time_elapsed     | 423      |
|    total_timesteps  | 88157    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.32e-05 |
|    n_updates        | 12039    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0321  |
|    exploration_rate | 0.835    |
| time/               |          |
|    episodes         | 4876     |
|    fps              | 208      |
|    time_elapsed     | 424      |
|    total_timesteps  | 88227    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0157   |
|    n_updates        | 12056    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0318  |
|    exploration_rate | 0.834    |
| time/               |          |
|    episodes         | 4880     |
|    fps              | 208      |
|    time_elapsed     | 424      |
|    total_timesteps  | 88302    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 12075    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0318  |
|    exploration_rate | 0.834    |
| time/               |          |
|    episodes         | 4884     |
|    fps              | 208      |
|    time_elapsed     | 424      |
|    total_timesteps  | 88370    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.76e-05 |
|    n_updates        | 12092    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0315  |
|    exploration_rate | 0.834    |
| time/               |          |
|    episodes         | 4888     |
|    fps              | 208      |
|    time_elapsed     | 424      |
|    total_timesteps  | 88450    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.91e-05 |
|    n_updates        | 12112    |
----------------------------------
Eval num_timesteps=88500, episode_reward=-0.03 +/- 0.20
Episode length: 17.10 +/- 0.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.1     |
|    mean_reward      | -0.0274  |
| rollout/            |          |
|    exploration_rate | 0.833    |
| time/               |          |
|    total_timesteps  | 88500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.21e-05 |
|    n_updates        | 12124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0309  |
|    exploration_rate | 0.833    |
| time/               |          |
|    episodes         | 4892     |
|    fps              | 208      |
|    time_elapsed     | 424      |
|    total_timesteps  | 88517    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00702  |
|    n_updates        | 12129    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0308  |
|    exploration_rate | 0.833    |
| time/               |          |
|    episodes         | 4896     |
|    fps              | 208      |
|    time_elapsed     | 424      |
|    total_timesteps  | 88587    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 12146    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0419  |
|    exploration_rate | 0.832    |
| time/               |          |
|    episodes         | 4900     |
|    fps              | 208      |
|    time_elapsed     | 425      |
|    total_timesteps  | 88676    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.23e-05 |
|    n_updates        | 12168    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0424  |
|    exploration_rate | 0.832    |
| time/               |          |
|    episodes         | 4904     |
|    fps              | 208      |
|    time_elapsed     | 425      |
|    total_timesteps  | 88755    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.22e-05 |
|    n_updates        | 12188    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0518  |
|    exploration_rate | 0.832    |
| time/               |          |
|    episodes         | 4908     |
|    fps              | 208      |
|    time_elapsed     | 425      |
|    total_timesteps  | 88826    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 12206    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0415  |
|    exploration_rate | 0.831    |
| time/               |          |
|    episodes         | 4912     |
|    fps              | 209      |
|    time_elapsed     | 425      |
|    total_timesteps  | 88893    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.16e-05 |
|    n_updates        | 12223    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0414  |
|    exploration_rate | 0.831    |
| time/               |          |
|    episodes         | 4916     |
|    fps              | 209      |
|    time_elapsed     | 425      |
|    total_timesteps  | 88970    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.92e-05 |
|    n_updates        | 12242    |
----------------------------------
Eval num_timesteps=89000, episode_reward=-0.26 +/- 0.18
Episode length: 69.02 +/- 17.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69       |
|    mean_reward      | -0.256   |
| rollout/            |          |
|    exploration_rate | 0.831    |
| time/               |          |
|    total_timesteps  | 89000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.6e-05  |
|    n_updates        | 12249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0414  |
|    exploration_rate | 0.83     |
| time/               |          |
|    episodes         | 4920     |
|    fps              | 208      |
|    time_elapsed     | 427      |
|    total_timesteps  | 89050    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.94e-05 |
|    n_updates        | 12262    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0424  |
|    exploration_rate | 0.83     |
| time/               |          |
|    episodes         | 4924     |
|    fps              | 208      |
|    time_elapsed     | 427      |
|    total_timesteps  | 89142    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 12285    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0325  |
|    exploration_rate | 0.83     |
| time/               |          |
|    episodes         | 4928     |
|    fps              | 208      |
|    time_elapsed     | 427      |
|    total_timesteps  | 89215    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 12303    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.033   |
|    exploration_rate | 0.829    |
| time/               |          |
|    episodes         | 4932     |
|    fps              | 208      |
|    time_elapsed     | 427      |
|    total_timesteps  | 89287    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.55e-05 |
|    n_updates        | 12321    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.033   |
|    exploration_rate | 0.829    |
| time/               |          |
|    episodes         | 4936     |
|    fps              | 208      |
|    time_elapsed     | 427      |
|    total_timesteps  | 89360    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.87e-05 |
|    n_updates        | 12339    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0435  |
|    exploration_rate | 0.828    |
| time/               |          |
|    episodes         | 4940     |
|    fps              | 209      |
|    time_elapsed     | 427      |
|    total_timesteps  | 89441    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00691  |
|    n_updates        | 12360    |
----------------------------------
Eval num_timesteps=89500, episode_reward=-0.12 +/- 0.06
Episode length: 29.88 +/- 14.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 29.9     |
|    mean_reward      | -0.119   |
| rollout/            |          |
|    exploration_rate | 0.828    |
| time/               |          |
|    total_timesteps  | 89500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00698  |
|    n_updates        | 12374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0445  |
|    exploration_rate | 0.828    |
| time/               |          |
|    episodes         | 4944     |
|    fps              | 208      |
|    time_elapsed     | 429      |
|    total_timesteps  | 89533    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.28e-05 |
|    n_updates        | 12383    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0443  |
|    exploration_rate | 0.828    |
| time/               |          |
|    episodes         | 4948     |
|    fps              | 208      |
|    time_elapsed     | 429      |
|    total_timesteps  | 89600    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 12399    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0439  |
|    exploration_rate | 0.827    |
| time/               |          |
|    episodes         | 4952     |
|    fps              | 208      |
|    time_elapsed     | 429      |
|    total_timesteps  | 89670    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 12417    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0438  |
|    exploration_rate | 0.827    |
| time/               |          |
|    episodes         | 4956     |
|    fps              | 209      |
|    time_elapsed     | 429      |
|    total_timesteps  | 89743    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.38e-05 |
|    n_updates        | 12435    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0435  |
|    exploration_rate | 0.826    |
| time/               |          |
|    episodes         | 4960     |
|    fps              | 209      |
|    time_elapsed     | 429      |
|    total_timesteps  | 89809    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.23e-05 |
|    n_updates        | 12452    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0435  |
|    exploration_rate | 0.826    |
| time/               |          |
|    episodes         | 4964     |
|    fps              | 209      |
|    time_elapsed     | 429      |
|    total_timesteps  | 89877    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.16e-05 |
|    n_updates        | 12469    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0534  |
|    exploration_rate | 0.826    |
| time/               |          |
|    episodes         | 4968     |
|    fps              | 209      |
|    time_elapsed     | 429      |
|    total_timesteps  | 89951    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000196 |
|    n_updates        | 12487    |
----------------------------------
Eval num_timesteps=90000, episode_reward=-0.26 +/- 0.08
Episode length: 65.92 +/- 20.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.9     |
|    mean_reward      | -0.264   |
| rollout/            |          |
|    exploration_rate | 0.825    |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.69e-05 |
|    n_updates        | 12499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0543  |
|    exploration_rate | 0.825    |
| time/               |          |
|    episodes         | 4972     |
|    fps              | 208      |
|    time_elapsed     | 431      |
|    total_timesteps  | 90041    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 12510    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0444  |
|    exploration_rate | 0.825    |
| time/               |          |
|    episodes         | 4976     |
|    fps              | 208      |
|    time_elapsed     | 431      |
|    total_timesteps  | 90113    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.87e-06 |
|    n_updates        | 12528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0441  |
|    exploration_rate | 0.825    |
| time/               |          |
|    episodes         | 4980     |
|    fps              | 208      |
|    time_elapsed     | 431      |
|    total_timesteps  | 90182    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.67e-05 |
|    n_updates        | 12545    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0443  |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 4984     |
|    fps              | 209      |
|    time_elapsed     | 431      |
|    total_timesteps  | 90255    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.91e-05 |
|    n_updates        | 12563    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0441  |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 4988     |
|    fps              | 209      |
|    time_elapsed     | 431      |
|    total_timesteps  | 90330    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 12582    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0447  |
|    exploration_rate | 0.823    |
| time/               |          |
|    episodes         | 4992     |
|    fps              | 209      |
|    time_elapsed     | 431      |
|    total_timesteps  | 90412    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0071   |
|    n_updates        | 12602    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.0349  |
|    exploration_rate | 0.823    |
| time/               |          |
|    episodes         | 4996     |
|    fps              | 209      |
|    time_elapsed     | 431      |
|    total_timesteps  | 90487    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00702  |
|    n_updates        | 12621    |
----------------------------------
Eval num_timesteps=90500, episode_reward=-0.13 +/- 0.19
Episode length: 36.68 +/- 27.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.7     |
|    mean_reward      | -0.126   |
| rollout/            |          |
|    exploration_rate | 0.823    |
| time/               |          |
|    total_timesteps  | 90500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.94e-05 |
|    n_updates        | 12624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0342  |
|    exploration_rate | 0.823    |
| time/               |          |
|    episodes         | 5000     |
|    fps              | 209      |
|    time_elapsed     | 433      |
|    total_timesteps  | 90558    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.98e-05 |
|    n_updates        | 12639    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0342  |
|    exploration_rate | 0.822    |
| time/               |          |
|    episodes         | 5004     |
|    fps              | 209      |
|    time_elapsed     | 433      |
|    total_timesteps  | 90638    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 12659    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0145  |
|    exploration_rate | 0.822    |
| time/               |          |
|    episodes         | 5008     |
|    fps              | 209      |
|    time_elapsed     | 433      |
|    total_timesteps  | 90716    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.05e-05 |
|    n_updates        | 12678    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0247  |
|    exploration_rate | 0.821    |
| time/               |          |
|    episodes         | 5012     |
|    fps              | 209      |
|    time_elapsed     | 433      |
|    total_timesteps  | 90788    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.14e-05 |
|    n_updates        | 12696    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0245  |
|    exploration_rate | 0.821    |
| time/               |          |
|    episodes         | 5016     |
|    fps              | 209      |
|    time_elapsed     | 433      |
|    total_timesteps  | 90860    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.49e-05 |
|    n_updates        | 12714    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0239  |
|    exploration_rate | 0.821    |
| time/               |          |
|    episodes         | 5020     |
|    fps              | 209      |
|    time_elapsed     | 433      |
|    total_timesteps  | 90925    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.08e-05 |
|    n_updates        | 12731    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.023   |
|    exploration_rate | 0.82     |
| time/               |          |
|    episodes         | 5024     |
|    fps              | 209      |
|    time_elapsed     | 433      |
|    total_timesteps  | 90994    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.83e-05 |
|    n_updates        | 12748    |
----------------------------------
Eval num_timesteps=91000, episode_reward=-0.10 +/- 0.35
Episode length: 56.06 +/- 21.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.1     |
|    mean_reward      | -0.104   |
| rollout/            |          |
|    exploration_rate | 0.82     |
| time/               |          |
|    total_timesteps  | 91000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.42e-05 |
|    n_updates        | 12749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0331  |
|    exploration_rate | 0.82     |
| time/               |          |
|    episodes         | 5028     |
|    fps              | 209      |
|    time_elapsed     | 435      |
|    total_timesteps  | 91069    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 12767    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0334  |
|    exploration_rate | 0.819    |
| time/               |          |
|    episodes         | 5032     |
|    fps              | 209      |
|    time_elapsed     | 435      |
|    total_timesteps  | 91148    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 12786    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0233  |
|    exploration_rate | 0.819    |
| time/               |          |
|    episodes         | 5036     |
|    fps              | 209      |
|    time_elapsed     | 435      |
|    total_timesteps  | 91220    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 12804    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0232  |
|    exploration_rate | 0.819    |
| time/               |          |
|    episodes         | 5040     |
|    fps              | 209      |
|    time_elapsed     | 435      |
|    total_timesteps  | 91299    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.11e-05 |
|    n_updates        | 12824    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0125  |
|    exploration_rate | 0.818    |
| time/               |          |
|    episodes         | 5044     |
|    fps              | 209      |
|    time_elapsed     | 435      |
|    total_timesteps  | 91373    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.65e-05 |
|    n_updates        | 12843    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00302 |
|    exploration_rate | 0.818    |
| time/               |          |
|    episodes         | 5048     |
|    fps              | 209      |
|    time_elapsed     | 435      |
|    total_timesteps  | 91453    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.58e-05 |
|    n_updates        | 12863    |
----------------------------------
Eval num_timesteps=91500, episode_reward=-0.18 +/- 0.31
Episode length: 68.86 +/- 12.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.9     |
|    mean_reward      | -0.175   |
| rollout/            |          |
|    exploration_rate | 0.818    |
| time/               |          |
|    total_timesteps  | 91500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.47e-05 |
|    n_updates        | 12874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00685  |
|    exploration_rate | 0.817    |
| time/               |          |
|    episodes         | 5052     |
|    fps              | 208      |
|    time_elapsed     | 438      |
|    total_timesteps  | 91526    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00779  |
|    n_updates        | 12881    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.00697  |
|    exploration_rate | 0.817    |
| time/               |          |
|    episodes         | 5056     |
|    fps              | 209      |
|    time_elapsed     | 438      |
|    total_timesteps  | 91596    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 12898    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00677  |
|    exploration_rate | 0.817    |
| time/               |          |
|    episodes         | 5060     |
|    fps              | 209      |
|    time_elapsed     | 438      |
|    total_timesteps  | 91667    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.13e-05 |
|    n_updates        | 12916    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00633  |
|    exploration_rate | 0.816    |
| time/               |          |
|    episodes         | 5064     |
|    fps              | 209      |
|    time_elapsed     | 438      |
|    total_timesteps  | 91746    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.06e-05 |
|    n_updates        | 12936    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00645  |
|    exploration_rate | 0.816    |
| time/               |          |
|    episodes         | 5068     |
|    fps              | 209      |
|    time_elapsed     | 438      |
|    total_timesteps  | 91817    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00691  |
|    n_updates        | 12954    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 5072     |
|    fps              | 209      |
|    time_elapsed     | 438      |
|    total_timesteps  | 91892    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 12972    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0174   |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 5076     |
|    fps              | 209      |
|    time_elapsed     | 438      |
|    total_timesteps  | 91955    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000163 |
|    n_updates        | 12988    |
----------------------------------
Eval num_timesteps=92000, episode_reward=-0.08 +/- 0.25
Episode length: 35.70 +/- 21.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.7     |
|    mean_reward      | -0.082   |
| rollout/            |          |
|    exploration_rate | 0.815    |
| time/               |          |
|    total_timesteps  | 92000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 12999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 5080     |
|    fps              | 209      |
|    time_elapsed     | 439      |
|    total_timesteps  | 92031    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.36e-05 |
|    n_updates        | 13007    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0174   |
|    exploration_rate | 0.814    |
| time/               |          |
|    episodes         | 5084     |
|    fps              | 209      |
|    time_elapsed     | 439      |
|    total_timesteps  | 92098    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 13024    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0173   |
|    exploration_rate | 0.814    |
| time/               |          |
|    episodes         | 5088     |
|    fps              | 209      |
|    time_elapsed     | 439      |
|    total_timesteps  | 92175    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.76e-05 |
|    n_updates        | 13043    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0169   |
|    exploration_rate | 0.813    |
| time/               |          |
|    episodes         | 5092     |
|    fps              | 209      |
|    time_elapsed     | 440      |
|    total_timesteps  | 92267    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.02e-05 |
|    n_updates        | 13066    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0275   |
|    exploration_rate | 0.813    |
| time/               |          |
|    episodes         | 5096     |
|    fps              | 209      |
|    time_elapsed     | 440      |
|    total_timesteps  | 92326    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.77e-05 |
|    n_updates        | 13081    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0373   |
|    exploration_rate | 0.813    |
| time/               |          |
|    episodes         | 5100     |
|    fps              | 209      |
|    time_elapsed     | 440      |
|    total_timesteps  | 92403    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.29e-05 |
|    n_updates        | 13100    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0477   |
|    exploration_rate | 0.812    |
| time/               |          |
|    episodes         | 5104     |
|    fps              | 210      |
|    time_elapsed     | 440      |
|    total_timesteps  | 92475    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.23e-05 |
|    n_updates        | 13118    |
----------------------------------
Eval num_timesteps=92500, episode_reward=-0.28 +/- 0.03
Episode length: 70.86 +/- 8.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.9     |
|    mean_reward      | -0.283   |
| rollout/            |          |
|    exploration_rate | 0.812    |
| time/               |          |
|    total_timesteps  | 92500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00695  |
|    n_updates        | 13124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0276   |
|    exploration_rate | 0.812    |
| time/               |          |
|    episodes         | 5108     |
|    fps              | 209      |
|    time_elapsed     | 442      |
|    total_timesteps  | 92552    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.24e-05 |
|    n_updates        | 13137    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0272   |
|    exploration_rate | 0.812    |
| time/               |          |
|    episodes         | 5112     |
|    fps              | 209      |
|    time_elapsed     | 442      |
|    total_timesteps  | 92636    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00708  |
|    n_updates        | 13158    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0273   |
|    exploration_rate | 0.811    |
| time/               |          |
|    episodes         | 5116     |
|    fps              | 209      |
|    time_elapsed     | 442      |
|    total_timesteps  | 92705    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.52e-05 |
|    n_updates        | 13176    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0369   |
|    exploration_rate | 0.811    |
| time/               |          |
|    episodes         | 5120     |
|    fps              | 209      |
|    time_elapsed     | 442      |
|    total_timesteps  | 92780    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.91e-05 |
|    n_updates        | 13194    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0366   |
|    exploration_rate | 0.81     |
| time/               |          |
|    episodes         | 5124     |
|    fps              | 209      |
|    time_elapsed     | 442      |
|    total_timesteps  | 92857    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00789  |
|    n_updates        | 13214    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0363   |
|    exploration_rate | 0.81     |
| time/               |          |
|    episodes         | 5128     |
|    fps              | 209      |
|    time_elapsed     | 442      |
|    total_timesteps  | 92938    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 13234    |
----------------------------------
Eval num_timesteps=93000, episode_reward=-0.08 +/- 0.15
Episode length: 24.98 +/- 10.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 25       |
|    mean_reward      | -0.0789  |
| rollout/            |          |
|    exploration_rate | 0.81     |
| time/               |          |
|    total_timesteps  | 93000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 13249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0362   |
|    exploration_rate | 0.809    |
| time/               |          |
|    episodes         | 5132     |
|    fps              | 209      |
|    time_elapsed     | 443      |
|    total_timesteps  | 93020    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00811  |
|    n_updates        | 13254    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0262   |
|    exploration_rate | 0.809    |
| time/               |          |
|    episodes         | 5136     |
|    fps              | 209      |
|    time_elapsed     | 443      |
|    total_timesteps  | 93093    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 13273    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0364   |
|    exploration_rate | 0.809    |
| time/               |          |
|    episodes         | 5140     |
|    fps              | 209      |
|    time_elapsed     | 443      |
|    total_timesteps  | 93166    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 13291    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0258   |
|    exploration_rate | 0.808    |
| time/               |          |
|    episodes         | 5144     |
|    fps              | 210      |
|    time_elapsed     | 444      |
|    total_timesteps  | 93256    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.13e-05 |
|    n_updates        | 13313    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0157   |
|    exploration_rate | 0.808    |
| time/               |          |
|    episodes         | 5148     |
|    fps              | 210      |
|    time_elapsed     | 444      |
|    total_timesteps  | 93337    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.98e-05 |
|    n_updates        | 13334    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00609  |
|    exploration_rate | 0.807    |
| time/               |          |
|    episodes         | 5152     |
|    fps              | 210      |
|    time_elapsed     | 444      |
|    total_timesteps  | 93401    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00704  |
|    n_updates        | 13350    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00633  |
|    exploration_rate | 0.807    |
| time/               |          |
|    episodes         | 5156     |
|    fps              | 210      |
|    time_elapsed     | 444      |
|    total_timesteps  | 93465    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0078   |
|    n_updates        | 13366    |
----------------------------------
Eval num_timesteps=93500, episode_reward=-0.30 +/- 0.02
Episode length: 73.98 +/- 5.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74       |
|    mean_reward      | -0.296   |
| rollout/            |          |
|    exploration_rate | 0.807    |
| time/               |          |
|    total_timesteps  | 93500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 13374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00653  |
|    exploration_rate | 0.807    |
| time/               |          |
|    episodes         | 5160     |
|    fps              | 209      |
|    time_elapsed     | 446      |
|    total_timesteps  | 93531    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7e-05    |
|    n_updates        | 13382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00673  |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 5164     |
|    fps              | 209      |
|    time_elapsed     | 446      |
|    total_timesteps  | 93605    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00684  |
|    n_updates        | 13401    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00685  |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 5168     |
|    fps              | 209      |
|    time_elapsed     | 446      |
|    total_timesteps  | 93673    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 13418    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00304 |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 5172     |
|    fps              | 209      |
|    time_elapsed     | 446      |
|    total_timesteps  | 93745    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.94e-05 |
|    n_updates        | 13436    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0139  |
|    exploration_rate | 0.805    |
| time/               |          |
|    episodes         | 5176     |
|    fps              | 209      |
|    time_elapsed     | 446      |
|    total_timesteps  | 93829    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.35e-05 |
|    n_updates        | 13457    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0138  |
|    exploration_rate | 0.805    |
| time/               |          |
|    episodes         | 5180     |
|    fps              | 210      |
|    time_elapsed     | 447      |
|    total_timesteps  | 93903    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.13e-05 |
|    n_updates        | 13475    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.00432 |
|    exploration_rate | 0.804    |
| time/               |          |
|    episodes         | 5184     |
|    fps              | 210      |
|    time_elapsed     | 447      |
|    total_timesteps  | 93983    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.64e-05 |
|    n_updates        | 13495    |
----------------------------------
Eval num_timesteps=94000, episode_reward=-0.03 +/- 0.28
Episode length: 27.82 +/- 15.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 27.8     |
|    mean_reward      | -0.0303  |
| rollout/            |          |
|    exploration_rate | 0.804    |
| time/               |          |
|    total_timesteps  | 94000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 13499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.00396 |
|    exploration_rate | 0.804    |
| time/               |          |
|    episodes         | 5188     |
|    fps              | 209      |
|    time_elapsed     | 448      |
|    total_timesteps  | 94051    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.48e-05 |
|    n_updates        | 13512    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.003   |
|    exploration_rate | 0.803    |
| time/               |          |
|    episodes         | 5192     |
|    fps              | 209      |
|    time_elapsed     | 448      |
|    total_timesteps  | 94119    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.36e-05 |
|    n_updates        | 13529    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0236  |
|    exploration_rate | 0.803    |
| time/               |          |
|    episodes         | 5196     |
|    fps              | 210      |
|    time_elapsed     | 448      |
|    total_timesteps  | 94193    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 13548    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0239  |
|    exploration_rate | 0.803    |
| time/               |          |
|    episodes         | 5200     |
|    fps              | 210      |
|    time_elapsed     | 448      |
|    total_timesteps  | 94278    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.55e-05 |
|    n_updates        | 13569    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0143  |
|    exploration_rate | 0.802    |
| time/               |          |
|    episodes         | 5204     |
|    fps              | 210      |
|    time_elapsed     | 448      |
|    total_timesteps  | 94360    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 13589    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.014   |
|    exploration_rate | 0.802    |
| time/               |          |
|    episodes         | 5208     |
|    fps              | 210      |
|    time_elapsed     | 448      |
|    total_timesteps  | 94429    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 13607    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0135  |
|    exploration_rate | 0.801    |
| time/               |          |
|    episodes         | 5212     |
|    fps              | 210      |
|    time_elapsed     | 448      |
|    total_timesteps  | 94499    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.08e-05 |
|    n_updates        | 13624    |
----------------------------------
Eval num_timesteps=94500, episode_reward=-0.30 +/- 0.02
Episode length: 74.36 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | -0.297   |
| time/              |          |
|    total_timesteps | 94500    |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00345 |
|    exploration_rate | 0.801    |
| time/               |          |
|    episodes         | 5216     |
|    fps              | 209      |
|    time_elapsed     | 451      |
|    total_timesteps  | 94568    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 13641    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.00324 |
|    exploration_rate | 0.801    |
| time/               |          |
|    episodes         | 5220     |
|    fps              | 209      |
|    time_elapsed     | 451      |
|    total_timesteps  | 94638    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.52e-05 |
|    n_updates        | 13659    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00276 |
|    exploration_rate | 0.8      |
| time/               |          |
|    episodes         | 5224     |
|    fps              | 209      |
|    time_elapsed     | 451      |
|    total_timesteps  | 94703    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 13675    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00224 |
|    exploration_rate | 0.8      |
| time/               |          |
|    episodes         | 5228     |
|    fps              | 210      |
|    time_elapsed     | 451      |
|    total_timesteps  | 94771    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.69e-05 |
|    n_updates        | 13692    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00256 |
|    exploration_rate | 0.799    |
| time/               |          |
|    episodes         | 5232     |
|    fps              | 210      |
|    time_elapsed     | 451      |
|    total_timesteps  | 94861    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000165 |
|    n_updates        | 13715    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00774  |
|    exploration_rate | 0.799    |
| time/               |          |
|    episodes         | 5236     |
|    fps              | 210      |
|    time_elapsed     | 451      |
|    total_timesteps  | 94927    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.52e-05 |
|    n_updates        | 13731    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00811  |
|    exploration_rate | 0.799    |
| time/               |          |
|    episodes         | 5240     |
|    fps              | 210      |
|    time_elapsed     | 451      |
|    total_timesteps  | 94991    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00783  |
|    n_updates        | 13747    |
----------------------------------
Eval num_timesteps=95000, episode_reward=0.05 +/- 0.36
Episode length: 23.00 +/- 12.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23       |
|    mean_reward      | 0.0491   |
| rollout/            |          |
|    exploration_rate | 0.799    |
| time/               |          |
|    total_timesteps  | 95000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00697  |
|    n_updates        | 13749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00803  |
|    exploration_rate | 0.798    |
| time/               |          |
|    episodes         | 5244     |
|    fps              | 210      |
|    time_elapsed     | 452      |
|    total_timesteps  | 95083    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.98e-05 |
|    n_updates        | 13770    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00831  |
|    exploration_rate | 0.798    |
| time/               |          |
|    episodes         | 5248     |
|    fps              | 210      |
|    time_elapsed     | 452      |
|    total_timesteps  | 95157    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.86e-05 |
|    n_updates        | 13789    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00827  |
|    exploration_rate | 0.797    |
| time/               |          |
|    episodes         | 5252     |
|    fps              | 210      |
|    time_elapsed     | 452      |
|    total_timesteps  | 95222    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.22e-05 |
|    n_updates        | 13805    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00791  |
|    exploration_rate | 0.797    |
| time/               |          |
|    episodes         | 5256     |
|    fps              | 210      |
|    time_elapsed     | 452      |
|    total_timesteps  | 95295    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00768  |
|    n_updates        | 13823    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00767  |
|    exploration_rate | 0.797    |
| time/               |          |
|    episodes         | 5260     |
|    fps              | 210      |
|    time_elapsed     | 452      |
|    total_timesteps  | 95367    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.98e-05 |
|    n_updates        | 13841    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00639  |
|    exploration_rate | 0.796    |
| time/               |          |
|    episodes         | 5264     |
|    fps              | 210      |
|    time_elapsed     | 452      |
|    total_timesteps  | 95473    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 13868    |
----------------------------------
Eval num_timesteps=95500, episode_reward=0.00 +/- 0.27
Episode length: 19.06 +/- 3.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 19.1     |
|    mean_reward      | 0.00472  |
| rollout/            |          |
|    exploration_rate | 0.796    |
| time/               |          |
|    total_timesteps  | 95500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.03e-05 |
|    n_updates        | 13874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.00539  |
|    exploration_rate | 0.796    |
| time/               |          |
|    episodes         | 5268     |
|    fps              | 210      |
|    time_elapsed     | 453      |
|    total_timesteps  | 95566    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.89e-05 |
|    n_updates        | 13891    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.00551  |
|    exploration_rate | 0.795    |
| time/               |          |
|    episodes         | 5272     |
|    fps              | 210      |
|    time_elapsed     | 453      |
|    total_timesteps  | 95635    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.79e-05 |
|    n_updates        | 13908    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00611  |
|    exploration_rate | 0.795    |
| time/               |          |
|    episodes         | 5276     |
|    fps              | 211      |
|    time_elapsed     | 453      |
|    total_timesteps  | 95704    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.57e-05 |
|    n_updates        | 13925    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00615  |
|    exploration_rate | 0.794    |
| time/               |          |
|    episodes         | 5280     |
|    fps              | 211      |
|    time_elapsed     | 453      |
|    total_timesteps  | 95777    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00797  |
|    n_updates        | 13944    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.00403 |
|    exploration_rate | 0.794    |
| time/               |          |
|    episodes         | 5284     |
|    fps              | 211      |
|    time_elapsed     | 453      |
|    total_timesteps  | 95861    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.32e-05 |
|    n_updates        | 13965    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.00391 |
|    exploration_rate | 0.794    |
| time/               |          |
|    episodes         | 5288     |
|    fps              | 211      |
|    time_elapsed     | 453      |
|    total_timesteps  | 95926    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 13981    |
----------------------------------
Eval num_timesteps=96000, episode_reward=-0.08 +/- 0.23
Episode length: 31.06 +/- 22.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 31.1     |
|    mean_reward      | -0.0834  |
| rollout/            |          |
|    exploration_rate | 0.793    |
| time/               |          |
|    total_timesteps  | 96000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.19e-05 |
|    n_updates        | 13999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.00499 |
|    exploration_rate | 0.793    |
| time/               |          |
|    episodes         | 5292     |
|    fps              | 211      |
|    time_elapsed     | 454      |
|    total_timesteps  | 96021    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7e-05    |
|    n_updates        | 14005    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.00523 |
|    exploration_rate | 0.793    |
| time/               |          |
|    episodes         | 5296     |
|    fps              | 211      |
|    time_elapsed     | 454      |
|    total_timesteps  | 96101    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.43e-05 |
|    n_updates        | 14025    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0145  |
|    exploration_rate | 0.792    |
| time/               |          |
|    episodes         | 5300     |
|    fps              | 211      |
|    time_elapsed     | 454      |
|    total_timesteps  | 96168    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.89e-05 |
|    n_updates        | 14041    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0341  |
|    exploration_rate | 0.792    |
| time/               |          |
|    episodes         | 5304     |
|    fps              | 211      |
|    time_elapsed     | 455      |
|    total_timesteps  | 96239    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.65e-05 |
|    n_updates        | 14059    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0238  |
|    exploration_rate | 0.791    |
| time/               |          |
|    episodes         | 5308     |
|    fps              | 211      |
|    time_elapsed     | 455      |
|    total_timesteps  | 96301    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 14075    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0137  |
|    exploration_rate | 0.791    |
| time/               |          |
|    episodes         | 5312     |
|    fps              | 211      |
|    time_elapsed     | 455      |
|    total_timesteps  | 96369    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.92e-05 |
|    n_updates        | 14092    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0239  |
|    exploration_rate | 0.791    |
| time/               |          |
|    episodes         | 5316     |
|    fps              | 211      |
|    time_elapsed     | 455      |
|    total_timesteps  | 96441    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.73e-05 |
|    n_updates        | 14110    |
----------------------------------
Eval num_timesteps=96500, episode_reward=0.02 +/- 0.30
Episode length: 20.08 +/- 11.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.1     |
|    mean_reward      | 0.0207   |
| rollout/            |          |
|    exploration_rate | 0.79     |
| time/               |          |
|    total_timesteps  | 96500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00694  |
|    n_updates        | 14124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0342  |
|    exploration_rate | 0.79     |
| time/               |          |
|    episodes         | 5320     |
|    fps              | 211      |
|    time_elapsed     | 455      |
|    total_timesteps  | 96520    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.97e-05 |
|    n_updates        | 14129    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0342  |
|    exploration_rate | 0.79     |
| time/               |          |
|    episodes         | 5324     |
|    fps              | 211      |
|    time_elapsed     | 456      |
|    total_timesteps  | 96585    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.58e-05 |
|    n_updates        | 14146    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0352  |
|    exploration_rate | 0.789    |
| time/               |          |
|    episodes         | 5328     |
|    fps              | 211      |
|    time_elapsed     | 456      |
|    total_timesteps  | 96676    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000134 |
|    n_updates        | 14168    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0244  |
|    exploration_rate | 0.789    |
| time/               |          |
|    episodes         | 5332     |
|    fps              | 212      |
|    time_elapsed     | 456      |
|    total_timesteps  | 96747    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 14186    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.035   |
|    exploration_rate | 0.789    |
| time/               |          |
|    episodes         | 5336     |
|    fps              | 212      |
|    time_elapsed     | 456      |
|    total_timesteps  | 96828    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.05e-05 |
|    n_updates        | 14206    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0454  |
|    exploration_rate | 0.788    |
| time/               |          |
|    episodes         | 5340     |
|    fps              | 212      |
|    time_elapsed     | 456      |
|    total_timesteps  | 96901    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000165 |
|    n_updates        | 14225    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0448  |
|    exploration_rate | 0.788    |
| time/               |          |
|    episodes         | 5344     |
|    fps              | 212      |
|    time_elapsed     | 456      |
|    total_timesteps  | 96977    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 14244    |
----------------------------------
Eval num_timesteps=97000, episode_reward=0.01 +/- 0.44
Episode length: 47.66 +/- 21.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.7     |
|    mean_reward      | 0.0102   |
| rollout/            |          |
|    exploration_rate | 0.788    |
| time/               |          |
|    total_timesteps  | 97000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00782  |
|    n_updates        | 14249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0348  |
|    exploration_rate | 0.787    |
| time/               |          |
|    episodes         | 5348     |
|    fps              | 211      |
|    time_elapsed     | 458      |
|    total_timesteps  | 97051    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.77e-06 |
|    n_updates        | 14262    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0356  |
|    exploration_rate | 0.787    |
| time/               |          |
|    episodes         | 5352     |
|    fps              | 212      |
|    time_elapsed     | 458      |
|    total_timesteps  | 97136    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00693  |
|    n_updates        | 14283    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.0361  |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 5356     |
|    fps              | 212      |
|    time_elapsed     | 458      |
|    total_timesteps  | 97223    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.44e-05 |
|    n_updates        | 14305    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.037   |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 5360     |
|    fps              | 212      |
|    time_elapsed     | 458      |
|    total_timesteps  | 97317    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.56e-05 |
|    n_updates        | 14329    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0353  |
|    exploration_rate | 0.785    |
| time/               |          |
|    episodes         | 5364     |
|    fps              | 212      |
|    time_elapsed     | 458      |
|    total_timesteps  | 97380    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00773  |
|    n_updates        | 14344    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.0352  |
|    exploration_rate | 0.785    |
| time/               |          |
|    episodes         | 5368     |
|    fps              | 212      |
|    time_elapsed     | 458      |
|    total_timesteps  | 97470    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 14367    |
----------------------------------
Eval num_timesteps=97500, episode_reward=-0.24 +/- 0.07
Episode length: 61.30 +/- 18.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.3     |
|    mean_reward      | -0.245   |
| rollout/            |          |
|    exploration_rate | 0.785    |
| time/               |          |
|    total_timesteps  | 97500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00773  |
|    n_updates        | 14374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0352  |
|    exploration_rate | 0.785    |
| time/               |          |
|    episodes         | 5372     |
|    fps              | 211      |
|    time_elapsed     | 460      |
|    total_timesteps  | 97540    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.06e-05 |
|    n_updates        | 14384    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0356  |
|    exploration_rate | 0.784    |
| time/               |          |
|    episodes         | 5376     |
|    fps              | 211      |
|    time_elapsed     | 460      |
|    total_timesteps  | 97618    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 14404    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0256  |
|    exploration_rate | 0.784    |
| time/               |          |
|    episodes         | 5380     |
|    fps              | 212      |
|    time_elapsed     | 460      |
|    total_timesteps  | 97691    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.98e-05 |
|    n_updates        | 14422    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0147  |
|    exploration_rate | 0.783    |
| time/               |          |
|    episodes         | 5384     |
|    fps              | 212      |
|    time_elapsed     | 460      |
|    total_timesteps  | 97755    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00704  |
|    n_updates        | 14438    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.00483 |
|    exploration_rate | 0.783    |
| time/               |          |
|    episodes         | 5388     |
|    fps              | 212      |
|    time_elapsed     | 460      |
|    total_timesteps  | 97822    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00694  |
|    n_updates        | 14455    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.00415 |
|    exploration_rate | 0.783    |
| time/               |          |
|    episodes         | 5392     |
|    fps              | 212      |
|    time_elapsed     | 460      |
|    total_timesteps  | 97900    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.2e-06  |
|    n_updates        | 14474    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0064   |
|    exploration_rate | 0.782    |
| time/               |          |
|    episodes         | 5396     |
|    fps              | 212      |
|    time_elapsed     | 460      |
|    total_timesteps  | 97966    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00695  |
|    n_updates        | 14491    |
----------------------------------
Eval num_timesteps=98000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.782    |
| time/               |          |
|    total_timesteps  | 98000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.86e-05 |
|    n_updates        | 14499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00592  |
|    exploration_rate | 0.782    |
| time/               |          |
|    episodes         | 5400     |
|    fps              | 211      |
|    time_elapsed     | 463      |
|    total_timesteps  | 98045    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.55e-05 |
|    n_updates        | 14511    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00624  |
|    exploration_rate | 0.781    |
| time/               |          |
|    episodes         | 5404     |
|    fps              | 211      |
|    time_elapsed     | 463      |
|    total_timesteps  | 98108    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 14526    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.00467 |
|    exploration_rate | 0.781    |
| time/               |          |
|    episodes         | 5408     |
|    fps              | 211      |
|    time_elapsed     | 463      |
|    total_timesteps  | 98193    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.45e-05 |
|    n_updates        | 14548    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0146  |
|    exploration_rate | 0.781    |
| time/               |          |
|    episodes         | 5412     |
|    fps              | 211      |
|    time_elapsed     | 463      |
|    total_timesteps  | 98259    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.77e-05 |
|    n_updates        | 14564    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0146  |
|    exploration_rate | 0.78     |
| time/               |          |
|    episodes         | 5416     |
|    fps              | 212      |
|    time_elapsed     | 463      |
|    total_timesteps  | 98332    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 14582    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0145  |
|    exploration_rate | 0.78     |
| time/               |          |
|    episodes         | 5420     |
|    fps              | 212      |
|    time_elapsed     | 463      |
|    total_timesteps  | 98407    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000191 |
|    n_updates        | 14601    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0145  |
|    exploration_rate | 0.779    |
| time/               |          |
|    episodes         | 5424     |
|    fps              | 212      |
|    time_elapsed     | 463      |
|    total_timesteps  | 98473    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 14618    |
----------------------------------
Eval num_timesteps=98500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.779    |
| time/               |          |
|    total_timesteps  | 98500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.27e-05 |
|    n_updates        | 14624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0143  |
|    exploration_rate | 0.779    |
| time/               |          |
|    episodes         | 5428     |
|    fps              | 211      |
|    time_elapsed     | 466      |
|    total_timesteps  | 98558    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 14639    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0243  |
|    exploration_rate | 0.778    |
| time/               |          |
|    episodes         | 5432     |
|    fps              | 211      |
|    time_elapsed     | 466      |
|    total_timesteps  | 98629    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 14657    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0238  |
|    exploration_rate | 0.778    |
| time/               |          |
|    episodes         | 5436     |
|    fps              | 211      |
|    time_elapsed     | 466      |
|    total_timesteps  | 98699    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.55e-05 |
|    n_updates        | 14674    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0236  |
|    exploration_rate | 0.778    |
| time/               |          |
|    episodes         | 5440     |
|    fps              | 211      |
|    time_elapsed     | 466      |
|    total_timesteps  | 98767    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.43e-05 |
|    n_updates        | 14691    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0138  |
|    exploration_rate | 0.777    |
| time/               |          |
|    episodes         | 5444     |
|    fps              | 211      |
|    time_elapsed     | 466      |
|    total_timesteps  | 98849    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 14712    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0134  |
|    exploration_rate | 0.777    |
| time/               |          |
|    episodes         | 5448     |
|    fps              | 212      |
|    time_elapsed     | 466      |
|    total_timesteps  | 98913    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.54e-05 |
|    n_updates        | 14728    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0134  |
|    exploration_rate | 0.776    |
| time/               |          |
|    episodes         | 5452     |
|    fps              | 212      |
|    time_elapsed     | 466      |
|    total_timesteps  | 98998    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.54e-05 |
|    n_updates        | 14749    |
----------------------------------
Eval num_timesteps=99000, episode_reward=-0.28 +/- 0.07
Episode length: 69.24 +/- 17.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.2     |
|    mean_reward      | -0.277   |
| rollout/            |          |
|    exploration_rate | 0.776    |
| time/               |          |
|    total_timesteps  | 99000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00242 |
|    exploration_rate | 0.776    |
| time/               |          |
|    episodes         | 5456     |
|    fps              | 211      |
|    time_elapsed     | 468      |
|    total_timesteps  | 99059    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000163 |
|    n_updates        | 14764    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00158 |
|    exploration_rate | 0.776    |
| time/               |          |
|    episodes         | 5460     |
|    fps              | 211      |
|    time_elapsed     | 468      |
|    total_timesteps  | 99132    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.01e-05 |
|    n_updates        | 14782    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00186 |
|    exploration_rate | 0.775    |
| time/               |          |
|    episodes         | 5464     |
|    fps              | 211      |
|    time_elapsed     | 469      |
|    total_timesteps  | 99202    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00793  |
|    n_updates        | 14800    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0191   |
|    exploration_rate | 0.775    |
| time/               |          |
|    episodes         | 5468     |
|    fps              | 211      |
|    time_elapsed     | 469      |
|    total_timesteps  | 99268    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.9e-05  |
|    n_updates        | 14816    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0183   |
|    exploration_rate | 0.774    |
| time/               |          |
|    episodes         | 5472     |
|    fps              | 211      |
|    time_elapsed     | 469      |
|    total_timesteps  | 99359    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.21e-05 |
|    n_updates        | 14839    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0287   |
|    exploration_rate | 0.774    |
| time/               |          |
|    episodes         | 5476     |
|    fps              | 211      |
|    time_elapsed     | 469      |
|    total_timesteps  | 99426    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.02e-05 |
|    n_updates        | 14856    |
----------------------------------
Eval num_timesteps=99500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.773    |
| time/               |          |
|    total_timesteps  | 99500    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.68e-05 |
|    n_updates        | 14874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0186   |
|    exploration_rate | 0.773    |
| time/               |          |
|    episodes         | 5480     |
|    fps              | 210      |
|    time_elapsed     | 471      |
|    total_timesteps  | 99501    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 14875    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0185   |
|    exploration_rate | 0.773    |
| time/               |          |
|    episodes         | 5484     |
|    fps              | 211      |
|    time_elapsed     | 471      |
|    total_timesteps  | 99567    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.53e-05 |
|    n_updates        | 14891    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0084   |
|    exploration_rate | 0.773    |
| time/               |          |
|    episodes         | 5488     |
|    fps              | 211      |
|    time_elapsed     | 471      |
|    total_timesteps  | 99637    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.15e-05 |
|    n_updates        | 14909    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0182   |
|    exploration_rate | 0.772    |
| time/               |          |
|    episodes         | 5492     |
|    fps              | 211      |
|    time_elapsed     | 471      |
|    total_timesteps  | 99721    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.39e-06 |
|    n_updates        | 14930    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00791  |
|    exploration_rate | 0.772    |
| time/               |          |
|    episodes         | 5496     |
|    fps              | 211      |
|    time_elapsed     | 471      |
|    total_timesteps  | 99794    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.88e-05 |
|    n_updates        | 14948    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00831  |
|    exploration_rate | 0.771    |
| time/               |          |
|    episodes         | 5500     |
|    fps              | 211      |
|    time_elapsed     | 472      |
|    total_timesteps  | 99863    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 14965    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00815  |
|    exploration_rate | 0.771    |
| time/               |          |
|    episodes         | 5504     |
|    fps              | 211      |
|    time_elapsed     | 472      |
|    total_timesteps  | 99930    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.26e-05 |
|    n_updates        | 14982    |
----------------------------------
Eval num_timesteps=100000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.771    |
| time/               |          |
|    total_timesteps  | 100000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 14999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00847  |
|    exploration_rate | 0.771    |
| time/               |          |
|    episodes         | 5508     |
|    fps              | 210      |
|    time_elapsed     | 474      |
|    total_timesteps  | 100007   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0069   |
|    n_updates        | 15001    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00763  |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 5512     |
|    fps              | 210      |
|    time_elapsed     | 474      |
|    total_timesteps  | 100094   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 15023    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00767  |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 5516     |
|    fps              | 210      |
|    time_elapsed     | 474      |
|    total_timesteps  | 100166   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.24e-05 |
|    n_updates        | 15041    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00783  |
|    exploration_rate | 0.769    |
| time/               |          |
|    episodes         | 5520     |
|    fps              | 211      |
|    time_elapsed     | 474      |
|    total_timesteps  | 100237   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 15059    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00791  |
|    exploration_rate | 0.769    |
| time/               |          |
|    episodes         | 5524     |
|    fps              | 211      |
|    time_elapsed     | 474      |
|    total_timesteps  | 100301   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 15075    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00871  |
|    exploration_rate | 0.768    |
| time/               |          |
|    episodes         | 5528     |
|    fps              | 211      |
|    time_elapsed     | 474      |
|    total_timesteps  | 100366   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.61e-05 |
|    n_updates        | 15091    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0183   |
|    exploration_rate | 0.768    |
| time/               |          |
|    episodes         | 5532     |
|    fps              | 211      |
|    time_elapsed     | 474      |
|    total_timesteps  | 100446   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.04e-05 |
|    n_updates        | 15111    |
----------------------------------
Eval num_timesteps=100500, episode_reward=-0.30 +/- 0.01
Episode length: 74.76 +/- 1.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.8     |
|    mean_reward      | -0.299   |
| rollout/            |          |
|    exploration_rate | 0.768    |
| time/               |          |
|    total_timesteps  | 100500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.15e-05 |
|    n_updates        | 15124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0287   |
|    exploration_rate | 0.768    |
| time/               |          |
|    episodes         | 5536     |
|    fps              | 210      |
|    time_elapsed     | 477      |
|    total_timesteps  | 100508   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.9e-05  |
|    n_updates        | 15126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0377   |
|    exploration_rate | 0.767    |
| time/               |          |
|    episodes         | 5540     |
|    fps              | 210      |
|    time_elapsed     | 477      |
|    total_timesteps  | 100599   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.51e-05 |
|    n_updates        | 15149    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0284   |
|    exploration_rate | 0.767    |
| time/               |          |
|    episodes         | 5544     |
|    fps              | 210      |
|    time_elapsed     | 477      |
|    total_timesteps  | 100665   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.05e-05 |
|    n_updates        | 15166    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0176   |
|    exploration_rate | 0.766    |
| time/               |          |
|    episodes         | 5548     |
|    fps              | 210      |
|    time_elapsed     | 477      |
|    total_timesteps  | 100748   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0149   |
|    n_updates        | 15186    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0183   |
|    exploration_rate | 0.766    |
| time/               |          |
|    episodes         | 5552     |
|    fps              | 211      |
|    time_elapsed     | 477      |
|    total_timesteps  | 100817   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.4e-05  |
|    n_updates        | 15204    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0279   |
|    exploration_rate | 0.765    |
| time/               |          |
|    episodes         | 5556     |
|    fps              | 211      |
|    time_elapsed     | 477      |
|    total_timesteps  | 100889   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 15222    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0278   |
|    exploration_rate | 0.765    |
| time/               |          |
|    episodes         | 5560     |
|    fps              | 211      |
|    time_elapsed     | 477      |
|    total_timesteps  | 100964   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00795  |
|    n_updates        | 15240    |
----------------------------------
Eval num_timesteps=101000, episode_reward=-0.17 +/- 0.18
Episode length: 48.70 +/- 24.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | -0.174   |
| rollout/            |          |
|    exploration_rate | 0.765    |
| time/               |          |
|    total_timesteps  | 101000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.31e-05 |
|    n_updates        | 15249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.027    |
|    exploration_rate | 0.765    |
| time/               |          |
|    episodes         | 5564     |
|    fps              | 210      |
|    time_elapsed     | 479      |
|    total_timesteps  | 101053   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.26e-05 |
|    n_updates        | 15263    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0268   |
|    exploration_rate | 0.764    |
| time/               |          |
|    episodes         | 5568     |
|    fps              | 210      |
|    time_elapsed     | 479      |
|    total_timesteps  | 101125   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.83e-05 |
|    n_updates        | 15281    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0476   |
|    exploration_rate | 0.764    |
| time/               |          |
|    episodes         | 5572     |
|    fps              | 210      |
|    time_elapsed     | 479      |
|    total_timesteps  | 101195   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.4e-05  |
|    n_updates        | 15298    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0375   |
|    exploration_rate | 0.763    |
| time/               |          |
|    episodes         | 5576     |
|    fps              | 211      |
|    time_elapsed     | 479      |
|    total_timesteps  | 101265   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.47e-05 |
|    n_updates        | 15316    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0582   |
|    exploration_rate | 0.763    |
| time/               |          |
|    episodes         | 5580     |
|    fps              | 211      |
|    time_elapsed     | 479      |
|    total_timesteps  | 101322   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 15330    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0482   |
|    exploration_rate | 0.763    |
| time/               |          |
|    episodes         | 5584     |
|    fps              | 211      |
|    time_elapsed     | 479      |
|    total_timesteps  | 101388   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00789  |
|    n_updates        | 15346    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0483   |
|    exploration_rate | 0.762    |
| time/               |          |
|    episodes         | 5588     |
|    fps              | 211      |
|    time_elapsed     | 479      |
|    total_timesteps  | 101457   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.53e-05 |
|    n_updates        | 15364    |
----------------------------------
Eval num_timesteps=101500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.762    |
| time/               |          |
|    total_timesteps  | 101500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.92e-05 |
|    n_updates        | 15374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0488   |
|    exploration_rate | 0.762    |
| time/               |          |
|    episodes         | 5592     |
|    fps              | 210      |
|    time_elapsed     | 482      |
|    total_timesteps  | 101527   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.46e-05 |
|    n_updates        | 15381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0589   |
|    exploration_rate | 0.761    |
| time/               |          |
|    episodes         | 5596     |
|    fps              | 210      |
|    time_elapsed     | 482      |
|    total_timesteps  | 101597   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 15399    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0591   |
|    exploration_rate | 0.761    |
| time/               |          |
|    episodes         | 5600     |
|    fps              | 210      |
|    time_elapsed     | 482      |
|    total_timesteps  | 101662   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000114 |
|    n_updates        | 15415    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0592   |
|    exploration_rate | 0.761    |
| time/               |          |
|    episodes         | 5604     |
|    fps              | 210      |
|    time_elapsed     | 482      |
|    total_timesteps  | 101725   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.66e-05 |
|    n_updates        | 15431    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0695   |
|    exploration_rate | 0.76     |
| time/               |          |
|    episodes         | 5608     |
|    fps              | 210      |
|    time_elapsed     | 482      |
|    total_timesteps  | 101796   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0145   |
|    n_updates        | 15448    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0698   |
|    exploration_rate | 0.76     |
| time/               |          |
|    episodes         | 5612     |
|    fps              | 211      |
|    time_elapsed     | 482      |
|    total_timesteps  | 101875   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 15468    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.069    |
|    exploration_rate | 0.759    |
| time/               |          |
|    episodes         | 5616     |
|    fps              | 211      |
|    time_elapsed     | 482      |
|    total_timesteps  | 101968   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.12e-05 |
|    n_updates        | 15491    |
----------------------------------
Eval num_timesteps=102000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.759    |
| time/               |          |
|    total_timesteps  | 102000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000193 |
|    n_updates        | 15499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0676   |
|    exploration_rate | 0.759    |
| time/               |          |
|    episodes         | 5620     |
|    fps              | 210      |
|    time_elapsed     | 485      |
|    total_timesteps  | 102074   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00678  |
|    n_updates        | 15518    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0776   |
|    exploration_rate | 0.758    |
| time/               |          |
|    episodes         | 5624     |
|    fps              | 210      |
|    time_elapsed     | 485      |
|    total_timesteps  | 102137   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00768  |
|    n_updates        | 15534    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0773   |
|    exploration_rate | 0.758    |
| time/               |          |
|    episodes         | 5628     |
|    fps              | 210      |
|    time_elapsed     | 485      |
|    total_timesteps  | 102211   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.4e-05  |
|    n_updates        | 15552    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0762   |
|    exploration_rate | 0.757    |
| time/               |          |
|    episodes         | 5632     |
|    fps              | 210      |
|    time_elapsed     | 485      |
|    total_timesteps  | 102317   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 15579    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0757   |
|    exploration_rate | 0.757    |
| time/               |          |
|    episodes         | 5636     |
|    fps              | 210      |
|    time_elapsed     | 485      |
|    total_timesteps  | 102392   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.42e-05 |
|    n_updates        | 15597    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0668   |
|    exploration_rate | 0.756    |
| time/               |          |
|    episodes         | 5640     |
|    fps              | 210      |
|    time_elapsed     | 485      |
|    total_timesteps  | 102457   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 15614    |
----------------------------------
Eval num_timesteps=102500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.756    |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00703  |
|    n_updates        | 15624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0667   |
|    exploration_rate | 0.756    |
| time/               |          |
|    episodes         | 5644     |
|    fps              | 209      |
|    time_elapsed     | 488      |
|    total_timesteps  | 102525   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.28e-05 |
|    n_updates        | 15631    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0675   |
|    exploration_rate | 0.756    |
| time/               |          |
|    episodes         | 5648     |
|    fps              | 210      |
|    time_elapsed     | 488      |
|    total_timesteps  | 102589   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 15647    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0672   |
|    exploration_rate | 0.755    |
| time/               |          |
|    episodes         | 5652     |
|    fps              | 210      |
|    time_elapsed     | 488      |
|    total_timesteps  | 102664   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.14e-05 |
|    n_updates        | 15665    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0473   |
|    exploration_rate | 0.755    |
| time/               |          |
|    episodes         | 5656     |
|    fps              | 210      |
|    time_elapsed     | 488      |
|    total_timesteps  | 102733   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.63e-05 |
|    n_updates        | 15683    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0574   |
|    exploration_rate | 0.754    |
| time/               |          |
|    episodes         | 5660     |
|    fps              | 210      |
|    time_elapsed     | 488      |
|    total_timesteps  | 102806   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 15701    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0576   |
|    exploration_rate | 0.754    |
| time/               |          |
|    episodes         | 5664     |
|    fps              | 210      |
|    time_elapsed     | 488      |
|    total_timesteps  | 102890   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.53e-05 |
|    n_updates        | 15722    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0376   |
|    exploration_rate | 0.753    |
| time/               |          |
|    episodes         | 5668     |
|    fps              | 210      |
|    time_elapsed     | 488      |
|    total_timesteps  | 102962   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.25e-05 |
|    n_updates        | 15740    |
----------------------------------
Eval num_timesteps=103000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.753    |
| time/               |          |
|    total_timesteps  | 103000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 15749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0174   |
|    exploration_rate | 0.753    |
| time/               |          |
|    episodes         | 5672     |
|    fps              | 209      |
|    time_elapsed     | 491      |
|    total_timesteps  | 103037   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00711  |
|    n_updates        | 15759    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0276   |
|    exploration_rate | 0.753    |
| time/               |          |
|    episodes         | 5676     |
|    fps              | 209      |
|    time_elapsed     | 491      |
|    total_timesteps  | 103102   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000139 |
|    n_updates        | 15775    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.752    |
| time/               |          |
|    episodes         | 5680     |
|    fps              | 209      |
|    time_elapsed     | 491      |
|    total_timesteps  | 103168   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000143 |
|    n_updates        | 15791    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 0.752    |
| time/               |          |
|    episodes         | 5684     |
|    fps              | 210      |
|    time_elapsed     | 491      |
|    total_timesteps  | 103237   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.57e-05 |
|    n_updates        | 15809    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.751    |
| time/               |          |
|    episodes         | 5688     |
|    fps              | 210      |
|    time_elapsed     | 491      |
|    total_timesteps  | 103303   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000136 |
|    n_updates        | 15825    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.751    |
| time/               |          |
|    episodes         | 5692     |
|    fps              | 210      |
|    time_elapsed     | 491      |
|    total_timesteps  | 103374   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.66e-05 |
|    n_updates        | 15843    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00669  |
|    exploration_rate | 0.75     |
| time/               |          |
|    episodes         | 5696     |
|    fps              | 210      |
|    time_elapsed     | 491      |
|    total_timesteps  | 103456   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000173 |
|    n_updates        | 15863    |
----------------------------------
Eval num_timesteps=103500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.75     |
| time/               |          |
|    total_timesteps  | 103500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 15874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00641  |
|    exploration_rate | 0.75     |
| time/               |          |
|    episodes         | 5700     |
|    fps              | 209      |
|    time_elapsed     | 494      |
|    total_timesteps  | 103528   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.04e-05 |
|    n_updates        | 15881    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00621  |
|    exploration_rate | 0.75     |
| time/               |          |
|    episodes         | 5704     |
|    fps              | 209      |
|    time_elapsed     | 494      |
|    total_timesteps  | 103596   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 15898    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.00531  |
|    exploration_rate | 0.749    |
| time/               |          |
|    episodes         | 5708     |
|    fps              | 209      |
|    time_elapsed     | 494      |
|    total_timesteps  | 103689   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.18e-05 |
|    n_updates        | 15922    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0154   |
|    exploration_rate | 0.749    |
| time/               |          |
|    episodes         | 5712     |
|    fps              | 209      |
|    time_elapsed     | 494      |
|    total_timesteps  | 103766   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00699  |
|    n_updates        | 15941    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0266   |
|    exploration_rate | 0.748    |
| time/               |          |
|    episodes         | 5716     |
|    fps              | 210      |
|    time_elapsed     | 494      |
|    total_timesteps  | 103830   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.21e-05 |
|    n_updates        | 15957    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.027    |
|    exploration_rate | 0.748    |
| time/               |          |
|    episodes         | 5720     |
|    fps              | 210      |
|    time_elapsed     | 494      |
|    total_timesteps  | 103926   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 15981    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0166   |
|    exploration_rate | 0.747    |
| time/               |          |
|    episodes         | 5724     |
|    fps              | 210      |
|    time_elapsed     | 494      |
|    total_timesteps  | 103999   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00674  |
|    n_updates        | 15999    |
----------------------------------
Eval num_timesteps=104000, episode_reward=-0.30 +/- 0.02
Episode length: 74.28 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.3     |
|    mean_reward     | -0.297   |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0165   |
|    exploration_rate | 0.747    |
| time/               |          |
|    episodes         | 5728     |
|    fps              | 209      |
|    time_elapsed     | 497      |
|    total_timesteps  | 104075   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00767  |
|    n_updates        | 16018    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00792  |
|    exploration_rate | 0.746    |
| time/               |          |
|    episodes         | 5732     |
|    fps              | 209      |
|    time_elapsed     | 497      |
|    total_timesteps  | 104145   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 16036    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00188 |
|    exploration_rate | 0.746    |
| time/               |          |
|    episodes         | 5736     |
|    fps              | 209      |
|    time_elapsed     | 497      |
|    total_timesteps  | 104215   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00696  |
|    n_updates        | 16053    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00805  |
|    exploration_rate | 0.746    |
| time/               |          |
|    episodes         | 5740     |
|    fps              | 209      |
|    time_elapsed     | 497      |
|    total_timesteps  | 104282   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0148   |
|    n_updates        | 16070    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00809  |
|    exploration_rate | 0.745    |
| time/               |          |
|    episodes         | 5744     |
|    fps              | 209      |
|    time_elapsed     | 497      |
|    total_timesteps  | 104349   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 16087    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00745  |
|    exploration_rate | 0.745    |
| time/               |          |
|    episodes         | 5748     |
|    fps              | 209      |
|    time_elapsed     | 497      |
|    total_timesteps  | 104429   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.06e-05 |
|    n_updates        | 16107    |
----------------------------------
Eval num_timesteps=104500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.744    |
| time/               |          |
|    total_timesteps  | 104500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 16124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00741  |
|    exploration_rate | 0.744    |
| time/               |          |
|    episodes         | 5752     |
|    fps              | 209      |
|    time_elapsed     | 499      |
|    total_timesteps  | 104505   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0153   |
|    n_updates        | 16126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0177   |
|    exploration_rate | 0.744    |
| time/               |          |
|    episodes         | 5756     |
|    fps              | 209      |
|    time_elapsed     | 499      |
|    total_timesteps  | 104568   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00725  |
|    n_updates        | 16141    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00772  |
|    exploration_rate | 0.743    |
| time/               |          |
|    episodes         | 5760     |
|    fps              | 209      |
|    time_elapsed     | 500      |
|    total_timesteps  | 104640   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 16159    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00788  |
|    exploration_rate | 0.743    |
| time/               |          |
|    episodes         | 5764     |
|    fps              | 209      |
|    time_elapsed     | 500      |
|    total_timesteps  | 104720   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00775  |
|    n_updates        | 16179    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00804  |
|    exploration_rate | 0.743    |
| time/               |          |
|    episodes         | 5768     |
|    fps              | 209      |
|    time_elapsed     | 500      |
|    total_timesteps  | 104788   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.86e-05 |
|    n_updates        | 16196    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00816  |
|    exploration_rate | 0.742    |
| time/               |          |
|    episodes         | 5772     |
|    fps              | 209      |
|    time_elapsed     | 500      |
|    total_timesteps  | 104860   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.07e-05 |
|    n_updates        | 16214    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00783  |
|    exploration_rate | 0.742    |
| time/               |          |
|    episodes         | 5776     |
|    fps              | 209      |
|    time_elapsed     | 500      |
|    total_timesteps  | 104933   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 16233    |
----------------------------------
Eval num_timesteps=105000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.741    |
| time/               |          |
|    total_timesteps  | 105000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000118 |
|    n_updates        | 16249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0034  |
|    exploration_rate | 0.741    |
| time/               |          |
|    episodes         | 5780     |
|    fps              | 208      |
|    time_elapsed     | 502      |
|    total_timesteps  | 105030   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0077   |
|    n_updates        | 16257    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00666  |
|    exploration_rate | 0.741    |
| time/               |          |
|    episodes         | 5784     |
|    fps              | 208      |
|    time_elapsed     | 502      |
|    total_timesteps  | 105098   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.49e-05 |
|    n_updates        | 16274    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00638  |
|    exploration_rate | 0.74     |
| time/               |          |
|    episodes         | 5788     |
|    fps              | 209      |
|    time_elapsed     | 502      |
|    total_timesteps  | 105171   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.62e-05 |
|    n_updates        | 16292    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0044  |
|    exploration_rate | 0.74     |
| time/               |          |
|    episodes         | 5792     |
|    fps              | 209      |
|    time_elapsed     | 503      |
|    total_timesteps  | 105261   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000166 |
|    n_updates        | 16315    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.00448 |
|    exploration_rate | 0.739    |
| time/               |          |
|    episodes         | 5796     |
|    fps              | 209      |
|    time_elapsed     | 503      |
|    total_timesteps  | 105345   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00697  |
|    n_updates        | 16336    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0044  |
|    exploration_rate | 0.739    |
| time/               |          |
|    episodes         | 5800     |
|    fps              | 209      |
|    time_elapsed     | 503      |
|    total_timesteps  | 105415   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.96e-05 |
|    n_updates        | 16353    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0046  |
|    exploration_rate | 0.738    |
| time/               |          |
|    episodes         | 5804     |
|    fps              | 209      |
|    time_elapsed     | 503      |
|    total_timesteps  | 105488   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.85e-05 |
|    n_updates        | 16371    |
----------------------------------
Eval num_timesteps=105500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.738    |
| time/               |          |
|    total_timesteps  | 105500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.96e-05 |
|    n_updates        | 16374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.00374 |
|    exploration_rate | 0.738    |
| time/               |          |
|    episodes         | 5808     |
|    fps              | 208      |
|    time_elapsed     | 505      |
|    total_timesteps  | 105560   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.84e-05 |
|    n_updates        | 16389    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0135  |
|    exploration_rate | 0.738    |
| time/               |          |
|    episodes         | 5812     |
|    fps              | 208      |
|    time_elapsed     | 505      |
|    total_timesteps  | 105630   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.05e-05 |
|    n_updates        | 16407    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0239  |
|    exploration_rate | 0.737    |
| time/               |          |
|    episodes         | 5816     |
|    fps              | 208      |
|    time_elapsed     | 505      |
|    total_timesteps  | 105703   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.21e-05 |
|    n_updates        | 16425    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0129  |
|    exploration_rate | 0.737    |
| time/               |          |
|    episodes         | 5820     |
|    fps              | 209      |
|    time_elapsed     | 505      |
|    total_timesteps  | 105776   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.12e-05 |
|    n_updates        | 16443    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.013   |
|    exploration_rate | 0.736    |
| time/               |          |
|    episodes         | 5824     |
|    fps              | 209      |
|    time_elapsed     | 506      |
|    total_timesteps  | 105851   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.33e-05 |
|    n_updates        | 16462    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.013   |
|    exploration_rate | 0.736    |
| time/               |          |
|    episodes         | 5828     |
|    fps              | 209      |
|    time_elapsed     | 506      |
|    total_timesteps  | 105926   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.88e-05 |
|    n_updates        | 16481    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00282 |
|    exploration_rate | 0.735    |
| time/               |          |
|    episodes         | 5832     |
|    fps              | 209      |
|    time_elapsed     | 506      |
|    total_timesteps  | 105992   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00766  |
|    n_updates        | 16497    |
----------------------------------
Eval num_timesteps=106000, episode_reward=-0.03 +/- 0.20
Episode length: 17.34 +/- 1.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.3     |
|    mean_reward      | -0.0283  |
| rollout/            |          |
|    exploration_rate | 0.735    |
| time/               |          |
|    total_timesteps  | 106000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 16499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00294 |
|    exploration_rate | 0.735    |
| time/               |          |
|    episodes         | 5836     |
|    fps              | 209      |
|    time_elapsed     | 506      |
|    total_timesteps  | 106065   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00659  |
|    n_updates        | 16516    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.013   |
|    exploration_rate | 0.735    |
| time/               |          |
|    episodes         | 5840     |
|    fps              | 209      |
|    time_elapsed     | 506      |
|    total_timesteps  | 106134   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 16533    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00296 |
|    exploration_rate | 0.734    |
| time/               |          |
|    episodes         | 5844     |
|    fps              | 209      |
|    time_elapsed     | 506      |
|    total_timesteps  | 106199   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000139 |
|    n_updates        | 16549    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00268 |
|    exploration_rate | 0.734    |
| time/               |          |
|    episodes         | 5848     |
|    fps              | 209      |
|    time_elapsed     | 506      |
|    total_timesteps  | 106272   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.81e-05 |
|    n_updates        | 16567    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00268 |
|    exploration_rate | 0.733    |
| time/               |          |
|    episodes         | 5852     |
|    fps              | 209      |
|    time_elapsed     | 507      |
|    total_timesteps  | 106348   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.18e-05 |
|    n_updates        | 16586    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0137  |
|    exploration_rate | 0.733    |
| time/               |          |
|    episodes         | 5856     |
|    fps              | 209      |
|    time_elapsed     | 507      |
|    total_timesteps  | 106435   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.3e-05  |
|    n_updates        | 16608    |
----------------------------------
Eval num_timesteps=106500, episode_reward=-0.04 +/- 0.21
Episode length: 20.56 +/- 16.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.6     |
|    mean_reward      | -0.0413  |
| rollout/            |          |
|    exploration_rate | 0.732    |
| time/               |          |
|    total_timesteps  | 106500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 16624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0135  |
|    exploration_rate | 0.732    |
| time/               |          |
|    episodes         | 5860     |
|    fps              | 209      |
|    time_elapsed     | 507      |
|    total_timesteps  | 106502   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.48e-05 |
|    n_updates        | 16625    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0137  |
|    exploration_rate | 0.732    |
| time/               |          |
|    episodes         | 5864     |
|    fps              | 209      |
|    time_elapsed     | 507      |
|    total_timesteps  | 106588   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.97e-05 |
|    n_updates        | 16646    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00627  |
|    exploration_rate | 0.731    |
| time/               |          |
|    episodes         | 5868     |
|    fps              | 209      |
|    time_elapsed     | 508      |
|    total_timesteps  | 106657   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000145 |
|    n_updates        | 16664    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00631  |
|    exploration_rate | 0.731    |
| time/               |          |
|    episodes         | 5872     |
|    fps              | 210      |
|    time_elapsed     | 508      |
|    total_timesteps  | 106728   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0148   |
|    n_updates        | 16681    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.00361 |
|    exploration_rate | 0.731    |
| time/               |          |
|    episodes         | 5876     |
|    fps              | 210      |
|    time_elapsed     | 508      |
|    total_timesteps  | 106799   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.94e-05 |
|    n_updates        | 16699    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00245 |
|    exploration_rate | 0.73     |
| time/               |          |
|    episodes         | 5880     |
|    fps              | 210      |
|    time_elapsed     | 508      |
|    total_timesteps  | 106867   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.6e-05  |
|    n_updates        | 16716    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0125  |
|    exploration_rate | 0.73     |
| time/               |          |
|    episodes         | 5884     |
|    fps              | 210      |
|    time_elapsed     | 508      |
|    total_timesteps  | 106936   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 16733    |
----------------------------------
Eval num_timesteps=107000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.729    |
| time/               |          |
|    total_timesteps  | 107000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00767  |
|    n_updates        | 16749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.0123  |
|    exploration_rate | 0.729    |
| time/               |          |
|    episodes         | 5888     |
|    fps              | 209      |
|    time_elapsed     | 510      |
|    total_timesteps  | 107005   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 16751    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00139 |
|    exploration_rate | 0.729    |
| time/               |          |
|    episodes         | 5892     |
|    fps              | 209      |
|    time_elapsed     | 510      |
|    total_timesteps  | 107071   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.014    |
|    n_updates        | 16767    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.00907  |
|    exploration_rate | 0.728    |
| time/               |          |
|    episodes         | 5896     |
|    fps              | 209      |
|    time_elapsed     | 510      |
|    total_timesteps  | 107144   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000118 |
|    n_updates        | 16785    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00851  |
|    exploration_rate | 0.728    |
| time/               |          |
|    episodes         | 5900     |
|    fps              | 209      |
|    time_elapsed     | 511      |
|    total_timesteps  | 107228   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00779  |
|    n_updates        | 16806    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.00843  |
|    exploration_rate | 0.727    |
| time/               |          |
|    episodes         | 5904     |
|    fps              | 209      |
|    time_elapsed     | 511      |
|    total_timesteps  | 107303   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.43e-05 |
|    n_updates        | 16825    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00203 |
|    exploration_rate | 0.727    |
| time/               |          |
|    episodes         | 5908     |
|    fps              | 210      |
|    time_elapsed     | 511      |
|    total_timesteps  | 107386   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.88e-05 |
|    n_updates        | 16846    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00211 |
|    exploration_rate | 0.727    |
| time/               |          |
|    episodes         | 5912     |
|    fps              | 210      |
|    time_elapsed     | 511      |
|    total_timesteps  | 107458   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 16864    |
----------------------------------
Eval num_timesteps=107500, episode_reward=-0.00 +/- 0.24
Episode length: 15.82 +/- 0.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 15.8     |
|    mean_reward      | -0.00222 |
| rollout/            |          |
|    exploration_rate | 0.726    |
| time/               |          |
|    total_timesteps  | 107500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.29e-05 |
|    n_updates        | 16874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00215 |
|    exploration_rate | 0.726    |
| time/               |          |
|    episodes         | 5916     |
|    fps              | 210      |
|    time_elapsed     | 511      |
|    total_timesteps  | 107532   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 16882    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00263 |
|    exploration_rate | 0.726    |
| time/               |          |
|    episodes         | 5920     |
|    fps              | 210      |
|    time_elapsed     | 511      |
|    total_timesteps  | 107617   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 16904    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0072   |
|    exploration_rate | 0.725    |
| time/               |          |
|    episodes         | 5924     |
|    fps              | 210      |
|    time_elapsed     | 511      |
|    total_timesteps  | 107696   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.77e-05 |
|    n_updates        | 16923    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.00704  |
|    exploration_rate | 0.725    |
| time/               |          |
|    episodes         | 5928     |
|    fps              | 210      |
|    time_elapsed     | 512      |
|    total_timesteps  | 107775   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 16943    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00739  |
|    exploration_rate | 0.724    |
| time/               |          |
|    episodes         | 5932     |
|    fps              | 210      |
|    time_elapsed     | 512      |
|    total_timesteps  | 107832   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.52e-05 |
|    n_updates        | 16957    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0177   |
|    exploration_rate | 0.724    |
| time/               |          |
|    episodes         | 5936     |
|    fps              | 210      |
|    time_elapsed     | 512      |
|    total_timesteps  | 107898   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.38e-05 |
|    n_updates        | 16974    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0176   |
|    exploration_rate | 0.723    |
| time/               |          |
|    episodes         | 5940     |
|    fps              | 210      |
|    time_elapsed     | 512      |
|    total_timesteps  | 107970   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00724  |
|    n_updates        | 16992    |
----------------------------------
Eval num_timesteps=108000, episode_reward=-0.05 +/- 0.14
Episode length: 17.00 +/- 1.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17       |
|    mean_reward      | -0.047   |
| rollout/            |          |
|    exploration_rate | 0.723    |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.27e-05 |
|    n_updates        | 16999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0174   |
|    exploration_rate | 0.723    |
| time/               |          |
|    episodes         | 5944     |
|    fps              | 210      |
|    time_elapsed     | 512      |
|    total_timesteps  | 108041   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0146   |
|    n_updates        | 17010    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0174   |
|    exploration_rate | 0.723    |
| time/               |          |
|    episodes         | 5948     |
|    fps              | 210      |
|    time_elapsed     | 512      |
|    total_timesteps  | 108113   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 17028    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0477   |
|    exploration_rate | 0.722    |
| time/               |          |
|    episodes         | 5952     |
|    fps              | 210      |
|    time_elapsed     | 513      |
|    total_timesteps  | 108182   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.4e-05  |
|    n_updates        | 17045    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0485   |
|    exploration_rate | 0.722    |
| time/               |          |
|    episodes         | 5956     |
|    fps              | 210      |
|    time_elapsed     | 513      |
|    total_timesteps  | 108249   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00684  |
|    n_updates        | 17062    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0584   |
|    exploration_rate | 0.721    |
| time/               |          |
|    episodes         | 5960     |
|    fps              | 211      |
|    time_elapsed     | 513      |
|    total_timesteps  | 108318   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.1e-05  |
|    n_updates        | 17079    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0686   |
|    exploration_rate | 0.721    |
| time/               |          |
|    episodes         | 5964     |
|    fps              | 211      |
|    time_elapsed     | 513      |
|    total_timesteps  | 108399   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.99e-05 |
|    n_updates        | 17099    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0486   |
|    exploration_rate | 0.72     |
| time/               |          |
|    episodes         | 5968     |
|    fps              | 211      |
|    time_elapsed     | 513      |
|    total_timesteps  | 108468   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.03e-05 |
|    n_updates        | 17116    |
----------------------------------
Eval num_timesteps=108500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.72     |
| time/               |          |
|    total_timesteps  | 108500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000147 |
|    n_updates        | 17124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0477   |
|    exploration_rate | 0.72     |
| time/               |          |
|    episodes         | 5972     |
|    fps              | 210      |
|    time_elapsed     | 515      |
|    total_timesteps  | 108563   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.46e-05 |
|    n_updates        | 17140    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0471   |
|    exploration_rate | 0.719    |
| time/               |          |
|    episodes         | 5976     |
|    fps              | 210      |
|    time_elapsed     | 515      |
|    total_timesteps  | 108647   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 17161    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0468   |
|    exploration_rate | 0.719    |
| time/               |          |
|    episodes         | 5980     |
|    fps              | 210      |
|    time_elapsed     | 515      |
|    total_timesteps  | 108723   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00707  |
|    n_updates        | 17180    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0463   |
|    exploration_rate | 0.718    |
| time/               |          |
|    episodes         | 5984     |
|    fps              | 210      |
|    time_elapsed     | 516      |
|    total_timesteps  | 108806   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.12e-05 |
|    n_updates        | 17201    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0563   |
|    exploration_rate | 0.718    |
| time/               |          |
|    episodes         | 5988     |
|    fps              | 210      |
|    time_elapsed     | 516      |
|    total_timesteps  | 108876   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000138 |
|    n_updates        | 17218    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0563   |
|    exploration_rate | 0.717    |
| time/               |          |
|    episodes         | 5992     |
|    fps              | 211      |
|    time_elapsed     | 516      |
|    total_timesteps  | 108942   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.12e-05 |
|    n_updates        | 17235    |
----------------------------------
Eval num_timesteps=109000, episode_reward=-0.26 +/- 0.07
Episode length: 65.06 +/- 18.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.1     |
|    mean_reward      | -0.26    |
| rollout/            |          |
|    exploration_rate | 0.717    |
| time/               |          |
|    total_timesteps  | 109000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 17249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0454   |
|    exploration_rate | 0.717    |
| time/               |          |
|    episodes         | 5996     |
|    fps              | 210      |
|    time_elapsed     | 518      |
|    total_timesteps  | 109036   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.19e-05 |
|    n_updates        | 17258    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0458   |
|    exploration_rate | 0.716    |
| time/               |          |
|    episodes         | 6000     |
|    fps              | 210      |
|    time_elapsed     | 518      |
|    total_timesteps  | 109110   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.94e-05 |
|    n_updates        | 17277    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0562   |
|    exploration_rate | 0.716    |
| time/               |          |
|    episodes         | 6004     |
|    fps              | 210      |
|    time_elapsed     | 518      |
|    total_timesteps  | 109175   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.89e-05 |
|    n_updates        | 17293    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.067    |
|    exploration_rate | 0.716    |
| time/               |          |
|    episodes         | 6008     |
|    fps              | 210      |
|    time_elapsed     | 518      |
|    total_timesteps  | 109239   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.1e-05  |
|    n_updates        | 17309    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0667   |
|    exploration_rate | 0.715    |
| time/               |          |
|    episodes         | 6012     |
|    fps              | 210      |
|    time_elapsed     | 518      |
|    total_timesteps  | 109317   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00704  |
|    n_updates        | 17329    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0664   |
|    exploration_rate | 0.715    |
| time/               |          |
|    episodes         | 6016     |
|    fps              | 210      |
|    time_elapsed     | 518      |
|    total_timesteps  | 109398   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.51e-05 |
|    n_updates        | 17349    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0571   |
|    exploration_rate | 0.714    |
| time/               |          |
|    episodes         | 6020     |
|    fps              | 211      |
|    time_elapsed     | 518      |
|    total_timesteps  | 109468   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000167 |
|    n_updates        | 17366    |
----------------------------------
Eval num_timesteps=109500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.714    |
| time/               |          |
|    total_timesteps  | 109500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000138 |
|    n_updates        | 17374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0463   |
|    exploration_rate | 0.714    |
| time/               |          |
|    episodes         | 6024     |
|    fps              | 210      |
|    time_elapsed     | 521      |
|    total_timesteps  | 109565   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.49e-05 |
|    n_updates        | 17391    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0459   |
|    exploration_rate | 0.713    |
| time/               |          |
|    episodes         | 6028     |
|    fps              | 210      |
|    time_elapsed     | 521      |
|    total_timesteps  | 109656   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0141   |
|    n_updates        | 17413    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0448   |
|    exploration_rate | 0.713    |
| time/               |          |
|    episodes         | 6032     |
|    fps              | 210      |
|    time_elapsed     | 521      |
|    total_timesteps  | 109741   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.8e-05  |
|    n_updates        | 17435    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0543   |
|    exploration_rate | 0.712    |
| time/               |          |
|    episodes         | 6036     |
|    fps              | 210      |
|    time_elapsed     | 521      |
|    total_timesteps  | 109819   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 17454    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0644   |
|    exploration_rate | 0.712    |
| time/               |          |
|    episodes         | 6040     |
|    fps              | 210      |
|    time_elapsed     | 521      |
|    total_timesteps  | 109889   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00708  |
|    n_updates        | 17472    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0543   |
|    exploration_rate | 0.711    |
| time/               |          |
|    episodes         | 6044     |
|    fps              | 210      |
|    time_elapsed     | 521      |
|    total_timesteps  | 109961   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.56e-05 |
|    n_updates        | 17490    |
----------------------------------
Eval num_timesteps=110000, episode_reward=-0.01 +/- 0.24
Episode length: 16.96 +/- 1.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17       |
|    mean_reward      | -0.00684 |
| rollout/            |          |
|    exploration_rate | 0.711    |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 17499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0545   |
|    exploration_rate | 0.711    |
| time/               |          |
|    episodes         | 6048     |
|    fps              | 210      |
|    time_elapsed     | 522      |
|    total_timesteps  | 110028   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.03e-05 |
|    n_updates        | 17506    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0242   |
|    exploration_rate | 0.71     |
| time/               |          |
|    episodes         | 6052     |
|    fps              | 210      |
|    time_elapsed     | 522      |
|    total_timesteps  | 110104   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.33e-05 |
|    n_updates        | 17525    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0237   |
|    exploration_rate | 0.71     |
| time/               |          |
|    episodes         | 6056     |
|    fps              | 210      |
|    time_elapsed     | 522      |
|    total_timesteps  | 110183   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000114 |
|    n_updates        | 17545    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0139   |
|    exploration_rate | 0.709    |
| time/               |          |
|    episodes         | 6060     |
|    fps              | 210      |
|    time_elapsed     | 522      |
|    total_timesteps  | 110248   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 17561    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.00386  |
|    exploration_rate | 0.709    |
| time/               |          |
|    episodes         | 6064     |
|    fps              | 211      |
|    time_elapsed     | 522      |
|    total_timesteps  | 110329   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9e-05    |
|    n_updates        | 17582    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0142   |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 6068     |
|    fps              | 211      |
|    time_elapsed     | 522      |
|    total_timesteps  | 110391   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000182 |
|    n_updates        | 17597    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.015    |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 6072     |
|    fps              | 211      |
|    time_elapsed     | 522      |
|    total_timesteps  | 110464   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.37e-05 |
|    n_updates        | 17615    |
----------------------------------
Eval num_timesteps=110500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.708    |
| time/               |          |
|    total_timesteps  | 110500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.1e-05  |
|    n_updates        | 17624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.0351   |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 6076     |
|    fps              | 210      |
|    time_elapsed     | 525      |
|    total_timesteps  | 110548   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 17636    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0447   |
|    exploration_rate | 0.707    |
| time/               |          |
|    episodes         | 6080     |
|    fps              | 210      |
|    time_elapsed     | 525      |
|    total_timesteps  | 110634   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.71e-05 |
|    n_updates        | 17658    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.0451   |
|    exploration_rate | 0.707    |
| time/               |          |
|    episodes         | 6084     |
|    fps              | 210      |
|    time_elapsed     | 525      |
|    total_timesteps  | 110707   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 17676    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.035    |
|    exploration_rate | 0.706    |
| time/               |          |
|    episodes         | 6088     |
|    fps              | 210      |
|    time_elapsed     | 525      |
|    total_timesteps  | 110779   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 17694    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.025    |
|    exploration_rate | 0.706    |
| time/               |          |
|    episodes         | 6092     |
|    fps              | 210      |
|    time_elapsed     | 525      |
|    total_timesteps  | 110845   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.25e-05 |
|    n_updates        | 17711    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0257   |
|    exploration_rate | 0.705    |
| time/               |          |
|    episodes         | 6096     |
|    fps              | 211      |
|    time_elapsed     | 525      |
|    total_timesteps  | 110921   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00794  |
|    n_updates        | 17730    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0259   |
|    exploration_rate | 0.705    |
| time/               |          |
|    episodes         | 6100     |
|    fps              | 211      |
|    time_elapsed     | 525      |
|    total_timesteps  | 110989   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.44e-05 |
|    n_updates        | 17747    |
----------------------------------
Eval num_timesteps=111000, episode_reward=0.02 +/- 0.28
Episode length: 16.28 +/- 1.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.3     |
|    mean_reward      | 0.0159   |
| rollout/            |          |
|    exploration_rate | 0.705    |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.78e-05 |
|    n_updates        | 17749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.036    |
|    exploration_rate | 0.704    |
| time/               |          |
|    episodes         | 6104     |
|    fps              | 211      |
|    time_elapsed     | 526      |
|    total_timesteps  | 111053   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 17763    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0258   |
|    exploration_rate | 0.704    |
| time/               |          |
|    episodes         | 6108     |
|    fps              | 211      |
|    time_elapsed     | 526      |
|    total_timesteps  | 111122   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00685  |
|    n_updates        | 17780    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.026    |
|    exploration_rate | 0.703    |
| time/               |          |
|    episodes         | 6112     |
|    fps              | 211      |
|    time_elapsed     | 526      |
|    total_timesteps  | 111193   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000159 |
|    n_updates        | 17798    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0254   |
|    exploration_rate | 0.703    |
| time/               |          |
|    episodes         | 6116     |
|    fps              | 211      |
|    time_elapsed     | 526      |
|    total_timesteps  | 111290   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 17822    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0254   |
|    exploration_rate | 0.702    |
| time/               |          |
|    episodes         | 6120     |
|    fps              | 211      |
|    time_elapsed     | 526      |
|    total_timesteps  | 111361   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 17840    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0266   |
|    exploration_rate | 0.702    |
| time/               |          |
|    episodes         | 6124     |
|    fps              | 211      |
|    time_elapsed     | 526      |
|    total_timesteps  | 111428   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000149 |
|    n_updates        | 17856    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0376   |
|    exploration_rate | 0.702    |
| time/               |          |
|    episodes         | 6128     |
|    fps              | 211      |
|    time_elapsed     | 526      |
|    total_timesteps  | 111493   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.39e-05 |
|    n_updates        | 17873    |
----------------------------------
Eval num_timesteps=111500, episode_reward=-0.22 +/- 0.22
Episode length: 66.32 +/- 17.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.3     |
|    mean_reward      | -0.225   |
| rollout/            |          |
|    exploration_rate | 0.702    |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00705  |
|    n_updates        | 17874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0276   |
|    exploration_rate | 0.701    |
| time/               |          |
|    episodes         | 6132     |
|    fps              | 210      |
|    time_elapsed     | 528      |
|    total_timesteps  | 111578   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000171 |
|    n_updates        | 17894    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00749  |
|    exploration_rate | 0.701    |
| time/               |          |
|    episodes         | 6136     |
|    fps              | 211      |
|    time_elapsed     | 529      |
|    total_timesteps  | 111658   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000159 |
|    n_updates        | 17914    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00252 |
|    exploration_rate | 0.7      |
| time/               |          |
|    episodes         | 6140     |
|    fps              | 211      |
|    time_elapsed     | 529      |
|    total_timesteps  | 111728   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0138   |
|    n_updates        | 17931    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00745  |
|    exploration_rate | 0.7      |
| time/               |          |
|    episodes         | 6144     |
|    fps              | 211      |
|    time_elapsed     | 529      |
|    total_timesteps  | 111801   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 17950    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.00705  |
|    exploration_rate | 0.699    |
| time/               |          |
|    episodes         | 6148     |
|    fps              | 211      |
|    time_elapsed     | 529      |
|    total_timesteps  | 111878   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.41e-05 |
|    n_updates        | 17969    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00617  |
|    exploration_rate | 0.699    |
| time/               |          |
|    episodes         | 6152     |
|    fps              | 211      |
|    time_elapsed     | 529      |
|    total_timesteps  | 111976   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00705  |
|    n_updates        | 17993    |
----------------------------------
Eval num_timesteps=112000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.698    |
| time/               |          |
|    total_timesteps  | 112000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.53e-05 |
|    n_updates        | 17999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.00653  |
|    exploration_rate | 0.698    |
| time/               |          |
|    episodes         | 6156     |
|    fps              | 210      |
|    time_elapsed     | 531      |
|    total_timesteps  | 112046   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00689  |
|    n_updates        | 18011    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0164   |
|    exploration_rate | 0.698    |
| time/               |          |
|    episodes         | 6160     |
|    fps              | 210      |
|    time_elapsed     | 531      |
|    total_timesteps  | 112116   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00767  |
|    n_updates        | 18028    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0167   |
|    exploration_rate | 0.697    |
| time/               |          |
|    episodes         | 6164     |
|    fps              | 210      |
|    time_elapsed     | 531      |
|    total_timesteps  | 112188   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.85e-05 |
|    n_updates        | 18046    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00577  |
|    exploration_rate | 0.697    |
| time/               |          |
|    episodes         | 6168     |
|    fps              | 211      |
|    time_elapsed     | 532      |
|    total_timesteps  | 112273   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 18068    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.00533  |
|    exploration_rate | 0.696    |
| time/               |          |
|    episodes         | 6172     |
|    fps              | 211      |
|    time_elapsed     | 532      |
|    total_timesteps  | 112357   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0077   |
|    n_updates        | 18089    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0146  |
|    exploration_rate | 0.696    |
| time/               |          |
|    episodes         | 6176     |
|    fps              | 211      |
|    time_elapsed     | 532      |
|    total_timesteps  | 112439   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00707  |
|    n_updates        | 18109    |
----------------------------------
Eval num_timesteps=112500, episode_reward=-0.29 +/- 0.04
Episode length: 72.72 +/- 11.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.7     |
|    mean_reward      | -0.291   |
| rollout/            |          |
|    exploration_rate | 0.695    |
| time/               |          |
|    total_timesteps  | 112500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 18124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0246  |
|    exploration_rate | 0.695    |
| time/               |          |
|    episodes         | 6180     |
|    fps              | 210      |
|    time_elapsed     | 534      |
|    total_timesteps  | 112524   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.51e-05 |
|    n_updates        | 18130    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.0249  |
|    exploration_rate | 0.695    |
| time/               |          |
|    episodes         | 6184     |
|    fps              | 210      |
|    time_elapsed     | 534      |
|    total_timesteps  | 112606   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.12e-05 |
|    n_updates        | 18151    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0247  |
|    exploration_rate | 0.694    |
| time/               |          |
|    episodes         | 6188     |
|    fps              | 210      |
|    time_elapsed     | 534      |
|    total_timesteps  | 112673   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00818  |
|    n_updates        | 18168    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.025   |
|    exploration_rate | 0.694    |
| time/               |          |
|    episodes         | 6192     |
|    fps              | 210      |
|    time_elapsed     | 534      |
|    total_timesteps  | 112745   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.92e-05 |
|    n_updates        | 18186    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0245  |
|    exploration_rate | 0.693    |
| time/               |          |
|    episodes         | 6196     |
|    fps              | 210      |
|    time_elapsed     | 534      |
|    total_timesteps  | 112810   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000153 |
|    n_updates        | 18202    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0246  |
|    exploration_rate | 0.693    |
| time/               |          |
|    episodes         | 6200     |
|    fps              | 211      |
|    time_elapsed     | 534      |
|    total_timesteps  | 112880   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.08e-05 |
|    n_updates        | 18219    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0346  |
|    exploration_rate | 0.692    |
| time/               |          |
|    episodes         | 6204     |
|    fps              | 211      |
|    time_elapsed     | 535      |
|    total_timesteps  | 112944   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.35e-05 |
|    n_updates        | 18235    |
----------------------------------
Eval num_timesteps=113000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.692    |
| time/               |          |
|    total_timesteps  | 113000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.28e-05 |
|    n_updates        | 18249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0353  |
|    exploration_rate | 0.692    |
| time/               |          |
|    episodes         | 6208     |
|    fps              | 210      |
|    time_elapsed     | 537      |
|    total_timesteps  | 113030   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.57e-05 |
|    n_updates        | 18257    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0356  |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 6212     |
|    fps              | 210      |
|    time_elapsed     | 537      |
|    total_timesteps  | 113108   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 18276    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0355  |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 6216     |
|    fps              | 210      |
|    time_elapsed     | 537      |
|    total_timesteps  | 113203   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 18300    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0355  |
|    exploration_rate | 0.69     |
| time/               |          |
|    episodes         | 6220     |
|    fps              | 210      |
|    time_elapsed     | 537      |
|    total_timesteps  | 113274   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.14e-05 |
|    n_updates        | 18318    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | -0.0357  |
|    exploration_rate | 0.69     |
| time/               |          |
|    episodes         | 6224     |
|    fps              | 210      |
|    time_elapsed     | 537      |
|    total_timesteps  | 113347   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00771  |
|    n_updates        | 18336    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | -0.0459  |
|    exploration_rate | 0.689    |
| time/               |          |
|    episodes         | 6228     |
|    fps              | 210      |
|    time_elapsed     | 537      |
|    total_timesteps  | 113417   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 18354    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0251  |
|    exploration_rate | 0.689    |
| time/               |          |
|    episodes         | 6232     |
|    fps              | 210      |
|    time_elapsed     | 537      |
|    total_timesteps  | 113483   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.81e-05 |
|    n_updates        | 18370    |
----------------------------------
Eval num_timesteps=113500, episode_reward=-0.27 +/- 0.18
Episode length: 71.44 +/- 14.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.4     |
|    mean_reward      | -0.266   |
| rollout/            |          |
|    exploration_rate | 0.689    |
| time/               |          |
|    total_timesteps  | 113500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00812  |
|    n_updates        | 18374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0247  |
|    exploration_rate | 0.689    |
| time/               |          |
|    episodes         | 6236     |
|    fps              | 210      |
|    time_elapsed     | 540      |
|    total_timesteps  | 113551   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 18387    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0145  |
|    exploration_rate | 0.688    |
| time/               |          |
|    episodes         | 6240     |
|    fps              | 210      |
|    time_elapsed     | 540      |
|    total_timesteps  | 113618   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 18404    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0246  |
|    exploration_rate | 0.688    |
| time/               |          |
|    episodes         | 6244     |
|    fps              | 210      |
|    time_elapsed     | 540      |
|    total_timesteps  | 113691   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.13e-05 |
|    n_updates        | 18422    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0246  |
|    exploration_rate | 0.687    |
| time/               |          |
|    episodes         | 6248     |
|    fps              | 210      |
|    time_elapsed     | 540      |
|    total_timesteps  | 113770   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.52e-05 |
|    n_updates        | 18442    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0237  |
|    exploration_rate | 0.687    |
| time/               |          |
|    episodes         | 6252     |
|    fps              | 210      |
|    time_elapsed     | 540      |
|    total_timesteps  | 113844   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.07e-06 |
|    n_updates        | 18460    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0236  |
|    exploration_rate | 0.686    |
| time/               |          |
|    episodes         | 6256     |
|    fps              | 210      |
|    time_elapsed     | 540      |
|    total_timesteps  | 113911   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00696  |
|    n_updates        | 18477    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0341  |
|    exploration_rate | 0.686    |
| time/               |          |
|    episodes         | 6260     |
|    fps              | 210      |
|    time_elapsed     | 540      |
|    total_timesteps  | 113995   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.43e-05 |
|    n_updates        | 18498    |
----------------------------------
Eval num_timesteps=114000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.686    |
| time/               |          |
|    total_timesteps  | 114000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 18499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.024   |
|    exploration_rate | 0.685    |
| time/               |          |
|    episodes         | 6264     |
|    fps              | 209      |
|    time_elapsed     | 543      |
|    total_timesteps  | 114064   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.7e-05  |
|    n_updates        | 18515    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0234  |
|    exploration_rate | 0.685    |
| time/               |          |
|    episodes         | 6268     |
|    fps              | 210      |
|    time_elapsed     | 543      |
|    total_timesteps  | 114134   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 18533    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0238  |
|    exploration_rate | 0.684    |
| time/               |          |
|    episodes         | 6272     |
|    fps              | 210      |
|    time_elapsed     | 543      |
|    total_timesteps  | 114227   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 18556    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00295 |
|    exploration_rate | 0.684    |
| time/               |          |
|    episodes         | 6276     |
|    fps              | 210      |
|    time_elapsed     | 543      |
|    total_timesteps  | 114289   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 18572    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00279 |
|    exploration_rate | 0.683    |
| time/               |          |
|    episodes         | 6280     |
|    fps              | 210      |
|    time_elapsed     | 543      |
|    total_timesteps  | 114370   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.47e-05 |
|    n_updates        | 18592    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00263 |
|    exploration_rate | 0.683    |
| time/               |          |
|    episodes         | 6284     |
|    fps              | 210      |
|    time_elapsed     | 543      |
|    total_timesteps  | 114448   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00696  |
|    n_updates        | 18611    |
----------------------------------
Eval num_timesteps=114500, episode_reward=-0.23 +/- 0.19
Episode length: 62.30 +/- 23.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.3     |
|    mean_reward      | -0.229   |
| rollout/            |          |
|    exploration_rate | 0.683    |
| time/               |          |
|    total_timesteps  | 114500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.22e-05 |
|    n_updates        | 18624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00735  |
|    exploration_rate | 0.682    |
| time/               |          |
|    episodes         | 6288     |
|    fps              | 209      |
|    time_elapsed     | 545      |
|    total_timesteps  | 114516   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000167 |
|    n_updates        | 18628    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.00619  |
|    exploration_rate | 0.682    |
| time/               |          |
|    episodes         | 6292     |
|    fps              | 209      |
|    time_elapsed     | 545      |
|    total_timesteps  | 114617   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.42e-05 |
|    n_updates        | 18654    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00607  |
|    exploration_rate | 0.681    |
| time/               |          |
|    episodes         | 6296     |
|    fps              | 210      |
|    time_elapsed     | 545      |
|    total_timesteps  | 114685   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.94e-05 |
|    n_updates        | 18671    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00571  |
|    exploration_rate | 0.681    |
| time/               |          |
|    episodes         | 6300     |
|    fps              | 210      |
|    time_elapsed     | 546      |
|    total_timesteps  | 114764   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00021  |
|    n_updates        | 18690    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.00457 |
|    exploration_rate | 0.68     |
| time/               |          |
|    episodes         | 6304     |
|    fps              | 210      |
|    time_elapsed     | 546      |
|    total_timesteps  | 114835   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 18708    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.00465 |
|    exploration_rate | 0.68     |
| time/               |          |
|    episodes         | 6308     |
|    fps              | 210      |
|    time_elapsed     | 546      |
|    total_timesteps  | 114923   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.77e-05 |
|    n_updates        | 18730    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00573  |
|    exploration_rate | 0.679    |
| time/               |          |
|    episodes         | 6312     |
|    fps              | 210      |
|    time_elapsed     | 546      |
|    total_timesteps  | 114992   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.8e-05  |
|    n_updates        | 18747    |
----------------------------------
Eval num_timesteps=115000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.679    |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.66e-05 |
|    n_updates        | 18749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00589  |
|    exploration_rate | 0.679    |
| time/               |          |
|    episodes         | 6316     |
|    fps              | 209      |
|    time_elapsed     | 548      |
|    total_timesteps  | 115083   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.89e-05 |
|    n_updates        | 18770    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0259   |
|    exploration_rate | 0.678    |
| time/               |          |
|    episodes         | 6320     |
|    fps              | 209      |
|    time_elapsed     | 548      |
|    total_timesteps  | 115154   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.96e-05 |
|    n_updates        | 18788    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.0253   |
|    exploration_rate | 0.678    |
| time/               |          |
|    episodes         | 6324     |
|    fps              | 209      |
|    time_elapsed     | 548      |
|    total_timesteps  | 115243   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.52e-05 |
|    n_updates        | 18810    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0245   |
|    exploration_rate | 0.677    |
| time/               |          |
|    episodes         | 6328     |
|    fps              | 210      |
|    time_elapsed     | 548      |
|    total_timesteps  | 115331   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.35e-05 |
|    n_updates        | 18832    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0244   |
|    exploration_rate | 0.677    |
| time/               |          |
|    episodes         | 6332     |
|    fps              | 210      |
|    time_elapsed     | 549      |
|    total_timesteps  | 115399   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00702  |
|    n_updates        | 18849    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0244   |
|    exploration_rate | 0.676    |
| time/               |          |
|    episodes         | 6336     |
|    fps              | 210      |
|    time_elapsed     | 549      |
|    total_timesteps  | 115469   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000114 |
|    n_updates        | 18867    |
----------------------------------
Eval num_timesteps=115500, episode_reward=-0.23 +/- 0.16
Episode length: 62.04 +/- 15.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62       |
|    mean_reward      | -0.228   |
| rollout/            |          |
|    exploration_rate | 0.676    |
| time/               |          |
|    total_timesteps  | 115500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00017  |
|    n_updates        | 18874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0135   |
|    exploration_rate | 0.676    |
| time/               |          |
|    episodes         | 6340     |
|    fps              | 209      |
|    time_elapsed     | 551      |
|    total_timesteps  | 115557   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0069   |
|    n_updates        | 18889    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0133   |
|    exploration_rate | 0.675    |
| time/               |          |
|    episodes         | 6344     |
|    fps              | 209      |
|    time_elapsed     | 551      |
|    total_timesteps  | 115635   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 18908    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0136   |
|    exploration_rate | 0.675    |
| time/               |          |
|    episodes         | 6348     |
|    fps              | 209      |
|    time_elapsed     | 551      |
|    total_timesteps  | 115706   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.52e-05 |
|    n_updates        | 18926    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0132   |
|    exploration_rate | 0.674    |
| time/               |          |
|    episodes         | 6352     |
|    fps              | 209      |
|    time_elapsed     | 551      |
|    total_timesteps  | 115791   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 18947    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0126   |
|    exploration_rate | 0.674    |
| time/               |          |
|    episodes         | 6356     |
|    fps              | 210      |
|    time_elapsed     | 551      |
|    total_timesteps  | 115873   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 18968    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0333   |
|    exploration_rate | 0.673    |
| time/               |          |
|    episodes         | 6360     |
|    fps              | 210      |
|    time_elapsed     | 551      |
|    total_timesteps  | 115938   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.02e-05 |
|    n_updates        | 18984    |
----------------------------------
Eval num_timesteps=116000, episode_reward=0.01 +/- 0.31
Episode length: 21.50 +/- 12.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21.5     |
|    mean_reward      | 0.0149   |
| rollout/            |          |
|    exploration_rate | 0.673    |
| time/               |          |
|    total_timesteps  | 116000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 18999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0229   |
|    exploration_rate | 0.673    |
| time/               |          |
|    episodes         | 6364     |
|    fps              | 210      |
|    time_elapsed     | 552      |
|    total_timesteps  | 116018   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 19004    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0223   |
|    exploration_rate | 0.672    |
| time/               |          |
|    episodes         | 6368     |
|    fps              | 210      |
|    time_elapsed     | 552      |
|    total_timesteps  | 116103   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.38e-05 |
|    n_updates        | 19025    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0429   |
|    exploration_rate | 0.672    |
| time/               |          |
|    episodes         | 6372     |
|    fps              | 210      |
|    time_elapsed     | 552      |
|    total_timesteps  | 116181   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 19045    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0321   |
|    exploration_rate | 0.671    |
| time/               |          |
|    episodes         | 6376     |
|    fps              | 210      |
|    time_elapsed     | 552      |
|    total_timesteps  | 116262   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.37e-05 |
|    n_updates        | 19065    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0324   |
|    exploration_rate | 0.671    |
| time/               |          |
|    episodes         | 6380     |
|    fps              | 210      |
|    time_elapsed     | 552      |
|    total_timesteps  | 116336   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0068   |
|    n_updates        | 19083    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0326   |
|    exploration_rate | 0.67     |
| time/               |          |
|    episodes         | 6384     |
|    fps              | 210      |
|    time_elapsed     | 552      |
|    total_timesteps  | 116409   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.62e-05 |
|    n_updates        | 19102    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0223   |
|    exploration_rate | 0.67     |
| time/               |          |
|    episodes         | 6388     |
|    fps              | 210      |
|    time_elapsed     | 552      |
|    total_timesteps  | 116485   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000189 |
|    n_updates        | 19121    |
----------------------------------
Eval num_timesteps=116500, episode_reward=-0.10 +/- 0.31
Episode length: 51.12 +/- 18.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.1     |
|    mean_reward      | -0.104   |
| rollout/            |          |
|    exploration_rate | 0.67     |
| time/               |          |
|    total_timesteps  | 116500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.88e-05 |
|    n_updates        | 19124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0333   |
|    exploration_rate | 0.669    |
| time/               |          |
|    episodes         | 6392     |
|    fps              | 210      |
|    time_elapsed     | 554      |
|    total_timesteps  | 116561   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.25e-05 |
|    n_updates        | 19140    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0428   |
|    exploration_rate | 0.669    |
| time/               |          |
|    episodes         | 6396     |
|    fps              | 210      |
|    time_elapsed     | 554      |
|    total_timesteps  | 116642   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.64e-05 |
|    n_updates        | 19160    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0532   |
|    exploration_rate | 0.668    |
| time/               |          |
|    episodes         | 6400     |
|    fps              | 210      |
|    time_elapsed     | 554      |
|    total_timesteps  | 116710   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 19177    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0631   |
|    exploration_rate | 0.668    |
| time/               |          |
|    episodes         | 6404     |
|    fps              | 210      |
|    time_elapsed     | 554      |
|    total_timesteps  | 116785   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.03e-05 |
|    n_updates        | 19196    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0637   |
|    exploration_rate | 0.667    |
| time/               |          |
|    episodes         | 6408     |
|    fps              | 210      |
|    time_elapsed     | 554      |
|    total_timesteps  | 116858   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.48e-05 |
|    n_updates        | 19214    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0633   |
|    exploration_rate | 0.667    |
| time/               |          |
|    episodes         | 6412     |
|    fps              | 210      |
|    time_elapsed     | 554      |
|    total_timesteps  | 116937   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.64e-05 |
|    n_updates        | 19234    |
----------------------------------
Eval num_timesteps=117000, episode_reward=-0.02 +/- 0.45
Episode length: 59.08 +/- 20.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.1     |
|    mean_reward      | -0.0157  |
| rollout/            |          |
|    exploration_rate | 0.666    |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 19249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0641   |
|    exploration_rate | 0.666    |
| time/               |          |
|    episodes         | 6416     |
|    fps              | 210      |
|    time_elapsed     | 556      |
|    total_timesteps  | 117008   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 19251    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0434   |
|    exploration_rate | 0.666    |
| time/               |          |
|    episodes         | 6420     |
|    fps              | 210      |
|    time_elapsed     | 557      |
|    total_timesteps  | 117096   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00671  |
|    n_updates        | 19273    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.044    |
|    exploration_rate | 0.665    |
| time/               |          |
|    episodes         | 6424     |
|    fps              | 210      |
|    time_elapsed     | 557      |
|    total_timesteps  | 117171   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00787  |
|    n_updates        | 19292    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.045    |
|    exploration_rate | 0.665    |
| time/               |          |
|    episodes         | 6428     |
|    fps              | 210      |
|    time_elapsed     | 557      |
|    total_timesteps  | 117233   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 19308    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0234   |
|    exploration_rate | 0.664    |
| time/               |          |
|    episodes         | 6432     |
|    fps              | 210      |
|    time_elapsed     | 557      |
|    total_timesteps  | 117342   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 19335    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0236   |
|    exploration_rate | 0.664    |
| time/               |          |
|    episodes         | 6436     |
|    fps              | 210      |
|    time_elapsed     | 557      |
|    total_timesteps  | 117407   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.04e-05 |
|    n_updates        | 19351    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0241   |
|    exploration_rate | 0.663    |
| time/               |          |
|    episodes         | 6440     |
|    fps              | 210      |
|    time_elapsed     | 557      |
|    total_timesteps  | 117482   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.81e-05 |
|    n_updates        | 19370    |
----------------------------------
Eval num_timesteps=117500, episode_reward=-0.29 +/- 0.04
Episode length: 72.50 +/- 9.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.5     |
|    mean_reward      | -0.29    |
| rollout/            |          |
|    exploration_rate | 0.663    |
| time/               |          |
|    total_timesteps  | 117500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00676  |
|    n_updates        | 19374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0236   |
|    exploration_rate | 0.663    |
| time/               |          |
|    episodes         | 6444     |
|    fps              | 210      |
|    time_elapsed     | 559      |
|    total_timesteps  | 117573   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000162 |
|    n_updates        | 19393    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0231   |
|    exploration_rate | 0.662    |
| time/               |          |
|    episodes         | 6448     |
|    fps              | 210      |
|    time_elapsed     | 559      |
|    total_timesteps  | 117656   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0148   |
|    n_updates        | 19413    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0234   |
|    exploration_rate | 0.662    |
| time/               |          |
|    episodes         | 6452     |
|    fps              | 210      |
|    time_elapsed     | 559      |
|    total_timesteps  | 117733   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.1e-05  |
|    n_updates        | 19433    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0234   |
|    exploration_rate | 0.661    |
| time/               |          |
|    episodes         | 6456     |
|    fps              | 210      |
|    time_elapsed     | 560      |
|    total_timesteps  | 117816   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 19453    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.00293  |
|    exploration_rate | 0.661    |
| time/               |          |
|    episodes         | 6460     |
|    fps              | 210      |
|    time_elapsed     | 560      |
|    total_timesteps  | 117892   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 19472    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0128   |
|    exploration_rate | 0.66     |
| time/               |          |
|    episodes         | 6464     |
|    fps              | 210      |
|    time_elapsed     | 560      |
|    total_timesteps  | 117975   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00689  |
|    n_updates        | 19493    |
----------------------------------
Eval num_timesteps=118000, episode_reward=-0.30 +/- 0.01
Episode length: 74.82 +/- 1.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.8     |
|    mean_reward      | -0.299   |
| rollout/            |          |
|    exploration_rate | 0.66     |
| time/               |          |
|    total_timesteps  | 118000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 19499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.013    |
|    exploration_rate | 0.66     |
| time/               |          |
|    episodes         | 6468     |
|    fps              | 209      |
|    time_elapsed     | 562      |
|    total_timesteps  | 118056   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 19513    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0131   |
|    exploration_rate | 0.659    |
| time/               |          |
|    episodes         | 6472     |
|    fps              | 209      |
|    time_elapsed     | 562      |
|    total_timesteps  | 118132   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 19532    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.00324  |
|    exploration_rate | 0.659    |
| time/               |          |
|    episodes         | 6476     |
|    fps              | 210      |
|    time_elapsed     | 562      |
|    total_timesteps  | 118209   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 19552    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0136   |
|    exploration_rate | 0.658    |
| time/               |          |
|    episodes         | 6480     |
|    fps              | 210      |
|    time_elapsed     | 562      |
|    total_timesteps  | 118275   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 19568    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0137   |
|    exploration_rate | 0.658    |
| time/               |          |
|    episodes         | 6484     |
|    fps              | 210      |
|    time_elapsed     | 562      |
|    total_timesteps  | 118345   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00701  |
|    n_updates        | 19586    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0336   |
|    exploration_rate | 0.657    |
| time/               |          |
|    episodes         | 6488     |
|    fps              | 210      |
|    time_elapsed     | 563      |
|    total_timesteps  | 118423   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.89e-05 |
|    n_updates        | 19605    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0241   |
|    exploration_rate | 0.657    |
| time/               |          |
|    episodes         | 6492     |
|    fps              | 210      |
|    time_elapsed     | 563      |
|    total_timesteps  | 118487   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00702  |
|    n_updates        | 19621    |
----------------------------------
Eval num_timesteps=118500, episode_reward=-0.16 +/- 0.24
Episode length: 49.58 +/- 26.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.6     |
|    mean_reward      | -0.158   |
| rollout/            |          |
|    exploration_rate | 0.657    |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0069   |
|    n_updates        | 19624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0142   |
|    exploration_rate | 0.656    |
| time/               |          |
|    episodes         | 6496     |
|    fps              | 209      |
|    time_elapsed     | 564      |
|    total_timesteps  | 118564   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000162 |
|    n_updates        | 19640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.00415  |
|    exploration_rate | 0.656    |
| time/               |          |
|    episodes         | 6500     |
|    fps              | 210      |
|    time_elapsed     | 564      |
|    total_timesteps  | 118634   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00024  |
|    n_updates        | 19658    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | -0.00759 |
|    exploration_rate | 0.655    |
| time/               |          |
|    episodes         | 6504     |
|    fps              | 210      |
|    time_elapsed     | 565      |
|    total_timesteps  | 118752   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 19687    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | -0.00735 |
|    exploration_rate | 0.655    |
| time/               |          |
|    episodes         | 6508     |
|    fps              | 210      |
|    time_elapsed     | 565      |
|    total_timesteps  | 118819   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00016  |
|    n_updates        | 19704    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.0169  |
|    exploration_rate | 0.654    |
| time/               |          |
|    episodes         | 6512     |
|    fps              | 210      |
|    time_elapsed     | 565      |
|    total_timesteps  | 118886   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.54e-05 |
|    n_updates        | 19721    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.0168  |
|    exploration_rate | 0.654    |
| time/               |          |
|    episodes         | 6516     |
|    fps              | 210      |
|    time_elapsed     | 565      |
|    total_timesteps  | 118954   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000183 |
|    n_updates        | 19738    |
----------------------------------
Eval num_timesteps=119000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.653    |
| time/               |          |
|    total_timesteps  | 119000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 19749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0164  |
|    exploration_rate | 0.653    |
| time/               |          |
|    episodes         | 6520     |
|    fps              | 209      |
|    time_elapsed     | 567      |
|    total_timesteps  | 119032   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.31e-05 |
|    n_updates        | 19757    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.00704 |
|    exploration_rate | 0.653    |
| time/               |          |
|    episodes         | 6524     |
|    fps              | 209      |
|    time_elapsed     | 567      |
|    total_timesteps  | 119124   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.96e-05 |
|    n_updates        | 19780    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.00244  |
|    exploration_rate | 0.652    |
| time/               |          |
|    episodes         | 6528     |
|    fps              | 209      |
|    time_elapsed     | 567      |
|    total_timesteps  | 119199   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000148 |
|    n_updates        | 19799    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.00336  |
|    exploration_rate | 0.652    |
| time/               |          |
|    episodes         | 6532     |
|    fps              | 210      |
|    time_elapsed     | 567      |
|    total_timesteps  | 119285   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 19821    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.00328  |
|    exploration_rate | 0.651    |
| time/               |          |
|    episodes         | 6536     |
|    fps              | 210      |
|    time_elapsed     | 568      |
|    total_timesteps  | 119352   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 19837    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0224   |
|    exploration_rate | 0.65     |
| time/               |          |
|    episodes         | 6540     |
|    fps              | 210      |
|    time_elapsed     | 568      |
|    total_timesteps  | 119450   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000118 |
|    n_updates        | 19862    |
----------------------------------
Eval num_timesteps=119500, episode_reward=-0.25 +/- 0.18
Episode length: 68.04 +/- 18.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68       |
|    mean_reward      | -0.252   |
| rollout/            |          |
|    exploration_rate | 0.65     |
| time/               |          |
|    total_timesteps  | 119500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.14e-05 |
|    n_updates        | 19874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0224   |
|    exploration_rate | 0.65     |
| time/               |          |
|    episodes         | 6544     |
|    fps              | 209      |
|    time_elapsed     | 570      |
|    total_timesteps  | 119541   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000159 |
|    n_updates        | 19885    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0227   |
|    exploration_rate | 0.649    |
| time/               |          |
|    episodes         | 6548     |
|    fps              | 209      |
|    time_elapsed     | 570      |
|    total_timesteps  | 119615   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 19903    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0227   |
|    exploration_rate | 0.649    |
| time/               |          |
|    episodes         | 6552     |
|    fps              | 209      |
|    time_elapsed     | 570      |
|    total_timesteps  | 119693   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.9e-05  |
|    n_updates        | 19923    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0534   |
|    exploration_rate | 0.648    |
| time/               |          |
|    episodes         | 6556     |
|    fps              | 209      |
|    time_elapsed     | 570      |
|    total_timesteps  | 119758   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 19939    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0532   |
|    exploration_rate | 0.648    |
| time/               |          |
|    episodes         | 6560     |
|    fps              | 209      |
|    time_elapsed     | 570      |
|    total_timesteps  | 119840   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0142   |
|    n_updates        | 19959    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.043    |
|    exploration_rate | 0.647    |
| time/               |          |
|    episodes         | 6564     |
|    fps              | 210      |
|    time_elapsed     | 570      |
|    total_timesteps  | 119927   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 19981    |
----------------------------------
Eval num_timesteps=120000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.647    |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00685  |
|    n_updates        | 19999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0531   |
|    exploration_rate | 0.647    |
| time/               |          |
|    episodes         | 6568     |
|    fps              | 209      |
|    time_elapsed     | 573      |
|    total_timesteps  | 120006   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 20001    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0333   |
|    exploration_rate | 0.646    |
| time/               |          |
|    episodes         | 6572     |
|    fps              | 209      |
|    time_elapsed     | 573      |
|    total_timesteps  | 120077   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0144   |
|    n_updates        | 20019    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.053    |
|    exploration_rate | 0.646    |
| time/               |          |
|    episodes         | 6576     |
|    fps              | 209      |
|    time_elapsed     | 573      |
|    total_timesteps  | 120161   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.27e-05 |
|    n_updates        | 20040    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0528   |
|    exploration_rate | 0.645    |
| time/               |          |
|    episodes         | 6580     |
|    fps              | 209      |
|    time_elapsed     | 573      |
|    total_timesteps  | 120232   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 20057    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0627   |
|    exploration_rate | 0.645    |
| time/               |          |
|    episodes         | 6584     |
|    fps              | 209      |
|    time_elapsed     | 573      |
|    total_timesteps  | 120305   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.01e-05 |
|    n_updates        | 20076    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0428   |
|    exploration_rate | 0.644    |
| time/               |          |
|    episodes         | 6588     |
|    fps              | 209      |
|    time_elapsed     | 573      |
|    total_timesteps  | 120380   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.99e-05 |
|    n_updates        | 20094    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0522   |
|    exploration_rate | 0.644    |
| time/               |          |
|    episodes         | 6592     |
|    fps              | 209      |
|    time_elapsed     | 573      |
|    total_timesteps  | 120460   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.65e-05 |
|    n_updates        | 20114    |
----------------------------------
Eval num_timesteps=120500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.643    |
| time/               |          |
|    total_timesteps  | 120500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000235 |
|    n_updates        | 20124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0522   |
|    exploration_rate | 0.643    |
| time/               |          |
|    episodes         | 6596     |
|    fps              | 209      |
|    time_elapsed     | 576      |
|    total_timesteps  | 120537   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 20134    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0523   |
|    exploration_rate | 0.643    |
| time/               |          |
|    episodes         | 6600     |
|    fps              | 209      |
|    time_elapsed     | 576      |
|    total_timesteps  | 120603   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 20150    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0537   |
|    exploration_rate | 0.642    |
| time/               |          |
|    episodes         | 6604     |
|    fps              | 209      |
|    time_elapsed     | 576      |
|    total_timesteps  | 120685   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.23e-05 |
|    n_updates        | 20171    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0536   |
|    exploration_rate | 0.642    |
| time/               |          |
|    episodes         | 6608     |
|    fps              | 209      |
|    time_elapsed     | 576      |
|    total_timesteps  | 120756   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.06e-05 |
|    n_updates        | 20188    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0629   |
|    exploration_rate | 0.641    |
| time/               |          |
|    episodes         | 6612     |
|    fps              | 209      |
|    time_elapsed     | 576      |
|    total_timesteps  | 120841   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000148 |
|    n_updates        | 20210    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0629   |
|    exploration_rate | 0.641    |
| time/               |          |
|    episodes         | 6616     |
|    fps              | 209      |
|    time_elapsed     | 576      |
|    total_timesteps  | 120909   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 20227    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0734   |
|    exploration_rate | 0.64     |
| time/               |          |
|    episodes         | 6620     |
|    fps              | 209      |
|    time_elapsed     | 576      |
|    total_timesteps  | 120975   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.72e-05 |
|    n_updates        | 20243    |
----------------------------------
Eval num_timesteps=121000, episode_reward=-0.29 +/- 0.05
Episode length: 71.70 +/- 11.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.7     |
|    mean_reward      | -0.287   |
| rollout/            |          |
|    exploration_rate | 0.64     |
| time/               |          |
|    total_timesteps  | 121000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000229 |
|    n_updates        | 20249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0642   |
|    exploration_rate | 0.64     |
| time/               |          |
|    episodes         | 6624     |
|    fps              | 209      |
|    time_elapsed     | 579      |
|    total_timesteps  | 121045   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000114 |
|    n_updates        | 20261    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.054    |
|    exploration_rate | 0.639    |
| time/               |          |
|    episodes         | 6628     |
|    fps              | 209      |
|    time_elapsed     | 579      |
|    total_timesteps  | 121127   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.06e-05 |
|    n_updates        | 20281    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0544   |
|    exploration_rate | 0.639    |
| time/               |          |
|    episodes         | 6632     |
|    fps              | 209      |
|    time_elapsed     | 579      |
|    total_timesteps  | 121203   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 20300    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0631   |
|    exploration_rate | 0.638    |
| time/               |          |
|    episodes         | 6636     |
|    fps              | 209      |
|    time_elapsed     | 579      |
|    total_timesteps  | 121302   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 20325    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0437   |
|    exploration_rate | 0.638    |
| time/               |          |
|    episodes         | 6640     |
|    fps              | 209      |
|    time_elapsed     | 579      |
|    total_timesteps  | 121385   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00653  |
|    n_updates        | 20346    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0438   |
|    exploration_rate | 0.637    |
| time/               |          |
|    episodes         | 6644     |
|    fps              | 209      |
|    time_elapsed     | 579      |
|    total_timesteps  | 121472   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0078   |
|    n_updates        | 20367    |
----------------------------------
Eval num_timesteps=121500, episode_reward=-0.02 +/- 0.28
Episode length: 24.20 +/- 16.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 24.2     |
|    mean_reward      | -0.0158  |
| rollout/            |          |
|    exploration_rate | 0.637    |
| time/               |          |
|    total_timesteps  | 121500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.32e-05 |
|    n_updates        | 20374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0428   |
|    exploration_rate | 0.636    |
| time/               |          |
|    episodes         | 6648     |
|    fps              | 209      |
|    time_elapsed     | 580      |
|    total_timesteps  | 121572   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.06e-05 |
|    n_updates        | 20392    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0427   |
|    exploration_rate | 0.636    |
| time/               |          |
|    episodes         | 6652     |
|    fps              | 209      |
|    time_elapsed     | 580      |
|    total_timesteps  | 121653   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 20413    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0126   |
|    exploration_rate | 0.635    |
| time/               |          |
|    episodes         | 6656     |
|    fps              | 209      |
|    time_elapsed     | 580      |
|    total_timesteps  | 121719   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.22e-05 |
|    n_updates        | 20429    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0228   |
|    exploration_rate | 0.635    |
| time/               |          |
|    episodes         | 6660     |
|    fps              | 209      |
|    time_elapsed     | 580      |
|    total_timesteps  | 121795   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000199 |
|    n_updates        | 20448    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0228   |
|    exploration_rate | 0.634    |
| time/               |          |
|    episodes         | 6664     |
|    fps              | 209      |
|    time_elapsed     | 580      |
|    total_timesteps  | 121884   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.86e-05 |
|    n_updates        | 20470    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0132   |
|    exploration_rate | 0.634    |
| time/               |          |
|    episodes         | 6668     |
|    fps              | 209      |
|    time_elapsed     | 580      |
|    total_timesteps  | 121953   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.45e-05 |
|    n_updates        | 20488    |
----------------------------------
Eval num_timesteps=122000, episode_reward=-0.19 +/- 0.31
Episode length: 67.48 +/- 16.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.5     |
|    mean_reward      | -0.19    |
| rollout/            |          |
|    exploration_rate | 0.633    |
| time/               |          |
|    total_timesteps  | 122000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.68e-05 |
|    n_updates        | 20499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0235   |
|    exploration_rate | 0.633    |
| time/               |          |
|    episodes         | 6672     |
|    fps              | 209      |
|    time_elapsed     | 583      |
|    total_timesteps  | 122016   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.27e-05 |
|    n_updates        | 20503    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.00358  |
|    exploration_rate | 0.633    |
| time/               |          |
|    episodes         | 6676     |
|    fps              | 209      |
|    time_elapsed     | 583      |
|    total_timesteps  | 122097   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00772  |
|    n_updates        | 20524    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.00671 |
|    exploration_rate | 0.632    |
| time/               |          |
|    episodes         | 6680     |
|    fps              | 209      |
|    time_elapsed     | 583      |
|    total_timesteps  | 122175   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0002   |
|    n_updates        | 20543    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0164  |
|    exploration_rate | 0.632    |
| time/               |          |
|    episodes         | 6684     |
|    fps              | 209      |
|    time_elapsed     | 583      |
|    total_timesteps  | 122240   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00773  |
|    n_updates        | 20559    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | -0.0159  |
|    exploration_rate | 0.631    |
| time/               |          |
|    episodes         | 6688     |
|    fps              | 209      |
|    time_elapsed     | 583      |
|    total_timesteps  | 122304   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 20575    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.0262  |
|    exploration_rate | 0.631    |
| time/               |          |
|    episodes         | 6692     |
|    fps              | 209      |
|    time_elapsed     | 583      |
|    total_timesteps  | 122389   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.61e-05 |
|    n_updates        | 20597    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | -0.0258  |
|    exploration_rate | 0.63     |
| time/               |          |
|    episodes         | 6696     |
|    fps              | 209      |
|    time_elapsed     | 583      |
|    total_timesteps  | 122457   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00717  |
|    n_updates        | 20614    |
----------------------------------
Eval num_timesteps=122500, episode_reward=-0.22 +/- 0.11
Episode length: 55.28 +/- 27.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.3     |
|    mean_reward      | -0.221   |
| rollout/            |          |
|    exploration_rate | 0.63     |
| time/               |          |
|    total_timesteps  | 122500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.45e-05 |
|    n_updates        | 20624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.0262  |
|    exploration_rate | 0.63     |
| time/               |          |
|    episodes         | 6700     |
|    fps              | 209      |
|    time_elapsed     | 585      |
|    total_timesteps  | 122533   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 20633    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | -0.016   |
|    exploration_rate | 0.629    |
| time/               |          |
|    episodes         | 6704     |
|    fps              | 209      |
|    time_elapsed     | 585      |
|    total_timesteps  | 122610   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.33e-05 |
|    n_updates        | 20652    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | -0.0159  |
|    exploration_rate | 0.629    |
| time/               |          |
|    episodes         | 6708     |
|    fps              | 209      |
|    time_elapsed     | 585      |
|    total_timesteps  | 122679   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0157   |
|    n_updates        | 20669    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0253  |
|    exploration_rate | 0.628    |
| time/               |          |
|    episodes         | 6712     |
|    fps              | 209      |
|    time_elapsed     | 585      |
|    total_timesteps  | 122749   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00772  |
|    n_updates        | 20687    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.026   |
|    exploration_rate | 0.628    |
| time/               |          |
|    episodes         | 6716     |
|    fps              | 209      |
|    time_elapsed     | 585      |
|    total_timesteps  | 122835   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.09e-05 |
|    n_updates        | 20708    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.0364  |
|    exploration_rate | 0.627    |
| time/               |          |
|    episodes         | 6720     |
|    fps              | 209      |
|    time_elapsed     | 585      |
|    total_timesteps  | 122909   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.36e-05 |
|    n_updates        | 20727    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0365  |
|    exploration_rate | 0.627    |
| time/               |          |
|    episodes         | 6724     |
|    fps              | 209      |
|    time_elapsed     | 585      |
|    total_timesteps  | 122982   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.58e-05 |
|    n_updates        | 20745    |
----------------------------------
Eval num_timesteps=123000, episode_reward=0.06 +/- 0.36
Episode length: 19.58 +/- 11.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 19.6     |
|    mean_reward      | 0.0627   |
| rollout/            |          |
|    exploration_rate | 0.627    |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 20749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.0364  |
|    exploration_rate | 0.626    |
| time/               |          |
|    episodes         | 6728     |
|    fps              | 209      |
|    time_elapsed     | 586      |
|    total_timesteps  | 123061   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00707  |
|    n_updates        | 20765    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0367  |
|    exploration_rate | 0.626    |
| time/               |          |
|    episodes         | 6732     |
|    fps              | 209      |
|    time_elapsed     | 586      |
|    total_timesteps  | 123146   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 20786    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0354  |
|    exploration_rate | 0.625    |
| time/               |          |
|    episodes         | 6736     |
|    fps              | 210      |
|    time_elapsed     | 586      |
|    total_timesteps  | 123211   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.04e-05 |
|    n_updates        | 20802    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.035   |
|    exploration_rate | 0.625    |
| time/               |          |
|    episodes         | 6740     |
|    fps              | 210      |
|    time_elapsed     | 586      |
|    total_timesteps  | 123286   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00694  |
|    n_updates        | 20821    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0347  |
|    exploration_rate | 0.624    |
| time/               |          |
|    episodes         | 6744     |
|    fps              | 210      |
|    time_elapsed     | 586      |
|    total_timesteps  | 123365   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000176 |
|    n_updates        | 20841    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0242  |
|    exploration_rate | 0.624    |
| time/               |          |
|    episodes         | 6748     |
|    fps              | 210      |
|    time_elapsed     | 586      |
|    total_timesteps  | 123451   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00713  |
|    n_updates        | 20862    |
----------------------------------
Eval num_timesteps=123500, episode_reward=-0.03 +/- 0.20
Episode length: 18.06 +/- 8.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18.1     |
|    mean_reward      | -0.0313  |
| rollout/            |          |
|    exploration_rate | 0.623    |
| time/               |          |
|    total_timesteps  | 123500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00795  |
|    n_updates        | 20874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0244  |
|    exploration_rate | 0.623    |
| time/               |          |
|    episodes         | 6752     |
|    fps              | 210      |
|    time_elapsed     | 587      |
|    total_timesteps  | 123539   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 20884    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0256  |
|    exploration_rate | 0.622    |
| time/               |          |
|    episodes         | 6756     |
|    fps              | 210      |
|    time_elapsed     | 587      |
|    total_timesteps  | 123634   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000186 |
|    n_updates        | 20908    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0254  |
|    exploration_rate | 0.622    |
| time/               |          |
|    episodes         | 6760     |
|    fps              | 210      |
|    time_elapsed     | 587      |
|    total_timesteps  | 123704   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 20925    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0145  |
|    exploration_rate | 0.622    |
| time/               |          |
|    episodes         | 6764     |
|    fps              | 210      |
|    time_elapsed     | 587      |
|    total_timesteps  | 123772   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000157 |
|    n_updates        | 20942    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.00548  |
|    exploration_rate | 0.621    |
| time/               |          |
|    episodes         | 6768     |
|    fps              | 210      |
|    time_elapsed     | 587      |
|    total_timesteps  | 123841   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00759  |
|    n_updates        | 20960    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.00468  |
|    exploration_rate | 0.62     |
| time/               |          |
|    episodes         | 6772     |
|    fps              | 210      |
|    time_elapsed     | 588      |
|    total_timesteps  | 123924   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.79e-05 |
|    n_updates        | 20980    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0153   |
|    exploration_rate | 0.62     |
| time/               |          |
|    episodes         | 6776     |
|    fps              | 210      |
|    time_elapsed     | 588      |
|    total_timesteps  | 123990   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00695  |
|    n_updates        | 20997    |
----------------------------------
Eval num_timesteps=124000, episode_reward=-0.01 +/- 0.24
Episode length: 16.94 +/- 0.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | -0.00674 |
| rollout/            |          |
|    exploration_rate | 0.62     |
| time/               |          |
|    total_timesteps  | 124000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.3e-05  |
|    n_updates        | 20999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0159   |
|    exploration_rate | 0.62     |
| time/               |          |
|    episodes         | 6780     |
|    fps              | 210      |
|    time_elapsed     | 588      |
|    total_timesteps  | 124054   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00664  |
|    n_updates        | 21013    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0355   |
|    exploration_rate | 0.619    |
| time/               |          |
|    episodes         | 6784     |
|    fps              | 210      |
|    time_elapsed     | 588      |
|    total_timesteps  | 124128   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000174 |
|    n_updates        | 21031    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.035    |
|    exploration_rate | 0.619    |
| time/               |          |
|    episodes         | 6788     |
|    fps              | 210      |
|    time_elapsed     | 588      |
|    total_timesteps  | 124204   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000255 |
|    n_updates        | 21050    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0358   |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 6792     |
|    fps              | 211      |
|    time_elapsed     | 588      |
|    total_timesteps  | 124270   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.68e-05 |
|    n_updates        | 21067    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0356   |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 6796     |
|    fps              | 211      |
|    time_elapsed     | 589      |
|    total_timesteps  | 124342   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 21085    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0458   |
|    exploration_rate | 0.617    |
| time/               |          |
|    episodes         | 6800     |
|    fps              | 211      |
|    time_elapsed     | 589      |
|    total_timesteps  | 124414   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00689  |
|    n_updates        | 21103    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0565   |
|    exploration_rate | 0.617    |
| time/               |          |
|    episodes         | 6804     |
|    fps              | 211      |
|    time_elapsed     | 589      |
|    total_timesteps  | 124474   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000154 |
|    n_updates        | 21118    |
----------------------------------
Eval num_timesteps=124500, episode_reward=-0.15 +/- 0.26
Episode length: 51.64 +/- 23.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.6     |
|    mean_reward      | -0.146   |
| rollout/            |          |
|    exploration_rate | 0.617    |
| time/               |          |
|    total_timesteps  | 124500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00677  |
|    n_updates        | 21124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0565   |
|    exploration_rate | 0.616    |
| time/               |          |
|    episodes         | 6808     |
|    fps              | 210      |
|    time_elapsed     | 590      |
|    total_timesteps  | 124541   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 21135    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0559   |
|    exploration_rate | 0.616    |
| time/               |          |
|    episodes         | 6812     |
|    fps              | 210      |
|    time_elapsed     | 590      |
|    total_timesteps  | 124627   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0071   |
|    n_updates        | 21156    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0561   |
|    exploration_rate | 0.615    |
| time/               |          |
|    episodes         | 6816     |
|    fps              | 210      |
|    time_elapsed     | 591      |
|    total_timesteps  | 124709   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.69e-05 |
|    n_updates        | 21177    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0562   |
|    exploration_rate | 0.615    |
| time/               |          |
|    episodes         | 6820     |
|    fps              | 211      |
|    time_elapsed     | 591      |
|    total_timesteps  | 124779   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.6e-05  |
|    n_updates        | 21194    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0561   |
|    exploration_rate | 0.614    |
| time/               |          |
|    episodes         | 6824     |
|    fps              | 211      |
|    time_elapsed     | 591      |
|    total_timesteps  | 124856   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000208 |
|    n_updates        | 21213    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0554   |
|    exploration_rate | 0.614    |
| time/               |          |
|    episodes         | 6828     |
|    fps              | 211      |
|    time_elapsed     | 591      |
|    total_timesteps  | 124951   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000186 |
|    n_updates        | 21237    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.613    |
| time/               |          |
|    total_timesteps  | 125000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 21249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.056    |
|    exploration_rate | 0.613    |
| time/               |          |
|    episodes         | 6832     |
|    fps              | 210      |
|    time_elapsed     | 593      |
|    total_timesteps  | 125021   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00768  |
|    n_updates        | 21255    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0454   |
|    exploration_rate | 0.612    |
| time/               |          |
|    episodes         | 6836     |
|    fps              | 210      |
|    time_elapsed     | 593      |
|    total_timesteps  | 125101   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00692  |
|    n_updates        | 21275    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0456   |
|    exploration_rate | 0.612    |
| time/               |          |
|    episodes         | 6840     |
|    fps              | 210      |
|    time_elapsed     | 593      |
|    total_timesteps  | 125172   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00686  |
|    n_updates        | 21292    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0659   |
|    exploration_rate | 0.612    |
| time/               |          |
|    episodes         | 6844     |
|    fps              | 210      |
|    time_elapsed     | 594      |
|    total_timesteps  | 125244   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 21310    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0659   |
|    exploration_rate | 0.611    |
| time/               |          |
|    episodes         | 6848     |
|    fps              | 210      |
|    time_elapsed     | 594      |
|    total_timesteps  | 125329   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0147   |
|    n_updates        | 21332    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0769   |
|    exploration_rate | 0.611    |
| time/               |          |
|    episodes         | 6852     |
|    fps              | 211      |
|    time_elapsed     | 594      |
|    total_timesteps  | 125391   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.74e-05 |
|    n_updates        | 21347    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.077    |
|    exploration_rate | 0.61     |
| time/               |          |
|    episodes         | 6856     |
|    fps              | 211      |
|    time_elapsed     | 594      |
|    total_timesteps  | 125484   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 21370    |
----------------------------------
Eval num_timesteps=125500, episode_reward=-0.12 +/- 0.17
Episode length: 35.22 +/- 20.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.2     |
|    mean_reward      | -0.12    |
| rollout/            |          |
|    exploration_rate | 0.61     |
| time/               |          |
|    total_timesteps  | 125500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.07e-05 |
|    n_updates        | 21374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.077    |
|    exploration_rate | 0.609    |
| time/               |          |
|    episodes         | 6860     |
|    fps              | 210      |
|    time_elapsed     | 595      |
|    total_timesteps  | 125555   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 21388    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0664   |
|    exploration_rate | 0.609    |
| time/               |          |
|    episodes         | 6864     |
|    fps              | 210      |
|    time_elapsed     | 595      |
|    total_timesteps  | 125639   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.1e-05  |
|    n_updates        | 21409    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0552   |
|    exploration_rate | 0.608    |
| time/               |          |
|    episodes         | 6868     |
|    fps              | 211      |
|    time_elapsed     | 595      |
|    total_timesteps  | 125736   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 21433    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0553   |
|    exploration_rate | 0.608    |
| time/               |          |
|    episodes         | 6872     |
|    fps              | 211      |
|    time_elapsed     | 595      |
|    total_timesteps  | 125817   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.71e-05 |
|    n_updates        | 21454    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0555   |
|    exploration_rate | 0.607    |
| time/               |          |
|    episodes         | 6876     |
|    fps              | 211      |
|    time_elapsed     | 595      |
|    total_timesteps  | 125878   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00686  |
|    n_updates        | 21469    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.0652   |
|    exploration_rate | 0.607    |
| time/               |          |
|    episodes         | 6880     |
|    fps              | 211      |
|    time_elapsed     | 595      |
|    total_timesteps  | 125950   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 21487    |
----------------------------------
Eval num_timesteps=126000, episode_reward=-0.18 +/- 0.16
Episode length: 49.12 +/- 19.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.1     |
|    mean_reward      | -0.176   |
| rollout/            |          |
|    exploration_rate | 0.606    |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.2e-05  |
|    n_updates        | 21499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0556   |
|    exploration_rate | 0.606    |
| time/               |          |
|    episodes         | 6884     |
|    fps              | 210      |
|    time_elapsed     | 597      |
|    total_timesteps  | 126013   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 21503    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0557   |
|    exploration_rate | 0.606    |
| time/               |          |
|    episodes         | 6888     |
|    fps              | 210      |
|    time_elapsed     | 597      |
|    total_timesteps  | 126087   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.64e-05 |
|    n_updates        | 21521    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.0552   |
|    exploration_rate | 0.605    |
| time/               |          |
|    episodes         | 6892     |
|    fps              | 211      |
|    time_elapsed     | 597      |
|    total_timesteps  | 126166   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00784  |
|    n_updates        | 21541    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.0552   |
|    exploration_rate | 0.605    |
| time/               |          |
|    episodes         | 6896     |
|    fps              | 211      |
|    time_elapsed     | 597      |
|    total_timesteps  | 126239   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00789  |
|    n_updates        | 21559    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.0451   |
|    exploration_rate | 0.604    |
| time/               |          |
|    episodes         | 6900     |
|    fps              | 211      |
|    time_elapsed     | 597      |
|    total_timesteps  | 126312   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000138 |
|    n_updates        | 21577    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0332   |
|    exploration_rate | 0.603    |
| time/               |          |
|    episodes         | 6904     |
|    fps              | 211      |
|    time_elapsed     | 597      |
|    total_timesteps  | 126420   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.42e-05 |
|    n_updates        | 21604    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.033    |
|    exploration_rate | 0.603    |
| time/               |          |
|    episodes         | 6908     |
|    fps              | 211      |
|    time_elapsed     | 598      |
|    total_timesteps  | 126491   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 21622    |
----------------------------------
Eval num_timesteps=126500, episode_reward=0.01 +/- 0.27
Episode length: 17.80 +/- 0.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.8     |
|    mean_reward      | 0.00984  |
| rollout/            |          |
|    exploration_rate | 0.603    |
| time/               |          |
|    total_timesteps  | 126500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 21624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0333   |
|    exploration_rate | 0.602    |
| time/               |          |
|    episodes         | 6912     |
|    fps              | 211      |
|    time_elapsed     | 598      |
|    total_timesteps  | 126571   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000165 |
|    n_updates        | 21642    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0339   |
|    exploration_rate | 0.602    |
| time/               |          |
|    episodes         | 6916     |
|    fps              | 211      |
|    time_elapsed     | 598      |
|    total_timesteps  | 126637   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00695  |
|    n_updates        | 21659    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0436   |
|    exploration_rate | 0.601    |
| time/               |          |
|    episodes         | 6920     |
|    fps              | 211      |
|    time_elapsed     | 598      |
|    total_timesteps  | 126715   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.9e-05  |
|    n_updates        | 21678    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0439   |
|    exploration_rate | 0.601    |
| time/               |          |
|    episodes         | 6924     |
|    fps              | 211      |
|    time_elapsed     | 598      |
|    total_timesteps  | 126785   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 21696    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0445   |
|    exploration_rate | 0.6      |
| time/               |          |
|    episodes         | 6928     |
|    fps              | 211      |
|    time_elapsed     | 599      |
|    total_timesteps  | 126864   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00696  |
|    n_updates        | 21715    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0438   |
|    exploration_rate | 0.6      |
| time/               |          |
|    episodes         | 6932     |
|    fps              | 211      |
|    time_elapsed     | 599      |
|    total_timesteps  | 126951   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000138 |
|    n_updates        | 21737    |
----------------------------------
Eval num_timesteps=127000, episode_reward=0.02 +/- 0.31
Episode length: 19.10 +/- 11.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 19.1     |
|    mean_reward      | 0.0246   |
| rollout/            |          |
|    exploration_rate | 0.599    |
| time/               |          |
|    total_timesteps  | 127000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000133 |
|    n_updates        | 21749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.044    |
|    exploration_rate | 0.599    |
| time/               |          |
|    episodes         | 6936     |
|    fps              | 211      |
|    time_elapsed     | 599      |
|    total_timesteps  | 127028   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.83e-05 |
|    n_updates        | 21756    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0743   |
|    exploration_rate | 0.599    |
| time/               |          |
|    episodes         | 6940     |
|    fps              | 211      |
|    time_elapsed     | 599      |
|    total_timesteps  | 127091   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00691  |
|    n_updates        | 21772    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0545   |
|    exploration_rate | 0.598    |
| time/               |          |
|    episodes         | 6944     |
|    fps              | 211      |
|    time_elapsed     | 599      |
|    total_timesteps  | 127159   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.21e-05 |
|    n_updates        | 21789    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.054    |
|    exploration_rate | 0.598    |
| time/               |          |
|    episodes         | 6948     |
|    fps              | 212      |
|    time_elapsed     | 600      |
|    total_timesteps  | 127255   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.29e-05 |
|    n_updates        | 21813    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0439   |
|    exploration_rate | 0.597    |
| time/               |          |
|    episodes         | 6952     |
|    fps              | 212      |
|    time_elapsed     | 600      |
|    total_timesteps  | 127320   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0154   |
|    n_updates        | 21829    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0447   |
|    exploration_rate | 0.597    |
| time/               |          |
|    episodes         | 6956     |
|    fps              | 212      |
|    time_elapsed     | 600      |
|    total_timesteps  | 127394   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0133   |
|    n_updates        | 21848    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0342   |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 6960     |
|    fps              | 212      |
|    time_elapsed     | 600      |
|    total_timesteps  | 127476   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0078   |
|    n_updates        | 21868    |
----------------------------------
Eval num_timesteps=127500, episode_reward=-0.30 +/- 0.02
Episode length: 73.82 +/- 5.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.8     |
|    mean_reward      | -0.295   |
| rollout/            |          |
|    exploration_rate | 0.596    |
| time/               |          |
|    total_timesteps  | 127500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.73e-05 |
|    n_updates        | 21874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0448   |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 6964     |
|    fps              | 211      |
|    time_elapsed     | 602      |
|    total_timesteps  | 127545   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00689  |
|    n_updates        | 21886    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0358   |
|    exploration_rate | 0.595    |
| time/               |          |
|    episodes         | 6968     |
|    fps              | 211      |
|    time_elapsed     | 602      |
|    total_timesteps  | 127619   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00782  |
|    n_updates        | 21904    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0258   |
|    exploration_rate | 0.595    |
| time/               |          |
|    episodes         | 6972     |
|    fps              | 211      |
|    time_elapsed     | 602      |
|    total_timesteps  | 127698   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00779  |
|    n_updates        | 21924    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0147   |
|    exploration_rate | 0.594    |
| time/               |          |
|    episodes         | 6976     |
|    fps              | 211      |
|    time_elapsed     | 602      |
|    total_timesteps  | 127787   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00683  |
|    n_updates        | 21946    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.00432  |
|    exploration_rate | 0.593    |
| time/               |          |
|    episodes         | 6980     |
|    fps              | 212      |
|    time_elapsed     | 603      |
|    total_timesteps  | 127869   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000182 |
|    n_updates        | 21967    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.00657 |
|    exploration_rate | 0.593    |
| time/               |          |
|    episodes         | 6984     |
|    fps              | 212      |
|    time_elapsed     | 603      |
|    total_timesteps  | 127954   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 21988    |
----------------------------------
Eval num_timesteps=128000, episode_reward=-0.05 +/- 0.14
Episode length: 18.08 +/- 0.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18.1     |
|    mean_reward      | -0.0513  |
| rollout/            |          |
|    exploration_rate | 0.593    |
| time/               |          |
|    total_timesteps  | 128000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000178 |
|    n_updates        | 21999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.00303  |
|    exploration_rate | 0.592    |
| time/               |          |
|    episodes         | 6988     |
|    fps              | 212      |
|    time_elapsed     | 603      |
|    total_timesteps  | 128038   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000178 |
|    n_updates        | 22009    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.00287  |
|    exploration_rate | 0.592    |
| time/               |          |
|    episodes         | 6992     |
|    fps              | 212      |
|    time_elapsed     | 603      |
|    total_timesteps  | 128121   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.46e-05 |
|    n_updates        | 22030    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.00227  |
|    exploration_rate | 0.591    |
| time/               |          |
|    episodes         | 6996     |
|    fps              | 212      |
|    time_elapsed     | 604      |
|    total_timesteps  | 128209   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00666  |
|    n_updates        | 22052    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0116   |
|    exploration_rate | 0.59     |
| time/               |          |
|    episodes         | 7000     |
|    fps              | 212      |
|    time_elapsed     | 604      |
|    total_timesteps  | 128298   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 22074    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0025   |
|    exploration_rate | 0.59     |
| time/               |          |
|    episodes         | 7004     |
|    fps              | 212      |
|    time_elapsed     | 604      |
|    total_timesteps  | 128384   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0207   |
|    n_updates        | 22095    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.00126  |
|    exploration_rate | 0.589    |
| time/               |          |
|    episodes         | 7008     |
|    fps              | 212      |
|    time_elapsed     | 604      |
|    total_timesteps  | 128486   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 22121    |
----------------------------------
Eval num_timesteps=128500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.589    |
| time/               |          |
|    total_timesteps  | 128500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000136 |
|    n_updates        | 22124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20       |
|    ep_rew_mean      | 0.00094  |
|    exploration_rate | 0.589    |
| time/               |          |
|    episodes         | 7012     |
|    fps              | 211      |
|    time_elapsed     | 606      |
|    total_timesteps  | 128574   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 22143    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.0107   |
|    exploration_rate | 0.588    |
| time/               |          |
|    episodes         | 7016     |
|    fps              | 211      |
|    time_elapsed     | 606      |
|    total_timesteps  | 128645   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000174 |
|    n_updates        | 22161    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0114   |
|    exploration_rate | 0.588    |
| time/               |          |
|    episodes         | 7020     |
|    fps              | 212      |
|    time_elapsed     | 606      |
|    total_timesteps  | 128707   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00684  |
|    n_updates        | 22176    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0215   |
|    exploration_rate | 0.587    |
| time/               |          |
|    episodes         | 7024     |
|    fps              | 212      |
|    time_elapsed     | 607      |
|    total_timesteps  | 128775   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00703  |
|    n_updates        | 22193    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.0206   |
|    exploration_rate | 0.586    |
| time/               |          |
|    episodes         | 7028     |
|    fps              | 212      |
|    time_elapsed     | 607      |
|    total_timesteps  | 128876   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.82e-05 |
|    n_updates        | 22218    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0213   |
|    exploration_rate | 0.586    |
| time/               |          |
|    episodes         | 7032     |
|    fps              | 212      |
|    time_elapsed     | 607      |
|    total_timesteps  | 128946   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.64e-05 |
|    n_updates        | 22236    |
----------------------------------
Eval num_timesteps=129000, episode_reward=0.01 +/- 0.27
Episode length: 16.86 +/- 0.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | 0.0135   |
| rollout/            |          |
|    exploration_rate | 0.586    |
| time/               |          |
|    total_timesteps  | 129000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000136 |
|    n_updates        | 22249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0214   |
|    exploration_rate | 0.585    |
| time/               |          |
|    episodes         | 7036     |
|    fps              | 212      |
|    time_elapsed     | 607      |
|    total_timesteps  | 129020   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.15e-05 |
|    n_updates        | 22254    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.0108   |
|    exploration_rate | 0.585    |
| time/               |          |
|    episodes         | 7040     |
|    fps              | 212      |
|    time_elapsed     | 607      |
|    total_timesteps  | 129098   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000136 |
|    n_updates        | 22274    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.0105   |
|    exploration_rate | 0.584    |
| time/               |          |
|    episodes         | 7044     |
|    fps              | 212      |
|    time_elapsed     | 607      |
|    total_timesteps  | 129174   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000182 |
|    n_updates        | 22293    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20       |
|    ep_rew_mean      | 0.00103  |
|    exploration_rate | 0.584    |
| time/               |          |
|    episodes         | 7048     |
|    fps              | 212      |
|    time_elapsed     | 608      |
|    total_timesteps  | 129255   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.99e-05 |
|    n_updates        | 22313    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.00051  |
|    exploration_rate | 0.583    |
| time/               |          |
|    episodes         | 7052     |
|    fps              | 212      |
|    time_elapsed     | 608      |
|    total_timesteps  | 129333   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00676  |
|    n_updates        | 22333    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.00067  |
|    exploration_rate | 0.583    |
| time/               |          |
|    episodes         | 7056     |
|    fps              | 212      |
|    time_elapsed     | 608      |
|    total_timesteps  | 129403   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00682  |
|    n_updates        | 22350    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20       |
|    ep_rew_mean      | 0.00099  |
|    exploration_rate | 0.582    |
| time/               |          |
|    episodes         | 7060     |
|    fps              | 212      |
|    time_elapsed     | 608      |
|    total_timesteps  | 129477   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.23e-05 |
|    n_updates        | 22369    |
----------------------------------
Eval num_timesteps=129500, episode_reward=-0.01 +/- 0.24
Episode length: 17.62 +/- 0.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.6     |
|    mean_reward      | -0.0095  |
| rollout/            |          |
|    exploration_rate | 0.582    |
| time/               |          |
|    total_timesteps  | 129500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00691  |
|    n_updates        | 22374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | -0.00951 |
|    exploration_rate | 0.582    |
| time/               |          |
|    episodes         | 7064     |
|    fps              | 212      |
|    time_elapsed     | 608      |
|    total_timesteps  | 129558   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.48e-05 |
|    n_updates        | 22389    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.2     |
|    ep_rew_mean      | -0.00987 |
|    exploration_rate | 0.581    |
| time/               |          |
|    episodes         | 7068     |
|    fps              | 212      |
|    time_elapsed     | 609      |
|    total_timesteps  | 129641   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.41e-05 |
|    n_updates        | 22410    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.4     |
|    ep_rew_mean      | -0.0106  |
|    exploration_rate | 0.58     |
| time/               |          |
|    episodes         | 7072     |
|    fps              | 212      |
|    time_elapsed     | 609      |
|    total_timesteps  | 129738   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0144   |
|    n_updates        | 22434    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.2     |
|    ep_rew_mean      | -0.00979 |
|    exploration_rate | 0.58     |
| time/               |          |
|    episodes         | 7076     |
|    fps              | 213      |
|    time_elapsed     | 609      |
|    total_timesteps  | 129807   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000214 |
|    n_updates        | 22451    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20       |
|    ep_rew_mean      | 0.011    |
|    exploration_rate | 0.579    |
| time/               |          |
|    episodes         | 7080     |
|    fps              | 213      |
|    time_elapsed     | 609      |
|    total_timesteps  | 129871   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00665  |
|    n_updates        | 22467    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | 0.0118   |
|    exploration_rate | 0.579    |
| time/               |          |
|    episodes         | 7084     |
|    fps              | 213      |
|    time_elapsed     | 609      |
|    total_timesteps  | 129936   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000201 |
|    n_updates        | 22483    |
----------------------------------
Eval num_timesteps=130000, episode_reward=0.03 +/- 0.30
Episode length: 17.00 +/- 0.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17       |
|    mean_reward      | 0.033    |
| rollout/            |          |
|    exploration_rate | 0.579    |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0138   |
|    n_updates        | 22499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.00212  |
|    exploration_rate | 0.578    |
| time/               |          |
|    episodes         | 7088     |
|    fps              | 213      |
|    time_elapsed     | 609      |
|    total_timesteps  | 130011   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000217 |
|    n_updates        | 22502    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0022   |
|    exploration_rate | 0.578    |
| time/               |          |
|    episodes         | 7092     |
|    fps              | 213      |
|    time_elapsed     | 610      |
|    total_timesteps  | 130092   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.81e-05 |
|    n_updates        | 22522    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0028   |
|    exploration_rate | 0.577    |
| time/               |          |
|    episodes         | 7096     |
|    fps              | 213      |
|    time_elapsed     | 610      |
|    total_timesteps  | 130165   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 22541    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.00643 |
|    exploration_rate | 0.577    |
| time/               |          |
|    episodes         | 7100     |
|    fps              | 213      |
|    time_elapsed     | 610      |
|    total_timesteps  | 130235   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0066   |
|    n_updates        | 22558    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.00607 |
|    exploration_rate | 0.576    |
| time/               |          |
|    episodes         | 7104     |
|    fps              | 213      |
|    time_elapsed     | 610      |
|    total_timesteps  | 130312   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00681  |
|    n_updates        | 22577    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.00498  |
|    exploration_rate | 0.576    |
| time/               |          |
|    episodes         | 7108     |
|    fps              | 213      |
|    time_elapsed     | 610      |
|    total_timesteps  | 130388   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 22596    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.00554  |
|    exploration_rate | 0.575    |
| time/               |          |
|    episodes         | 7112     |
|    fps              | 213      |
|    time_elapsed     | 610      |
|    total_timesteps  | 130462   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 22615    |
----------------------------------
Eval num_timesteps=130500, episode_reward=0.05 +/- 0.33
Episode length: 16.68 +/- 0.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.7     |
|    mean_reward      | 0.0543   |
| rollout/            |          |
|    exploration_rate | 0.575    |
| time/               |          |
|    total_timesteps  | 130500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.1e-05  |
|    n_updates        | 22624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.00566  |
|    exploration_rate | 0.575    |
| time/               |          |
|    episodes         | 7116     |
|    fps              | 213      |
|    time_elapsed     | 611      |
|    total_timesteps  | 130530   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.78e-05 |
|    n_updates        | 22632    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.00492 |
|    exploration_rate | 0.574    |
| time/               |          |
|    episodes         | 7120     |
|    fps              | 213      |
|    time_elapsed     | 611      |
|    total_timesteps  | 130606   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000136 |
|    n_updates        | 22651    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.015   |
|    exploration_rate | 0.574    |
| time/               |          |
|    episodes         | 7124     |
|    fps              | 213      |
|    time_elapsed     | 611      |
|    total_timesteps  | 130676   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 22668    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0142  |
|    exploration_rate | 0.573    |
| time/               |          |
|    episodes         | 7128     |
|    fps              | 213      |
|    time_elapsed     | 611      |
|    total_timesteps  | 130757   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0071   |
|    n_updates        | 22689    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.00397 |
|    exploration_rate | 0.573    |
| time/               |          |
|    episodes         | 7132     |
|    fps              | 213      |
|    time_elapsed     | 611      |
|    total_timesteps  | 130821   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00686  |
|    n_updates        | 22705    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.00377 |
|    exploration_rate | 0.572    |
| time/               |          |
|    episodes         | 7136     |
|    fps              | 214      |
|    time_elapsed     | 611      |
|    total_timesteps  | 130890   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0155   |
|    n_updates        | 22722    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0234  |
|    exploration_rate | 0.572    |
| time/               |          |
|    episodes         | 7140     |
|    fps              | 214      |
|    time_elapsed     | 611      |
|    total_timesteps  | 130958   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000141 |
|    n_updates        | 22739    |
----------------------------------
Eval num_timesteps=131000, episode_reward=-0.05 +/- 0.30
Episode length: 33.10 +/- 24.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.1     |
|    mean_reward      | -0.0515  |
| rollout/            |          |
|    exploration_rate | 0.572    |
| time/               |          |
|    total_timesteps  | 131000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 22749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0233  |
|    exploration_rate | 0.571    |
| time/               |          |
|    episodes         | 7144     |
|    fps              | 213      |
|    time_elapsed     | 612      |
|    total_timesteps  | 131032   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 22757    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0229  |
|    exploration_rate | 0.571    |
| time/               |          |
|    episodes         | 7148     |
|    fps              | 213      |
|    time_elapsed     | 612      |
|    total_timesteps  | 131102   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 22775    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.0231  |
|    exploration_rate | 0.57     |
| time/               |          |
|    episodes         | 7152     |
|    fps              | 214      |
|    time_elapsed     | 612      |
|    total_timesteps  | 131185   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.94e-05 |
|    n_updates        | 22796    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0235  |
|    exploration_rate | 0.57     |
| time/               |          |
|    episodes         | 7156     |
|    fps              | 214      |
|    time_elapsed     | 612      |
|    total_timesteps  | 131267   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00675  |
|    n_updates        | 22816    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0237  |
|    exploration_rate | 0.569    |
| time/               |          |
|    episodes         | 7160     |
|    fps              | 214      |
|    time_elapsed     | 612      |
|    total_timesteps  | 131345   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000149 |
|    n_updates        | 22836    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0234  |
|    exploration_rate | 0.569    |
| time/               |          |
|    episodes         | 7164     |
|    fps              | 214      |
|    time_elapsed     | 613      |
|    total_timesteps  | 131418   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0079   |
|    n_updates        | 22854    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00281 |
|    exploration_rate | 0.568    |
| time/               |          |
|    episodes         | 7168     |
|    fps              | 214      |
|    time_elapsed     | 613      |
|    total_timesteps  | 131488   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00672  |
|    n_updates        | 22871    |
----------------------------------
Eval num_timesteps=131500, episode_reward=-0.09 +/- 0.35
Episode length: 52.42 +/- 19.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.4     |
|    mean_reward      | -0.0888  |
| rollout/            |          |
|    exploration_rate | 0.568    |
| time/               |          |
|    total_timesteps  | 131500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00673  |
|    n_updates        | 22874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00169 |
|    exploration_rate | 0.568    |
| time/               |          |
|    episodes         | 7172     |
|    fps              | 213      |
|    time_elapsed     | 614      |
|    total_timesteps  | 131557   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.75e-05 |
|    n_updates        | 22889    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00145 |
|    exploration_rate | 0.567    |
| time/               |          |
|    episodes         | 7176     |
|    fps              | 214      |
|    time_elapsed     | 614      |
|    total_timesteps  | 131620   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00696  |
|    n_updates        | 22904    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0219  |
|    exploration_rate | 0.567    |
| time/               |          |
|    episodes         | 7180     |
|    fps              | 214      |
|    time_elapsed     | 615      |
|    total_timesteps  | 131695   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.97e-05 |
|    n_updates        | 22923    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.00171 |
|    exploration_rate | 0.566    |
| time/               |          |
|    episodes         | 7184     |
|    fps              | 214      |
|    time_elapsed     | 615      |
|    total_timesteps  | 131755   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000192 |
|    n_updates        | 22938    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00215 |
|    exploration_rate | 0.566    |
| time/               |          |
|    episodes         | 7188     |
|    fps              | 214      |
|    time_elapsed     | 615      |
|    total_timesteps  | 131841   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000203 |
|    n_updates        | 22960    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00263 |
|    exploration_rate | 0.565    |
| time/               |          |
|    episodes         | 7192     |
|    fps              | 214      |
|    time_elapsed     | 615      |
|    total_timesteps  | 131934   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.86e-05 |
|    n_updates        | 22983    |
----------------------------------
Eval num_timesteps=132000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.564    |
| time/               |          |
|    total_timesteps  | 132000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000214 |
|    n_updates        | 22999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00283 |
|    exploration_rate | 0.564    |
| time/               |          |
|    episodes         | 7196     |
|    fps              | 213      |
|    time_elapsed     | 617      |
|    total_timesteps  | 132012   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 23002    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.00299 |
|    exploration_rate | 0.564    |
| time/               |          |
|    episodes         | 7200     |
|    fps              | 213      |
|    time_elapsed     | 617      |
|    total_timesteps  | 132086   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 23021    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0175   |
|    exploration_rate | 0.563    |
| time/               |          |
|    episodes         | 7204     |
|    fps              | 213      |
|    time_elapsed     | 617      |
|    total_timesteps  | 132150   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 23037    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.00717  |
|    exploration_rate | 0.563    |
| time/               |          |
|    episodes         | 7208     |
|    fps              | 213      |
|    time_elapsed     | 618      |
|    total_timesteps  | 132235   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00673  |
|    n_updates        | 23058    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00737  |
|    exploration_rate | 0.562    |
| time/               |          |
|    episodes         | 7212     |
|    fps              | 214      |
|    time_elapsed     | 618      |
|    total_timesteps  | 132304   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00673  |
|    n_updates        | 23075    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.00258 |
|    exploration_rate | 0.562    |
| time/               |          |
|    episodes         | 7216     |
|    fps              | 214      |
|    time_elapsed     | 618      |
|    total_timesteps  | 132371   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00678  |
|    n_updates        | 23092    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.00218 |
|    exploration_rate | 0.561    |
| time/               |          |
|    episodes         | 7220     |
|    fps              | 214      |
|    time_elapsed     | 618      |
|    total_timesteps  | 132437   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000176 |
|    n_updates        | 23109    |
----------------------------------
Eval num_timesteps=132500, episode_reward=0.03 +/- 0.30
Episode length: 16.84 +/- 0.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.8     |
|    mean_reward      | 0.0337   |
| rollout/            |          |
|    exploration_rate | 0.561    |
| time/               |          |
|    total_timesteps  | 132500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00683  |
|    n_updates        | 23124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.00759  |
|    exploration_rate | 0.561    |
| time/               |          |
|    episodes         | 7224     |
|    fps              | 214      |
|    time_elapsed     | 618      |
|    total_timesteps  | 132513   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 23128    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00795  |
|    exploration_rate | 0.56     |
| time/               |          |
|    episodes         | 7228     |
|    fps              | 214      |
|    time_elapsed     | 618      |
|    total_timesteps  | 132585   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.12e-05 |
|    n_updates        | 23146    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0281   |
|    exploration_rate | 0.56     |
| time/               |          |
|    episodes         | 7232     |
|    fps              | 214      |
|    time_elapsed     | 619      |
|    total_timesteps  | 132646   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00685  |
|    n_updates        | 23161    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.028    |
|    exploration_rate | 0.559    |
| time/               |          |
|    episodes         | 7236     |
|    fps              | 214      |
|    time_elapsed     | 619      |
|    total_timesteps  | 132716   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 23178    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.028    |
|    exploration_rate | 0.559    |
| time/               |          |
|    episodes         | 7240     |
|    fps              | 214      |
|    time_elapsed     | 619      |
|    total_timesteps  | 132784   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00697  |
|    n_updates        | 23195    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0276   |
|    exploration_rate | 0.558    |
| time/               |          |
|    episodes         | 7244     |
|    fps              | 214      |
|    time_elapsed     | 619      |
|    total_timesteps  | 132868   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00774  |
|    n_updates        | 23216    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0277   |
|    exploration_rate | 0.558    |
| time/               |          |
|    episodes         | 7248     |
|    fps              | 214      |
|    time_elapsed     | 619      |
|    total_timesteps  | 132937   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 23234    |
----------------------------------
Eval num_timesteps=133000, episode_reward=0.10 +/- 0.42
Episode length: 29.66 +/- 16.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 29.7     |
|    mean_reward      | 0.102    |
| rollout/            |          |
|    exploration_rate | 0.557    |
| time/               |          |
|    total_timesteps  | 133000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000227 |
|    n_updates        | 23249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0277   |
|    exploration_rate | 0.557    |
| time/               |          |
|    episodes         | 7252     |
|    fps              | 214      |
|    time_elapsed     | 620      |
|    total_timesteps  | 133020   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.73e-05 |
|    n_updates        | 23254    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0276   |
|    exploration_rate | 0.557    |
| time/               |          |
|    episodes         | 7256     |
|    fps              | 214      |
|    time_elapsed     | 620      |
|    total_timesteps  | 133105   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 23276    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.028    |
|    exploration_rate | 0.556    |
| time/               |          |
|    episodes         | 7260     |
|    fps              | 214      |
|    time_elapsed     | 620      |
|    total_timesteps  | 133171   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 23292    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.028    |
|    exploration_rate | 0.556    |
| time/               |          |
|    episodes         | 7264     |
|    fps              | 214      |
|    time_elapsed     | 620      |
|    total_timesteps  | 133244   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.77e-05 |
|    n_updates        | 23310    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.00799  |
|    exploration_rate | 0.555    |
| time/               |          |
|    episodes         | 7268     |
|    fps              | 214      |
|    time_elapsed     | 620      |
|    total_timesteps  | 133314   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.19e-05 |
|    n_updates        | 23328    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0179   |
|    exploration_rate | 0.555    |
| time/               |          |
|    episodes         | 7272     |
|    fps              | 214      |
|    time_elapsed     | 620      |
|    total_timesteps  | 133386   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.36e-05 |
|    n_updates        | 23346    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0178   |
|    exploration_rate | 0.554    |
| time/               |          |
|    episodes         | 7276     |
|    fps              | 214      |
|    time_elapsed     | 620      |
|    total_timesteps  | 133452   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000154 |
|    n_updates        | 23362    |
----------------------------------
Eval num_timesteps=133500, episode_reward=-0.09 +/- 0.25
Episode length: 36.88 +/- 18.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.9     |
|    mean_reward      | -0.0865  |
| rollout/            |          |
|    exploration_rate | 0.554    |
| time/               |          |
|    total_timesteps  | 133500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 23374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0179   |
|    exploration_rate | 0.554    |
| time/               |          |
|    episodes         | 7280     |
|    fps              | 214      |
|    time_elapsed     | 622      |
|    total_timesteps  | 133523   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.05e-05 |
|    n_updates        | 23380    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0074   |
|    exploration_rate | 0.553    |
| time/               |          |
|    episodes         | 7284     |
|    fps              | 214      |
|    time_elapsed     | 622      |
|    total_timesteps  | 133596   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.45e-05 |
|    n_updates        | 23398    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.00808  |
|    exploration_rate | 0.553    |
| time/               |          |
|    episodes         | 7288     |
|    fps              | 214      |
|    time_elapsed     | 622      |
|    total_timesteps  | 133665   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.07e-05 |
|    n_updates        | 23416    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0193   |
|    exploration_rate | 0.552    |
| time/               |          |
|    episodes         | 7292     |
|    fps              | 214      |
|    time_elapsed     | 622      |
|    total_timesteps  | 133729   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000254 |
|    n_updates        | 23432    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | 0.0196   |
|    exploration_rate | 0.552    |
| time/               |          |
|    episodes         | 7296     |
|    fps              | 214      |
|    time_elapsed     | 622      |
|    total_timesteps  | 133799   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00696  |
|    n_updates        | 23449    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.8     |
|    ep_rew_mean      | 0.0199   |
|    exploration_rate | 0.551    |
| time/               |          |
|    episodes         | 7300     |
|    fps              | 215      |
|    time_elapsed     | 622      |
|    total_timesteps  | 133865   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 23466    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.00104 |
|    exploration_rate | 0.55     |
| time/               |          |
|    episodes         | 7304     |
|    fps              | 215      |
|    time_elapsed     | 622      |
|    total_timesteps  | 133952   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.94e-05 |
|    n_updates        | 23487    |
----------------------------------
Eval num_timesteps=134000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.55     |
| time/               |          |
|    total_timesteps  | 134000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0078   |
|    n_updates        | 23499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 17.9     |
|    ep_rew_mean      | -0.00064 |
|    exploration_rate | 0.55     |
| time/               |          |
|    episodes         | 7308     |
|    fps              | 214      |
|    time_elapsed     | 625      |
|    total_timesteps  | 134027   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 23506    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.001   |
|    exploration_rate | 0.549    |
| time/               |          |
|    episodes         | 7312     |
|    fps              | 214      |
|    time_elapsed     | 625      |
|    total_timesteps  | 134105   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000207 |
|    n_updates        | 23526    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00116 |
|    exploration_rate | 0.549    |
| time/               |          |
|    episodes         | 7316     |
|    fps              | 214      |
|    time_elapsed     | 625      |
|    total_timesteps  | 134176   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.38e-05 |
|    n_updates        | 23543    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.00128 |
|    exploration_rate | 0.548    |
| time/               |          |
|    episodes         | 7320     |
|    fps              | 214      |
|    time_elapsed     | 625      |
|    total_timesteps  | 134245   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 23561    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | -0.0111  |
|    exploration_rate | 0.548    |
| time/               |          |
|    episodes         | 7324     |
|    fps              | 214      |
|    time_elapsed     | 625      |
|    total_timesteps  | 134316   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.06e-05 |
|    n_updates        | 23578    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0112  |
|    exploration_rate | 0.547    |
| time/               |          |
|    episodes         | 7328     |
|    fps              | 214      |
|    time_elapsed     | 625      |
|    total_timesteps  | 134392   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00692  |
|    n_updates        | 23597    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | -0.0414  |
|    exploration_rate | 0.547    |
| time/               |          |
|    episodes         | 7332     |
|    fps              | 214      |
|    time_elapsed     | 625      |
|    total_timesteps  | 134457   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.8e-05  |
|    n_updates        | 23614    |
----------------------------------
Eval num_timesteps=134500, episode_reward=-0.27 +/- 0.07
Episode length: 68.38 +/- 17.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.4     |
|    mean_reward      | -0.273   |
| rollout/            |          |
|    exploration_rate | 0.547    |
| time/               |          |
|    total_timesteps  | 134500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.18e-05 |
|    n_updates        | 23624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0417  |
|    exploration_rate | 0.546    |
| time/               |          |
|    episodes         | 7336     |
|    fps              | 214      |
|    time_elapsed     | 627      |
|    total_timesteps  | 134535   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.05e-05 |
|    n_updates        | 23633    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0425  |
|    exploration_rate | 0.546    |
| time/               |          |
|    episodes         | 7340     |
|    fps              | 214      |
|    time_elapsed     | 627      |
|    total_timesteps  | 134623   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 23655    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.042   |
|    exploration_rate | 0.545    |
| time/               |          |
|    episodes         | 7344     |
|    fps              | 214      |
|    time_elapsed     | 627      |
|    total_timesteps  | 134694   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.7e-05  |
|    n_updates        | 23673    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0424  |
|    exploration_rate | 0.545    |
| time/               |          |
|    episodes         | 7348     |
|    fps              | 214      |
|    time_elapsed     | 628      |
|    total_timesteps  | 134773   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.83e-05 |
|    n_updates        | 23693    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.042   |
|    exploration_rate | 0.544    |
| time/               |          |
|    episodes         | 7352     |
|    fps              | 214      |
|    time_elapsed     | 628      |
|    total_timesteps  | 134846   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0142   |
|    n_updates        | 23711    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.0419  |
|    exploration_rate | 0.543    |
| time/               |          |
|    episodes         | 7356     |
|    fps              | 214      |
|    time_elapsed     | 628      |
|    total_timesteps  | 134929   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 23732    |
----------------------------------
Eval num_timesteps=135000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.543    |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000223 |
|    n_updates        | 23749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | -0.0424  |
|    exploration_rate | 0.543    |
| time/               |          |
|    episodes         | 7360     |
|    fps              | 214      |
|    time_elapsed     | 630      |
|    total_timesteps  | 135007   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.52e-05 |
|    n_updates        | 23751    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | -0.032   |
|    exploration_rate | 0.542    |
| time/               |          |
|    episodes         | 7364     |
|    fps              | 214      |
|    time_elapsed     | 630      |
|    total_timesteps  | 135070   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00702  |
|    n_updates        | 23767    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | -0.022   |
|    exploration_rate | 0.542    |
| time/               |          |
|    episodes         | 7368     |
|    fps              | 214      |
|    time_elapsed     | 630      |
|    total_timesteps  | 135139   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.17e-05 |
|    n_updates        | 23784    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | -0.033   |
|    exploration_rate | 0.541    |
| time/               |          |
|    episodes         | 7372     |
|    fps              | 214      |
|    time_elapsed     | 630      |
|    total_timesteps  | 135236   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.28e-05 |
|    n_updates        | 23808    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | -0.0334  |
|    exploration_rate | 0.541    |
| time/               |          |
|    episodes         | 7376     |
|    fps              | 214      |
|    time_elapsed     | 631      |
|    total_timesteps  | 135314   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000148 |
|    n_updates        | 23828    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0345  |
|    exploration_rate | 0.54     |
| time/               |          |
|    episodes         | 7380     |
|    fps              | 214      |
|    time_elapsed     | 631      |
|    total_timesteps  | 135412   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000176 |
|    n_updates        | 23852    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.0349  |
|    exploration_rate | 0.539    |
| time/               |          |
|    episodes         | 7384     |
|    fps              | 214      |
|    time_elapsed     | 631      |
|    total_timesteps  | 135493   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 23873    |
----------------------------------
Eval num_timesteps=135500, episode_reward=0.27 +/- 0.51
Episode length: 26.92 +/- 12.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 26.9     |
|    mean_reward      | 0.274    |
| rollout/            |          |
|    exploration_rate | 0.539    |
| time/               |          |
|    total_timesteps  | 135500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 23874    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0353  |
|    exploration_rate | 0.539    |
| time/               |          |
|    episodes         | 7388     |
|    fps              | 214      |
|    time_elapsed     | 632      |
|    total_timesteps  | 135573   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0068   |
|    n_updates        | 23893    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0454  |
|    exploration_rate | 0.538    |
| time/               |          |
|    episodes         | 7392     |
|    fps              | 214      |
|    time_elapsed     | 632      |
|    total_timesteps  | 135638   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00691  |
|    n_updates        | 23909    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | -0.0456  |
|    exploration_rate | 0.538    |
| time/               |          |
|    episodes         | 7396     |
|    fps              | 214      |
|    time_elapsed     | 632      |
|    total_timesteps  | 135715   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.37e-05 |
|    n_updates        | 23928    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | -0.0357  |
|    exploration_rate | 0.537    |
| time/               |          |
|    episodes         | 7400     |
|    fps              | 214      |
|    time_elapsed     | 632      |
|    total_timesteps  | 135782   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00683  |
|    n_updates        | 23945    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.0251  |
|    exploration_rate | 0.537    |
| time/               |          |
|    episodes         | 7404     |
|    fps              | 214      |
|    time_elapsed     | 632      |
|    total_timesteps  | 135854   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.68e-05 |
|    n_updates        | 23963    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.0249  |
|    exploration_rate | 0.536    |
| time/               |          |
|    episodes         | 7408     |
|    fps              | 214      |
|    time_elapsed     | 632      |
|    total_timesteps  | 135923   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 23980    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0143  |
|    exploration_rate | 0.536    |
| time/               |          |
|    episodes         | 7412     |
|    fps              | 214      |
|    time_elapsed     | 632      |
|    total_timesteps  | 135987   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.55e-05 |
|    n_updates        | 23996    |
----------------------------------
Eval num_timesteps=136000, episode_reward=-0.30 +/- 0.01
Episode length: 74.62 +/- 1.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.6     |
|    mean_reward      | -0.298   |
| rollout/            |          |
|    exploration_rate | 0.536    |
| time/               |          |
|    total_timesteps  | 136000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.78e-05 |
|    n_updates        | 23999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.00435 |
|    exploration_rate | 0.535    |
| time/               |          |
|    episodes         | 7416     |
|    fps              | 214      |
|    time_elapsed     | 635      |
|    total_timesteps  | 136060   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.74e-05 |
|    n_updates        | 24014    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.00507 |
|    exploration_rate | 0.535    |
| time/               |          |
|    episodes         | 7420     |
|    fps              | 214      |
|    time_elapsed     | 635      |
|    total_timesteps  | 136147   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 24036    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.00443  |
|    exploration_rate | 0.534    |
| time/               |          |
|    episodes         | 7424     |
|    fps              | 214      |
|    time_elapsed     | 635      |
|    total_timesteps  | 136231   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 24057    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.014    |
|    exploration_rate | 0.533    |
| time/               |          |
|    episodes         | 7428     |
|    fps              | 214      |
|    time_elapsed     | 635      |
|    total_timesteps  | 136319   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000258 |
|    n_updates        | 24079    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0236   |
|    exploration_rate | 0.533    |
| time/               |          |
|    episodes         | 7432     |
|    fps              | 214      |
|    time_elapsed     | 635      |
|    total_timesteps  | 136392   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.86e-05 |
|    n_updates        | 24097    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0231   |
|    exploration_rate | 0.532    |
| time/               |          |
|    episodes         | 7436     |
|    fps              | 214      |
|    time_elapsed     | 635      |
|    total_timesteps  | 136483   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0136   |
|    n_updates        | 24120    |
----------------------------------
Eval num_timesteps=136500, episode_reward=0.08 +/- 0.35
Episode length: 14.56 +/- 1.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 14.6     |
|    mean_reward      | 0.0828   |
| rollout/            |          |
|    exploration_rate | 0.532    |
| time/               |          |
|    total_timesteps  | 136500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.89e-05 |
|    n_updates        | 24124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0238   |
|    exploration_rate | 0.532    |
| time/               |          |
|    episodes         | 7440     |
|    fps              | 214      |
|    time_elapsed     | 636      |
|    total_timesteps  | 136554   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000213 |
|    n_updates        | 24138    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0338   |
|    exploration_rate | 0.531    |
| time/               |          |
|    episodes         | 7444     |
|    fps              | 214      |
|    time_elapsed     | 636      |
|    total_timesteps  | 136625   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.1e-05  |
|    n_updates        | 24156    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0439   |
|    exploration_rate | 0.531    |
| time/               |          |
|    episodes         | 7448     |
|    fps              | 214      |
|    time_elapsed     | 636      |
|    total_timesteps  | 136701   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000186 |
|    n_updates        | 24175    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0438   |
|    exploration_rate | 0.53     |
| time/               |          |
|    episodes         | 7452     |
|    fps              | 214      |
|    time_elapsed     | 636      |
|    total_timesteps  | 136777   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00656  |
|    n_updates        | 24194    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0642   |
|    exploration_rate | 0.53     |
| time/               |          |
|    episodes         | 7456     |
|    fps              | 215      |
|    time_elapsed     | 636      |
|    total_timesteps  | 136850   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000219 |
|    n_updates        | 24212    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0644   |
|    exploration_rate | 0.529    |
| time/               |          |
|    episodes         | 7460     |
|    fps              | 215      |
|    time_elapsed     | 636      |
|    total_timesteps  | 136923   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00791  |
|    n_updates        | 24230    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0641   |
|    exploration_rate | 0.528    |
| time/               |          |
|    episodes         | 7464     |
|    fps              | 215      |
|    time_elapsed     | 636      |
|    total_timesteps  | 136993   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000169 |
|    n_updates        | 24248    |
----------------------------------
Eval num_timesteps=137000, episode_reward=-0.05 +/- 0.34
Episode length: 37.04 +/- 23.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37       |
|    mean_reward      | -0.0473  |
| rollout/            |          |
|    exploration_rate | 0.528    |
| time/               |          |
|    total_timesteps  | 137000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 24249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0636   |
|    exploration_rate | 0.528    |
| time/               |          |
|    episodes         | 7468     |
|    fps              | 214      |
|    time_elapsed     | 637      |
|    total_timesteps  | 137075   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 24268    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.075    |
|    exploration_rate | 0.527    |
| time/               |          |
|    episodes         | 7472     |
|    fps              | 214      |
|    time_elapsed     | 637      |
|    total_timesteps  | 137136   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000178 |
|    n_updates        | 24283    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0756   |
|    exploration_rate | 0.527    |
| time/               |          |
|    episodes         | 7476     |
|    fps              | 215      |
|    time_elapsed     | 638      |
|    total_timesteps  | 137201   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0145   |
|    n_updates        | 24300    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0867   |
|    exploration_rate | 0.526    |
| time/               |          |
|    episodes         | 7480     |
|    fps              | 215      |
|    time_elapsed     | 638      |
|    total_timesteps  | 137270   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.44e-05 |
|    n_updates        | 24317    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0768   |
|    exploration_rate | 0.526    |
| time/               |          |
|    episodes         | 7484     |
|    fps              | 215      |
|    time_elapsed     | 638      |
|    total_timesteps  | 137349   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00703  |
|    n_updates        | 24337    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0765   |
|    exploration_rate | 0.525    |
| time/               |          |
|    episodes         | 7488     |
|    fps              | 215      |
|    time_elapsed     | 638      |
|    total_timesteps  | 137437   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.97e-05 |
|    n_updates        | 24359    |
----------------------------------
Eval num_timesteps=137500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.525    |
| time/               |          |
|    total_timesteps  | 137500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 24374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0857   |
|    exploration_rate | 0.525    |
| time/               |          |
|    episodes         | 7492     |
|    fps              | 214      |
|    time_elapsed     | 640      |
|    total_timesteps  | 137522   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000183 |
|    n_updates        | 24380    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0961   |
|    exploration_rate | 0.524    |
| time/               |          |
|    episodes         | 7496     |
|    fps              | 214      |
|    time_elapsed     | 640      |
|    total_timesteps  | 137588   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 24396    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.086    |
|    exploration_rate | 0.524    |
| time/               |          |
|    episodes         | 7500     |
|    fps              | 214      |
|    time_elapsed     | 640      |
|    total_timesteps  | 137658   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 24414    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.076    |
|    exploration_rate | 0.523    |
| time/               |          |
|    episodes         | 7504     |
|    fps              | 214      |
|    time_elapsed     | 641      |
|    total_timesteps  | 137731   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.83e-05 |
|    n_updates        | 24432    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0758   |
|    exploration_rate | 0.523    |
| time/               |          |
|    episodes         | 7508     |
|    fps              | 214      |
|    time_elapsed     | 641      |
|    total_timesteps  | 137804   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000215 |
|    n_updates        | 24450    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0755   |
|    exploration_rate | 0.522    |
| time/               |          |
|    episodes         | 7512     |
|    fps              | 215      |
|    time_elapsed     | 641      |
|    total_timesteps  | 137877   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.28e-05 |
|    n_updates        | 24469    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0653   |
|    exploration_rate | 0.521    |
| time/               |          |
|    episodes         | 7516     |
|    fps              | 215      |
|    time_elapsed     | 641      |
|    total_timesteps  | 137954   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000298 |
|    n_updates        | 24488    |
----------------------------------
Eval num_timesteps=138000, episode_reward=0.02 +/- 0.27
Episode length: 15.46 +/- 1.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 15.5     |
|    mean_reward      | 0.0192   |
| rollout/            |          |
|    exploration_rate | 0.521    |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 24499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0661   |
|    exploration_rate | 0.521    |
| time/               |          |
|    episodes         | 7520     |
|    fps              | 215      |
|    time_elapsed     | 641      |
|    total_timesteps  | 138021   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 24505    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0669   |
|    exploration_rate | 0.52     |
| time/               |          |
|    episodes         | 7524     |
|    fps              | 215      |
|    time_elapsed     | 641      |
|    total_timesteps  | 138085   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.48e-05 |
|    n_updates        | 24521    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0567   |
|    exploration_rate | 0.52     |
| time/               |          |
|    episodes         | 7528     |
|    fps              | 215      |
|    time_elapsed     | 642      |
|    total_timesteps  | 138176   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.78e-05 |
|    n_updates        | 24543    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0462   |
|    exploration_rate | 0.519    |
| time/               |          |
|    episodes         | 7532     |
|    fps              | 215      |
|    time_elapsed     | 642      |
|    total_timesteps  | 138261   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00702  |
|    n_updates        | 24565    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.047    |
|    exploration_rate | 0.519    |
| time/               |          |
|    episodes         | 7536     |
|    fps              | 215      |
|    time_elapsed     | 642      |
|    total_timesteps  | 138333   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.63e-05 |
|    n_updates        | 24583    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0569   |
|    exploration_rate | 0.518    |
| time/               |          |
|    episodes         | 7540     |
|    fps              | 215      |
|    time_elapsed     | 642      |
|    total_timesteps  | 138407   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000177 |
|    n_updates        | 24601    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0462   |
|    exploration_rate | 0.517    |
| time/               |          |
|    episodes         | 7544     |
|    fps              | 215      |
|    time_elapsed     | 642      |
|    total_timesteps  | 138495   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.87e-05 |
|    n_updates        | 24623    |
----------------------------------
Eval num_timesteps=138500, episode_reward=-0.06 +/- 0.16
Episode length: 21.18 +/- 17.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21.2     |
|    mean_reward      | -0.0638  |
| rollout/            |          |
|    exploration_rate | 0.517    |
| time/               |          |
|    total_timesteps  | 138500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00684  |
|    n_updates        | 24624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0366   |
|    exploration_rate | 0.517    |
| time/               |          |
|    episodes         | 7548     |
|    fps              | 215      |
|    time_elapsed     | 643      |
|    total_timesteps  | 138561   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00686  |
|    n_updates        | 24640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0361   |
|    exploration_rate | 0.516    |
| time/               |          |
|    episodes         | 7552     |
|    fps              | 215      |
|    time_elapsed     | 643      |
|    total_timesteps  | 138650   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00659  |
|    n_updates        | 24662    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0162   |
|    exploration_rate | 0.516    |
| time/               |          |
|    episodes         | 7556     |
|    fps              | 215      |
|    time_elapsed     | 643      |
|    total_timesteps  | 138720   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000253 |
|    n_updates        | 24679    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0259   |
|    exploration_rate | 0.515    |
| time/               |          |
|    episodes         | 7560     |
|    fps              | 215      |
|    time_elapsed     | 643      |
|    total_timesteps  | 138800   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.99e-05 |
|    n_updates        | 24699    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0154   |
|    exploration_rate | 0.515    |
| time/               |          |
|    episodes         | 7564     |
|    fps              | 215      |
|    time_elapsed     | 643      |
|    total_timesteps  | 138883   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 24720    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00588  |
|    exploration_rate | 0.514    |
| time/               |          |
|    episodes         | 7568     |
|    fps              | 215      |
|    time_elapsed     | 643      |
|    total_timesteps  | 138953   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000263 |
|    n_updates        | 24738    |
----------------------------------
Eval num_timesteps=139000, episode_reward=-0.08 +/- 0.33
Episode length: 39.42 +/- 29.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.4     |
|    mean_reward      | -0.0771  |
| rollout/            |          |
|    exploration_rate | 0.514    |
| time/               |          |
|    total_timesteps  | 139000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 24749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.00489 |
|    exploration_rate | 0.514    |
| time/               |          |
|    episodes         | 7572     |
|    fps              | 215      |
|    time_elapsed     | 644      |
|    total_timesteps  | 139033   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000224 |
|    n_updates        | 24758    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.00434  |
|    exploration_rate | 0.513    |
| time/               |          |
|    episodes         | 7576     |
|    fps              | 215      |
|    time_elapsed     | 645      |
|    total_timesteps  | 139117   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0002   |
|    n_updates        | 24779    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.00563 |
|    exploration_rate | 0.512    |
| time/               |          |
|    episodes         | 7580     |
|    fps              | 215      |
|    time_elapsed     | 645      |
|    total_timesteps  | 139185   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 24796    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.00511 |
|    exploration_rate | 0.512    |
| time/               |          |
|    episodes         | 7584     |
|    fps              | 215      |
|    time_elapsed     | 645      |
|    total_timesteps  | 139251   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000169 |
|    n_updates        | 24812    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.00596  |
|    exploration_rate | 0.511    |
| time/               |          |
|    episodes         | 7588     |
|    fps              | 215      |
|    time_elapsed     | 645      |
|    total_timesteps  | 139312   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000198 |
|    n_updates        | 24827    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.00407 |
|    exploration_rate | 0.511    |
| time/               |          |
|    episodes         | 7592     |
|    fps              | 216      |
|    time_elapsed     | 645      |
|    total_timesteps  | 139398   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 24849    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0141  |
|    exploration_rate | 0.51     |
| time/               |          |
|    episodes         | 7596     |
|    fps              | 216      |
|    time_elapsed     | 645      |
|    total_timesteps  | 139465   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00778  |
|    n_updates        | 24866    |
----------------------------------
Eval num_timesteps=139500, episode_reward=-0.25 +/- 0.16
Episode length: 68.46 +/- 12.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.5     |
|    mean_reward      | -0.254   |
| rollout/            |          |
|    exploration_rate | 0.51     |
| time/               |          |
|    total_timesteps  | 139500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0136   |
|    n_updates        | 24874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0146  |
|    exploration_rate | 0.51     |
| time/               |          |
|    episodes         | 7600     |
|    fps              | 215      |
|    time_elapsed     | 647      |
|    total_timesteps  | 139548   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00678  |
|    n_updates        | 24886    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.0143  |
|    exploration_rate | 0.509    |
| time/               |          |
|    episodes         | 7604     |
|    fps              | 215      |
|    time_elapsed     | 647      |
|    total_timesteps  | 139613   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000176 |
|    n_updates        | 24903    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.00384 |
|    exploration_rate | 0.509    |
| time/               |          |
|    episodes         | 7608     |
|    fps              | 215      |
|    time_elapsed     | 647      |
|    total_timesteps  | 139674   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0001   |
|    n_updates        | 24918    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | -0.014   |
|    exploration_rate | 0.508    |
| time/               |          |
|    episodes         | 7612     |
|    fps              | 215      |
|    time_elapsed     | 647      |
|    total_timesteps  | 139752   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.83e-05 |
|    n_updates        | 24937    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | -0.0139  |
|    exploration_rate | 0.508    |
| time/               |          |
|    episodes         | 7616     |
|    fps              | 215      |
|    time_elapsed     | 647      |
|    total_timesteps  | 139826   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.3e-05  |
|    n_updates        | 24956    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | -0.0147  |
|    exploration_rate | 0.507    |
| time/               |          |
|    episodes         | 7620     |
|    fps              | 215      |
|    time_elapsed     | 648      |
|    total_timesteps  | 139913   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00784  |
|    n_updates        | 24978    |
----------------------------------
Eval num_timesteps=140000, episode_reward=-0.14 +/- 0.28
Episode length: 51.16 +/- 25.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | -0.144   |
| rollout/            |          |
|    exploration_rate | 0.506    |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00796  |
|    n_updates        | 24999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.0263  |
|    exploration_rate | 0.506    |
| time/               |          |
|    episodes         | 7624     |
|    fps              | 215      |
|    time_elapsed     | 649      |
|    total_timesteps  | 140017   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 25004    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.0269  |
|    exploration_rate | 0.505    |
| time/               |          |
|    episodes         | 7628     |
|    fps              | 215      |
|    time_elapsed     | 649      |
|    total_timesteps  | 140124   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 25030    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0166  |
|    exploration_rate | 0.505    |
| time/               |          |
|    episodes         | 7632     |
|    fps              | 215      |
|    time_elapsed     | 650      |
|    total_timesteps  | 140200   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0137   |
|    n_updates        | 25049    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0168  |
|    exploration_rate | 0.504    |
| time/               |          |
|    episodes         | 7636     |
|    fps              | 215      |
|    time_elapsed     | 650      |
|    total_timesteps  | 140277   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 25069    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0164  |
|    exploration_rate | 0.504    |
| time/               |          |
|    episodes         | 7640     |
|    fps              | 215      |
|    time_elapsed     | 650      |
|    total_timesteps  | 140342   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 25085    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0156  |
|    exploration_rate | 0.503    |
| time/               |          |
|    episodes         | 7644     |
|    fps              | 215      |
|    time_elapsed     | 650      |
|    total_timesteps  | 140410   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.73e-05 |
|    n_updates        | 25102    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | -0.0158  |
|    exploration_rate | 0.503    |
| time/               |          |
|    episodes         | 7648     |
|    fps              | 216      |
|    time_elapsed     | 650      |
|    total_timesteps  | 140481   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 25120    |
----------------------------------
Eval num_timesteps=140500, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 14.6     |
|    mean_reward      | 0.0426   |
| rollout/            |          |
|    exploration_rate | 0.503    |
| time/               |          |
|    total_timesteps  | 140500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 25124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0155  |
|    exploration_rate | 0.502    |
| time/               |          |
|    episodes         | 7652     |
|    fps              | 215      |
|    time_elapsed     | 650      |
|    total_timesteps  | 140562   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.6e-05  |
|    n_updates        | 25140    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.0151  |
|    exploration_rate | 0.502    |
| time/               |          |
|    episodes         | 7656     |
|    fps              | 216      |
|    time_elapsed     | 650      |
|    total_timesteps  | 140622   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.16e-05 |
|    n_updates        | 25155    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | -0.00483 |
|    exploration_rate | 0.501    |
| time/               |          |
|    episodes         | 7660     |
|    fps              | 216      |
|    time_elapsed     | 651      |
|    total_timesteps  | 140696   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 25173    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.00535  |
|    exploration_rate | 0.501    |
| time/               |          |
|    episodes         | 7664     |
|    fps              | 216      |
|    time_elapsed     | 651      |
|    total_timesteps  | 140775   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0134   |
|    n_updates        | 25193    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0154   |
|    exploration_rate | 0.5      |
| time/               |          |
|    episodes         | 7668     |
|    fps              | 216      |
|    time_elapsed     | 651      |
|    total_timesteps  | 140843   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.98e-05 |
|    n_updates        | 25210    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0263   |
|    exploration_rate | 0.5      |
| time/               |          |
|    episodes         | 7672     |
|    fps              | 216      |
|    time_elapsed     | 651      |
|    total_timesteps  | 140900   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.85e-05 |
|    n_updates        | 25224    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0169   |
|    exploration_rate | 0.499    |
| time/               |          |
|    episodes         | 7676     |
|    fps              | 216      |
|    time_elapsed     | 651      |
|    total_timesteps  | 140969   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.26e-05 |
|    n_updates        | 25242    |
----------------------------------
Eval num_timesteps=141000, episode_reward=0.08 +/- 0.35
Episode length: 14.52 +/- 1.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 14.5     |
|    mean_reward      | 0.0831   |
| rollout/            |          |
|    exploration_rate | 0.499    |
| time/               |          |
|    total_timesteps  | 141000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.67e-05 |
|    n_updates        | 25249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0173   |
|    exploration_rate | 0.499    |
| time/               |          |
|    episodes         | 7680     |
|    fps              | 216      |
|    time_elapsed     | 651      |
|    total_timesteps  | 141029   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000234 |
|    n_updates        | 25257    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.017    |
|    exploration_rate | 0.498    |
| time/               |          |
|    episodes         | 7684     |
|    fps              | 216      |
|    time_elapsed     | 651      |
|    total_timesteps  | 141101   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000136 |
|    n_updates        | 25275    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0169   |
|    exploration_rate | 0.498    |
| time/               |          |
|    episodes         | 7688     |
|    fps              | 216      |
|    time_elapsed     | 652      |
|    total_timesteps  | 141166   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.65e-05 |
|    n_updates        | 25291    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 0.497    |
| time/               |          |
|    episodes         | 7692     |
|    fps              | 216      |
|    time_elapsed     | 652      |
|    total_timesteps  | 141246   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.55e-05 |
|    n_updates        | 25311    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.5     |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.497    |
| time/               |          |
|    episodes         | 7696     |
|    fps              | 216      |
|    time_elapsed     | 652      |
|    total_timesteps  | 141311   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 25327    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.028    |
|    exploration_rate | 0.496    |
| time/               |          |
|    episodes         | 7700     |
|    fps              | 216      |
|    time_elapsed     | 652      |
|    total_timesteps  | 141374   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.14e-05 |
|    n_updates        | 25343    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0267   |
|    exploration_rate | 0.495    |
| time/               |          |
|    episodes         | 7704     |
|    fps              | 216      |
|    time_elapsed     | 652      |
|    total_timesteps  | 141471   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 25367    |
----------------------------------
Eval num_timesteps=141500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.495    |
| time/               |          |
|    total_timesteps  | 141500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 25374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0258   |
|    exploration_rate | 0.495    |
| time/               |          |
|    episodes         | 7708     |
|    fps              | 216      |
|    time_elapsed     | 654      |
|    total_timesteps  | 141555   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.16e-05 |
|    n_updates        | 25388    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0255   |
|    exploration_rate | 0.494    |
| time/               |          |
|    episodes         | 7712     |
|    fps              | 216      |
|    time_elapsed     | 654      |
|    total_timesteps  | 141640   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00701  |
|    n_updates        | 25409    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0555   |
|    exploration_rate | 0.494    |
| time/               |          |
|    episodes         | 7716     |
|    fps              | 216      |
|    time_elapsed     | 655      |
|    total_timesteps  | 141714   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000193 |
|    n_updates        | 25428    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0561   |
|    exploration_rate | 0.493    |
| time/               |          |
|    episodes         | 7720     |
|    fps              | 216      |
|    time_elapsed     | 655      |
|    total_timesteps  | 141788   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.26e-05 |
|    n_updates        | 25446    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0573   |
|    exploration_rate | 0.493    |
| time/               |          |
|    episodes         | 7724     |
|    fps              | 216      |
|    time_elapsed     | 655      |
|    total_timesteps  | 141862   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 25465    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.0582   |
|    exploration_rate | 0.492    |
| time/               |          |
|    episodes         | 7728     |
|    fps              | 216      |
|    time_elapsed     | 655      |
|    total_timesteps  | 141946   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000156 |
|    n_updates        | 25486    |
----------------------------------
Eval num_timesteps=142000, episode_reward=-0.07 +/- 0.16
Episode length: 22.46 +/- 17.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22.5     |
|    mean_reward      | -0.0689  |
| rollout/            |          |
|    exploration_rate | 0.492    |
| time/               |          |
|    total_timesteps  | 142000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000134 |
|    n_updates        | 25499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0486   |
|    exploration_rate | 0.491    |
| time/               |          |
|    episodes         | 7732     |
|    fps              | 216      |
|    time_elapsed     | 656      |
|    total_timesteps  | 142012   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00783  |
|    n_updates        | 25502    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18       |
|    ep_rew_mean      | 0.0589   |
|    exploration_rate | 0.491    |
| time/               |          |
|    episodes         | 7736     |
|    fps              | 216      |
|    time_elapsed     | 656      |
|    total_timesteps  | 142081   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.75e-05 |
|    n_updates        | 25520    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0485   |
|    exploration_rate | 0.49     |
| time/               |          |
|    episodes         | 7740     |
|    fps              | 216      |
|    time_elapsed     | 656      |
|    total_timesteps  | 142155   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 25538    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.1     |
|    ep_rew_mean      | 0.0585   |
|    exploration_rate | 0.49     |
| time/               |          |
|    episodes         | 7744     |
|    fps              | 216      |
|    time_elapsed     | 656      |
|    total_timesteps  | 142223   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00678  |
|    n_updates        | 25555    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.2     |
|    ep_rew_mean      | 0.058    |
|    exploration_rate | 0.489    |
| time/               |          |
|    episodes         | 7748     |
|    fps              | 216      |
|    time_elapsed     | 656      |
|    total_timesteps  | 142306   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00688  |
|    n_updates        | 25576    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.3     |
|    ep_rew_mean      | 0.0577   |
|    exploration_rate | 0.489    |
| time/               |          |
|    episodes         | 7752     |
|    fps              | 216      |
|    time_elapsed     | 656      |
|    total_timesteps  | 142395   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 25598    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0675   |
|    exploration_rate | 0.488    |
| time/               |          |
|    episodes         | 7756     |
|    fps              | 216      |
|    time_elapsed     | 656      |
|    total_timesteps  | 142461   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 25615    |
----------------------------------
Eval num_timesteps=142500, episode_reward=-0.01 +/- 0.29
Episode length: 23.64 +/- 14.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23.6     |
|    mean_reward      | -0.0136  |
| rollout/            |          |
|    exploration_rate | 0.488    |
| time/               |          |
|    total_timesteps  | 142500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00675  |
|    n_updates        | 25624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0461   |
|    exploration_rate | 0.487    |
| time/               |          |
|    episodes         | 7760     |
|    fps              | 216      |
|    time_elapsed     | 657      |
|    total_timesteps  | 142569   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000141 |
|    n_updates        | 25642    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0361   |
|    exploration_rate | 0.487    |
| time/               |          |
|    episodes         | 7764     |
|    fps              | 216      |
|    time_elapsed     | 657      |
|    total_timesteps  | 142648   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 25661    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0258   |
|    exploration_rate | 0.486    |
| time/               |          |
|    episodes         | 7768     |
|    fps              | 217      |
|    time_elapsed     | 657      |
|    total_timesteps  | 142723   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.24e-05 |
|    n_updates        | 25680    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.0149   |
|    exploration_rate | 0.485    |
| time/               |          |
|    episodes         | 7772     |
|    fps              | 217      |
|    time_elapsed     | 657      |
|    total_timesteps  | 142802   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 25700    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0142   |
|    exploration_rate | 0.485    |
| time/               |          |
|    episodes         | 7776     |
|    fps              | 217      |
|    time_elapsed     | 657      |
|    total_timesteps  | 142890   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.14e-05 |
|    n_updates        | 25722    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0228   |
|    exploration_rate | 0.484    |
| time/               |          |
|    episodes         | 7780     |
|    fps              | 217      |
|    time_elapsed     | 657      |
|    total_timesteps  | 142986   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000303 |
|    n_updates        | 25746    |
----------------------------------
Eval num_timesteps=143000, episode_reward=-0.08 +/- 0.24
Episode length: 34.44 +/- 16.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.4     |
|    mean_reward      | -0.0768  |
| rollout/            |          |
|    exploration_rate | 0.484    |
| time/               |          |
|    total_timesteps  | 143000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00683  |
|    n_updates        | 25749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.023    |
|    exploration_rate | 0.484    |
| time/               |          |
|    episodes         | 7784     |
|    fps              | 217      |
|    time_elapsed     | 659      |
|    total_timesteps  | 143051   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00699  |
|    n_updates        | 25762    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0227   |
|    exploration_rate | 0.483    |
| time/               |          |
|    episodes         | 7788     |
|    fps              | 217      |
|    time_elapsed     | 659      |
|    total_timesteps  | 143124   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 25780    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0222   |
|    exploration_rate | 0.482    |
| time/               |          |
|    episodes         | 7792     |
|    fps              | 217      |
|    time_elapsed     | 659      |
|    total_timesteps  | 143216   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.91e-05 |
|    n_updates        | 25803    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0312   |
|    exploration_rate | 0.482    |
| time/               |          |
|    episodes         | 7796     |
|    fps              | 217      |
|    time_elapsed     | 659      |
|    total_timesteps  | 143306   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 25826    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.0205   |
|    exploration_rate | 0.481    |
| time/               |          |
|    episodes         | 7800     |
|    fps              | 217      |
|    time_elapsed     | 659      |
|    total_timesteps  | 143388   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.29e-05 |
|    n_updates        | 25846    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | 0.0317   |
|    exploration_rate | 0.481    |
| time/               |          |
|    episodes         | 7804     |
|    fps              | 217      |
|    time_elapsed     | 659      |
|    total_timesteps  | 143455   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.75e-05 |
|    n_updates        | 25863    |
----------------------------------
Eval num_timesteps=143500, episode_reward=0.10 +/- 0.45
Episode length: 41.28 +/- 17.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 0.0959   |
| rollout/            |          |
|    exploration_rate | 0.48     |
| time/               |          |
|    total_timesteps  | 143500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 25874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0324   |
|    exploration_rate | 0.48     |
| time/               |          |
|    episodes         | 7808     |
|    fps              | 217      |
|    time_elapsed     | 661      |
|    total_timesteps  | 143522   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00029  |
|    n_updates        | 25880    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0329   |
|    exploration_rate | 0.48     |
| time/               |          |
|    episodes         | 7812     |
|    fps              | 217      |
|    time_elapsed     | 661      |
|    total_timesteps  | 143594   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00697  |
|    n_updates        | 25898    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.00318  |
|    exploration_rate | 0.479    |
| time/               |          |
|    episodes         | 7816     |
|    fps              | 217      |
|    time_elapsed     | 661      |
|    total_timesteps  | 143661   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00674  |
|    n_updates        | 25915    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.013    |
|    exploration_rate | 0.478    |
| time/               |          |
|    episodes         | 7820     |
|    fps              | 217      |
|    time_elapsed     | 661      |
|    total_timesteps  | 143740   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.79e-05 |
|    n_updates        | 25934    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | 0.012    |
|    exploration_rate | 0.478    |
| time/               |          |
|    episodes         | 7824     |
|    fps              | 217      |
|    time_elapsed     | 661      |
|    total_timesteps  | 143838   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.46e-05 |
|    n_updates        | 25959    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0229   |
|    exploration_rate | 0.477    |
| time/               |          |
|    episodes         | 7828     |
|    fps              | 217      |
|    time_elapsed     | 661      |
|    total_timesteps  | 143901   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 25975    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | 0.0321   |
|    exploration_rate | 0.477    |
| time/               |          |
|    episodes         | 7832     |
|    fps              | 217      |
|    time_elapsed     | 661      |
|    total_timesteps  | 143987   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000153 |
|    n_updates        | 25996    |
----------------------------------
Eval num_timesteps=144000, episode_reward=-0.21 +/- 0.10
Episode length: 53.44 +/- 25.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.4     |
|    mean_reward      | -0.213   |
| rollout/            |          |
|    exploration_rate | 0.476    |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00685  |
|    n_updates        | 25999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0217   |
|    exploration_rate | 0.476    |
| time/               |          |
|    episodes         | 7836     |
|    fps              | 217      |
|    time_elapsed     | 663      |
|    total_timesteps  | 144067   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.08e-05 |
|    n_updates        | 26016    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0213   |
|    exploration_rate | 0.475    |
| time/               |          |
|    episodes         | 7840     |
|    fps              | 217      |
|    time_elapsed     | 663      |
|    total_timesteps  | 144149   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 26037    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0215   |
|    exploration_rate | 0.475    |
| time/               |          |
|    episodes         | 7844     |
|    fps              | 217      |
|    time_elapsed     | 663      |
|    total_timesteps  | 144213   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.55e-05 |
|    n_updates        | 26053    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | 0.0217   |
|    exploration_rate | 0.474    |
| time/               |          |
|    episodes         | 7848     |
|    fps              | 217      |
|    time_elapsed     | 663      |
|    total_timesteps  | 144290   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00689  |
|    n_updates        | 26072    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0224   |
|    exploration_rate | 0.474    |
| time/               |          |
|    episodes         | 7852     |
|    fps              | 217      |
|    time_elapsed     | 663      |
|    total_timesteps  | 144362   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.48e-05 |
|    n_updates        | 26090    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | 0.012    |
|    exploration_rate | 0.473    |
| time/               |          |
|    episodes         | 7856     |
|    fps              | 217      |
|    time_elapsed     | 663      |
|    total_timesteps  | 144439   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.39e-05 |
|    n_updates        | 26109    |
----------------------------------
Eval num_timesteps=144500, episode_reward=-0.05 +/- 0.21
Episode length: 22.26 +/- 10.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22.3     |
|    mean_reward      | -0.0481  |
| rollout/            |          |
|    exploration_rate | 0.473    |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000261 |
|    n_updates        | 26124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0237   |
|    exploration_rate | 0.473    |
| time/               |          |
|    episodes         | 7860     |
|    fps              | 217      |
|    time_elapsed     | 664      |
|    total_timesteps  | 144505   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.58e-05 |
|    n_updates        | 26126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0243   |
|    exploration_rate | 0.472    |
| time/               |          |
|    episodes         | 7864     |
|    fps              | 217      |
|    time_elapsed     | 664      |
|    total_timesteps  | 144567   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000202 |
|    n_updates        | 26141    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0231   |
|    exploration_rate | 0.471    |
| time/               |          |
|    episodes         | 7868     |
|    fps              | 217      |
|    time_elapsed     | 664      |
|    total_timesteps  | 144673   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0068   |
|    n_updates        | 26168    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | 0.032    |
|    exploration_rate | 0.471    |
| time/               |          |
|    episodes         | 7872     |
|    fps              | 217      |
|    time_elapsed     | 664      |
|    total_timesteps  | 144779   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.66e-05 |
|    n_updates        | 26194    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0427   |
|    exploration_rate | 0.47     |
| time/               |          |
|    episodes         | 7876     |
|    fps              | 217      |
|    time_elapsed     | 664      |
|    total_timesteps  | 144850   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000167 |
|    n_updates        | 26212    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0338   |
|    exploration_rate | 0.47     |
| time/               |          |
|    episodes         | 7880     |
|    fps              | 217      |
|    time_elapsed     | 664      |
|    total_timesteps  | 144918   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00789  |
|    n_updates        | 26229    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0537   |
|    exploration_rate | 0.469    |
| time/               |          |
|    episodes         | 7884     |
|    fps              | 218      |
|    time_elapsed     | 665      |
|    total_timesteps  | 144988   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000199 |
|    n_updates        | 26246    |
----------------------------------
Eval num_timesteps=145000, episode_reward=-0.20 +/- 0.17
Episode length: 54.84 +/- 23.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.8     |
|    mean_reward      | -0.199   |
| rollout/            |          |
|    exploration_rate | 0.469    |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00683  |
|    n_updates        | 26249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0438   |
|    exploration_rate | 0.468    |
| time/               |          |
|    episodes         | 7888     |
|    fps              | 217      |
|    time_elapsed     | 666      |
|    total_timesteps  | 145058   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.35e-05 |
|    n_updates        | 26264    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0442   |
|    exploration_rate | 0.468    |
| time/               |          |
|    episodes         | 7892     |
|    fps              | 217      |
|    time_elapsed     | 667      |
|    total_timesteps  | 145141   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00693  |
|    n_updates        | 26285    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0348   |
|    exploration_rate | 0.467    |
| time/               |          |
|    episodes         | 7896     |
|    fps              | 217      |
|    time_elapsed     | 667      |
|    total_timesteps  | 145214   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 26303    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19       |
|    ep_rew_mean      | 0.0452   |
|    exploration_rate | 0.467    |
| time/               |          |
|    episodes         | 7900     |
|    fps              | 217      |
|    time_elapsed     | 667      |
|    total_timesteps  | 145287   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00666  |
|    n_updates        | 26321    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0342   |
|    exploration_rate | 0.466    |
| time/               |          |
|    episodes         | 7904     |
|    fps              | 217      |
|    time_elapsed     | 667      |
|    total_timesteps  | 145377   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00694  |
|    n_updates        | 26344    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0331   |
|    exploration_rate | 0.465    |
| time/               |          |
|    episodes         | 7908     |
|    fps              | 217      |
|    time_elapsed     | 667      |
|    total_timesteps  | 145473   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00657  |
|    n_updates        | 26368    |
----------------------------------
Eval num_timesteps=145500, episode_reward=-0.00 +/- 0.24
Episode length: 16.20 +/- 1.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.2     |
|    mean_reward      | -0.00372 |
| rollout/            |          |
|    exploration_rate | 0.465    |
| time/               |          |
|    total_timesteps  | 145500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 26374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0329   |
|    exploration_rate | 0.465    |
| time/               |          |
|    episodes         | 7912     |
|    fps              | 217      |
|    time_elapsed     | 668      |
|    total_timesteps  | 145550   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0146   |
|    n_updates        | 26387    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0427   |
|    exploration_rate | 0.464    |
| time/               |          |
|    episodes         | 7916     |
|    fps              | 217      |
|    time_elapsed     | 668      |
|    total_timesteps  | 145622   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000241 |
|    n_updates        | 26405    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.033    |
|    exploration_rate | 0.464    |
| time/               |          |
|    episodes         | 7920     |
|    fps              | 218      |
|    time_elapsed     | 668      |
|    total_timesteps  | 145692   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.29e-05 |
|    n_updates        | 26422    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0339   |
|    exploration_rate | 0.463    |
| time/               |          |
|    episodes         | 7924     |
|    fps              | 218      |
|    time_elapsed     | 668      |
|    total_timesteps  | 145768   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.18e-05 |
|    n_updates        | 26441    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0323   |
|    exploration_rate | 0.462    |
| time/               |          |
|    episodes         | 7928     |
|    fps              | 218      |
|    time_elapsed     | 668      |
|    total_timesteps  | 145871   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.007    |
|    n_updates        | 26467    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0229   |
|    exploration_rate | 0.462    |
| time/               |          |
|    episodes         | 7932     |
|    fps              | 218      |
|    time_elapsed     | 668      |
|    total_timesteps  | 145943   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.97e-05 |
|    n_updates        | 26485    |
----------------------------------
Eval num_timesteps=146000, episode_reward=-0.09 +/- 0.39
Episode length: 61.74 +/- 17.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.7     |
|    mean_reward      | -0.0863  |
| rollout/            |          |
|    exploration_rate | 0.461    |
| time/               |          |
|    total_timesteps  | 146000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00772  |
|    n_updates        | 26499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.023    |
|    exploration_rate | 0.461    |
| time/               |          |
|    episodes         | 7936     |
|    fps              | 217      |
|    time_elapsed     | 670      |
|    total_timesteps  | 146020   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.78e-05 |
|    n_updates        | 26504    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0228   |
|    exploration_rate | 0.46     |
| time/               |          |
|    episodes         | 7940     |
|    fps              | 217      |
|    time_elapsed     | 670      |
|    total_timesteps  | 146106   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000186 |
|    n_updates        | 26526    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0127   |
|    exploration_rate | 0.46     |
| time/               |          |
|    episodes         | 7944     |
|    fps              | 217      |
|    time_elapsed     | 670      |
|    total_timesteps  | 146175   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000134 |
|    n_updates        | 26543    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0129   |
|    exploration_rate | 0.459    |
| time/               |          |
|    episodes         | 7948     |
|    fps              | 218      |
|    time_elapsed     | 670      |
|    total_timesteps  | 146246   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.68e-05 |
|    n_updates        | 26561    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0233   |
|    exploration_rate | 0.459    |
| time/               |          |
|    episodes         | 7952     |
|    fps              | 218      |
|    time_elapsed     | 670      |
|    total_timesteps  | 146309   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000185 |
|    n_updates        | 26577    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.0231   |
|    exploration_rate | 0.458    |
| time/               |          |
|    episodes         | 7956     |
|    fps              | 218      |
|    time_elapsed     | 670      |
|    total_timesteps  | 146390   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00708  |
|    n_updates        | 26597    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0126   |
|    exploration_rate | 0.458    |
| time/               |          |
|    episodes         | 7960     |
|    fps              | 218      |
|    time_elapsed     | 670      |
|    total_timesteps  | 146468   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 26616    |
----------------------------------
Eval num_timesteps=146500, episode_reward=-0.06 +/- 0.34
Episode length: 45.94 +/- 20.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | -0.0629  |
| rollout/            |          |
|    exploration_rate | 0.457    |
| time/               |          |
|    total_timesteps  | 146500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.95e-05 |
|    n_updates        | 26624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0116   |
|    exploration_rate | 0.457    |
| time/               |          |
|    episodes         | 7964     |
|    fps              | 217      |
|    time_elapsed     | 672      |
|    total_timesteps  | 146554   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00698  |
|    n_updates        | 26638    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0133   |
|    exploration_rate | 0.457    |
| time/               |          |
|    episodes         | 7968     |
|    fps              | 217      |
|    time_elapsed     | 672      |
|    total_timesteps  | 146617   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00683  |
|    n_updates        | 26654    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.00469  |
|    exploration_rate | 0.456    |
| time/               |          |
|    episodes         | 7972     |
|    fps              | 218      |
|    time_elapsed     | 672      |
|    total_timesteps  | 146689   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.41e-05 |
|    n_updates        | 26672    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.00661 |
|    exploration_rate | 0.455    |
| time/               |          |
|    episodes         | 7976     |
|    fps              | 218      |
|    time_elapsed     | 672      |
|    total_timesteps  | 146792   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.49e-05 |
|    n_updates        | 26697    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.00701 |
|    exploration_rate | 0.455    |
| time/               |          |
|    episodes         | 7980     |
|    fps              | 218      |
|    time_elapsed     | 672      |
|    total_timesteps  | 146870   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00018  |
|    n_updates        | 26717    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.0269  |
|    exploration_rate | 0.454    |
| time/               |          |
|    episodes         | 7984     |
|    fps              | 218      |
|    time_elapsed     | 672      |
|    total_timesteps  | 146937   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00681  |
|    n_updates        | 26734    |
----------------------------------
Eval num_timesteps=147000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.454    |
| time/               |          |
|    total_timesteps  | 147000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.94e-05 |
|    n_updates        | 26749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.0168  |
|    exploration_rate | 0.454    |
| time/               |          |
|    episodes         | 7988     |
|    fps              | 217      |
|    time_elapsed     | 675      |
|    total_timesteps  | 147004   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00786  |
|    n_updates        | 26750    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | -0.0175  |
|    exploration_rate | 0.453    |
| time/               |          |
|    episodes         | 7992     |
|    fps              | 217      |
|    time_elapsed     | 675      |
|    total_timesteps  | 147104   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00674  |
|    n_updates        | 26775    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | -0.00793 |
|    exploration_rate | 0.452    |
| time/               |          |
|    episodes         | 7996     |
|    fps              | 217      |
|    time_elapsed     | 675      |
|    total_timesteps  | 147188   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.18e-05 |
|    n_updates        | 26796    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | -0.0182  |
|    exploration_rate | 0.452    |
| time/               |          |
|    episodes         | 8000     |
|    fps              | 217      |
|    time_elapsed     | 675      |
|    total_timesteps  | 147269   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.97e-05 |
|    n_updates        | 26817    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | -0.0174  |
|    exploration_rate | 0.451    |
| time/               |          |
|    episodes         | 8004     |
|    fps              | 218      |
|    time_elapsed     | 675      |
|    total_timesteps  | 147339   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000174 |
|    n_updates        | 26834    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0266  |
|    exploration_rate | 0.45     |
| time/               |          |
|    episodes         | 8008     |
|    fps              | 218      |
|    time_elapsed     | 675      |
|    total_timesteps  | 147413   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 26853    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.0263  |
|    exploration_rate | 0.45     |
| time/               |          |
|    episodes         | 8012     |
|    fps              | 218      |
|    time_elapsed     | 675      |
|    total_timesteps  | 147484   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.7e-05  |
|    n_updates        | 26870    |
----------------------------------
Eval num_timesteps=147500, episode_reward=-0.15 +/- 0.07
Episode length: 37.82 +/- 17.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.8     |
|    mean_reward      | -0.15    |
| rollout/            |          |
|    exploration_rate | 0.45     |
| time/               |          |
|    total_timesteps  | 147500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 26874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0365  |
|    exploration_rate | 0.449    |
| time/               |          |
|    episodes         | 8016     |
|    fps              | 217      |
|    time_elapsed     | 677      |
|    total_timesteps  | 147560   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.17e-05 |
|    n_updates        | 26889    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0264  |
|    exploration_rate | 0.449    |
| time/               |          |
|    episodes         | 8020     |
|    fps              | 217      |
|    time_elapsed     | 677      |
|    total_timesteps  | 147627   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 26906    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0267  |
|    exploration_rate | 0.448    |
| time/               |          |
|    episodes         | 8024     |
|    fps              | 218      |
|    time_elapsed     | 677      |
|    total_timesteps  | 147711   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00784  |
|    n_updates        | 26927    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0354  |
|    exploration_rate | 0.448    |
| time/               |          |
|    episodes         | 8028     |
|    fps              | 218      |
|    time_elapsed     | 677      |
|    total_timesteps  | 147782   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.83e-05 |
|    n_updates        | 26945    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0253  |
|    exploration_rate | 0.447    |
| time/               |          |
|    episodes         | 8032     |
|    fps              | 218      |
|    time_elapsed     | 677      |
|    total_timesteps  | 147852   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.16e-05 |
|    n_updates        | 26962    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0252  |
|    exploration_rate | 0.447    |
| time/               |          |
|    episodes         | 8036     |
|    fps              | 218      |
|    time_elapsed     | 677      |
|    total_timesteps  | 147925   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.01e-05 |
|    n_updates        | 26981    |
----------------------------------
Eval num_timesteps=148000, episode_reward=-0.06 +/- 0.35
Episode length: 44.26 +/- 25.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | -0.0565  |
| rollout/            |          |
|    exploration_rate | 0.446    |
| time/               |          |
|    total_timesteps  | 148000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 26999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | -0.0252  |
|    exploration_rate | 0.446    |
| time/               |          |
|    episodes         | 8040     |
|    fps              | 217      |
|    time_elapsed     | 679      |
|    total_timesteps  | 148012   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000167 |
|    n_updates        | 27002    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.026   |
|    exploration_rate | 0.445    |
| time/               |          |
|    episodes         | 8044     |
|    fps              | 218      |
|    time_elapsed     | 679      |
|    total_timesteps  | 148101   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0002   |
|    n_updates        | 27025    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | -0.0162  |
|    exploration_rate | 0.445    |
| time/               |          |
|    episodes         | 8048     |
|    fps              | 218      |
|    time_elapsed     | 679      |
|    total_timesteps  | 148177   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.18e-05 |
|    n_updates        | 27044    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0267  |
|    exploration_rate | 0.444    |
| time/               |          |
|    episodes         | 8052     |
|    fps              | 218      |
|    time_elapsed     | 679      |
|    total_timesteps  | 148252   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.49e-05 |
|    n_updates        | 27062    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | -0.0274  |
|    exploration_rate | 0.443    |
| time/               |          |
|    episodes         | 8056     |
|    fps              | 218      |
|    time_elapsed     | 679      |
|    total_timesteps  | 148350   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000186 |
|    n_updates        | 27087    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | -0.0278  |
|    exploration_rate | 0.443    |
| time/               |          |
|    episodes         | 8060     |
|    fps              | 218      |
|    time_elapsed     | 679      |
|    total_timesteps  | 148439   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00783  |
|    n_updates        | 27109    |
----------------------------------
Eval num_timesteps=148500, episode_reward=-0.20 +/- 0.27
Episode length: 65.30 +/- 17.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.3     |
|    mean_reward      | -0.201   |
| rollout/            |          |
|    exploration_rate | 0.442    |
| time/               |          |
|    total_timesteps  | 148500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.48e-05 |
|    n_updates        | 27124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | -0.0276  |
|    exploration_rate | 0.442    |
| time/               |          |
|    episodes         | 8064     |
|    fps              | 217      |
|    time_elapsed     | 681      |
|    total_timesteps  | 148519   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0146   |
|    n_updates        | 27129    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | -0.0275  |
|    exploration_rate | 0.442    |
| time/               |          |
|    episodes         | 8068     |
|    fps              | 217      |
|    time_elapsed     | 681      |
|    total_timesteps  | 148581   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 27145    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | -0.0278  |
|    exploration_rate | 0.441    |
| time/               |          |
|    episodes         | 8072     |
|    fps              | 217      |
|    time_elapsed     | 682      |
|    total_timesteps  | 148661   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.33e-05 |
|    n_updates        | 27165    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.0271  |
|    exploration_rate | 0.44     |
| time/               |          |
|    episodes         | 8076     |
|    fps              | 218      |
|    time_elapsed     | 682      |
|    total_timesteps  | 148744   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 27185    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | -0.0274  |
|    exploration_rate | 0.44     |
| time/               |          |
|    episodes         | 8080     |
|    fps              | 218      |
|    time_elapsed     | 682      |
|    total_timesteps  | 148832   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 27207    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | -0.0202  |
|    exploration_rate | 0.439    |
| time/               |          |
|    episodes         | 8084     |
|    fps              | 218      |
|    time_elapsed     | 682      |
|    total_timesteps  | 148968   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.34e-05 |
|    n_updates        | 27241    |
----------------------------------
Eval num_timesteps=149000, episode_reward=-0.26 +/- 0.09
Episode length: 64.68 +/- 22.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.7     |
|    mean_reward      | -0.259   |
| rollout/            |          |
|    exploration_rate | 0.438    |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.14e-05 |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.8     |
|    ep_rew_mean      | -0.0319  |
|    exploration_rate | 0.438    |
| time/               |          |
|    episodes         | 8088     |
|    fps              | 217      |
|    time_elapsed     | 684      |
|    total_timesteps  | 149079   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.64e-05 |
|    n_updates        | 27269    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.9     |
|    ep_rew_mean      | -0.0327  |
|    exploration_rate | 0.437    |
| time/               |          |
|    episodes         | 8092     |
|    fps              | 217      |
|    time_elapsed     | 684      |
|    total_timesteps  | 149198   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.19e-05 |
|    n_updates        | 27299    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.9     |
|    ep_rew_mean      | -0.0425  |
|    exploration_rate | 0.436    |
| time/               |          |
|    episodes         | 8096     |
|    fps              | 217      |
|    time_elapsed     | 684      |
|    total_timesteps  | 149277   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 27319    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.9     |
|    ep_rew_mean      | -0.0426  |
|    exploration_rate | 0.435    |
| time/               |          |
|    episodes         | 8100     |
|    fps              | 218      |
|    time_elapsed     | 684      |
|    total_timesteps  | 149360   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000175 |
|    n_updates        | 27339    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21       |
|    ep_rew_mean      | -0.0428  |
|    exploration_rate | 0.435    |
| time/               |          |
|    episodes         | 8104     |
|    fps              | 218      |
|    time_elapsed     | 684      |
|    total_timesteps  | 149435   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.18e-05 |
|    n_updates        | 27358    |
----------------------------------
Eval num_timesteps=149500, episode_reward=-0.26 +/- 0.18
Episode length: 70.28 +/- 16.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.3     |
|    mean_reward      | -0.261   |
| rollout/            |          |
|    exploration_rate | 0.434    |
| time/               |          |
|    total_timesteps  | 149500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00692  |
|    n_updates        | 27374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.9     |
|    ep_rew_mean      | -0.0426  |
|    exploration_rate | 0.434    |
| time/               |          |
|    episodes         | 8108     |
|    fps              | 217      |
|    time_elapsed     | 687      |
|    total_timesteps  | 149504   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000197 |
|    n_updates        | 27375    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21       |
|    ep_rew_mean      | -0.043   |
|    exploration_rate | 0.434    |
| time/               |          |
|    episodes         | 8112     |
|    fps              | 217      |
|    time_elapsed     | 687      |
|    total_timesteps  | 149584   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 27395    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | -0.0434  |
|    exploration_rate | 0.433    |
| time/               |          |
|    episodes         | 8116     |
|    fps              | 217      |
|    time_elapsed     | 687      |
|    total_timesteps  | 149670   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00707  |
|    n_updates        | 27417    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | -0.0541  |
|    exploration_rate | 0.432    |
| time/               |          |
|    episodes         | 8120     |
|    fps              | 217      |
|    time_elapsed     | 687      |
|    total_timesteps  | 149755   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000149 |
|    n_updates        | 27438    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | -0.0536  |
|    exploration_rate | 0.432    |
| time/               |          |
|    episodes         | 8124     |
|    fps              | 217      |
|    time_elapsed     | 687      |
|    total_timesteps  | 149826   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.22e-05 |
|    n_updates        | 27456    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.4     |
|    ep_rew_mean      | -0.0546  |
|    exploration_rate | 0.431    |
| time/               |          |
|    episodes         | 8128     |
|    fps              | 217      |
|    time_elapsed     | 687      |
|    total_timesteps  | 149922   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000172 |
|    n_updates        | 27480    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.4     |
|    ep_rew_mean      | -0.0646  |
|    exploration_rate | 0.431    |
| time/               |          |
|    episodes         | 8132     |
|    fps              | 218      |
|    time_elapsed     | 687      |
|    total_timesteps  | 149993   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.04e-05 |
|    n_updates        | 27498    |
----------------------------------
Eval num_timesteps=150000, episode_reward=0.01 +/- 0.43
Episode length: 48.06 +/- 22.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 0.0084   |
| rollout/            |          |
|    exploration_rate | 0.431    |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.91e-05 |
|    n_updates        | 27499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.5     |
|    ep_rew_mean      | -0.0651  |
|    exploration_rate | 0.43     |
| time/               |          |
|    episodes         | 8136     |
|    fps              | 217      |
|    time_elapsed     | 689      |
|    total_timesteps  | 150078   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00776  |
|    n_updates        | 27519    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.4     |
|    ep_rew_mean      | -0.0448  |
|    exploration_rate | 0.429    |
| time/               |          |
|    episodes         | 8140     |
|    fps              | 217      |
|    time_elapsed     | 689      |
|    total_timesteps  | 150156   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.74e-05 |
|    n_updates        | 27538    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.2     |
|    ep_rew_mean      | -0.0438  |
|    exploration_rate | 0.429    |
| time/               |          |
|    episodes         | 8144     |
|    fps              | 217      |
|    time_elapsed     | 689      |
|    total_timesteps  | 150222   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0067   |
|    n_updates        | 27555    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | -0.0536  |
|    exploration_rate | 0.428    |
| time/               |          |
|    episodes         | 8148     |
|    fps              | 217      |
|    time_elapsed     | 689      |
|    total_timesteps  | 150292   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 27572    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | -0.0332  |
|    exploration_rate | 0.428    |
| time/               |          |
|    episodes         | 8152     |
|    fps              | 217      |
|    time_elapsed     | 689      |
|    total_timesteps  | 150357   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.89e-05 |
|    n_updates        | 27589    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.8     |
|    ep_rew_mean      | -0.0322  |
|    exploration_rate | 0.427    |
| time/               |          |
|    episodes         | 8156     |
|    fps              | 218      |
|    time_elapsed     | 689      |
|    total_timesteps  | 150431   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000258 |
|    n_updates        | 27607    |
----------------------------------
Eval num_timesteps=150500, episode_reward=0.08 +/- 0.35
Episode length: 14.64 +/- 1.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 14.6     |
|    mean_reward      | 0.0825   |
| rollout/            |          |
|    exploration_rate | 0.427    |
| time/               |          |
|    total_timesteps  | 150500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000139 |
|    n_updates        | 27624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.9     |
|    ep_rew_mean      | -0.0324  |
|    exploration_rate | 0.426    |
| time/               |          |
|    episodes         | 8160     |
|    fps              | 217      |
|    time_elapsed     | 690      |
|    total_timesteps  | 150524   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 27630    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.7     |
|    ep_rew_mean      | -0.0317  |
|    exploration_rate | 0.426    |
| time/               |          |
|    episodes         | 8164     |
|    fps              | 218      |
|    time_elapsed     | 690      |
|    total_timesteps  | 150587   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.64e-05 |
|    n_updates        | 27646    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | -0.0334  |
|    exploration_rate | 0.425    |
| time/               |          |
|    episodes         | 8168     |
|    fps              | 218      |
|    time_elapsed     | 690      |
|    total_timesteps  | 150691   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00736  |
|    n_updates        | 27672    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | -0.0261  |
|    exploration_rate | 0.424    |
| time/               |          |
|    episodes         | 8172     |
|    fps              | 218      |
|    time_elapsed     | 690      |
|    total_timesteps  | 150838   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0078   |
|    n_updates        | 27709    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.9     |
|    ep_rew_mean      | -0.0268  |
|    exploration_rate | 0.423    |
| time/               |          |
|    episodes         | 8176     |
|    fps              | 218      |
|    time_elapsed     | 690      |
|    total_timesteps  | 150938   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.59e-05 |
|    n_updates        | 27734    |
----------------------------------
Eval num_timesteps=151000, episode_reward=-0.24 +/- 0.10
Episode length: 60.00 +/- 25.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60       |
|    mean_reward      | -0.24    |
| rollout/            |          |
|    exploration_rate | 0.423    |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00783  |
|    n_updates        | 27749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | -0.0158  |
|    exploration_rate | 0.423    |
| time/               |          |
|    episodes         | 8180     |
|    fps              | 217      |
|    time_elapsed     | 692      |
|    total_timesteps  | 151002   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000162 |
|    n_updates        | 27750    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21       |
|    ep_rew_mean      | -0.0231  |
|    exploration_rate | 0.422    |
| time/               |          |
|    episodes         | 8184     |
|    fps              | 217      |
|    time_elapsed     | 693      |
|    total_timesteps  | 151069   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 27767    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.6     |
|    ep_rew_mean      | -0.0216  |
|    exploration_rate | 0.422    |
| time/               |          |
|    episodes         | 8188     |
|    fps              | 218      |
|    time_elapsed     | 693      |
|    total_timesteps  | 151144   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00688  |
|    n_updates        | 27785    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.2     |
|    ep_rew_mean      | -0.0199  |
|    exploration_rate | 0.421    |
| time/               |          |
|    episodes         | 8192     |
|    fps              | 218      |
|    time_elapsed     | 693      |
|    total_timesteps  | 151219   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.01e-05 |
|    n_updates        | 27804    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | -0.00944 |
|    exploration_rate | 0.42     |
| time/               |          |
|    episodes         | 8196     |
|    fps              | 218      |
|    time_elapsed     | 693      |
|    total_timesteps  | 151288   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.96e-05 |
|    n_updates        | 27821    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | -0.00936 |
|    exploration_rate | 0.42     |
| time/               |          |
|    episodes         | 8200     |
|    fps              | 218      |
|    time_elapsed     | 693      |
|    total_timesteps  | 151369   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000161 |
|    n_updates        | 27842    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20       |
|    ep_rew_mean      | -0.00908 |
|    exploration_rate | 0.419    |
| time/               |          |
|    episodes         | 8204     |
|    fps              | 218      |
|    time_elapsed     | 693      |
|    total_timesteps  | 151437   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.23e-05 |
|    n_updates        | 27859    |
----------------------------------
Eval num_timesteps=151500, episode_reward=-0.29 +/- 0.04
Episode length: 72.10 +/- 10.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.1     |
|    mean_reward      | -0.288   |
| rollout/            |          |
|    exploration_rate | 0.419    |
| time/               |          |
|    total_timesteps  | 151500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.38e-05 |
|    n_updates        | 27874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | -0.00928 |
|    exploration_rate | 0.419    |
| time/               |          |
|    episodes         | 8208     |
|    fps              | 217      |
|    time_elapsed     | 695      |
|    total_timesteps  | 151511   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00778  |
|    n_updates        | 27877    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20       |
|    ep_rew_mean      | 0.00099  |
|    exploration_rate | 0.418    |
| time/               |          |
|    episodes         | 8212     |
|    fps              | 217      |
|    time_elapsed     | 695      |
|    total_timesteps  | 151584   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00661  |
|    n_updates        | 27895    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.2     |
|    ep_rew_mean      | 0.00011  |
|    exploration_rate | 0.417    |
| time/               |          |
|    episodes         | 8216     |
|    fps              | 217      |
|    time_elapsed     | 696      |
|    total_timesteps  | 151692   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.23e-05 |
|    n_updates        | 27922    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.00075  |
|    exploration_rate | 0.417    |
| time/               |          |
|    episodes         | 8220     |
|    fps              | 218      |
|    time_elapsed     | 696      |
|    total_timesteps  | 151761   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.4e-05  |
|    n_updates        | 27940    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | -9e-05   |
|    exploration_rate | 0.416    |
| time/               |          |
|    episodes         | 8224     |
|    fps              | 218      |
|    time_elapsed     | 696      |
|    total_timesteps  | 151853   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00695  |
|    n_updates        | 27963    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0214   |
|    exploration_rate | 0.416    |
| time/               |          |
|    episodes         | 8228     |
|    fps              | 218      |
|    time_elapsed     | 696      |
|    total_timesteps  | 151912   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000232 |
|    n_updates        | 27977    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | 0.0318   |
|    exploration_rate | 0.415    |
| time/               |          |
|    episodes         | 8232     |
|    fps              | 218      |
|    time_elapsed     | 696      |
|    total_timesteps  | 151973   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 27993    |
----------------------------------
Eval num_timesteps=152000, episode_reward=-0.28 +/- 0.15
Episode length: 74.58 +/- 2.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.6     |
|    mean_reward      | -0.278   |
| rollout/            |          |
|    exploration_rate | 0.415    |
| time/               |          |
|    total_timesteps  | 152000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.72e-05 |
|    n_updates        | 27999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0313   |
|    exploration_rate | 0.414    |
| time/               |          |
|    episodes         | 8236     |
|    fps              | 217      |
|    time_elapsed     | 698      |
|    total_timesteps  | 152069   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 28017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20       |
|    ep_rew_mean      | 0.011    |
|    exploration_rate | 0.414    |
| time/               |          |
|    episodes         | 8240     |
|    fps              | 217      |
|    time_elapsed     | 698      |
|    total_timesteps  | 152157   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 28039    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.4     |
|    ep_rew_mean      | 0.00932  |
|    exploration_rate | 0.413    |
| time/               |          |
|    episodes         | 8244     |
|    fps              | 217      |
|    time_elapsed     | 699      |
|    total_timesteps  | 152264   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.06e-05 |
|    n_updates        | 28065    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.4     |
|    ep_rew_mean      | 0.0195   |
|    exploration_rate | 0.412    |
| time/               |          |
|    episodes         | 8248     |
|    fps              | 217      |
|    time_elapsed     | 699      |
|    total_timesteps  | 152330   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00673  |
|    n_updates        | 28082    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.4     |
|    ep_rew_mean      | -0.00081 |
|    exploration_rate | 0.412    |
| time/               |          |
|    episodes         | 8252     |
|    fps              | 217      |
|    time_elapsed     | 699      |
|    total_timesteps  | 152402   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0078   |
|    n_updates        | 28100    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.4     |
|    ep_rew_mean      | -0.00081 |
|    exploration_rate | 0.411    |
| time/               |          |
|    episodes         | 8256     |
|    fps              | 218      |
|    time_elapsed     | 699      |
|    total_timesteps  | 152476   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000118 |
|    n_updates        | 28118    |
----------------------------------
Eval num_timesteps=152500, episode_reward=-0.09 +/- 0.26
Episode length: 36.62 +/- 23.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.6     |
|    mean_reward      | -0.0856  |
| rollout/            |          |
|    exploration_rate | 0.411    |
| time/               |          |
|    total_timesteps  | 152500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 28124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | -0.00013 |
|    exploration_rate | 0.411    |
| time/               |          |
|    episodes         | 8260     |
|    fps              | 217      |
|    time_elapsed     | 700      |
|    total_timesteps  | 152552   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000167 |
|    n_updates        | 28137    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | -0.00037 |
|    exploration_rate | 0.41     |
| time/               |          |
|    episodes         | 8264     |
|    fps              | 217      |
|    time_elapsed     | 700      |
|    total_timesteps  | 152621   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00686  |
|    n_updates        | 28155    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.00067  |
|    exploration_rate | 0.409    |
| time/               |          |
|    episodes         | 8268     |
|    fps              | 217      |
|    time_elapsed     | 700      |
|    total_timesteps  | 152699   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.84e-05 |
|    n_updates        | 28174    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.00669 |
|    exploration_rate | 0.409    |
| time/               |          |
|    episodes         | 8272     |
|    fps              | 218      |
|    time_elapsed     | 700      |
|    total_timesteps  | 152780   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.75e-05 |
|    n_updates        | 28194    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | -0.00589 |
|    exploration_rate | 0.408    |
| time/               |          |
|    episodes         | 8276     |
|    fps              | 218      |
|    time_elapsed     | 700      |
|    total_timesteps  | 152860   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.36e-05 |
|    n_updates        | 28214    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | -0.0168  |
|    exploration_rate | 0.407    |
| time/               |          |
|    episodes         | 8280     |
|    fps              | 218      |
|    time_elapsed     | 700      |
|    total_timesteps  | 152946   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000225 |
|    n_updates        | 28236    |
----------------------------------
Eval num_timesteps=153000, episode_reward=-0.08 +/- 0.27
Episode length: 39.04 +/- 20.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39       |
|    mean_reward      | -0.0752  |
| rollout/            |          |
|    exploration_rate | 0.407    |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000153 |
|    n_updates        | 28249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.00709 |
|    exploration_rate | 0.407    |
| time/               |          |
|    episodes         | 8284     |
|    fps              | 217      |
|    time_elapsed     | 702      |
|    total_timesteps  | 153021   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0067   |
|    n_updates        | 28255    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.00713 |
|    exploration_rate | 0.406    |
| time/               |          |
|    episodes         | 8288     |
|    fps              | 217      |
|    time_elapsed     | 702      |
|    total_timesteps  | 153097   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00684  |
|    n_updates        | 28274    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0028   |
|    exploration_rate | 0.406    |
| time/               |          |
|    episodes         | 8292     |
|    fps              | 218      |
|    time_elapsed     | 702      |
|    total_timesteps  | 153174   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000161 |
|    n_updates        | 28293    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | -0.00774 |
|    exploration_rate | 0.405    |
| time/               |          |
|    episodes         | 8296     |
|    fps              | 218      |
|    time_elapsed     | 702      |
|    total_timesteps  | 153256   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 28313    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | -0.00762 |
|    exploration_rate | 0.404    |
| time/               |          |
|    episodes         | 8300     |
|    fps              | 218      |
|    time_elapsed     | 702      |
|    total_timesteps  | 153334   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00676  |
|    n_updates        | 28333    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | -0.00926 |
|    exploration_rate | 0.404    |
| time/               |          |
|    episodes         | 8304     |
|    fps              | 218      |
|    time_elapsed     | 702      |
|    total_timesteps  | 153443   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.33e-05 |
|    n_updates        | 28360    |
----------------------------------
Eval num_timesteps=153500, episode_reward=-0.03 +/- 0.30
Episode length: 28.74 +/- 23.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 28.7     |
|    mean_reward      | -0.034   |
| rollout/            |          |
|    exploration_rate | 0.403    |
| time/               |          |
|    total_timesteps  | 153500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.02e-05 |
|    n_updates        | 28374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | -0.0093  |
|    exploration_rate | 0.403    |
| time/               |          |
|    episodes         | 8308     |
|    fps              | 218      |
|    time_elapsed     | 703      |
|    total_timesteps  | 153518   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000153 |
|    n_updates        | 28379    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20       |
|    ep_rew_mean      | -0.019   |
|    exploration_rate | 0.402    |
| time/               |          |
|    episodes         | 8312     |
|    fps              | 218      |
|    time_elapsed     | 703      |
|    total_timesteps  | 153584   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000179 |
|    n_updates        | 28395    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | -0.018   |
|    exploration_rate | 0.402    |
| time/               |          |
|    episodes         | 8316     |
|    fps              | 218      |
|    time_elapsed     | 703      |
|    total_timesteps  | 153666   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 28416    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | -0.0183  |
|    exploration_rate | 0.401    |
| time/               |          |
|    episodes         | 8320     |
|    fps              | 218      |
|    time_elapsed     | 704      |
|    total_timesteps  | 153743   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00695  |
|    n_updates        | 28435    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | -0.00769 |
|    exploration_rate | 0.401    |
| time/               |          |
|    episodes         | 8324     |
|    fps              | 218      |
|    time_elapsed     | 704      |
|    total_timesteps  | 153820   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 28454    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20       |
|    ep_rew_mean      | -0.0289  |
|    exploration_rate | 0.4      |
| time/               |          |
|    episodes         | 8328     |
|    fps              | 218      |
|    time_elapsed     | 704      |
|    total_timesteps  | 153909   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.08e-05 |
|    n_updates        | 28477    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.2     |
|    ep_rew_mean      | -0.0297  |
|    exploration_rate | 0.399    |
| time/               |          |
|    episodes         | 8332     |
|    fps              | 218      |
|    time_elapsed     | 704      |
|    total_timesteps  | 153991   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000172 |
|    n_updates        | 28497    |
----------------------------------
Eval num_timesteps=154000, episode_reward=-0.04 +/- 0.26
Episode length: 26.36 +/- 17.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 26.4     |
|    mean_reward      | -0.0445  |
| rollout/            |          |
|    exploration_rate | 0.399    |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000154 |
|    n_updates        | 28499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20       |
|    ep_rew_mean      | -0.0291  |
|    exploration_rate | 0.399    |
| time/               |          |
|    episodes         | 8336     |
|    fps              | 218      |
|    time_elapsed     | 705      |
|    total_timesteps  | 154072   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 28517    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | -0.00844 |
|    exploration_rate | 0.398    |
| time/               |          |
|    episodes         | 8340     |
|    fps              | 218      |
|    time_elapsed     | 705      |
|    total_timesteps  | 154143   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00662  |
|    n_updates        | 28535    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.00309  |
|    exploration_rate | 0.398    |
| time/               |          |
|    episodes         | 8344     |
|    fps              | 218      |
|    time_elapsed     | 705      |
|    total_timesteps  | 154212   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000176 |
|    n_updates        | 28552    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.00314  |
|    exploration_rate | 0.397    |
| time/               |          |
|    episodes         | 8348     |
|    fps              | 218      |
|    time_elapsed     | 705      |
|    total_timesteps  | 154277   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 28569    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0133   |
|    exploration_rate | 0.396    |
| time/               |          |
|    episodes         | 8352     |
|    fps              | 218      |
|    time_elapsed     | 705      |
|    total_timesteps  | 154345   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0142   |
|    n_updates        | 28586    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0238   |
|    exploration_rate | 0.396    |
| time/               |          |
|    episodes         | 8356     |
|    fps              | 218      |
|    time_elapsed     | 705      |
|    total_timesteps  | 154407   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000218 |
|    n_updates        | 28601    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0242   |
|    exploration_rate | 0.395    |
| time/               |          |
|    episodes         | 8360     |
|    fps              | 218      |
|    time_elapsed     | 705      |
|    total_timesteps  | 154473   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.6e-05  |
|    n_updates        | 28618    |
----------------------------------
Eval num_timesteps=154500, episode_reward=0.08 +/- 0.38
Episode length: 20.08 +/- 13.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.1     |
|    mean_reward      | 0.0807   |
| rollout/            |          |
|    exploration_rate | 0.395    |
| time/               |          |
|    total_timesteps  | 154500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.98e-05 |
|    n_updates        | 28624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.3     |
|    ep_rew_mean      | 0.0237   |
|    exploration_rate | 0.395    |
| time/               |          |
|    episodes         | 8364     |
|    fps              | 218      |
|    time_elapsed     | 706      |
|    total_timesteps  | 154555   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000193 |
|    n_updates        | 28638    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0241   |
|    exploration_rate | 0.394    |
| time/               |          |
|    episodes         | 8368     |
|    fps              | 218      |
|    time_elapsed     | 706      |
|    total_timesteps  | 154623   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 28655    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0246   |
|    exploration_rate | 0.394    |
| time/               |          |
|    episodes         | 8372     |
|    fps              | 218      |
|    time_elapsed     | 706      |
|    total_timesteps  | 154692   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.85e-05 |
|    n_updates        | 28672    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0348   |
|    exploration_rate | 0.393    |
| time/               |          |
|    episodes         | 8376     |
|    fps              | 219      |
|    time_elapsed     | 706      |
|    total_timesteps  | 154765   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.37e-05 |
|    n_updates        | 28691    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0354   |
|    exploration_rate | 0.393    |
| time/               |          |
|    episodes         | 8380     |
|    fps              | 219      |
|    time_elapsed     | 706      |
|    total_timesteps  | 154837   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00017  |
|    n_updates        | 28709    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0253   |
|    exploration_rate | 0.392    |
| time/               |          |
|    episodes         | 8384     |
|    fps              | 219      |
|    time_elapsed     | 706      |
|    total_timesteps  | 154914   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 28728    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0253   |
|    exploration_rate | 0.391    |
| time/               |          |
|    episodes         | 8388     |
|    fps              | 219      |
|    time_elapsed     | 706      |
|    total_timesteps  | 154991   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 28747    |
----------------------------------
Eval num_timesteps=155000, episode_reward=0.02 +/- 0.31
Episode length: 25.60 +/- 14.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 25.6     |
|    mean_reward      | 0.0187   |
| rollout/            |          |
|    exploration_rate | 0.391    |
| time/               |          |
|    total_timesteps  | 155000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 28749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0155   |
|    exploration_rate | 0.391    |
| time/               |          |
|    episodes         | 8392     |
|    fps              | 219      |
|    time_elapsed     | 707      |
|    total_timesteps  | 155063   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000264 |
|    n_updates        | 28765    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0159   |
|    exploration_rate | 0.39     |
| time/               |          |
|    episodes         | 8396     |
|    fps              | 219      |
|    time_elapsed     | 707      |
|    total_timesteps  | 155134   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000214 |
|    n_updates        | 28783    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0162   |
|    exploration_rate | 0.39     |
| time/               |          |
|    episodes         | 8400     |
|    fps              | 219      |
|    time_elapsed     | 708      |
|    total_timesteps  | 155205   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000228 |
|    n_updates        | 28801    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0173   |
|    exploration_rate | 0.389    |
| time/               |          |
|    episodes         | 8404     |
|    fps              | 219      |
|    time_elapsed     | 708      |
|    total_timesteps  | 155287   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00681  |
|    n_updates        | 28821    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.4     |
|    ep_rew_mean      | 0.0174   |
|    exploration_rate | 0.388    |
| time/               |          |
|    episodes         | 8408     |
|    fps              | 219      |
|    time_elapsed     | 708      |
|    total_timesteps  | 155360   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00675  |
|    n_updates        | 28839    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.7     |
|    ep_rew_mean      | 0.0261   |
|    exploration_rate | 0.388    |
| time/               |          |
|    episodes         | 8412     |
|    fps              | 219      |
|    time_elapsed     | 708      |
|    total_timesteps  | 155457   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.95e-05 |
|    n_updates        | 28864    |
----------------------------------
Eval num_timesteps=155500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.387    |
| time/               |          |
|    total_timesteps  | 155500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0145   |
|    n_updates        | 28874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.6     |
|    ep_rew_mean      | 0.0267   |
|    exploration_rate | 0.387    |
| time/               |          |
|    episodes         | 8416     |
|    fps              | 218      |
|    time_elapsed     | 710      |
|    total_timesteps  | 155526   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 28881    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.9     |
|    ep_rew_mean      | 0.0254   |
|    exploration_rate | 0.386    |
| time/               |          |
|    episodes         | 8420     |
|    fps              | 218      |
|    time_elapsed     | 710      |
|    total_timesteps  | 155634   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.16e-05 |
|    n_updates        | 28908    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0147   |
|    exploration_rate | 0.385    |
| time/               |          |
|    episodes         | 8424     |
|    fps              | 219      |
|    time_elapsed     | 711      |
|    total_timesteps  | 155729   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00712  |
|    n_updates        | 28932    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0147   |
|    exploration_rate | 0.385    |
| time/               |          |
|    episodes         | 8428     |
|    fps              | 219      |
|    time_elapsed     | 711      |
|    total_timesteps  | 155818   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 28954    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.00442  |
|    exploration_rate | 0.384    |
| time/               |          |
|    episodes         | 8432     |
|    fps              | 219      |
|    time_elapsed     | 711      |
|    total_timesteps  | 155907   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 28976    |
----------------------------------
Eval num_timesteps=156000, episode_reward=-0.12 +/- 0.31
Episode length: 54.36 +/- 23.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.4     |
|    mean_reward      | -0.117   |
| rollout/            |          |
|    exploration_rate | 0.383    |
| time/               |          |
|    total_timesteps  | 156000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000277 |
|    n_updates        | 28999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0133   |
|    exploration_rate | 0.383    |
| time/               |          |
|    episodes         | 8436     |
|    fps              | 218      |
|    time_elapsed     | 713      |
|    total_timesteps  | 156016   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00701  |
|    n_updates        | 29003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | -0.00689 |
|    exploration_rate | 0.383    |
| time/               |          |
|    episodes         | 8440     |
|    fps              | 218      |
|    time_elapsed     | 713      |
|    total_timesteps  | 156092   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 29022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | -0.00716 |
|    exploration_rate | 0.382    |
| time/               |          |
|    episodes         | 8444     |
|    fps              | 218      |
|    time_elapsed     | 713      |
|    total_timesteps  | 156168   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 29041    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | -0.00763 |
|    exploration_rate | 0.381    |
| time/               |          |
|    episodes         | 8448     |
|    fps              | 219      |
|    time_elapsed     | 713      |
|    total_timesteps  | 156245   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 29061    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | -0.0176  |
|    exploration_rate | 0.381    |
| time/               |          |
|    episodes         | 8452     |
|    fps              | 219      |
|    time_elapsed     | 713      |
|    total_timesteps  | 156313   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.21e-05 |
|    n_updates        | 29078    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | -0.03    |
|    exploration_rate | 0.38     |
| time/               |          |
|    episodes         | 8456     |
|    fps              | 219      |
|    time_elapsed     | 713      |
|    total_timesteps  | 156433   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.81e-05 |
|    n_updates        | 29108    |
----------------------------------
Eval num_timesteps=156500, episode_reward=0.08 +/- 0.38
Episode length: 20.46 +/- 11.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.5     |
|    mean_reward      | 0.0792   |
| rollout/            |          |
|    exploration_rate | 0.379    |
| time/               |          |
|    total_timesteps  | 156500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00025  |
|    n_updates        | 29124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | -0.0303  |
|    exploration_rate | 0.379    |
| time/               |          |
|    episodes         | 8460     |
|    fps              | 219      |
|    time_elapsed     | 714      |
|    total_timesteps  | 156507   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00678  |
|    n_updates        | 29126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.2     |
|    ep_rew_mean      | -0.0298  |
|    exploration_rate | 0.379    |
| time/               |          |
|    episodes         | 8464     |
|    fps              | 219      |
|    time_elapsed     | 714      |
|    total_timesteps  | 156577   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00778  |
|    n_updates        | 29144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.5     |
|    ep_rew_mean      | -0.0309  |
|    exploration_rate | 0.378    |
| time/               |          |
|    episodes         | 8468     |
|    fps              | 219      |
|    time_elapsed     | 714      |
|    total_timesteps  | 156672   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00773  |
|    n_updates        | 29167    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.8     |
|    ep_rew_mean      | -0.0323  |
|    exploration_rate | 0.377    |
| time/               |          |
|    episodes         | 8472     |
|    fps              | 219      |
|    time_elapsed     | 714      |
|    total_timesteps  | 156776   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.86e-05 |
|    n_updates        | 29193    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.2     |
|    ep_rew_mean      | -0.0436  |
|    exploration_rate | 0.376    |
| time/               |          |
|    episodes         | 8476     |
|    fps              | 219      |
|    time_elapsed     | 714      |
|    total_timesteps  | 156883   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.28e-05 |
|    n_updates        | 29220    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | -0.0443  |
|    exploration_rate | 0.376    |
| time/               |          |
|    episodes         | 8480     |
|    fps              | 219      |
|    time_elapsed     | 714      |
|    total_timesteps  | 156971   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.29e-05 |
|    n_updates        | 29242    |
----------------------------------
Eval num_timesteps=157000, episode_reward=0.03 +/- 0.41
Episode length: 42.74 +/- 23.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 0.03     |
| rollout/            |          |
|    exploration_rate | 0.375    |
| time/               |          |
|    total_timesteps  | 157000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 29249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | -0.0457  |
|    exploration_rate | 0.375    |
| time/               |          |
|    episodes         | 8484     |
|    fps              | 219      |
|    time_elapsed     | 716      |
|    total_timesteps  | 157084   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 29270    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.6     |
|    ep_rew_mean      | -0.0353  |
|    exploration_rate | 0.374    |
| time/               |          |
|    episodes         | 8488     |
|    fps              | 219      |
|    time_elapsed     | 716      |
|    total_timesteps  | 157149   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000173 |
|    n_updates        | 29287    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | -0.0358  |
|    exploration_rate | 0.373    |
| time/               |          |
|    episodes         | 8492     |
|    fps              | 219      |
|    time_elapsed     | 716      |
|    total_timesteps  | 157235   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00786  |
|    n_updates        | 29308    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | -0.0262  |
|    exploration_rate | 0.373    |
| time/               |          |
|    episodes         | 8496     |
|    fps              | 219      |
|    time_elapsed     | 716      |
|    total_timesteps  | 157315   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.92e-05 |
|    n_updates        | 29328    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | -0.0159  |
|    exploration_rate | 0.372    |
| time/               |          |
|    episodes         | 8500     |
|    fps              | 219      |
|    time_elapsed     | 716      |
|    total_timesteps  | 157380   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00697  |
|    n_updates        | 29344    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.6     |
|    ep_rew_mean      | -0.0153  |
|    exploration_rate | 0.372    |
| time/               |          |
|    episodes         | 8504     |
|    fps              | 219      |
|    time_elapsed     | 716      |
|    total_timesteps  | 157447   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 29361    |
----------------------------------
Eval num_timesteps=157500, episode_reward=-0.03 +/- 0.20
Episode length: 16.88 +/- 0.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | -0.0265  |
| rollout/            |          |
|    exploration_rate | 0.371    |
| time/               |          |
|    total_timesteps  | 157500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00692  |
|    n_updates        | 29374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.6     |
|    ep_rew_mean      | -0.0153  |
|    exploration_rate | 0.371    |
| time/               |          |
|    episodes         | 8508     |
|    fps              | 219      |
|    time_elapsed     | 717      |
|    total_timesteps  | 157519   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0142   |
|    n_updates        | 29379    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | -0.0241  |
|    exploration_rate | 0.371    |
| time/               |          |
|    episodes         | 8512     |
|    fps              | 219      |
|    time_elapsed     | 717      |
|    total_timesteps  | 157585   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.78e-05 |
|    n_updates        | 29396    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | -0.0141  |
|    exploration_rate | 0.37     |
| time/               |          |
|    episodes         | 8516     |
|    fps              | 219      |
|    time_elapsed     | 717      |
|    total_timesteps  | 157655   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.96e-05 |
|    n_updates        | 29413    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.9     |
|    ep_rew_mean      | -0.0125  |
|    exploration_rate | 0.37     |
| time/               |          |
|    episodes         | 8520     |
|    fps              | 219      |
|    time_elapsed     | 717      |
|    total_timesteps  | 157723   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 29430    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.6     |
|    ep_rew_mean      | -0.00123 |
|    exploration_rate | 0.369    |
| time/               |          |
|    episodes         | 8524     |
|    fps              | 219      |
|    time_elapsed     | 717      |
|    total_timesteps  | 157786   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00688  |
|    n_updates        | 29446    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.6     |
|    ep_rew_mean      | -0.00131 |
|    exploration_rate | 0.368    |
| time/               |          |
|    episodes         | 8528     |
|    fps              | 219      |
|    time_elapsed     | 717      |
|    total_timesteps  | 157877   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.79e-05 |
|    n_updates        | 29469    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.4     |
|    ep_rew_mean      | -0.00055 |
|    exploration_rate | 0.368    |
| time/               |          |
|    episodes         | 8532     |
|    fps              | 220      |
|    time_elapsed     | 717      |
|    total_timesteps  | 157947   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000197 |
|    n_updates        | 29486    |
----------------------------------
Eval num_timesteps=158000, episode_reward=-0.04 +/- 0.37
Episode length: 46.08 +/- 24.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | -0.0436  |
| rollout/            |          |
|    exploration_rate | 0.367    |
| time/               |          |
|    total_timesteps  | 158000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000149 |
|    n_updates        | 29499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.2     |
|    ep_rew_mean      | 0.00028  |
|    exploration_rate | 0.367    |
| time/               |          |
|    episodes         | 8536     |
|    fps              | 219      |
|    time_elapsed     | 719      |
|    total_timesteps  | 158035   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 29508    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | -4e-05   |
|    exploration_rate | 0.366    |
| time/               |          |
|    episodes         | 8540     |
|    fps              | 219      |
|    time_elapsed     | 719      |
|    total_timesteps  | 158119   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.007    |
|    n_updates        | 29529    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.2     |
|    ep_rew_mean      | 0.0102   |
|    exploration_rate | 0.366    |
| time/               |          |
|    episodes         | 8544     |
|    fps              | 219      |
|    time_elapsed     | 719      |
|    total_timesteps  | 158189   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.48e-05 |
|    n_updates        | 29547    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.0106   |
|    exploration_rate | 0.365    |
| time/               |          |
|    episodes         | 8548     |
|    fps              | 219      |
|    time_elapsed     | 719      |
|    total_timesteps  | 158254   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 29563    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.0205   |
|    exploration_rate | 0.365    |
| time/               |          |
|    episodes         | 8552     |
|    fps              | 219      |
|    time_elapsed     | 719      |
|    total_timesteps  | 158326   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 29581    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0222   |
|    exploration_rate | 0.364    |
| time/               |          |
|    episodes         | 8556     |
|    fps              | 220      |
|    time_elapsed     | 719      |
|    total_timesteps  | 158404   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.02e-05 |
|    n_updates        | 29600    |
----------------------------------
Eval num_timesteps=158500, episode_reward=-0.19 +/- 0.32
Episode length: 67.08 +/- 15.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.1     |
|    mean_reward      | -0.188   |
| rollout/            |          |
|    exploration_rate | 0.363    |
| time/               |          |
|    total_timesteps  | 158500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 29624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0213   |
|    exploration_rate | 0.363    |
| time/               |          |
|    episodes         | 8560     |
|    fps              | 219      |
|    time_elapsed     | 722      |
|    total_timesteps  | 158500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.0207   |
|    exploration_rate | 0.363    |
| time/               |          |
|    episodes         | 8564     |
|    fps              | 219      |
|    time_elapsed     | 722      |
|    total_timesteps  | 158584   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0155   |
|    n_updates        | 29645    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.0213   |
|    exploration_rate | 0.362    |
| time/               |          |
|    episodes         | 8568     |
|    fps              | 219      |
|    time_elapsed     | 722      |
|    total_timesteps  | 158665   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00697  |
|    n_updates        | 29666    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.8     |
|    ep_rew_mean      | 0.0319   |
|    exploration_rate | 0.361    |
| time/               |          |
|    episodes         | 8572     |
|    fps              | 219      |
|    time_elapsed     | 722      |
|    total_timesteps  | 158753   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000175 |
|    n_updates        | 29688    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0335   |
|    exploration_rate | 0.361    |
| time/               |          |
|    episodes         | 8576     |
|    fps              | 219      |
|    time_elapsed     | 722      |
|    total_timesteps  | 158822   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.74e-05 |
|    n_updates        | 29705    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.2     |
|    ep_rew_mean      | 0.0342   |
|    exploration_rate | 0.36     |
| time/               |          |
|    episodes         | 8580     |
|    fps              | 219      |
|    time_elapsed     | 722      |
|    total_timesteps  | 158892   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00024  |
|    n_updates        | 29722    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.036    |
|    exploration_rate | 0.36     |
| time/               |          |
|    episodes         | 8584     |
|    fps              | 219      |
|    time_elapsed     | 722      |
|    total_timesteps  | 158960   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.5e-05  |
|    n_updates        | 29739    |
----------------------------------
Eval num_timesteps=159000, episode_reward=-0.15 +/- 0.31
Episode length: 58.06 +/- 20.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.1     |
|    mean_reward      | -0.152   |
| rollout/            |          |
|    exploration_rate | 0.359    |
| time/               |          |
|    total_timesteps  | 159000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.48e-05 |
|    n_updates        | 29749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 18.8     |
|    ep_rew_mean      | 0.0257   |
|    exploration_rate | 0.359    |
| time/               |          |
|    episodes         | 8588     |
|    fps              | 219      |
|    time_elapsed     | 724      |
|    total_timesteps  | 159032   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00672  |
|    n_updates        | 29757    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0248   |
|    exploration_rate | 0.358    |
| time/               |          |
|    episodes         | 8592     |
|    fps              | 219      |
|    time_elapsed     | 724      |
|    total_timesteps  | 159140   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00676  |
|    n_updates        | 29784    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0148   |
|    exploration_rate | 0.357    |
| time/               |          |
|    episodes         | 8596     |
|    fps              | 219      |
|    time_elapsed     | 724      |
|    total_timesteps  | 159221   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000217 |
|    n_updates        | 29805    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.00232  |
|    exploration_rate | 0.356    |
| time/               |          |
|    episodes         | 8600     |
|    fps              | 219      |
|    time_elapsed     | 724      |
|    total_timesteps  | 159348   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 29836    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.00164  |
|    exploration_rate | 0.356    |
| time/               |          |
|    episodes         | 8604     |
|    fps              | 219      |
|    time_elapsed     | 725      |
|    total_timesteps  | 159432   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00681  |
|    n_updates        | 29857    |
----------------------------------
Eval num_timesteps=159500, episode_reward=-0.00 +/- 0.38
Episode length: 35.32 +/- 21.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.3     |
|    mean_reward      | -0.0004  |
| rollout/            |          |
|    exploration_rate | 0.355    |
| time/               |          |
|    total_timesteps  | 159500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000195 |
|    n_updates        | 29874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.9     |
|    ep_rew_mean      | 0.00132  |
|    exploration_rate | 0.355    |
| time/               |          |
|    episodes         | 8608     |
|    fps              | 219      |
|    time_elapsed     | 726      |
|    total_timesteps  | 159512   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00781  |
|    n_updates        | 29877    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.2     |
|    ep_rew_mean      | 0.00032  |
|    exploration_rate | 0.354    |
| time/               |          |
|    episodes         | 8612     |
|    fps              | 219      |
|    time_elapsed     | 726      |
|    total_timesteps  | 159603   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 29900    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.2     |
|    ep_rew_mean      | 0.00035  |
|    exploration_rate | 0.354    |
| time/               |          |
|    episodes         | 8616     |
|    fps              | 219      |
|    time_elapsed     | 726      |
|    total_timesteps  | 159672   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.42e-05 |
|    n_updates        | 29917    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | 0.0097   |
|    exploration_rate | 0.353    |
| time/               |          |
|    episodes         | 8620     |
|    fps              | 219      |
|    time_elapsed     | 726      |
|    total_timesteps  | 159756   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 29938    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.7     |
|    ep_rew_mean      | -0.00191 |
|    exploration_rate | 0.352    |
| time/               |          |
|    episodes         | 8624     |
|    fps              | 219      |
|    time_elapsed     | 726      |
|    total_timesteps  | 159859   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 29964    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.8     |
|    ep_rew_mean      | -0.00211 |
|    exploration_rate | 0.351    |
| time/               |          |
|    episodes         | 8628     |
|    fps              | 220      |
|    time_elapsed     | 726      |
|    total_timesteps  | 159955   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000156 |
|    n_updates        | 29988    |
----------------------------------
Eval num_timesteps=160000, episode_reward=0.03 +/- 0.30
Episode length: 16.92 +/- 0.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | 0.0334   |
| rollout/            |          |
|    exploration_rate | 0.351    |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00688  |
|    n_updates        | 29999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.8     |
|    ep_rew_mean      | -0.00199 |
|    exploration_rate | 0.351    |
| time/               |          |
|    episodes         | 8632     |
|    fps              | 219      |
|    time_elapsed     | 727      |
|    total_timesteps  | 160022   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000151 |
|    n_updates        | 30005    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.7     |
|    ep_rew_mean      | -0.00184 |
|    exploration_rate | 0.35     |
| time/               |          |
|    episodes         | 8636     |
|    fps              | 220      |
|    time_elapsed     | 727      |
|    total_timesteps  | 160106   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000218 |
|    n_updates        | 30026    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.7     |
|    ep_rew_mean      | -0.00196 |
|    exploration_rate | 0.35     |
| time/               |          |
|    episodes         | 8640     |
|    fps              | 220      |
|    time_elapsed     | 727      |
|    total_timesteps  | 160193   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.57e-05 |
|    n_updates        | 30048    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.9     |
|    ep_rew_mean      | -0.0127  |
|    exploration_rate | 0.349    |
| time/               |          |
|    episodes         | 8644     |
|    fps              | 220      |
|    time_elapsed     | 727      |
|    total_timesteps  | 160281   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000219 |
|    n_updates        | 30070    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.2     |
|    ep_rew_mean      | -0.014   |
|    exploration_rate | 0.348    |
| time/               |          |
|    episodes         | 8648     |
|    fps              | 220      |
|    time_elapsed     | 727      |
|    total_timesteps  | 160379   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000141 |
|    n_updates        | 30094    |
----------------------------------
Eval num_timesteps=160500, episode_reward=-0.30 +/- 0.00
Episode length: 74.88 +/- 0.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.9     |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.347    |
| time/               |          |
|    total_timesteps  | 160500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.09e-05 |
|    n_updates        | 30124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | -0.026   |
|    exploration_rate | 0.347    |
| time/               |          |
|    episodes         | 8652     |
|    fps              | 219      |
|    time_elapsed     | 730      |
|    total_timesteps  | 160502   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0138   |
|    n_updates        | 30125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | -0.0261  |
|    exploration_rate | 0.346    |
| time/               |          |
|    episodes         | 8656     |
|    fps              | 219      |
|    time_elapsed     | 730      |
|    total_timesteps  | 160583   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000214 |
|    n_updates        | 30145    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.6     |
|    ep_rew_mean      | -0.0252  |
|    exploration_rate | 0.346    |
| time/               |          |
|    episodes         | 8660     |
|    fps              | 219      |
|    time_elapsed     | 730      |
|    total_timesteps  | 160656   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0135   |
|    n_updates        | 30163    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.5     |
|    ep_rew_mean      | -0.0149  |
|    exploration_rate | 0.345    |
| time/               |          |
|    episodes         | 8664     |
|    fps              | 219      |
|    time_elapsed     | 730      |
|    total_timesteps  | 160732   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000151 |
|    n_updates        | 30182    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.4     |
|    ep_rew_mean      | -0.0146  |
|    exploration_rate | 0.345    |
| time/               |          |
|    episodes         | 8668     |
|    fps              | 220      |
|    time_elapsed     | 730      |
|    total_timesteps  | 160807   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.31e-05 |
|    n_updates        | 30201    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | -0.0241  |
|    exploration_rate | 0.344    |
| time/               |          |
|    episodes         | 8672     |
|    fps              | 220      |
|    time_elapsed     | 730      |
|    total_timesteps  | 160880   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00674  |
|    n_updates        | 30219    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | -0.0242  |
|    exploration_rate | 0.343    |
| time/               |          |
|    episodes         | 8676     |
|    fps              | 220      |
|    time_elapsed     | 730      |
|    total_timesteps  | 160953   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.91e-05 |
|    n_updates        | 30238    |
----------------------------------
Eval num_timesteps=161000, episode_reward=0.01 +/- 0.27
Episode length: 16.80 +/- 0.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.8     |
|    mean_reward      | 0.0139   |
| rollout/            |          |
|    exploration_rate | 0.343    |
| time/               |          |
|    total_timesteps  | 161000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00018  |
|    n_updates        | 30249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | -0.0243  |
|    exploration_rate | 0.343    |
| time/               |          |
|    episodes         | 8680     |
|    fps              | 220      |
|    time_elapsed     | 731      |
|    total_timesteps  | 161026   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00695  |
|    n_updates        | 30256    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | -0.00409 |
|    exploration_rate | 0.342    |
| time/               |          |
|    episodes         | 8684     |
|    fps              | 220      |
|    time_elapsed     | 731      |
|    total_timesteps  | 161088   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00684  |
|    n_updates        | 30271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | -0.00597 |
|    exploration_rate | 0.341    |
| time/               |          |
|    episodes         | 8688     |
|    fps              | 220      |
|    time_elapsed     | 731      |
|    total_timesteps  | 161207   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.74e-05 |
|    n_updates        | 30301    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | -0.00565 |
|    exploration_rate | 0.341    |
| time/               |          |
|    episodes         | 8692     |
|    fps              | 220      |
|    time_elapsed     | 731      |
|    total_timesteps  | 161307   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 30326    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.6     |
|    ep_rew_mean      | 0.00452  |
|    exploration_rate | 0.34     |
| time/               |          |
|    episodes         | 8696     |
|    fps              | 220      |
|    time_elapsed     | 731      |
|    total_timesteps  | 161384   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.47e-05 |
|    n_updates        | 30345    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.2     |
|    ep_rew_mean      | 0.00616  |
|    exploration_rate | 0.339    |
| time/               |          |
|    episodes         | 8700     |
|    fps              | 220      |
|    time_elapsed     | 731      |
|    total_timesteps  | 161470   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.75e-05 |
|    n_updates        | 30367    |
----------------------------------
Eval num_timesteps=161500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.339    |
| time/               |          |
|    total_timesteps  | 161500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 30374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | 0.0259   |
|    exploration_rate | 0.338    |
| time/               |          |
|    episodes         | 8704     |
|    fps              | 219      |
|    time_elapsed     | 734      |
|    total_timesteps  | 161562   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 30390    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.4     |
|    ep_rew_mean      | 0.0257   |
|    exploration_rate | 0.338    |
| time/               |          |
|    episodes         | 8708     |
|    fps              | 220      |
|    time_elapsed     | 734      |
|    total_timesteps  | 161647   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.74e-05 |
|    n_updates        | 30411    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | 0.0259   |
|    exploration_rate | 0.337    |
| time/               |          |
|    episodes         | 8712     |
|    fps              | 220      |
|    time_elapsed     | 734      |
|    total_timesteps  | 161732   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 30432    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.4     |
|    ep_rew_mean      | 0.0155   |
|    exploration_rate | 0.336    |
| time/               |          |
|    episodes         | 8716     |
|    fps              | 220      |
|    time_elapsed     | 734      |
|    total_timesteps  | 161812   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00689  |
|    n_updates        | 30452    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | 0.00589  |
|    exploration_rate | 0.336    |
| time/               |          |
|    episodes         | 8720     |
|    fps              | 220      |
|    time_elapsed     | 734      |
|    total_timesteps  | 161886   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.36e-05 |
|    n_updates        | 30471    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.2     |
|    ep_rew_mean      | 0.00633  |
|    exploration_rate | 0.335    |
| time/               |          |
|    episodes         | 8724     |
|    fps              | 220      |
|    time_elapsed     | 734      |
|    total_timesteps  | 161978   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00742  |
|    n_updates        | 30494    |
----------------------------------
Eval num_timesteps=162000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.335    |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0148   |
|    n_updates        | 30499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | 0.00681  |
|    exploration_rate | 0.334    |
| time/               |          |
|    episodes         | 8728     |
|    fps              | 219      |
|    time_elapsed     | 737      |
|    total_timesteps  | 162062   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00706  |
|    n_updates        | 30515    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | 0.00657  |
|    exploration_rate | 0.334    |
| time/               |          |
|    episodes         | 8732     |
|    fps              | 219      |
|    time_elapsed     | 737      |
|    total_timesteps  | 162135   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000153 |
|    n_updates        | 30533    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21       |
|    ep_rew_mean      | 0.00704  |
|    exploration_rate | 0.333    |
| time/               |          |
|    episodes         | 8736     |
|    fps              | 219      |
|    time_elapsed     | 737      |
|    total_timesteps  | 162207   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.92e-05 |
|    n_updates        | 30551    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.8     |
|    ep_rew_mean      | 0.0178   |
|    exploration_rate | 0.333    |
| time/               |          |
|    episodes         | 8740     |
|    fps              | 219      |
|    time_elapsed     | 737      |
|    total_timesteps  | 162276   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 30568    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21       |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 0.332    |
| time/               |          |
|    episodes         | 8744     |
|    fps              | 220      |
|    time_elapsed     | 737      |
|    total_timesteps  | 162382   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.86e-05 |
|    n_updates        | 30595    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21       |
|    ep_rew_mean      | 0.00717  |
|    exploration_rate | 0.331    |
| time/               |          |
|    episodes         | 8748     |
|    fps              | 220      |
|    time_elapsed     | 737      |
|    total_timesteps  | 162477   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.37e-05 |
|    n_updates        | 30619    |
----------------------------------
Eval num_timesteps=162500, episode_reward=-0.29 +/- 0.04
Episode length: 71.40 +/- 8.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.4     |
|    mean_reward      | -0.285   |
| rollout/            |          |
|    exploration_rate | 0.331    |
| time/               |          |
|    total_timesteps  | 162500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.57e-05 |
|    n_updates        | 30624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.6     |
|    ep_rew_mean      | 0.00853  |
|    exploration_rate | 0.33     |
| time/               |          |
|    episodes         | 8752     |
|    fps              | 219      |
|    time_elapsed     | 740      |
|    total_timesteps  | 162566   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000179 |
|    n_updates        | 30641    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.5     |
|    ep_rew_mean      | 0.019    |
|    exploration_rate | 0.33     |
| time/               |          |
|    episodes         | 8756     |
|    fps              | 219      |
|    time_elapsed     | 740      |
|    total_timesteps  | 162636   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.88e-05 |
|    n_updates        | 30658    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.7     |
|    ep_rew_mean      | 0.0183   |
|    exploration_rate | 0.329    |
| time/               |          |
|    episodes         | 8760     |
|    fps              | 219      |
|    time_elapsed     | 740      |
|    total_timesteps  | 162726   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000246 |
|    n_updates        | 30681    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | 0.00681  |
|    exploration_rate | 0.328    |
| time/               |          |
|    episodes         | 8764     |
|    fps              | 219      |
|    time_elapsed     | 740      |
|    total_timesteps  | 162839   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00694  |
|    n_updates        | 30709    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.2     |
|    ep_rew_mean      | 0.00621  |
|    exploration_rate | 0.327    |
| time/               |          |
|    episodes         | 8768     |
|    fps              | 219      |
|    time_elapsed     | 740      |
|    total_timesteps  | 162929   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 30732    |
----------------------------------
Eval num_timesteps=163000, episode_reward=-0.22 +/- 0.15
Episode length: 58.94 +/- 20.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.9     |
|    mean_reward      | -0.215   |
| rollout/            |          |
|    exploration_rate | 0.327    |
| time/               |          |
|    total_timesteps  | 163000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00685  |
|    n_updates        | 30749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | 0.00589  |
|    exploration_rate | 0.327    |
| time/               |          |
|    episodes         | 8772     |
|    fps              | 219      |
|    time_elapsed     | 742      |
|    total_timesteps  | 163010   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 30752    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.5     |
|    ep_rew_mean      | 0.00497  |
|    exploration_rate | 0.326    |
| time/               |          |
|    episodes         | 8776     |
|    fps              | 219      |
|    time_elapsed     | 742      |
|    total_timesteps  | 163106   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 30776    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | 0.00425  |
|    exploration_rate | 0.325    |
| time/               |          |
|    episodes         | 8780     |
|    fps              | 219      |
|    time_elapsed     | 743      |
|    total_timesteps  | 163197   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 30799    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.2     |
|    ep_rew_mean      | -0.0176  |
|    exploration_rate | 0.324    |
| time/               |          |
|    episodes         | 8784     |
|    fps              | 219      |
|    time_elapsed     | 743      |
|    total_timesteps  | 163304   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.34e-05 |
|    n_updates        | 30825    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | -0.017   |
|    exploration_rate | 0.323    |
| time/               |          |
|    episodes         | 8788     |
|    fps              | 219      |
|    time_elapsed     | 743      |
|    total_timesteps  | 163410   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000234 |
|    n_updates        | 30852    |
----------------------------------
Eval num_timesteps=163500, episode_reward=-0.19 +/- 0.27
Episode length: 62.18 +/- 21.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.2     |
|    mean_reward      | -0.188   |
| rollout/            |          |
|    exploration_rate | 0.323    |
| time/               |          |
|    total_timesteps  | 163500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.47e-05 |
|    n_updates        | 30874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.1     |
|    ep_rew_mean      | -0.0171  |
|    exploration_rate | 0.322    |
| time/               |          |
|    episodes         | 8792     |
|    fps              | 219      |
|    time_elapsed     | 745      |
|    total_timesteps  | 163512   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.35e-05 |
|    n_updates        | 30877    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | -0.0268  |
|    exploration_rate | 0.322    |
| time/               |          |
|    episodes         | 8796     |
|    fps              | 219      |
|    time_elapsed     | 745      |
|    total_timesteps  | 163581   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.14e-05 |
|    n_updates        | 30895    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.9     |
|    ep_rew_mean      | -0.0265  |
|    exploration_rate | 0.321    |
| time/               |          |
|    episodes         | 8800     |
|    fps              | 219      |
|    time_elapsed     | 745      |
|    total_timesteps  | 163659   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00787  |
|    n_updates        | 30914    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | -0.0459  |
|    exploration_rate | 0.321    |
| time/               |          |
|    episodes         | 8804     |
|    fps              | 219      |
|    time_elapsed     | 745      |
|    total_timesteps  | 163736   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 30933    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | -0.0461  |
|    exploration_rate | 0.32     |
| time/               |          |
|    episodes         | 8808     |
|    fps              | 219      |
|    time_elapsed     | 745      |
|    total_timesteps  | 163825   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000181 |
|    n_updates        | 30956    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.1     |
|    ep_rew_mean      | -0.0475  |
|    exploration_rate | 0.319    |
| time/               |          |
|    episodes         | 8812     |
|    fps              | 219      |
|    time_elapsed     | 745      |
|    total_timesteps  | 163945   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0067   |
|    n_updates        | 30986    |
----------------------------------
Eval num_timesteps=164000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.318    |
| time/               |          |
|    total_timesteps  | 164000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.96e-05 |
|    n_updates        | 30999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | -0.0282  |
|    exploration_rate | 0.318    |
| time/               |          |
|    episodes         | 8816     |
|    fps              | 219      |
|    time_elapsed     | 748      |
|    total_timesteps  | 164044   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00699  |
|    n_updates        | 31010    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | -0.0298  |
|    exploration_rate | 0.317    |
| time/               |          |
|    episodes         | 8820     |
|    fps              | 219      |
|    time_elapsed     | 748      |
|    total_timesteps  | 164156   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00666  |
|    n_updates        | 31038    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | -0.0309  |
|    exploration_rate | 0.316    |
| time/               |          |
|    episodes         | 8824     |
|    fps              | 219      |
|    time_elapsed     | 748      |
|    total_timesteps  | 164277   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 31069    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | -0.031   |
|    exploration_rate | 0.315    |
| time/               |          |
|    episodes         | 8828     |
|    fps              | 219      |
|    time_elapsed     | 748      |
|    total_timesteps  | 164364   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.89e-05 |
|    n_updates        | 31090    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | -0.033   |
|    exploration_rate | 0.314    |
| time/               |          |
|    episodes         | 8832     |
|    fps              | 219      |
|    time_elapsed     | 748      |
|    total_timesteps  | 164487   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00704  |
|    n_updates        | 31121    |
----------------------------------
Eval num_timesteps=164500, episode_reward=0.01 +/- 0.27
Episode length: 17.66 +/- 1.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.7     |
|    mean_reward      | 0.0103   |
| rollout/            |          |
|    exploration_rate | 0.314    |
| time/               |          |
|    total_timesteps  | 164500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.35e-05 |
|    n_updates        | 31124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | -0.0328  |
|    exploration_rate | 0.314    |
| time/               |          |
|    episodes         | 8836     |
|    fps              | 219      |
|    time_elapsed     | 749      |
|    total_timesteps  | 164553   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.16e-05 |
|    n_updates        | 31138    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | -0.0445  |
|    exploration_rate | 0.313    |
| time/               |          |
|    episodes         | 8840     |
|    fps              | 219      |
|    time_elapsed     | 749      |
|    total_timesteps  | 164664   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 31165    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | -0.0431  |
|    exploration_rate | 0.312    |
| time/               |          |
|    episodes         | 8844     |
|    fps              | 219      |
|    time_elapsed     | 749      |
|    total_timesteps  | 164736   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000207 |
|    n_updates        | 31183    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.4     |
|    ep_rew_mean      | -0.0324  |
|    exploration_rate | 0.312    |
| time/               |          |
|    episodes         | 8848     |
|    fps              | 219      |
|    time_elapsed     | 749      |
|    total_timesteps  | 164813   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.58e-05 |
|    n_updates        | 31203    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | -0.0249  |
|    exploration_rate | 0.31     |
| time/               |          |
|    episodes         | 8852     |
|    fps              | 219      |
|    time_elapsed     | 749      |
|    total_timesteps  | 164966   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000162 |
|    n_updates        | 31241    |
----------------------------------
Eval num_timesteps=165000, episode_reward=-0.00 +/- 0.37
Episode length: 35.76 +/- 19.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.8     |
|    mean_reward      | -0.00224 |
| rollout/            |          |
|    exploration_rate | 0.31     |
| time/               |          |
|    total_timesteps  | 165000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0072   |
|    n_updates        | 31249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | -0.0353  |
|    exploration_rate | 0.31     |
| time/               |          |
|    episodes         | 8856     |
|    fps              | 219      |
|    time_elapsed     | 751      |
|    total_timesteps  | 165044   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 31260    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | -0.0355  |
|    exploration_rate | 0.309    |
| time/               |          |
|    episodes         | 8860     |
|    fps              | 219      |
|    time_elapsed     | 751      |
|    total_timesteps  | 165140   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000218 |
|    n_updates        | 31284    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | -0.0347  |
|    exploration_rate | 0.308    |
| time/               |          |
|    episodes         | 8864     |
|    fps              | 219      |
|    time_elapsed     | 751      |
|    total_timesteps  | 165233   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000156 |
|    n_updates        | 31308    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | -0.0253  |
|    exploration_rate | 0.307    |
| time/               |          |
|    episodes         | 8868     |
|    fps              | 219      |
|    time_elapsed     | 751      |
|    total_timesteps  | 165339   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 31334    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | -0.016   |
|    exploration_rate | 0.307    |
| time/               |          |
|    episodes         | 8872     |
|    fps              | 220      |
|    time_elapsed     | 751      |
|    total_timesteps  | 165437   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.97e-05 |
|    n_updates        | 31359    |
----------------------------------
Eval num_timesteps=165500, episode_reward=-0.13 +/- 0.24
Episode length: 43.56 +/- 25.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | -0.134   |
| rollout/            |          |
|    exploration_rate | 0.306    |
| time/               |          |
|    total_timesteps  | 165500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 31374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.6     |
|    ep_rew_mean      | -0.0075  |
|    exploration_rate | 0.305    |
| time/               |          |
|    episodes         | 8876     |
|    fps              | 219      |
|    time_elapsed     | 753      |
|    total_timesteps  | 165570   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.86e-05 |
|    n_updates        | 31392    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | -0.00794 |
|    exploration_rate | 0.305    |
| time/               |          |
|    episodes         | 8880     |
|    fps              | 219      |
|    time_elapsed     | 753      |
|    total_timesteps  | 165672   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 31417    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.9     |
|    ep_rew_mean      | 0.00159  |
|    exploration_rate | 0.304    |
| time/               |          |
|    episodes         | 8884     |
|    fps              | 220      |
|    time_elapsed     | 753      |
|    total_timesteps  | 165791   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 31447    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25       |
|    ep_rew_mean      | 0.00091  |
|    exploration_rate | 0.303    |
| time/               |          |
|    episodes         | 8888     |
|    fps              | 220      |
|    time_elapsed     | 753      |
|    total_timesteps  | 165914   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.02e-05 |
|    n_updates        | 31478    |
----------------------------------
Eval num_timesteps=166000, episode_reward=-0.25 +/- 0.18
Episode length: 68.48 +/- 17.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.5     |
|    mean_reward      | -0.254   |
| rollout/            |          |
|    exploration_rate | 0.302    |
| time/               |          |
|    total_timesteps  | 166000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 31499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.9     |
|    ep_rew_mean      | 0.00131  |
|    exploration_rate | 0.302    |
| time/               |          |
|    episodes         | 8892     |
|    fps              | 219      |
|    time_elapsed     | 756      |
|    total_timesteps  | 166006   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00789  |
|    n_updates        | 31501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.2     |
|    ep_rew_mean      | 0.00039  |
|    exploration_rate | 0.301    |
| time/               |          |
|    episodes         | 8896     |
|    fps              | 219      |
|    time_elapsed     | 756      |
|    total_timesteps  | 166098   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0146   |
|    n_updates        | 31524    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | 0.00738  |
|    exploration_rate | 0.3      |
| time/               |          |
|    episodes         | 8900     |
|    fps              | 219      |
|    time_elapsed     | 756      |
|    total_timesteps  | 166251   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 31562    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.4     |
|    ep_rew_mean      | 0.00566  |
|    exploration_rate | 0.299    |
| time/               |          |
|    episodes         | 8904     |
|    fps              | 219      |
|    time_elapsed     | 756      |
|    total_timesteps  | 166371   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.72e-05 |
|    n_updates        | 31592    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.3     |
|    ep_rew_mean      | 0.00578  |
|    exploration_rate | 0.298    |
| time/               |          |
|    episodes         | 8908     |
|    fps              | 220      |
|    time_elapsed     | 756      |
|    total_timesteps  | 166457   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0136   |
|    n_updates        | 31614    |
----------------------------------
Eval num_timesteps=166500, episode_reward=-0.27 +/- 0.15
Episode length: 71.30 +/- 11.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.3     |
|    mean_reward      | -0.265   |
| rollout/            |          |
|    exploration_rate | 0.298    |
| time/               |          |
|    total_timesteps  | 166500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 31624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.1     |
|    ep_rew_mean      | 0.0165   |
|    exploration_rate | 0.297    |
| time/               |          |
|    episodes         | 8912     |
|    fps              | 219      |
|    time_elapsed     | 758      |
|    total_timesteps  | 166559   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8e-05    |
|    n_updates        | 31639    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | 0.00733  |
|    exploration_rate | 0.297    |
| time/               |          |
|    episodes         | 8916     |
|    fps              | 219      |
|    time_elapsed     | 759      |
|    total_timesteps  | 166637   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.49e-05 |
|    n_updates        | 31659    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.8     |
|    ep_rew_mean      | 0.0179   |
|    exploration_rate | 0.296    |
| time/               |          |
|    episodes         | 8920     |
|    fps              | 219      |
|    time_elapsed     | 759      |
|    total_timesteps  | 166736   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.34e-05 |
|    n_updates        | 31683    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | 0.0188   |
|    exploration_rate | 0.295    |
| time/               |          |
|    episodes         | 8924     |
|    fps              | 219      |
|    time_elapsed     | 759      |
|    total_timesteps  | 166833   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000175 |
|    n_updates        | 31708    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | 0.0186   |
|    exploration_rate | 0.294    |
| time/               |          |
|    episodes         | 8928     |
|    fps              | 219      |
|    time_elapsed     | 759      |
|    total_timesteps  | 166925   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.65e-05 |
|    n_updates        | 31731    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | 0.0307   |
|    exploration_rate | 0.294    |
| time/               |          |
|    episodes         | 8932     |
|    fps              | 219      |
|    time_elapsed     | 759      |
|    total_timesteps  | 166995   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000118 |
|    n_updates        | 31748    |
----------------------------------
Eval num_timesteps=167000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.294    |
| time/               |          |
|    total_timesteps  | 167000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.77e-05 |
|    n_updates        | 31749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | 0.0195   |
|    exploration_rate | 0.293    |
| time/               |          |
|    episodes         | 8936     |
|    fps              | 219      |
|    time_elapsed     | 761      |
|    total_timesteps  | 167092   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.72e-05 |
|    n_updates        | 31772    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | 0.0207   |
|    exploration_rate | 0.292    |
| time/               |          |
|    episodes         | 8940     |
|    fps              | 219      |
|    time_elapsed     | 762      |
|    total_timesteps  | 167173   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.44e-05 |
|    n_updates        | 31793    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.2     |
|    ep_rew_mean      | 0.0203   |
|    exploration_rate | 0.291    |
| time/               |          |
|    episodes         | 8944     |
|    fps              | 219      |
|    time_elapsed     | 762      |
|    total_timesteps  | 167256   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00016  |
|    n_updates        | 31813    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.3     |
|    ep_rew_mean      | 0.00999  |
|    exploration_rate | 0.291    |
| time/               |          |
|    episodes         | 8948     |
|    fps              | 219      |
|    time_elapsed     | 762      |
|    total_timesteps  | 167339   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000215 |
|    n_updates        | 31834    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.6     |
|    ep_rew_mean      | 0.00253  |
|    exploration_rate | 0.29     |
| time/               |          |
|    episodes         | 8952     |
|    fps              | 219      |
|    time_elapsed     | 762      |
|    total_timesteps  | 167428   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.1e-05  |
|    n_updates        | 31856    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | 0.00285  |
|    exploration_rate | 0.289    |
| time/               |          |
|    episodes         | 8956     |
|    fps              | 219      |
|    time_elapsed     | 762      |
|    total_timesteps  | 167498   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.34e-05 |
|    n_updates        | 31874    |
----------------------------------
Eval num_timesteps=167500, episode_reward=0.05 +/- 0.36
Episode length: 22.66 +/- 13.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22.7     |
|    mean_reward      | 0.0504   |
| rollout/            |          |
|    exploration_rate | 0.289    |
| time/               |          |
|    total_timesteps  | 167500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | 0.00365  |
|    exploration_rate | 0.289    |
| time/               |          |
|    episodes         | 8960     |
|    fps              | 219      |
|    time_elapsed     | 763      |
|    total_timesteps  | 167574   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.79e-05 |
|    n_updates        | 31893    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.9     |
|    ep_rew_mean      | 0.00129  |
|    exploration_rate | 0.287    |
| time/               |          |
|    episodes         | 8964     |
|    fps              | 219      |
|    time_elapsed     | 763      |
|    total_timesteps  | 167726   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.19e-05 |
|    n_updates        | 31931    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | -0.00787 |
|    exploration_rate | 0.287    |
| time/               |          |
|    episodes         | 8968     |
|    fps              | 219      |
|    time_elapsed     | 763      |
|    total_timesteps  | 167811   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000133 |
|    n_updates        | 31952    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | -0.0178  |
|    exploration_rate | 0.286    |
| time/               |          |
|    episodes         | 8972     |
|    fps              | 219      |
|    time_elapsed     | 763      |
|    total_timesteps  | 167908   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000156 |
|    n_updates        | 31976    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | -0.0253  |
|    exploration_rate | 0.285    |
| time/               |          |
|    episodes         | 8976     |
|    fps              | 219      |
|    time_elapsed     | 763      |
|    total_timesteps  | 167979   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0137   |
|    n_updates        | 31994    |
----------------------------------
Eval num_timesteps=168000, episode_reward=0.10 +/- 0.37
Episode length: 16.42 +/- 2.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.4     |
|    mean_reward      | 0.0954   |
| rollout/            |          |
|    exploration_rate | 0.285    |
| time/               |          |
|    total_timesteps  | 168000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.32e-05 |
|    n_updates        | 31999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | -0.025   |
|    exploration_rate | 0.285    |
| time/               |          |
|    episodes         | 8980     |
|    fps              | 219      |
|    time_elapsed     | 764      |
|    total_timesteps  | 168073   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 32018    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | -0.0338  |
|    exploration_rate | 0.284    |
| time/               |          |
|    episodes         | 8984     |
|    fps              | 219      |
|    time_elapsed     | 764      |
|    total_timesteps  | 168162   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.61e-05 |
|    n_updates        | 32040    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | -0.0317  |
|    exploration_rate | 0.283    |
| time/               |          |
|    episodes         | 8988     |
|    fps              | 220      |
|    time_elapsed     | 764      |
|    total_timesteps  | 168231   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 32057    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | -0.0307  |
|    exploration_rate | 0.283    |
| time/               |          |
|    episodes         | 8992     |
|    fps              | 220      |
|    time_elapsed     | 764      |
|    total_timesteps  | 168299   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 32074    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | -0.02    |
|    exploration_rate | 0.282    |
| time/               |          |
|    episodes         | 8996     |
|    fps              | 220      |
|    time_elapsed     | 764      |
|    total_timesteps  | 168373   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00782  |
|    n_updates        | 32093    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | -0.0186  |
|    exploration_rate | 0.281    |
| time/               |          |
|    episodes         | 9000     |
|    fps              | 220      |
|    time_elapsed     | 764      |
|    total_timesteps  | 168490   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000239 |
|    n_updates        | 32122    |
----------------------------------
Eval num_timesteps=168500, episode_reward=-0.28 +/- 0.06
Episode length: 69.02 +/- 14.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69       |
|    mean_reward      | -0.276   |
| rollout/            |          |
|    exploration_rate | 0.281    |
| time/               |          |
|    total_timesteps  | 168500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 32124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | -0.0171  |
|    exploration_rate | 0.28     |
| time/               |          |
|    episodes         | 9004     |
|    fps              | 219      |
|    time_elapsed     | 767      |
|    total_timesteps  | 168574   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.61e-05 |
|    n_updates        | 32143    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.9     |
|    ep_rew_mean      | -0.00681 |
|    exploration_rate | 0.28     |
| time/               |          |
|    episodes         | 9008     |
|    fps              | 219      |
|    time_elapsed     | 767      |
|    total_timesteps  | 168652   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00793  |
|    n_updates        | 32162    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | -0.017   |
|    exploration_rate | 0.279    |
| time/               |          |
|    episodes         | 9012     |
|    fps              | 219      |
|    time_elapsed     | 767      |
|    total_timesteps  | 168759   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00021  |
|    n_updates        | 32189    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.9     |
|    ep_rew_mean      | -0.0268  |
|    exploration_rate | 0.278    |
| time/               |          |
|    episodes         | 9016     |
|    fps              | 219      |
|    time_elapsed     | 767      |
|    total_timesteps  | 168831   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00694  |
|    n_updates        | 32207    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | -0.0357  |
|    exploration_rate | 0.278    |
| time/               |          |
|    episodes         | 9020     |
|    fps              | 220      |
|    time_elapsed     | 767      |
|    total_timesteps  | 168902   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 32225    |
----------------------------------
Eval num_timesteps=169000, episode_reward=-0.29 +/- 0.03
Episode length: 72.22 +/- 7.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.2     |
|    mean_reward      | -0.289   |
| rollout/            |          |
|    exploration_rate | 0.277    |
| time/               |          |
|    total_timesteps  | 169000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 32249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | -0.0362  |
|    exploration_rate | 0.277    |
| time/               |          |
|    episodes         | 9024     |
|    fps              | 219      |
|    time_elapsed     | 770      |
|    total_timesteps  | 169012   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000199 |
|    n_updates        | 32252    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.6     |
|    ep_rew_mean      | -0.0356  |
|    exploration_rate | 0.276    |
| time/               |          |
|    episodes         | 9028     |
|    fps              | 219      |
|    time_elapsed     | 770      |
|    total_timesteps  | 169089   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000185 |
|    n_updates        | 32272    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.9     |
|    ep_rew_mean      | -0.0365  |
|    exploration_rate | 0.275    |
| time/               |          |
|    episodes         | 9032     |
|    fps              | 219      |
|    time_elapsed     | 770      |
|    total_timesteps  | 169183   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00792  |
|    n_updates        | 32295    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.6     |
|    ep_rew_mean      | -0.0255  |
|    exploration_rate | 0.275    |
| time/               |          |
|    episodes         | 9036     |
|    fps              | 219      |
|    time_elapsed     | 770      |
|    total_timesteps  | 169254   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000168 |
|    n_updates        | 32313    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | -0.0157  |
|    exploration_rate | 0.274    |
| time/               |          |
|    episodes         | 9040     |
|    fps              | 219      |
|    time_elapsed     | 770      |
|    total_timesteps  | 169340   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00676  |
|    n_updates        | 32334    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | -0.0269  |
|    exploration_rate | 0.273    |
| time/               |          |
|    episodes         | 9044     |
|    fps              | 219      |
|    time_elapsed     | 770      |
|    total_timesteps  | 169452   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 32362    |
----------------------------------
Eval num_timesteps=169500, episode_reward=-0.17 +/- 0.33
Episode length: 63.16 +/- 22.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.2     |
|    mean_reward      | -0.172   |
| rollout/            |          |
|    exploration_rate | 0.273    |
| time/               |          |
|    total_timesteps  | 169500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 32374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | -0.0281  |
|    exploration_rate | 0.272    |
| time/               |          |
|    episodes         | 9048     |
|    fps              | 219      |
|    time_elapsed     | 772      |
|    total_timesteps  | 169566   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.53e-05 |
|    n_updates        | 32391    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | -0.0292  |
|    exploration_rate | 0.271    |
| time/               |          |
|    episodes         | 9052     |
|    fps              | 219      |
|    time_elapsed     | 772      |
|    total_timesteps  | 169683   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.79e-05 |
|    n_updates        | 32420    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | -0.0198  |
|    exploration_rate | 0.27     |
| time/               |          |
|    episodes         | 9056     |
|    fps              | 219      |
|    time_elapsed     | 772      |
|    total_timesteps  | 169767   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 32441    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | -0.0198  |
|    exploration_rate | 0.27     |
| time/               |          |
|    episodes         | 9060     |
|    fps              | 219      |
|    time_elapsed     | 772      |
|    total_timesteps  | 169843   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.67e-05 |
|    n_updates        | 32460    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.2     |
|    ep_rew_mean      | -0.00778 |
|    exploration_rate | 0.269    |
| time/               |          |
|    episodes         | 9064     |
|    fps              | 219      |
|    time_elapsed     | 773      |
|    total_timesteps  | 169945   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.77e-05 |
|    n_updates        | 32486    |
----------------------------------
Eval num_timesteps=170000, episode_reward=-0.26 +/- 0.18
Episode length: 70.50 +/- 15.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.5     |
|    mean_reward      | -0.262   |
| rollout/            |          |
|    exploration_rate | 0.268    |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.19e-05 |
|    n_updates        | 32499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | -0.00834 |
|    exploration_rate | 0.268    |
| time/               |          |
|    episodes         | 9068     |
|    fps              | 219      |
|    time_elapsed     | 775      |
|    total_timesteps  | 170044   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.93e-05 |
|    n_updates        | 32510    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | -0.0089  |
|    exploration_rate | 0.267    |
| time/               |          |
|    episodes         | 9072     |
|    fps              | 219      |
|    time_elapsed     | 775      |
|    total_timesteps  | 170155   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 32538    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | -0.0102  |
|    exploration_rate | 0.266    |
| time/               |          |
|    episodes         | 9076     |
|    fps              | 219      |
|    time_elapsed     | 775      |
|    total_timesteps  | 170259   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.12e-05 |
|    n_updates        | 32564    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.3     |
|    ep_rew_mean      | -0.00218 |
|    exploration_rate | 0.265    |
| time/               |          |
|    episodes         | 9080     |
|    fps              | 219      |
|    time_elapsed     | 775      |
|    total_timesteps  | 170402   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 32600    |
----------------------------------
Eval num_timesteps=170500, episode_reward=-0.27 +/- 0.18
Episode length: 71.62 +/- 13.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.6     |
|    mean_reward      | -0.266   |
| rollout/            |          |
|    exploration_rate | 0.264    |
| time/               |          |
|    total_timesteps  | 170500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000251 |
|    n_updates        | 32624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.4     |
|    ep_rew_mean      | -0.00274 |
|    exploration_rate | 0.264    |
| time/               |          |
|    episodes         | 9084     |
|    fps              | 219      |
|    time_elapsed     | 778      |
|    total_timesteps  | 170505   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.46e-05 |
|    n_updates        | 32626    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | -0.00454 |
|    exploration_rate | 0.263    |
| time/               |          |
|    episodes         | 9088     |
|    fps              | 219      |
|    time_elapsed     | 778      |
|    total_timesteps  | 170619   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 32654    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | -0.00506 |
|    exploration_rate | 0.262    |
| time/               |          |
|    episodes         | 9092     |
|    fps              | 219      |
|    time_elapsed     | 778      |
|    total_timesteps  | 170700   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 32674    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | -0.015   |
|    exploration_rate | 0.262    |
| time/               |          |
|    episodes         | 9096     |
|    fps              | 219      |
|    time_elapsed     | 778      |
|    total_timesteps  | 170774   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000198 |
|    n_updates        | 32693    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | -0.0236  |
|    exploration_rate | 0.261    |
| time/               |          |
|    episodes         | 9100     |
|    fps              | 219      |
|    time_elapsed     | 778      |
|    total_timesteps  | 170856   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00681  |
|    n_updates        | 32713    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | -0.00283 |
|    exploration_rate | 0.261    |
| time/               |          |
|    episodes         | 9104     |
|    fps              | 219      |
|    time_elapsed     | 778      |
|    total_timesteps  | 170920   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.68e-05 |
|    n_updates        | 32729    |
----------------------------------
Eval num_timesteps=171000, episode_reward=-0.26 +/- 0.18
Episode length: 69.42 +/- 16.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.4     |
|    mean_reward      | -0.258   |
| rollout/            |          |
|    exploration_rate | 0.26     |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.13e-05 |
|    n_updates        | 32749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | -0.0131  |
|    exploration_rate | 0.26     |
| time/               |          |
|    episodes         | 9108     |
|    fps              | 218      |
|    time_elapsed     | 781      |
|    total_timesteps  | 171004   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.61e-05 |
|    n_updates        | 32750    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | -0.0118  |
|    exploration_rate | 0.259    |
| time/               |          |
|    episodes         | 9112     |
|    fps              | 218      |
|    time_elapsed     | 781      |
|    total_timesteps  | 171080   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 32769    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.4     |
|    ep_rew_mean      | -0.0125  |
|    exploration_rate | 0.258    |
| time/               |          |
|    episodes         | 9116     |
|    fps              | 219      |
|    time_elapsed     | 781      |
|    total_timesteps  | 171169   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00656  |
|    n_updates        | 32792    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | -0.0134  |
|    exploration_rate | 0.258    |
| time/               |          |
|    episodes         | 9120     |
|    fps              | 219      |
|    time_elapsed     | 781      |
|    total_timesteps  | 171262   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.13e-05 |
|    n_updates        | 32815    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | -0.00285 |
|    exploration_rate | 0.257    |
| time/               |          |
|    episodes         | 9124     |
|    fps              | 219      |
|    time_elapsed     | 781      |
|    total_timesteps  | 171359   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 32839    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 0.0166   |
|    exploration_rate | 0.256    |
| time/               |          |
|    episodes         | 9128     |
|    fps              | 219      |
|    time_elapsed     | 781      |
|    total_timesteps  | 171449   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00683  |
|    n_updates        | 32862    |
----------------------------------
Eval num_timesteps=171500, episode_reward=-0.25 +/- 0.24
Episode length: 72.72 +/- 11.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.7     |
|    mean_reward      | -0.251   |
| rollout/            |          |
|    exploration_rate | 0.256    |
| time/               |          |
|    total_timesteps  | 171500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 32874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 0.00659  |
|    exploration_rate | 0.255    |
| time/               |          |
|    episodes         | 9132     |
|    fps              | 218      |
|    time_elapsed     | 784      |
|    total_timesteps  | 171544   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000155 |
|    n_updates        | 32885    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | -0.0038  |
|    exploration_rate | 0.255    |
| time/               |          |
|    episodes         | 9136     |
|    fps              | 218      |
|    time_elapsed     | 784      |
|    total_timesteps  | 171625   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.96e-05 |
|    n_updates        | 32906    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | -0.0133  |
|    exploration_rate | 0.254    |
| time/               |          |
|    episodes         | 9140     |
|    fps              | 218      |
|    time_elapsed     | 784      |
|    total_timesteps  | 171698   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 32924    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | -0.00327 |
|    exploration_rate | 0.253    |
| time/               |          |
|    episodes         | 9144     |
|    fps              | 219      |
|    time_elapsed     | 784      |
|    total_timesteps  | 171810   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0134   |
|    n_updates        | 32952    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.4     |
|    ep_rew_mean      | -0.00243 |
|    exploration_rate | 0.252    |
| time/               |          |
|    episodes         | 9148     |
|    fps              | 219      |
|    time_elapsed     | 784      |
|    total_timesteps  | 171903   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.39e-05 |
|    n_updates        | 32975    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 0.00907  |
|    exploration_rate | 0.252    |
| time/               |          |
|    episodes         | 9152     |
|    fps              | 219      |
|    time_elapsed     | 784      |
|    total_timesteps  | 171983   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000192 |
|    n_updates        | 32995    |
----------------------------------
Eval num_timesteps=172000, episode_reward=0.05 +/- 0.33
Episode length: 16.70 +/- 0.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.7     |
|    mean_reward      | 0.0543   |
| rollout/            |          |
|    exploration_rate | 0.251    |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000195 |
|    n_updates        | 32999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.8     |
|    ep_rew_mean      | 0.0398   |
|    exploration_rate | 0.251    |
| time/               |          |
|    episodes         | 9156     |
|    fps              | 219      |
|    time_elapsed     | 785      |
|    total_timesteps  | 172051   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000213 |
|    n_updates        | 33012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 0.0392   |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 9160     |
|    fps              | 219      |
|    time_elapsed     | 785      |
|    total_timesteps  | 172140   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 33034    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 0.0402   |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 9164     |
|    fps              | 219      |
|    time_elapsed     | 785      |
|    total_timesteps  | 172217   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.23e-05 |
|    n_updates        | 33054    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 0.0506   |
|    exploration_rate | 0.249    |
| time/               |          |
|    episodes         | 9168     |
|    fps              | 219      |
|    time_elapsed     | 785      |
|    total_timesteps  | 172307   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.92e-05 |
|    n_updates        | 33076    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0517   |
|    exploration_rate | 0.248    |
| time/               |          |
|    episodes         | 9172     |
|    fps              | 219      |
|    time_elapsed     | 785      |
|    total_timesteps  | 172391   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.31e-05 |
|    n_updates        | 33097    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 0.0619   |
|    exploration_rate | 0.247    |
| time/               |          |
|    episodes         | 9176     |
|    fps              | 219      |
|    time_elapsed     | 785      |
|    total_timesteps  | 172488   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00682  |
|    n_updates        | 33121    |
----------------------------------
Eval num_timesteps=172500, episode_reward=-0.29 +/- 0.03
Episode length: 73.66 +/- 7.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.7     |
|    mean_reward      | -0.295   |
| rollout/            |          |
|    exploration_rate | 0.247    |
| time/               |          |
|    total_timesteps  | 172500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.45e-05 |
|    n_updates        | 33124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.1     |
|    ep_rew_mean      | 0.0529   |
|    exploration_rate | 0.246    |
| time/               |          |
|    episodes         | 9180     |
|    fps              | 218      |
|    time_elapsed     | 788      |
|    total_timesteps  | 172607   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.4e-05  |
|    n_updates        | 33151    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | 0.0538   |
|    exploration_rate | 0.246    |
| time/               |          |
|    episodes         | 9184     |
|    fps              | 219      |
|    time_elapsed     | 788      |
|    total_timesteps  | 172688   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.29e-05 |
|    n_updates        | 33171    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.5     |
|    ep_rew_mean      | 0.0553   |
|    exploration_rate | 0.245    |
| time/               |          |
|    episodes         | 9188     |
|    fps              | 219      |
|    time_elapsed     | 788      |
|    total_timesteps  | 172765   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 33191    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.5     |
|    ep_rew_mean      | 0.0551   |
|    exploration_rate | 0.244    |
| time/               |          |
|    episodes         | 9192     |
|    fps              | 219      |
|    time_elapsed     | 788      |
|    total_timesteps  | 172851   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.31e-05 |
|    n_updates        | 33212    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | 0.0543   |
|    exploration_rate | 0.243    |
| time/               |          |
|    episodes         | 9196     |
|    fps              | 219      |
|    time_elapsed     | 788      |
|    total_timesteps  | 172943   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.4e-05  |
|    n_updates        | 33235    |
----------------------------------
Eval num_timesteps=173000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.243    |
| time/               |          |
|    total_timesteps  | 173000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000274 |
|    n_updates        | 33249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | 0.0637   |
|    exploration_rate | 0.243    |
| time/               |          |
|    episodes         | 9200     |
|    fps              | 218      |
|    time_elapsed     | 791      |
|    total_timesteps  | 173040   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 33259    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0414   |
|    exploration_rate | 0.241    |
| time/               |          |
|    episodes         | 9204     |
|    fps              | 218      |
|    time_elapsed     | 791      |
|    total_timesteps  | 173162   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.73e-05 |
|    n_updates        | 33290    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 0.0519   |
|    exploration_rate | 0.241    |
| time/               |          |
|    episodes         | 9208     |
|    fps              | 218      |
|    time_elapsed     | 791      |
|    total_timesteps  | 173233   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.78e-05 |
|    n_updates        | 33308    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0515   |
|    exploration_rate | 0.24     |
| time/               |          |
|    episodes         | 9212     |
|    fps              | 218      |
|    time_elapsed     | 791      |
|    total_timesteps  | 173320   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.2e-05  |
|    n_updates        | 33329    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 0.0495   |
|    exploration_rate | 0.239    |
| time/               |          |
|    episodes         | 9216     |
|    fps              | 219      |
|    time_elapsed     | 791      |
|    total_timesteps  | 173459   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.14e-05 |
|    n_updates        | 33364    |
----------------------------------
Eval num_timesteps=173500, episode_reward=-0.30 +/- 0.01
Episode length: 74.78 +/- 1.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.8     |
|    mean_reward      | -0.299   |
| rollout/            |          |
|    exploration_rate | 0.239    |
| time/               |          |
|    total_timesteps  | 173500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 33374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 0.049    |
|    exploration_rate | 0.238    |
| time/               |          |
|    episodes         | 9220     |
|    fps              | 218      |
|    time_elapsed     | 794      |
|    total_timesteps  | 173564   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.23e-05 |
|    n_updates        | 33390    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 0.0401   |
|    exploration_rate | 0.237    |
| time/               |          |
|    episodes         | 9224     |
|    fps              | 218      |
|    time_elapsed     | 794      |
|    total_timesteps  | 173633   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.63e-05 |
|    n_updates        | 33408    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 0.019    |
|    exploration_rate | 0.236    |
| time/               |          |
|    episodes         | 9228     |
|    fps              | 218      |
|    time_elapsed     | 794      |
|    total_timesteps  | 173749   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.34e-05 |
|    n_updates        | 33437    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 0.0286   |
|    exploration_rate | 0.236    |
| time/               |          |
|    episodes         | 9232     |
|    fps              | 218      |
|    time_elapsed     | 794      |
|    total_timesteps  | 173856   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000212 |
|    n_updates        | 33463    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.3     |
|    ep_rew_mean      | 0.0279   |
|    exploration_rate | 0.235    |
| time/               |          |
|    episodes         | 9236     |
|    fps              | 218      |
|    time_elapsed     | 794      |
|    total_timesteps  | 173954   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.78e-05 |
|    n_updates        | 33488    |
----------------------------------
Eval num_timesteps=174000, episode_reward=0.13 +/- 0.46
Episode length: 33.86 +/- 17.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.9     |
|    mean_reward      | 0.126    |
| rollout/            |          |
|    exploration_rate | 0.234    |
| time/               |          |
|    total_timesteps  | 174000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.39e-05 |
|    n_updates        | 33499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.8     |
|    ep_rew_mean      | 0.0259   |
|    exploration_rate | 0.234    |
| time/               |          |
|    episodes         | 9240     |
|    fps              | 218      |
|    time_elapsed     | 795      |
|    total_timesteps  | 174076   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 33518    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | 0.0171   |
|    exploration_rate | 0.233    |
| time/               |          |
|    episodes         | 9244     |
|    fps              | 218      |
|    time_elapsed     | 796      |
|    total_timesteps  | 174160   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 33539    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.232    |
| time/               |          |
|    episodes         | 9248     |
|    fps              | 218      |
|    time_elapsed     | 796      |
|    total_timesteps  | 174249   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.71e-05 |
|    n_updates        | 33562    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | 0.00683  |
|    exploration_rate | 0.231    |
| time/               |          |
|    episodes         | 9252     |
|    fps              | 218      |
|    time_elapsed     | 796      |
|    total_timesteps  | 174338   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 33584    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | -0.0254  |
|    exploration_rate | 0.23     |
| time/               |          |
|    episodes         | 9256     |
|    fps              | 219      |
|    time_elapsed     | 796      |
|    total_timesteps  | 174462   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.23e-05 |
|    n_updates        | 33615    |
----------------------------------
Eval num_timesteps=174500, episode_reward=0.06 +/- 0.42
Episode length: 35.96 +/- 19.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36       |
|    mean_reward      | 0.0572   |
| rollout/            |          |
|    exploration_rate | 0.23     |
| time/               |          |
|    total_timesteps  | 174500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.01e-05 |
|    n_updates        | 33624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | -0.0248  |
|    exploration_rate | 0.23     |
| time/               |          |
|    episodes         | 9260     |
|    fps              | 218      |
|    time_elapsed     | 797      |
|    total_timesteps  | 174535   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00686  |
|    n_updates        | 33633    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | -0.037   |
|    exploration_rate | 0.229    |
| time/               |          |
|    episodes         | 9264     |
|    fps              | 218      |
|    time_elapsed     | 797      |
|    total_timesteps  | 174667   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 33666    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.6     |
|    ep_rew_mean      | -0.0474  |
|    exploration_rate | 0.228    |
| time/               |          |
|    episodes         | 9268     |
|    fps              | 219      |
|    time_elapsed     | 797      |
|    total_timesteps  | 174767   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000143 |
|    n_updates        | 33691    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | -0.0469  |
|    exploration_rate | 0.227    |
| time/               |          |
|    episodes         | 9272     |
|    fps              | 219      |
|    time_elapsed     | 798      |
|    total_timesteps  | 174838   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 33709    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | -0.046   |
|    exploration_rate | 0.226    |
| time/               |          |
|    episodes         | 9276     |
|    fps              | 219      |
|    time_elapsed     | 798      |
|    total_timesteps  | 174915   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.19e-05 |
|    n_updates        | 33728    |
----------------------------------
Eval num_timesteps=175000, episode_reward=-0.03 +/- 0.38
Episode length: 42.66 +/- 23.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | -0.0297  |
| rollout/            |          |
|    exploration_rate | 0.226    |
| time/               |          |
|    total_timesteps  | 175000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 33749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | -0.045   |
|    exploration_rate | 0.226    |
| time/               |          |
|    episodes         | 9280     |
|    fps              | 218      |
|    time_elapsed     | 799      |
|    total_timesteps  | 175007   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 33751    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | -0.046   |
|    exploration_rate | 0.225    |
| time/               |          |
|    episodes         | 9284     |
|    fps              | 218      |
|    time_elapsed     | 799      |
|    total_timesteps  | 175114   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.09e-05 |
|    n_updates        | 33778    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | -0.0457  |
|    exploration_rate | 0.224    |
| time/               |          |
|    episodes         | 9288     |
|    fps              | 219      |
|    time_elapsed     | 799      |
|    total_timesteps  | 175183   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 33795    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | -0.0349  |
|    exploration_rate | 0.224    |
| time/               |          |
|    episodes         | 9292     |
|    fps              | 219      |
|    time_elapsed     | 799      |
|    total_timesteps  | 175249   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0135   |
|    n_updates        | 33812    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | -0.0353  |
|    exploration_rate | 0.223    |
| time/               |          |
|    episodes         | 9296     |
|    fps              | 219      |
|    time_elapsed     | 800      |
|    total_timesteps  | 175352   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 33837    |
----------------------------------
Eval num_timesteps=175500, episode_reward=0.01 +/- 0.27
Episode length: 18.24 +/- 3.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18.2     |
|    mean_reward      | 0.00806  |
| rollout/            |          |
|    exploration_rate | 0.221    |
| time/               |          |
|    total_timesteps  | 175500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.16e-05 |
|    n_updates        | 33874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | -0.048   |
|    exploration_rate | 0.221    |
| time/               |          |
|    episodes         | 9300     |
|    fps              | 219      |
|    time_elapsed     | 800      |
|    total_timesteps  | 175517   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.96e-05 |
|    n_updates        | 33879    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | -0.0361  |
|    exploration_rate | 0.221    |
| time/               |          |
|    episodes         | 9304     |
|    fps              | 219      |
|    time_elapsed     | 800      |
|    total_timesteps  | 175591   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.42e-05 |
|    n_updates        | 33897    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | -0.0494  |
|    exploration_rate | 0.219    |
| time/               |          |
|    episodes         | 9308     |
|    fps              | 219      |
|    time_elapsed     | 801      |
|    total_timesteps  | 175744   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 33935    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.3     |
|    ep_rew_mean      | -0.0503  |
|    exploration_rate | 0.218    |
| time/               |          |
|    episodes         | 9312     |
|    fps              | 219      |
|    time_elapsed     | 801      |
|    total_timesteps  | 175854   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.2e-05  |
|    n_updates        | 33963    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | -0.0478  |
|    exploration_rate | 0.218    |
| time/               |          |
|    episodes         | 9316     |
|    fps              | 219      |
|    time_elapsed     | 801      |
|    total_timesteps  | 175929   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.55e-05 |
|    n_updates        | 33982    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | -0.0363  |
|    exploration_rate | 0.217    |
| time/               |          |
|    episodes         | 9320     |
|    fps              | 219      |
|    time_elapsed     | 801      |
|    total_timesteps  | 175997   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 33999    |
----------------------------------
Eval num_timesteps=176000, episode_reward=0.05 +/- 0.33
Episode length: 17.64 +/- 1.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.6     |
|    mean_reward      | 0.0505   |
| rollout/            |          |
|    exploration_rate | 0.217    |
| time/               |          |
|    total_timesteps  | 176000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | -0.0368  |
|    exploration_rate | 0.216    |
| time/               |          |
|    episodes         | 9324     |
|    fps              | 219      |
|    time_elapsed     | 802      |
|    total_timesteps  | 176080   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.45e-05 |
|    n_updates        | 34019    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | -0.0353  |
|    exploration_rate | 0.216    |
| time/               |          |
|    episodes         | 9328     |
|    fps              | 219      |
|    time_elapsed     | 802      |
|    total_timesteps  | 176157   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 34039    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.8     |
|    ep_rew_mean      | -0.0441  |
|    exploration_rate | 0.215    |
| time/               |          |
|    episodes         | 9332     |
|    fps              | 219      |
|    time_elapsed     | 802      |
|    total_timesteps  | 176235   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 34058    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | -0.0457  |
|    exploration_rate | 0.214    |
| time/               |          |
|    episodes         | 9336     |
|    fps              | 219      |
|    time_elapsed     | 802      |
|    total_timesteps  | 176373   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000138 |
|    n_updates        | 34093    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | -0.0446  |
|    exploration_rate | 0.213    |
| time/               |          |
|    episodes         | 9340     |
|    fps              | 219      |
|    time_elapsed     | 802      |
|    total_timesteps  | 176466   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.66e-05 |
|    n_updates        | 34116    |
----------------------------------
Eval num_timesteps=176500, episode_reward=0.00 +/- 0.39
Episode length: 39.74 +/- 19.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.7     |
|    mean_reward      | 0.00208  |
| rollout/            |          |
|    exploration_rate | 0.213    |
| time/               |          |
|    total_timesteps  | 176500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.36e-05 |
|    n_updates        | 34124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | -0.0357  |
|    exploration_rate | 0.212    |
| time/               |          |
|    episodes         | 9344     |
|    fps              | 219      |
|    time_elapsed     | 803      |
|    total_timesteps  | 176578   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 34144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | -0.0271  |
|    exploration_rate | 0.211    |
| time/               |          |
|    episodes         | 9348     |
|    fps              | 219      |
|    time_elapsed     | 804      |
|    total_timesteps  | 176702   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.96e-05 |
|    n_updates        | 34175    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.6     |
|    ep_rew_mean      | -0.0274  |
|    exploration_rate | 0.21     |
| time/               |          |
|    episodes         | 9352     |
|    fps              | 219      |
|    time_elapsed     | 804      |
|    total_timesteps  | 176797   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000165 |
|    n_updates        | 34199    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | -0.0253  |
|    exploration_rate | 0.21     |
| time/               |          |
|    episodes         | 9356     |
|    fps              | 219      |
|    time_elapsed     | 804      |
|    total_timesteps  | 176869   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000205 |
|    n_updates        | 34217    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | -0.0251  |
|    exploration_rate | 0.209    |
| time/               |          |
|    episodes         | 9360     |
|    fps              | 219      |
|    time_elapsed     | 804      |
|    total_timesteps  | 176937   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 34234    |
----------------------------------
Eval num_timesteps=177000, episode_reward=-0.05 +/- 0.26
Episode length: 27.22 +/- 20.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 27.2     |
|    mean_reward      | -0.0479  |
| rollout/            |          |
|    exploration_rate | 0.208    |
| time/               |          |
|    total_timesteps  | 177000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00791  |
|    n_updates        | 34249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | -0.0134  |
|    exploration_rate | 0.208    |
| time/               |          |
|    episodes         | 9364     |
|    fps              | 219      |
|    time_elapsed     | 805      |
|    total_timesteps  | 177028   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0144   |
|    n_updates        | 34256    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.4     |
|    ep_rew_mean      | -0.00242 |
|    exploration_rate | 0.208    |
| time/               |          |
|    episodes         | 9368     |
|    fps              | 219      |
|    time_elapsed     | 805      |
|    total_timesteps  | 177103   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.95e-05 |
|    n_updates        | 34275    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | -0.00334 |
|    exploration_rate | 0.207    |
| time/               |          |
|    episodes         | 9372     |
|    fps              | 219      |
|    time_elapsed     | 805      |
|    total_timesteps  | 177197   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00792  |
|    n_updates        | 34299    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | -0.00345 |
|    exploration_rate | 0.206    |
| time/               |          |
|    episodes         | 9376     |
|    fps              | 220      |
|    time_elapsed     | 805      |
|    total_timesteps  | 177277   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00705  |
|    n_updates        | 34319    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 0.0163   |
|    exploration_rate | 0.205    |
| time/               |          |
|    episodes         | 9380     |
|    fps              | 220      |
|    time_elapsed     | 805      |
|    total_timesteps  | 177376   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.84e-05 |
|    n_updates        | 34343    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.3     |
|    ep_rew_mean      | 0.0177   |
|    exploration_rate | 0.205    |
| time/               |          |
|    episodes         | 9384     |
|    fps              | 220      |
|    time_elapsed     | 805      |
|    total_timesteps  | 177447   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00683  |
|    n_updates        | 34361    |
----------------------------------
Eval num_timesteps=177500, episode_reward=0.10 +/- 0.46
Episode length: 36.28 +/- 18.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.3     |
|    mean_reward      | 0.0959   |
| rollout/            |          |
|    exploration_rate | 0.204    |
| time/               |          |
|    total_timesteps  | 177500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.52e-05 |
|    n_updates        | 34374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.204    |
| time/               |          |
|    episodes         | 9388     |
|    fps              | 219      |
|    time_elapsed     | 807      |
|    total_timesteps  | 177530   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000118 |
|    n_updates        | 34382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.5     |
|    ep_rew_mean      | 0.00708  |
|    exploration_rate | 0.203    |
| time/               |          |
|    episodes         | 9392     |
|    fps              | 220      |
|    time_elapsed     | 807      |
|    total_timesteps  | 177598   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000222 |
|    n_updates        | 34399    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | 0.00836  |
|    exploration_rate | 0.203    |
| time/               |          |
|    episodes         | 9396     |
|    fps              | 220      |
|    time_elapsed     | 807      |
|    total_timesteps  | 177669   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 34417    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 0.0118   |
|    exploration_rate | 0.202    |
| time/               |          |
|    episodes         | 9400     |
|    fps              | 220      |
|    time_elapsed     | 807      |
|    total_timesteps  | 177747   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.58e-05 |
|    n_updates        | 34436    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0114   |
|    exploration_rate | 0.201    |
| time/               |          |
|    episodes         | 9404     |
|    fps              | 220      |
|    time_elapsed     | 807      |
|    total_timesteps  | 177833   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 34458    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | 0.0142   |
|    exploration_rate | 0.201    |
| time/               |          |
|    episodes         | 9408     |
|    fps              | 220      |
|    time_elapsed     | 807      |
|    total_timesteps  | 177915   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00689  |
|    n_updates        | 34478    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.3     |
|    ep_rew_mean      | 0.0158   |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 9412     |
|    fps              | 220      |
|    time_elapsed     | 807      |
|    total_timesteps  | 177986   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 34496    |
----------------------------------
Eval num_timesteps=178000, episode_reward=0.15 +/- 0.50
Episode length: 41.88 +/- 22.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 0.154    |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 178000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00698  |
|    n_updates        | 34499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.6     |
|    ep_rew_mean      | 0.0148   |
|    exploration_rate | 0.199    |
| time/               |          |
|    episodes         | 9416     |
|    fps              | 220      |
|    time_elapsed     | 809      |
|    total_timesteps  | 178085   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.95e-05 |
|    n_updates        | 34521    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | 0.0242   |
|    exploration_rate | 0.198    |
| time/               |          |
|    episodes         | 9420     |
|    fps              | 220      |
|    time_elapsed     | 809      |
|    total_timesteps  | 178170   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00687  |
|    n_updates        | 34542    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | 0.024    |
|    exploration_rate | 0.198    |
| time/               |          |
|    episodes         | 9424     |
|    fps              | 220      |
|    time_elapsed     | 809      |
|    total_timesteps  | 178257   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.39e-05 |
|    n_updates        | 34564    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 0.0231   |
|    exploration_rate | 0.197    |
| time/               |          |
|    episodes         | 9428     |
|    fps              | 220      |
|    time_elapsed     | 809      |
|    total_timesteps  | 178357   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.86e-05 |
|    n_updates        | 34589    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.1     |
|    ep_rew_mean      | 0.0329   |
|    exploration_rate | 0.196    |
| time/               |          |
|    episodes         | 9432     |
|    fps              | 220      |
|    time_elapsed     | 809      |
|    total_timesteps  | 178440   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00686  |
|    n_updates        | 34609    |
----------------------------------
Eval num_timesteps=178500, episode_reward=-0.05 +/- 0.26
Episode length: 28.34 +/- 19.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 28.3     |
|    mean_reward      | -0.0526  |
| rollout/            |          |
|    exploration_rate | 0.195    |
| time/               |          |
|    total_timesteps  | 178500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00669  |
|    n_updates        | 34624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.5     |
|    ep_rew_mean      | 0.035    |
|    exploration_rate | 0.195    |
| time/               |          |
|    episodes         | 9436     |
|    fps              | 220      |
|    time_elapsed     | 810      |
|    total_timesteps  | 178525   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 34631    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | 0.0338   |
|    exploration_rate | 0.194    |
| time/               |          |
|    episodes         | 9440     |
|    fps              | 220      |
|    time_elapsed     | 810      |
|    total_timesteps  | 178647   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.35e-05 |
|    n_updates        | 34661    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.9     |
|    ep_rew_mean      | 0.0235   |
|    exploration_rate | 0.193    |
| time/               |          |
|    episodes         | 9444     |
|    fps              | 220      |
|    time_elapsed     | 810      |
|    total_timesteps  | 178768   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000114 |
|    n_updates        | 34691    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.1     |
|    ep_rew_mean      | 0.0127   |
|    exploration_rate | 0.192    |
| time/               |          |
|    episodes         | 9448     |
|    fps              | 220      |
|    time_elapsed     | 810      |
|    total_timesteps  | 178911   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.74e-05 |
|    n_updates        | 34727    |
----------------------------------
Eval num_timesteps=179000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.191    |
| time/               |          |
|    total_timesteps  | 179000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 34749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.1     |
|    ep_rew_mean      | 0.0229   |
|    exploration_rate | 0.191    |
| time/               |          |
|    episodes         | 9452     |
|    fps              | 220      |
|    time_elapsed     | 813      |
|    total_timesteps  | 179002   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00683  |
|    n_updates        | 34750    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0215   |
|    exploration_rate | 0.19     |
| time/               |          |
|    episodes         | 9456     |
|    fps              | 220      |
|    time_elapsed     | 813      |
|    total_timesteps  | 179109   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.65e-05 |
|    n_updates        | 34777    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 0.0188   |
|    exploration_rate | 0.189    |
| time/               |          |
|    episodes         | 9460     |
|    fps              | 220      |
|    time_elapsed     | 813      |
|    total_timesteps  | 179243   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.01e-05 |
|    n_updates        | 34810    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | 0.00833  |
|    exploration_rate | 0.188    |
| time/               |          |
|    episodes         | 9464     |
|    fps              | 220      |
|    time_elapsed     | 813      |
|    total_timesteps  | 179346   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.77e-05 |
|    n_updates        | 34836    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | -0.00382 |
|    exploration_rate | 0.187    |
| time/               |          |
|    episodes         | 9468     |
|    fps              | 220      |
|    time_elapsed     | 814      |
|    total_timesteps  | 179475   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000157 |
|    n_updates        | 34868    |
----------------------------------
Eval num_timesteps=179500, episode_reward=0.00 +/- 0.44
Episode length: 54.14 +/- 21.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.1     |
|    mean_reward      | 0.00422  |
| rollout/            |          |
|    exploration_rate | 0.187    |
| time/               |          |
|    total_timesteps  | 179500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.76e-05 |
|    n_updates        | 34874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 0.0048   |
|    exploration_rate | 0.186    |
| time/               |          |
|    episodes         | 9472     |
|    fps              | 220      |
|    time_elapsed     | 816      |
|    total_timesteps  | 179604   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000167 |
|    n_updates        | 34900    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.4     |
|    ep_rew_mean      | 0.0036   |
|    exploration_rate | 0.185    |
| time/               |          |
|    episodes         | 9476     |
|    fps              | 220      |
|    time_elapsed     | 816      |
|    total_timesteps  | 179714   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 34928    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | -0.00472 |
|    exploration_rate | 0.184    |
| time/               |          |
|    episodes         | 9480     |
|    fps              | 220      |
|    time_elapsed     | 816      |
|    total_timesteps  | 179771   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.76e-05 |
|    n_updates        | 34942    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24       |
|    ep_rew_mean      | -0.00484 |
|    exploration_rate | 0.184    |
| time/               |          |
|    episodes         | 9484     |
|    fps              | 220      |
|    time_elapsed     | 816      |
|    total_timesteps  | 179845   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0146   |
|    n_updates        | 34961    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | -0.00588 |
|    exploration_rate | 0.183    |
| time/               |          |
|    episodes         | 9488     |
|    fps              | 220      |
|    time_elapsed     | 816      |
|    total_timesteps  | 179954   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000136 |
|    n_updates        | 34988    |
----------------------------------
Eval num_timesteps=180000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.182    |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 34999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | -0.00804 |
|    exploration_rate | 0.182    |
| time/               |          |
|    episodes         | 9492     |
|    fps              | 219      |
|    time_elapsed     | 819      |
|    total_timesteps  | 180076   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.47e-05 |
|    n_updates        | 35018    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | -0.0103  |
|    exploration_rate | 0.181    |
| time/               |          |
|    episodes         | 9496     |
|    fps              | 219      |
|    time_elapsed     | 819      |
|    total_timesteps  | 180204   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00686  |
|    n_updates        | 35050    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.3     |
|    ep_rew_mean      | -0.0102  |
|    exploration_rate | 0.18     |
| time/               |          |
|    episodes         | 9500     |
|    fps              | 220      |
|    time_elapsed     | 819      |
|    total_timesteps  | 180279   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 35069    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | -0.0205  |
|    exploration_rate | 0.179    |
| time/               |          |
|    episodes         | 9504     |
|    fps              | 220      |
|    time_elapsed     | 819      |
|    total_timesteps  | 180372   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 35092    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.2     |
|    ep_rew_mean      | -0.00992 |
|    exploration_rate | 0.178    |
| time/               |          |
|    episodes         | 9508     |
|    fps              | 220      |
|    time_elapsed     | 819      |
|    total_timesteps  | 180440   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 35109    |
----------------------------------
Eval num_timesteps=180500, episode_reward=-0.01 +/- 0.24
Episode length: 16.98 +/- 0.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17       |
|    mean_reward      | -0.0069  |
| rollout/            |          |
|    exploration_rate | 0.178    |
| time/               |          |
|    total_timesteps  | 180500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.83e-05 |
|    n_updates        | 35124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.2     |
|    ep_rew_mean      | -0.0098  |
|    exploration_rate | 0.178    |
| time/               |          |
|    episodes         | 9512     |
|    fps              | 220      |
|    time_elapsed     | 820      |
|    total_timesteps  | 180508   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00674  |
|    n_updates        | 35126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | -0.00944 |
|    exploration_rate | 0.177    |
| time/               |          |
|    episodes         | 9516     |
|    fps              | 220      |
|    time_elapsed     | 820      |
|    total_timesteps  | 180598   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.81e-05 |
|    n_updates        | 35149    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | -0.0191  |
|    exploration_rate | 0.176    |
| time/               |          |
|    episodes         | 9520     |
|    fps              | 220      |
|    time_elapsed     | 820      |
|    total_timesteps  | 180675   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.49e-05 |
|    n_updates        | 35168    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | -0.0192  |
|    exploration_rate | 0.176    |
| time/               |          |
|    episodes         | 9524     |
|    fps              | 220      |
|    time_elapsed     | 820      |
|    total_timesteps  | 180764   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.93e-05 |
|    n_updates        | 35190    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.2     |
|    ep_rew_mean      | -0.00987 |
|    exploration_rate | 0.175    |
| time/               |          |
|    episodes         | 9528     |
|    fps              | 220      |
|    time_elapsed     | 820      |
|    total_timesteps  | 180880   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 35219    |
----------------------------------
Eval num_timesteps=181000, episode_reward=0.03 +/- 0.32
Episode length: 22.04 +/- 11.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22       |
|    mean_reward      | 0.0328   |
| rollout/            |          |
|    exploration_rate | 0.174    |
| time/               |          |
|    total_timesteps  | 181000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000222 |
|    n_updates        | 35249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.7     |
|    ep_rew_mean      | -0.0219  |
|    exploration_rate | 0.173    |
| time/               |          |
|    episodes         | 9532     |
|    fps              | 220      |
|    time_elapsed     | 821      |
|    total_timesteps  | 181014   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 35253    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.1     |
|    ep_rew_mean      | -0.0134  |
|    exploration_rate | 0.172    |
| time/               |          |
|    episodes         | 9536     |
|    fps              | 220      |
|    time_elapsed     | 821      |
|    total_timesteps  | 181135   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.57e-05 |
|    n_updates        | 35283    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.2     |
|    ep_rew_mean      | -0.0136  |
|    exploration_rate | 0.171    |
| time/               |          |
|    episodes         | 9540     |
|    fps              | 220      |
|    time_elapsed     | 821      |
|    total_timesteps  | 181264   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000114 |
|    n_updates        | 35315    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.4     |
|    ep_rew_mean      | -0.0146  |
|    exploration_rate | 0.17     |
| time/               |          |
|    episodes         | 9544     |
|    fps              | 220      |
|    time_elapsed     | 821      |
|    total_timesteps  | 181410   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 35352    |
----------------------------------
Eval num_timesteps=181500, episode_reward=-0.03 +/- 0.20
Episode length: 17.90 +/- 0.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.9     |
|    mean_reward      | -0.0306  |
| rollout/            |          |
|    exploration_rate | 0.169    |
| time/               |          |
|    total_timesteps  | 181500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 35374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26       |
|    ep_rew_mean      | -0.0029  |
|    exploration_rate | 0.169    |
| time/               |          |
|    episodes         | 9548     |
|    fps              | 220      |
|    time_elapsed     | 822      |
|    total_timesteps  | 181510   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00776  |
|    n_updates        | 35377    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.8     |
|    ep_rew_mean      | -0.0123  |
|    exploration_rate | 0.168    |
| time/               |          |
|    episodes         | 9552     |
|    fps              | 220      |
|    time_elapsed     | 822      |
|    total_timesteps  | 181586   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 35396    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | -0.00131 |
|    exploration_rate | 0.168    |
| time/               |          |
|    episodes         | 9556     |
|    fps              | 220      |
|    time_elapsed     | 822      |
|    total_timesteps  | 181669   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8e-05    |
|    n_updates        | 35417    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | -0.00119 |
|    exploration_rate | 0.167    |
| time/               |          |
|    episodes         | 9560     |
|    fps              | 220      |
|    time_elapsed     | 822      |
|    total_timesteps  | 181800   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.58e-05 |
|    n_updates        | 35449    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.4     |
|    ep_rew_mean      | -0.00043 |
|    exploration_rate | 0.166    |
| time/               |          |
|    episodes         | 9564     |
|    fps              | 220      |
|    time_elapsed     | 823      |
|    total_timesteps  | 181884   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.07e-05 |
|    n_updates        | 35470    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | 0.00049  |
|    exploration_rate | 0.165    |
| time/               |          |
|    episodes         | 9568     |
|    fps              | 221      |
|    time_elapsed     | 823      |
|    total_timesteps  | 181990   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000295 |
|    n_updates        | 35497    |
----------------------------------
Eval num_timesteps=182000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.165    |
| time/               |          |
|    total_timesteps  | 182000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00675  |
|    n_updates        | 35499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | -0.0112  |
|    exploration_rate | 0.163    |
| time/               |          |
|    episodes         | 9572     |
|    fps              | 220      |
|    time_elapsed     | 825      |
|    total_timesteps  | 182160   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.24e-05 |
|    n_updates        | 35539    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.2     |
|    ep_rew_mean      | -0.02    |
|    exploration_rate | 0.163    |
| time/               |          |
|    episodes         | 9576     |
|    fps              | 220      |
|    time_elapsed     | 825      |
|    total_timesteps  | 182239   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.48e-05 |
|    n_updates        | 35559    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | -0.0214  |
|    exploration_rate | 0.162    |
| time/               |          |
|    episodes         | 9580     |
|    fps              | 220      |
|    time_elapsed     | 825      |
|    total_timesteps  | 182332   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.18e-05 |
|    n_updates        | 35582    |
----------------------------------
Eval num_timesteps=182500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.16     |
| time/               |          |
|    total_timesteps  | 182500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.94e-05 |
|    n_updates        | 35624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.6     |
|    ep_rew_mean      | -0.0154  |
|    exploration_rate | 0.16     |
| time/               |          |
|    episodes         | 9584     |
|    fps              | 220      |
|    time_elapsed     | 828      |
|    total_timesteps  | 182507   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.47e-05 |
|    n_updates        | 35626    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.2     |
|    ep_rew_mean      | -0.0138  |
|    exploration_rate | 0.16     |
| time/               |          |
|    episodes         | 9588     |
|    fps              | 220      |
|    time_elapsed     | 828      |
|    total_timesteps  | 182576   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.53e-05 |
|    n_updates        | 35643    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.8     |
|    ep_rew_mean      | -0.012   |
|    exploration_rate | 0.159    |
| time/               |          |
|    episodes         | 9592     |
|    fps              | 220      |
|    time_elapsed     | 828      |
|    total_timesteps  | 182652   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.42e-05 |
|    n_updates        | 35662    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.2     |
|    ep_rew_mean      | 0.00028  |
|    exploration_rate | 0.158    |
| time/               |          |
|    episodes         | 9596     |
|    fps              | 220      |
|    time_elapsed     | 828      |
|    total_timesteps  | 182723   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00691  |
|    n_updates        | 35680    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | -0.00148 |
|    exploration_rate | 0.157    |
| time/               |          |
|    episodes         | 9600     |
|    fps              | 220      |
|    time_elapsed     | 829      |
|    total_timesteps  | 182842   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 35710    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | -0.00152 |
|    exploration_rate | 0.156    |
| time/               |          |
|    episodes         | 9604     |
|    fps              | 220      |
|    time_elapsed     | 829      |
|    total_timesteps  | 182936   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.32e-05 |
|    n_updates        | 35733    |
----------------------------------
Eval num_timesteps=183000, episode_reward=-0.05 +/- 0.14
Episode length: 18.38 +/- 1.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18.4     |
|    mean_reward      | -0.0525  |
| rollout/            |          |
|    exploration_rate | 0.156    |
| time/               |          |
|    total_timesteps  | 183000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.75e-05 |
|    n_updates        | 35749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.7     |
|    ep_rew_mean      | -0.0117  |
|    exploration_rate | 0.156    |
| time/               |          |
|    episodes         | 9608     |
|    fps              | 220      |
|    time_elapsed     | 829      |
|    total_timesteps  | 183009   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.62e-05 |
|    n_updates        | 35752    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.8     |
|    ep_rew_mean      | -0.0123  |
|    exploration_rate | 0.155    |
| time/               |          |
|    episodes         | 9612     |
|    fps              | 220      |
|    time_elapsed     | 829      |
|    total_timesteps  | 183092   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000188 |
|    n_updates        | 35772    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.7     |
|    ep_rew_mean      | -0.0119  |
|    exploration_rate | 0.154    |
| time/               |          |
|    episodes         | 9616     |
|    fps              | 220      |
|    time_elapsed     | 830      |
|    total_timesteps  | 183171   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000185 |
|    n_updates        | 35792    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.7     |
|    ep_rew_mean      | -0.0216  |
|    exploration_rate | 0.154    |
| time/               |          |
|    episodes         | 9620     |
|    fps              | 220      |
|    time_elapsed     | 830      |
|    total_timesteps  | 183241   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.81e-05 |
|    n_updates        | 35810    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | -0.0227  |
|    exploration_rate | 0.153    |
| time/               |          |
|    episodes         | 9624     |
|    fps              | 220      |
|    time_elapsed     | 830      |
|    total_timesteps  | 183357   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00683  |
|    n_updates        | 35839    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | -0.0228  |
|    exploration_rate | 0.152    |
| time/               |          |
|    episodes         | 9628     |
|    fps              | 220      |
|    time_elapsed     | 830      |
|    total_timesteps  | 183475   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 35868    |
----------------------------------
Eval num_timesteps=183500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.151    |
| time/               |          |
|    total_timesteps  | 183500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 35874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.1     |
|    ep_rew_mean      | -0.0136  |
|    exploration_rate | 0.15     |
| time/               |          |
|    episodes         | 9632     |
|    fps              | 220      |
|    time_elapsed     | 833      |
|    total_timesteps  | 183629   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000172 |
|    n_updates        | 35907    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.7     |
|    ep_rew_mean      | -0.0218  |
|    exploration_rate | 0.15     |
| time/               |          |
|    episodes         | 9636     |
|    fps              | 220      |
|    time_elapsed     | 833      |
|    total_timesteps  | 183707   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 35926    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.1     |
|    ep_rew_mean      | -0.0196  |
|    exploration_rate | 0.149    |
| time/               |          |
|    episodes         | 9640     |
|    fps              | 220      |
|    time_elapsed     | 833      |
|    total_timesteps  | 183779   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00676  |
|    n_updates        | 35944    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.4     |
|    ep_rew_mean      | -0.0166  |
|    exploration_rate | 0.148    |
| time/               |          |
|    episodes         | 9644     |
|    fps              | 220      |
|    time_elapsed     | 833      |
|    total_timesteps  | 183851   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.41e-05 |
|    n_updates        | 35962    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | -0.0156  |
|    exploration_rate | 0.148    |
| time/               |          |
|    episodes         | 9648     |
|    fps              | 220      |
|    time_elapsed     | 833      |
|    total_timesteps  | 183925   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.3e-05  |
|    n_updates        | 35981    |
----------------------------------
Eval num_timesteps=184000, episode_reward=-0.09 +/- 0.36
Episode length: 52.34 +/- 23.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.3     |
|    mean_reward      | -0.0888  |
| rollout/            |          |
|    exploration_rate | 0.147    |
| time/               |          |
|    total_timesteps  | 184000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00685  |
|    n_updates        | 35999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | -0.0162  |
|    exploration_rate | 0.147    |
| time/               |          |
|    episodes         | 9652     |
|    fps              | 220      |
|    time_elapsed     | 835      |
|    total_timesteps  | 184018   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.37e-05 |
|    n_updates        | 36004    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.4     |
|    ep_rew_mean      | -0.0364  |
|    exploration_rate | 0.146    |
| time/               |          |
|    episodes         | 9656     |
|    fps              | 220      |
|    time_elapsed     | 835      |
|    total_timesteps  | 184104   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00022  |
|    n_updates        | 36025    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | -0.0248  |
|    exploration_rate | 0.145    |
| time/               |          |
|    episodes         | 9660     |
|    fps              | 220      |
|    time_elapsed     | 835      |
|    total_timesteps  | 184194   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.47e-05 |
|    n_updates        | 36048    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | -0.0271  |
|    exploration_rate | 0.144    |
| time/               |          |
|    episodes         | 9664     |
|    fps              | 220      |
|    time_elapsed     | 835      |
|    total_timesteps  | 184336   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 36083    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.3     |
|    ep_rew_mean      | -0.0163  |
|    exploration_rate | 0.143    |
| time/               |          |
|    episodes         | 9668     |
|    fps              | 220      |
|    time_elapsed     | 835      |
|    total_timesteps  | 184422   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 36105    |
----------------------------------
Eval num_timesteps=184500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.143    |
| time/               |          |
|    total_timesteps  | 184500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 36124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.9     |
|    ep_rew_mean      | -0.00469 |
|    exploration_rate | 0.142    |
| time/               |          |
|    episodes         | 9672     |
|    fps              | 220      |
|    time_elapsed     | 838      |
|    total_timesteps  | 184553   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00662  |
|    n_updates        | 36138    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.2     |
|    ep_rew_mean      | -0.00577 |
|    exploration_rate | 0.141    |
| time/               |          |
|    episodes         | 9676     |
|    fps              | 220      |
|    time_elapsed     | 838      |
|    total_timesteps  | 184659   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.44e-05 |
|    n_updates        | 36164    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.7     |
|    ep_rew_mean      | -0.0179  |
|    exploration_rate | 0.14     |
| time/               |          |
|    episodes         | 9680     |
|    fps              | 220      |
|    time_elapsed     | 838      |
|    total_timesteps  | 184805   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 36201    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.8     |
|    ep_rew_mean      | -0.0243  |
|    exploration_rate | 0.139    |
| time/               |          |
|    episodes         | 9684     |
|    fps              | 220      |
|    time_elapsed     | 838      |
|    total_timesteps  | 184890   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00666  |
|    n_updates        | 36222    |
----------------------------------
Eval num_timesteps=185000, episode_reward=-0.29 +/- 0.03
Episode length: 73.26 +/- 6.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.3     |
|    mean_reward      | -0.293   |
| rollout/            |          |
|    exploration_rate | 0.138    |
| time/               |          |
|    total_timesteps  | 185000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000161 |
|    n_updates        | 36249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.8     |
|    ep_rew_mean      | -0.028   |
|    exploration_rate | 0.138    |
| time/               |          |
|    episodes         | 9688     |
|    fps              | 219      |
|    time_elapsed     | 841      |
|    total_timesteps  | 185051   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.31e-05 |
|    n_updates        | 36262    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.3     |
|    ep_rew_mean      | -0.0303  |
|    exploration_rate | 0.136    |
| time/               |          |
|    episodes         | 9692     |
|    fps              | 220      |
|    time_elapsed     | 841      |
|    total_timesteps  | 185184   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 36295    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | -0.0425  |
|    exploration_rate | 0.135    |
| time/               |          |
|    episodes         | 9696     |
|    fps              | 220      |
|    time_elapsed     | 841      |
|    total_timesteps  | 185310   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.99e-05 |
|    n_updates        | 36327    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.9     |
|    ep_rew_mean      | -0.0425  |
|    exploration_rate | 0.134    |
| time/               |          |
|    episodes         | 9700     |
|    fps              | 220      |
|    time_elapsed     | 841      |
|    total_timesteps  | 185430   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00813  |
|    n_updates        | 36357    |
----------------------------------
Eval num_timesteps=185500, episode_reward=-0.30 +/- 0.00
Episode length: 74.98 +/- 0.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.134    |
| time/               |          |
|    total_timesteps  | 185500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.64e-05 |
|    n_updates        | 36374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26       |
|    ep_rew_mean      | -0.0431  |
|    exploration_rate | 0.133    |
| time/               |          |
|    episodes         | 9704     |
|    fps              | 219      |
|    time_elapsed     | 844      |
|    total_timesteps  | 185540   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.64e-05 |
|    n_updates        | 36384    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.4     |
|    ep_rew_mean      | -0.0447  |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 9708     |
|    fps              | 219      |
|    time_elapsed     | 844      |
|    total_timesteps  | 185651   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.56e-05 |
|    n_updates        | 36412    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.4     |
|    ep_rew_mean      | -0.0344  |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 9712     |
|    fps              | 219      |
|    time_elapsed     | 844      |
|    total_timesteps  | 185727   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.6e-05  |
|    n_updates        | 36431    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.3     |
|    ep_rew_mean      | -0.0344  |
|    exploration_rate | 0.131    |
| time/               |          |
|    episodes         | 9716     |
|    fps              | 220      |
|    time_elapsed     | 844      |
|    total_timesteps  | 185805   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 36451    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.9     |
|    ep_rew_mean      | -0.0364  |
|    exploration_rate | 0.13     |
| time/               |          |
|    episodes         | 9720     |
|    fps              | 220      |
|    time_elapsed     | 844      |
|    total_timesteps  | 185926   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 36481    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.4     |
|    ep_rew_mean      | -0.0346  |
|    exploration_rate | 0.129    |
| time/               |          |
|    episodes         | 9724     |
|    fps              | 220      |
|    time_elapsed     | 844      |
|    total_timesteps  | 185997   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000148 |
|    n_updates        | 36499    |
----------------------------------
Eval num_timesteps=186000, episode_reward=-0.05 +/- 0.14
Episode length: 18.96 +/- 1.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 19       |
|    mean_reward      | -0.0548  |
| rollout/            |          |
|    exploration_rate | 0.129    |
| time/               |          |
|    total_timesteps  | 186000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.1     |
|    ep_rew_mean      | -0.0235  |
|    exploration_rate | 0.128    |
| time/               |          |
|    episodes         | 9728     |
|    fps              | 220      |
|    time_elapsed     | 845      |
|    total_timesteps  | 186088   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000139 |
|    n_updates        | 36521    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.7     |
|    ep_rew_mean      | -0.0216  |
|    exploration_rate | 0.127    |
| time/               |          |
|    episodes         | 9732     |
|    fps              | 220      |
|    time_elapsed     | 845      |
|    total_timesteps  | 186195   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.36e-05 |
|    n_updates        | 36548    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.7     |
|    ep_rew_mean      | -0.0118  |
|    exploration_rate | 0.127    |
| time/               |          |
|    episodes         | 9736     |
|    fps              | 220      |
|    time_elapsed     | 845      |
|    total_timesteps  | 186280   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 36569    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.4     |
|    ep_rew_mean      | -0.0147  |
|    exploration_rate | 0.125    |
| time/               |          |
|    episodes         | 9740     |
|    fps              | 220      |
|    time_elapsed     | 845      |
|    total_timesteps  | 186424   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00675  |
|    n_updates        | 36605    |
----------------------------------
Eval num_timesteps=186500, episode_reward=-0.27 +/- 0.14
Episode length: 73.22 +/- 7.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.2     |
|    mean_reward      | -0.273   |
| rollout/            |          |
|    exploration_rate | 0.125    |
| time/               |          |
|    total_timesteps  | 186500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.03e-05 |
|    n_updates        | 36624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.6     |
|    ep_rew_mean      | -0.0154  |
|    exploration_rate | 0.125    |
| time/               |          |
|    episodes         | 9744     |
|    fps              | 219      |
|    time_elapsed     | 848      |
|    total_timesteps  | 186512   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00674  |
|    n_updates        | 36627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.1     |
|    ep_rew_mean      | -0.0273  |
|    exploration_rate | 0.124    |
| time/               |          |
|    episodes         | 9748     |
|    fps              | 219      |
|    time_elapsed     | 848      |
|    total_timesteps  | 186633   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000145 |
|    n_updates        | 36658    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.1     |
|    ep_rew_mean      | -0.0172  |
|    exploration_rate | 0.123    |
| time/               |          |
|    episodes         | 9752     |
|    fps              | 220      |
|    time_elapsed     | 848      |
|    total_timesteps  | 186724   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.31e-05 |
|    n_updates        | 36680    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.9     |
|    ep_rew_mean      | 0.00367  |
|    exploration_rate | 0.122    |
| time/               |          |
|    episodes         | 9756     |
|    fps              | 220      |
|    time_elapsed     | 848      |
|    total_timesteps  | 186789   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000136 |
|    n_updates        | 36697    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.9     |
|    ep_rew_mean      | -0.00632 |
|    exploration_rate | 0.121    |
| time/               |          |
|    episodes         | 9760     |
|    fps              | 220      |
|    time_elapsed     | 848      |
|    total_timesteps  | 186879   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000243 |
|    n_updates        | 36719    |
----------------------------------
Eval num_timesteps=187000, episode_reward=-0.28 +/- 0.06
Episode length: 69.52 +/- 13.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.5     |
|    mean_reward      | -0.278   |
| rollout/            |          |
|    exploration_rate | 0.12     |
| time/               |          |
|    total_timesteps  | 187000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0143   |
|    n_updates        | 36749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.9     |
|    ep_rew_mean      | -0.00664 |
|    exploration_rate | 0.12     |
| time/               |          |
|    episodes         | 9764     |
|    fps              | 219      |
|    time_elapsed     | 851      |
|    total_timesteps  | 187029   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.72e-05 |
|    n_updates        | 36757    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.4     |
|    ep_rew_mean      | -0.0184  |
|    exploration_rate | 0.119    |
| time/               |          |
|    episodes         | 9768     |
|    fps              | 219      |
|    time_elapsed     | 851      |
|    total_timesteps  | 187160   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.67e-05 |
|    n_updates        | 36789    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 27.7     |
|    ep_rew_mean      | -0.0298  |
|    exploration_rate | 0.117    |
| time/               |          |
|    episodes         | 9772     |
|    fps              | 219      |
|    time_elapsed     | 851      |
|    total_timesteps  | 187325   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00776  |
|    n_updates        | 36831    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.4     |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 0.116    |
| time/               |          |
|    episodes         | 9776     |
|    fps              | 220      |
|    time_elapsed     | 851      |
|    total_timesteps  | 187499   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.24e-05 |
|    n_updates        | 36874    |
----------------------------------
Eval num_timesteps=187500, episode_reward=-0.30 +/- 0.01
Episode length: 74.68 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | -0.299   |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.5     |
|    ep_rew_mean      | -0.0231  |
|    exploration_rate | 0.114    |
| time/               |          |
|    episodes         | 9780     |
|    fps              | 219      |
|    time_elapsed     | 854      |
|    total_timesteps  | 187659   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.14e-05 |
|    n_updates        | 36914    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29       |
|    ep_rew_mean      | -0.015   |
|    exploration_rate | 0.113    |
| time/               |          |
|    episodes         | 9784     |
|    fps              | 219      |
|    time_elapsed     | 854      |
|    total_timesteps  | 187791   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.36e-05 |
|    n_updates        | 36947    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.5     |
|    ep_rew_mean      | -0.013   |
|    exploration_rate | 0.112    |
| time/               |          |
|    episodes         | 9788     |
|    fps              | 219      |
|    time_elapsed     | 854      |
|    total_timesteps  | 187902   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 36975    |
----------------------------------
Eval num_timesteps=188000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.111    |
| time/               |          |
|    total_timesteps  | 188000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000168 |
|    n_updates        | 36999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.2     |
|    ep_rew_mean      | -0.0118  |
|    exploration_rate | 0.111    |
| time/               |          |
|    episodes         | 9792     |
|    fps              | 219      |
|    time_elapsed     | 857      |
|    total_timesteps  | 188005   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.95e-05 |
|    n_updates        | 37001    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.7     |
|    ep_rew_mean      | -0.0139  |
|    exploration_rate | 0.11     |
| time/               |          |
|    episodes         | 9796     |
|    fps              | 219      |
|    time_elapsed     | 857      |
|    total_timesteps  | 188184   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.52e-05 |
|    n_updates        | 37045    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.2     |
|    ep_rew_mean      | -0.016   |
|    exploration_rate | 0.108    |
| time/               |          |
|    episodes         | 9800     |
|    fps              | 219      |
|    time_elapsed     | 857      |
|    total_timesteps  | 188355   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 37088    |
----------------------------------
Eval num_timesteps=188500, episode_reward=-0.30 +/- 0.03
Episode length: 73.88 +/- 7.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.9     |
|    mean_reward      | -0.296   |
| rollout/            |          |
|    exploration_rate | 0.107    |
| time/               |          |
|    total_timesteps  | 188500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000153 |
|    n_updates        | 37124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.9     |
|    ep_rew_mean      | -0.00868 |
|    exploration_rate | 0.106    |
| time/               |          |
|    episodes         | 9804     |
|    fps              | 219      |
|    time_elapsed     | 860      |
|    total_timesteps  | 188533   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 37133    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.6     |
|    ep_rew_mean      | -0.00736 |
|    exploration_rate | 0.106    |
| time/               |          |
|    episodes         | 9808     |
|    fps              | 219      |
|    time_elapsed     | 860      |
|    total_timesteps  | 188611   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000213 |
|    n_updates        | 37152    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.6     |
|    ep_rew_mean      | -0.0175  |
|    exploration_rate | 0.105    |
| time/               |          |
|    episodes         | 9812     |
|    fps              | 219      |
|    time_elapsed     | 860      |
|    total_timesteps  | 188691   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.18e-05 |
|    n_updates        | 37172    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.6     |
|    ep_rew_mean      | -0.00738 |
|    exploration_rate | 0.104    |
| time/               |          |
|    episodes         | 9816     |
|    fps              | 219      |
|    time_elapsed     | 860      |
|    total_timesteps  | 188766   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00771  |
|    n_updates        | 37191    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.4     |
|    ep_rew_mean      | -0.0108  |
|    exploration_rate | 0.102    |
| time/               |          |
|    episodes         | 9820     |
|    fps              | 219      |
|    time_elapsed     | 860      |
|    total_timesteps  | 188971   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000175 |
|    n_updates        | 37242    |
----------------------------------
Eval num_timesteps=189000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.102    |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.06e-05 |
|    n_updates        | 37249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.3     |
|    ep_rew_mean      | -0.0141  |
|    exploration_rate | 0.101    |
| time/               |          |
|    episodes         | 9824     |
|    fps              | 219      |
|    time_elapsed     | 863      |
|    total_timesteps  | 189126   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 37281    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.4     |
|    ep_rew_mean      | -0.0287  |
|    exploration_rate | 0.0992   |
| time/               |          |
|    episodes         | 9828     |
|    fps              | 219      |
|    time_elapsed     | 863      |
|    total_timesteps  | 189332   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.1e-05  |
|    n_updates        | 37332    |
----------------------------------
Eval num_timesteps=189500, episode_reward=-0.19 +/- 0.17
Episode length: 52.48 +/- 21.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.5     |
|    mean_reward      | -0.189   |
| rollout/            |          |
|    exploration_rate | 0.0977   |
| time/               |          |
|    total_timesteps  | 189500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.94e-05 |
|    n_updates        | 37374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.1     |
|    ep_rew_mean      | -0.0414  |
|    exploration_rate | 0.0977   |
| time/               |          |
|    episodes         | 9832     |
|    fps              | 218      |
|    time_elapsed     | 865      |
|    total_timesteps  | 189504   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.01e-05 |
|    n_updates        | 37375    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.4     |
|    ep_rew_mean      | -0.0525  |
|    exploration_rate | 0.0967   |
| time/               |          |
|    episodes         | 9836     |
|    fps              | 219      |
|    time_elapsed     | 865      |
|    total_timesteps  | 189616   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000163 |
|    n_updates        | 37403    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.6     |
|    ep_rew_mean      | -0.0496  |
|    exploration_rate | 0.096    |
| time/               |          |
|    episodes         | 9840     |
|    fps              | 219      |
|    time_elapsed     | 865      |
|    total_timesteps  | 189688   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.61e-05 |
|    n_updates        | 37421    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.7     |
|    ep_rew_mean      | -0.0536  |
|    exploration_rate | 0.0943   |
| time/               |          |
|    episodes         | 9844     |
|    fps              | 219      |
|    time_elapsed     | 865      |
|    total_timesteps  | 189878   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.61e-05 |
|    n_updates        | 37469    |
----------------------------------
Eval num_timesteps=190000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.0932   |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000171 |
|    n_updates        | 37499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.4     |
|    ep_rew_mean      | -0.0566  |
|    exploration_rate | 0.0925   |
| time/               |          |
|    episodes         | 9848     |
|    fps              | 218      |
|    time_elapsed     | 868      |
|    total_timesteps  | 190074   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.92e-05 |
|    n_updates        | 37518    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.8     |
|    ep_rew_mean      | -0.068   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 9852     |
|    fps              | 218      |
|    time_elapsed     | 868      |
|    total_timesteps  | 190200   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00779  |
|    n_updates        | 37549    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -0.0918  |
|    exploration_rate | 0.0899   |
| time/               |          |
|    episodes         | 9856     |
|    fps              | 219      |
|    time_elapsed     | 868      |
|    total_timesteps  | 190359   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.06e-05 |
|    n_updates        | 37589    |
----------------------------------
Eval num_timesteps=190500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.0886   |
| time/               |          |
|    total_timesteps  | 190500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.37e-05 |
|    n_updates        | 37624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.4     |
|    ep_rew_mean      | -0.0846  |
|    exploration_rate | 0.0885   |
| time/               |          |
|    episodes         | 9860     |
|    fps              | 218      |
|    time_elapsed     | 871      |
|    total_timesteps  | 190518   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0067   |
|    n_updates        | 37629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.9     |
|    ep_rew_mean      | -0.0827  |
|    exploration_rate | 0.0875   |
| time/               |          |
|    episodes         | 9864     |
|    fps              | 218      |
|    time_elapsed     | 871      |
|    total_timesteps  | 190620   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.33e-05 |
|    n_updates        | 37654    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -0.0617  |
|    exploration_rate | 0.0866   |
| time/               |          |
|    episodes         | 9868     |
|    fps              | 218      |
|    time_elapsed     | 871      |
|    total_timesteps  | 190727   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 37681    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -0.0596  |
|    exploration_rate | 0.0855   |
| time/               |          |
|    episodes         | 9872     |
|    fps              | 218      |
|    time_elapsed     | 871      |
|    total_timesteps  | 190840   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.3e-05  |
|    n_updates        | 37709    |
----------------------------------
Eval num_timesteps=191000, episode_reward=-0.15 +/- 0.34
Episode length: 63.40 +/- 17.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.4     |
|    mean_reward      | -0.153   |
| rollout/            |          |
|    exploration_rate | 0.0841   |
| time/               |          |
|    total_timesteps  | 191000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0068   |
|    n_updates        | 37749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -0.0696  |
|    exploration_rate | 0.084    |
| time/               |          |
|    episodes         | 9876     |
|    fps              | 218      |
|    time_elapsed     | 874      |
|    total_timesteps  | 191013   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.01e-05 |
|    n_updates        | 37753    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.1     |
|    ep_rew_mean      | -0.0693  |
|    exploration_rate | 0.0826   |
| time/               |          |
|    episodes         | 9880     |
|    fps              | 218      |
|    time_elapsed     | 874      |
|    total_timesteps  | 191167   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000176 |
|    n_updates        | 37791    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.3     |
|    ep_rew_mean      | -0.0841  |
|    exploration_rate | 0.0803   |
| time/               |          |
|    episodes         | 9884     |
|    fps              | 218      |
|    time_elapsed     | 874      |
|    total_timesteps  | 191419   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.95e-05 |
|    n_updates        | 37854    |
----------------------------------
Eval num_timesteps=191500, episode_reward=-0.10 +/- 0.36
Episode length: 55.74 +/- 24.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.7     |
|    mean_reward      | -0.102   |
| rollout/            |          |
|    exploration_rate | 0.0796   |
| time/               |          |
|    total_timesteps  | 191500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00773  |
|    n_updates        | 37874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.9     |
|    ep_rew_mean      | -0.0866  |
|    exploration_rate | 0.0787   |
| time/               |          |
|    episodes         | 9888     |
|    fps              | 218      |
|    time_elapsed     | 876      |
|    total_timesteps  | 191591   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000248 |
|    n_updates        | 37897    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.5     |
|    ep_rew_mean      | -0.089   |
|    exploration_rate | 0.0772   |
| time/               |          |
|    episodes         | 9892     |
|    fps              | 218      |
|    time_elapsed     | 876      |
|    total_timesteps  | 191755   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.03e-05 |
|    n_updates        | 37938    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.5     |
|    ep_rew_mean      | -0.085   |
|    exploration_rate | 0.0765   |
| time/               |          |
|    episodes         | 9896     |
|    fps              | 218      |
|    time_elapsed     | 877      |
|    total_timesteps  | 191833   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00699  |
|    n_updates        | 37958    |
----------------------------------
Eval num_timesteps=192000, episode_reward=-0.04 +/- 0.39
Episode length: 51.22 +/- 22.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | -0.0443  |
| rollout/            |          |
|    exploration_rate | 0.075    |
| time/               |          |
|    total_timesteps  | 192000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.44e-05 |
|    n_updates        | 37999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.6     |
|    ep_rew_mean      | -0.0754  |
|    exploration_rate | 0.0749   |
| time/               |          |
|    episodes         | 9900     |
|    fps              | 218      |
|    time_elapsed     | 878      |
|    total_timesteps  | 192014   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 38003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36       |
|    ep_rew_mean      | -0.0729  |
|    exploration_rate | 0.0738   |
| time/               |          |
|    episodes         | 9904     |
|    fps              | 218      |
|    time_elapsed     | 879      |
|    total_timesteps  | 192129   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000174 |
|    n_updates        | 38032    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.2     |
|    ep_rew_mean      | -0.0778  |
|    exploration_rate | 0.072    |
| time/               |          |
|    episodes         | 9908     |
|    fps              | 218      |
|    time_elapsed     | 879      |
|    total_timesteps  | 192329   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 38082    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.3     |
|    ep_rew_mean      | -0.0784  |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 9912     |
|    fps              | 218      |
|    time_elapsed     | 879      |
|    total_timesteps  | 192424   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000202 |
|    n_updates        | 38105    |
----------------------------------
Eval num_timesteps=192500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.0704   |
| time/               |          |
|    total_timesteps  | 192500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000141 |
|    n_updates        | 38124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.1     |
|    ep_rew_mean      | -0.0913  |
|    exploration_rate | 0.0698   |
| time/               |          |
|    episodes         | 9916     |
|    fps              | 218      |
|    time_elapsed     | 882      |
|    total_timesteps  | 192572   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.78e-05 |
|    n_updates        | 38142    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.8     |
|    ep_rew_mean      | -0.0804  |
|    exploration_rate | 0.0681   |
| time/               |          |
|    episodes         | 9920     |
|    fps              | 218      |
|    time_elapsed     | 882      |
|    total_timesteps  | 192754   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00774  |
|    n_updates        | 38188    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.8     |
|    ep_rew_mean      | -0.0803  |
|    exploration_rate | 0.0667   |
| time/               |          |
|    episodes         | 9924     |
|    fps              | 218      |
|    time_elapsed     | 882      |
|    total_timesteps  | 192906   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.5e-05  |
|    n_updates        | 38226    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.5     |
|    ep_rew_mean      | -0.0849  |
|    exploration_rate | 0.066    |
| time/               |          |
|    episodes         | 9928     |
|    fps              | 218      |
|    time_elapsed     | 882      |
|    total_timesteps  | 192978   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.83e-05 |
|    n_updates        | 38244    |
----------------------------------
Eval num_timesteps=193000, episode_reward=-0.01 +/- 0.44
Episode length: 53.72 +/- 20.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.7     |
|    mean_reward      | -0.0142  |
| rollout/            |          |
|    exploration_rate | 0.0658   |
| time/               |          |
|    total_timesteps  | 193000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.09e-05 |
|    n_updates        | 38249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.4     |
|    ep_rew_mean      | -0.0848  |
|    exploration_rate | 0.0645   |
| time/               |          |
|    episodes         | 9932     |
|    fps              | 218      |
|    time_elapsed     | 884      |
|    total_timesteps  | 193147   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.2e-05  |
|    n_updates        | 38286    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.4     |
|    ep_rew_mean      | -0.0746  |
|    exploration_rate | 0.0635   |
| time/               |          |
|    episodes         | 9936     |
|    fps              | 218      |
|    time_elapsed     | 884      |
|    total_timesteps  | 193254   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.4e-05  |
|    n_updates        | 38313    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.2     |
|    ep_rew_mean      | -0.078   |
|    exploration_rate | 0.0621   |
| time/               |          |
|    episodes         | 9940     |
|    fps              | 218      |
|    time_elapsed     | 884      |
|    total_timesteps  | 193411   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.84e-05 |
|    n_updates        | 38352    |
----------------------------------
Eval num_timesteps=193500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.0613   |
| time/               |          |
|    total_timesteps  | 193500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.73e-05 |
|    n_updates        | 38374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.6     |
|    ep_rew_mean      | -0.0694  |
|    exploration_rate | 0.06     |
| time/               |          |
|    episodes         | 9944     |
|    fps              | 218      |
|    time_elapsed     | 887      |
|    total_timesteps  | 193636   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.04e-05 |
|    n_updates        | 38408    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.5     |
|    ep_rew_mean      | -0.0692  |
|    exploration_rate | 0.0583   |
| time/               |          |
|    episodes         | 9948     |
|    fps              | 218      |
|    time_elapsed     | 887      |
|    total_timesteps  | 193827   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.01e-05 |
|    n_updates        | 38456    |
----------------------------------
Eval num_timesteps=194000, episode_reward=0.05 +/- 0.36
Episode length: 23.66 +/- 17.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23.7     |
|    mean_reward      | 0.0465   |
| rollout/            |          |
|    exploration_rate | 0.0567   |
| time/               |          |
|    total_timesteps  | 194000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.44e-05 |
|    n_updates        | 38499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.2     |
|    ep_rew_mean      | -0.0718  |
|    exploration_rate | 0.0565   |
| time/               |          |
|    episodes         | 9952     |
|    fps              | 218      |
|    time_elapsed     | 888      |
|    total_timesteps  | 194017   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.07e-05 |
|    n_updates        | 38504    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.6     |
|    ep_rew_mean      | -0.0694  |
|    exploration_rate | 0.0556   |
| time/               |          |
|    episodes         | 9956     |
|    fps              | 218      |
|    time_elapsed     | 888      |
|    total_timesteps  | 194118   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00708  |
|    n_updates        | 38529    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37       |
|    ep_rew_mean      | -0.0771  |
|    exploration_rate | 0.0547   |
| time/               |          |
|    episodes         | 9960     |
|    fps              | 218      |
|    time_elapsed     | 888      |
|    total_timesteps  | 194217   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.34e-05 |
|    n_updates        | 38554    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.4     |
|    ep_rew_mean      | -0.0785  |
|    exploration_rate | 0.0534   |
| time/               |          |
|    episodes         | 9964     |
|    fps              | 218      |
|    time_elapsed     | 889      |
|    total_timesteps  | 194356   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00689  |
|    n_updates        | 38588    |
----------------------------------
Eval num_timesteps=194500, episode_reward=-0.14 +/- 0.32
Episode length: 55.16 +/- 20.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.2     |
|    mean_reward      | -0.14    |
| rollout/            |          |
|    exploration_rate | 0.0521   |
| time/               |          |
|    total_timesteps  | 194500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00678  |
|    n_updates        | 38624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.9     |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0519   |
| time/               |          |
|    episodes         | 9968     |
|    fps              | 218      |
|    time_elapsed     | 891      |
|    total_timesteps  | 194518   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 38629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.1     |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.0506   |
| time/               |          |
|    episodes         | 9972     |
|    fps              | 218      |
|    time_elapsed     | 891      |
|    total_timesteps  | 194654   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.9e-05  |
|    n_updates        | 38663    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.9     |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0492   |
| time/               |          |
|    episodes         | 9976     |
|    fps              | 218      |
|    time_elapsed     | 891      |
|    total_timesteps  | 194806   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00667  |
|    n_updates        | 38701    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.6     |
|    ep_rew_mean      | -0.0993  |
|    exploration_rate | 0.0482   |
| time/               |          |
|    episodes         | 9980     |
|    fps              | 218      |
|    time_elapsed     | 891      |
|    total_timesteps  | 194923   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.6e-05  |
|    n_updates        | 38730    |
----------------------------------
Eval num_timesteps=195000, episode_reward=-0.06 +/- 0.27
Episode length: 29.76 +/- 22.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 29.8     |
|    mean_reward      | -0.0581  |
| rollout/            |          |
|    exploration_rate | 0.0475   |
| time/               |          |
|    total_timesteps  | 195000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00674  |
|    n_updates        | 38749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.1     |
|    ep_rew_mean      | -0.0834  |
|    exploration_rate | 0.0472   |
| time/               |          |
|    episodes         | 9984     |
|    fps              | 218      |
|    time_elapsed     | 892      |
|    total_timesteps  | 195026   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.78e-05 |
|    n_updates        | 38756    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.2     |
|    ep_rew_mean      | -0.0738  |
|    exploration_rate | 0.0456   |
| time/               |          |
|    episodes         | 9988     |
|    fps              | 218      |
|    time_elapsed     | 892      |
|    total_timesteps  | 195207   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00664  |
|    n_updates        | 38801    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.7     |
|    ep_rew_mean      | -0.072   |
|    exploration_rate | 0.0444   |
| time/               |          |
|    episodes         | 9992     |
|    fps              | 218      |
|    time_elapsed     | 893      |
|    total_timesteps  | 195327   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.78e-05 |
|    n_updates        | 38831    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.2     |
|    ep_rew_mean      | -0.074   |
|    exploration_rate | 0.0433   |
| time/               |          |
|    episodes         | 9996     |
|    fps              | 218      |
|    time_elapsed     | 893      |
|    total_timesteps  | 195456   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000165 |
|    n_updates        | 38863    |
----------------------------------
Eval num_timesteps=195500, episode_reward=-0.02 +/- 0.38
Episode length: 41.30 +/- 20.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | -0.0242  |
| rollout/            |          |
|    exploration_rate | 0.0429   |
| time/               |          |
|    total_timesteps  | 195500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.27e-05 |
|    n_updates        | 38874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.1     |
|    ep_rew_mean      | -0.0775  |
|    exploration_rate | 0.0408   |
| time/               |          |
|    episodes         | 10000    |
|    fps              | 218      |
|    time_elapsed     | 894      |
|    total_timesteps  | 195725   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.35e-05 |
|    n_updates        | 38931    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.9     |
|    ep_rew_mean      | -0.0866  |
|    exploration_rate | 0.0399   |
| time/               |          |
|    episodes         | 10004    |
|    fps              | 218      |
|    time_elapsed     | 894      |
|    total_timesteps  | 195816   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.12e-05 |
|    n_updates        | 38953    |
----------------------------------
Eval num_timesteps=196000, episode_reward=-0.10 +/- 0.30
Episode length: 50.66 +/- 21.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.7     |
|    mean_reward      | -0.102   |
| rollout/            |          |
|    exploration_rate | 0.0382   |
| time/               |          |
|    total_timesteps  | 196000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000179 |
|    n_updates        | 38999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.2     |
|    ep_rew_mean      | -0.0781  |
|    exploration_rate | 0.0377   |
| time/               |          |
|    episodes         | 10008    |
|    fps              | 218      |
|    time_elapsed     | 896      |
|    total_timesteps  | 196053   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.87e-05 |
|    n_updates        | 39013    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38       |
|    ep_rew_mean      | -0.061   |
|    exploration_rate | 0.0362   |
| time/               |          |
|    episodes         | 10012    |
|    fps              | 218      |
|    time_elapsed     | 897      |
|    total_timesteps  | 196220   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 39054    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38       |
|    ep_rew_mean      | -0.0512  |
|    exploration_rate | 0.0348   |
| time/               |          |
|    episodes         | 10016    |
|    fps              | 218      |
|    time_elapsed     | 897      |
|    total_timesteps  | 196373   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.36e-06 |
|    n_updates        | 39093    |
----------------------------------
Eval num_timesteps=196500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.0336   |
| time/               |          |
|    total_timesteps  | 196500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.72e-05 |
|    n_updates        | 39124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.2     |
|    ep_rew_mean      | -0.062   |
|    exploration_rate | 0.0329   |
| time/               |          |
|    episodes         | 10020    |
|    fps              | 218      |
|    time_elapsed     | 899      |
|    total_timesteps  | 196577   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00657  |
|    n_updates        | 39144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.9     |
|    ep_rew_mean      | -0.0507  |
|    exploration_rate | 0.0318   |
| time/               |          |
|    episodes         | 10024    |
|    fps              | 218      |
|    time_elapsed     | 900      |
|    total_timesteps  | 196695   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.59e-05 |
|    n_updates        | 39173    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.7     |
|    ep_rew_mean      | -0.044   |
|    exploration_rate | 0.0303   |
| time/               |          |
|    episodes         | 10028    |
|    fps              | 218      |
|    time_elapsed     | 900      |
|    total_timesteps  | 196851   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00662  |
|    n_updates        | 39212    |
----------------------------------
Eval num_timesteps=197000, episode_reward=-0.29 +/- 0.05
Episode length: 72.64 +/- 11.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.6     |
|    mean_reward      | -0.291   |
| rollout/            |          |
|    exploration_rate | 0.029    |
| time/               |          |
|    total_timesteps  | 197000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.08e-05 |
|    n_updates        | 39249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.6     |
|    ep_rew_mean      | -0.0477  |
|    exploration_rate | 0.0279   |
| time/               |          |
|    episodes         | 10032    |
|    fps              | 218      |
|    time_elapsed     | 903      |
|    total_timesteps  | 197112   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00692  |
|    n_updates        | 39277    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.1     |
|    ep_rew_mean      | -0.0638  |
|    exploration_rate | 0.0255   |
| time/               |          |
|    episodes         | 10036    |
|    fps              | 218      |
|    time_elapsed     | 903      |
|    total_timesteps  | 197369   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.41e-05 |
|    n_updates        | 39342    |
----------------------------------
Eval num_timesteps=197500, episode_reward=-0.28 +/- 0.14
Episode length: 74.86 +/- 0.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.9     |
|    mean_reward      | -0.279   |
| rollout/            |          |
|    exploration_rate | 0.0243   |
| time/               |          |
|    total_timesteps  | 197500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.91e-05 |
|    n_updates        | 39374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.7     |
|    ep_rew_mean      | -0.0658  |
|    exploration_rate | 0.0236   |
| time/               |          |
|    episodes         | 10040    |
|    fps              | 218      |
|    time_elapsed     | 906      |
|    total_timesteps  | 197577   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.81e-05 |
|    n_updates        | 39394    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41       |
|    ep_rew_mean      | -0.0633  |
|    exploration_rate | 0.0221   |
| time/               |          |
|    episodes         | 10044    |
|    fps              | 218      |
|    time_elapsed     | 906      |
|    total_timesteps  | 197740   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00677  |
|    n_updates        | 39434    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41       |
|    ep_rew_mean      | -0.0633  |
|    exploration_rate | 0.0203   |
| time/               |          |
|    episodes         | 10048    |
|    fps              | 218      |
|    time_elapsed     | 906      |
|    total_timesteps  | 197930   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.43e-05 |
|    n_updates        | 39482    |
----------------------------------
Eval num_timesteps=198000, episode_reward=-0.01 +/- 0.44
Episode length: 57.02 +/- 19.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57       |
|    mean_reward      | -0.00732 |
| rollout/            |          |
|    exploration_rate | 0.0197   |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000159 |
|    n_updates        | 39499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.2     |
|    ep_rew_mean      | -0.064   |
|    exploration_rate | 0.0184   |
| time/               |          |
|    episodes         | 10052    |
|    fps              | 218      |
|    time_elapsed     | 908      |
|    total_timesteps  | 198137   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.16e-05 |
|    n_updates        | 39534    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.5     |
|    ep_rew_mean      | -0.0652  |
|    exploration_rate | 0.0172   |
| time/               |          |
|    episodes         | 10056    |
|    fps              | 218      |
|    time_elapsed     | 908      |
|    total_timesteps  | 198269   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.04e-05 |
|    n_updates        | 39567    |
----------------------------------
Eval num_timesteps=198500, episode_reward=-0.24 +/- 0.10
Episode length: 59.60 +/- 23.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.6     |
|    mean_reward      | -0.238   |
| rollout/            |          |
|    exploration_rate | 0.015    |
| time/               |          |
|    total_timesteps  | 198500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.34e-05 |
|    n_updates        | 39624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.1     |
|    ep_rew_mean      | -0.0718  |
|    exploration_rate | 0.0147   |
| time/               |          |
|    episodes         | 10060    |
|    fps              | 217      |
|    time_elapsed     | 910      |
|    total_timesteps  | 198531   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.91e-05 |
|    n_updates        | 39632    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.8     |
|    ep_rew_mean      | -0.0745  |
|    exploration_rate | 0.0128   |
| time/               |          |
|    episodes         | 10064    |
|    fps              | 218      |
|    time_elapsed     | 911      |
|    total_timesteps  | 198739   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.98e-05 |
|    n_updates        | 39684    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.1     |
|    ep_rew_mean      | -0.0617  |
|    exploration_rate | 0.0119   |
| time/               |          |
|    episodes         | 10068    |
|    fps              | 218      |
|    time_elapsed     | 911      |
|    total_timesteps  | 198830   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00657  |
|    n_updates        | 39707    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.3     |
|    ep_rew_mean      | -0.0524  |
|    exploration_rate | 0.0105   |
| time/               |          |
|    episodes         | 10072    |
|    fps              | 218      |
|    time_elapsed     | 911      |
|    total_timesteps  | 198984   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00682  |
|    n_updates        | 39745    |
----------------------------------
Eval num_timesteps=199000, episode_reward=-0.23 +/- 0.20
Episode length: 68.70 +/- 11.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.7     |
|    mean_reward      | -0.234   |
| rollout/            |          |
|    exploration_rate | 0.0104   |
| time/               |          |
|    total_timesteps  | 199000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.19e-05 |
|    n_updates        | 39749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.3     |
|    ep_rew_mean      | -0.0565  |
|    exploration_rate | 0.00813  |
| time/               |          |
|    episodes         | 10076    |
|    fps              | 217      |
|    time_elapsed     | 913      |
|    total_timesteps  | 199238   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.68e-05 |
|    n_updates        | 39809    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45       |
|    ep_rew_mean      | -0.0593  |
|    exploration_rate | 0.00639  |
| time/               |          |
|    episodes         | 10080    |
|    fps              | 218      |
|    time_elapsed     | 914      |
|    total_timesteps  | 199424   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.97e-05 |
|    n_updates        | 39855    |
----------------------------------
Eval num_timesteps=199500, episode_reward=0.03 +/- 0.34
Episode length: 23.92 +/- 13.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 23.9     |
|    mean_reward      | 0.0253   |
| rollout/            |          |
|    exploration_rate | 0.00569  |
| time/               |          |
|    total_timesteps  | 199500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000154 |
|    n_updates        | 39874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.1     |
|    ep_rew_mean      | -0.0697  |
|    exploration_rate | 0.00533  |
| time/               |          |
|    episodes         | 10084    |
|    fps              | 218      |
|    time_elapsed     | 915      |
|    total_timesteps  | 199537   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.01e-05 |
|    n_updates        | 39884    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.6     |
|    ep_rew_mean      | -0.0715  |
|    exploration_rate | 0.00321  |
| time/               |          |
|    episodes         | 10088    |
|    fps              | 218      |
|    time_elapsed     | 915      |
|    total_timesteps  | 199764   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.78e-05 |
|    n_updates        | 39940    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.1     |
|    ep_rew_mean      | -0.0736  |
|    exploration_rate | 0.00161  |
| time/               |          |
|    episodes         | 10092    |
|    fps              | 218      |
|    time_elapsed     | 915      |
|    total_timesteps  | 199935   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.05e-05 |
|    n_updates        | 39983    |
----------------------------------
Eval num_timesteps=200000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.00101  |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 39999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.5     |
|    ep_rew_mean      | -0.0793  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10096    |
|    fps              | 217      |
|    time_elapsed     | 918      |
|    total_timesteps  | 200208   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.04e-05 |
|    n_updates        | 40051    |
----------------------------------
Eval num_timesteps=200500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 200500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00659  |
|    n_updates        | 40124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.8     |
|    ep_rew_mean      | -0.0906  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10100    |
|    fps              | 217      |
|    time_elapsed     | 921      |
|    total_timesteps  | 200508   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.25e-05 |
|    n_updates        | 40126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.9     |
|    ep_rew_mean      | -0.099   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10104    |
|    fps              | 217      |
|    time_elapsed     | 921      |
|    total_timesteps  | 200808   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.33e-05 |
|    n_updates        | 40201    |
----------------------------------
Eval num_timesteps=201000, episode_reward=-0.09 +/- 0.29
Episode length: 43.32 +/- 16.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | -0.0923  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 201000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.43e-05 |
|    n_updates        | 40249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.8     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10108    |
|    fps              | 217      |
|    time_elapsed     | 923      |
|    total_timesteps  | 201033   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.27e-06 |
|    n_updates        | 40258    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10112    |
|    fps              | 217      |
|    time_elapsed     | 923      |
|    total_timesteps  | 201250   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.64e-05 |
|    n_updates        | 40312    |
----------------------------------
Eval num_timesteps=201500, episode_reward=-0.28 +/- 0.06
Episode length: 70.32 +/- 15.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.3     |
|    mean_reward      | -0.281   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 201500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.1e-05  |
|    n_updates        | 40374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.8     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10116    |
|    fps              | 217      |
|    time_elapsed     | 926      |
|    total_timesteps  | 201550   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.91e-05 |
|    n_updates        | 40387    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.2     |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10120    |
|    fps              | 217      |
|    time_elapsed     | 926      |
|    total_timesteps  | 201795   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.63e-05 |
|    n_updates        | 40448    |
----------------------------------
Eval num_timesteps=202000, episode_reward=-0.04 +/- 0.20
Episode length: 21.08 +/- 8.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 21.1     |
|    mean_reward      | -0.0433  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 202000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.09e-05 |
|    n_updates        | 40499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.9     |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10124    |
|    fps              | 217      |
|    time_elapsed     | 927      |
|    total_timesteps  | 202081   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00684  |
|    n_updates        | 40520    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.3     |
|    ep_rew_mean      | -0.161   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10128    |
|    fps              | 218      |
|    time_elapsed     | 927      |
|    total_timesteps  | 202381   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.14e-05 |
|    n_updates        | 40595    |
----------------------------------
Eval num_timesteps=202500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 202500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.29e-06 |
|    n_updates        | 40624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.7     |
|    ep_rew_mean      | -0.162   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10132    |
|    fps              | 217      |
|    time_elapsed     | 930      |
|    total_timesteps  | 202681   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.11e-05 |
|    n_updates        | 40670    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.9     |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10136    |
|    fps              | 218      |
|    time_elapsed     | 930      |
|    total_timesteps  | 202957   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.4e-05  |
|    n_updates        | 40739    |
----------------------------------
Eval num_timesteps=203000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 203000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.1e-05  |
|    n_updates        | 40749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.3     |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10140    |
|    fps              | 217      |
|    time_elapsed     | 933      |
|    total_timesteps  | 203208   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.74e-05 |
|    n_updates        | 40801    |
----------------------------------
Eval num_timesteps=203500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 203500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.48e-05 |
|    n_updates        | 40874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.7     |
|    ep_rew_mean      | -0.18    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10144    |
|    fps              | 217      |
|    time_elapsed     | 936      |
|    total_timesteps  | 203508   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.78e-06 |
|    n_updates        | 40876    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.3     |
|    ep_rew_mean      | -0.183   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10148    |
|    fps              | 217      |
|    time_elapsed     | 936      |
|    total_timesteps  | 203756   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.83e-05 |
|    n_updates        | 40938    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.2     |
|    ep_rew_mean      | -0.182   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10152    |
|    fps              | 217      |
|    time_elapsed     | 936      |
|    total_timesteps  | 203953   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 40988    |
----------------------------------
Eval num_timesteps=204000, episode_reward=0.07 +/- 0.35
Episode length: 17.04 +/- 1.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17       |
|    mean_reward      | 0.0729   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 204000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.71e-05 |
|    n_updates        | 40999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.5     |
|    ep_rew_mean      | -0.18    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10156    |
|    fps              | 217      |
|    time_elapsed     | 937      |
|    total_timesteps  | 204023   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 41005    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.9     |
|    ep_rew_mean      | -0.181   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10160    |
|    fps              | 217      |
|    time_elapsed     | 937      |
|    total_timesteps  | 204323   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.41e-05 |
|    n_updates        | 41080    |
----------------------------------
Eval num_timesteps=204500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.48e-05 |
|    n_updates        | 41124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.8     |
|    ep_rew_mean      | -0.185   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10164    |
|    fps              | 217      |
|    time_elapsed     | 940      |
|    total_timesteps  | 204623   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.07e-05 |
|    n_updates        | 41155    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.5     |
|    ep_rew_mean      | -0.192   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10168    |
|    fps              | 217      |
|    time_elapsed     | 941      |
|    total_timesteps  | 204884   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.88e-05 |
|    n_updates        | 41220    |
----------------------------------
Eval num_timesteps=205000, episode_reward=-0.29 +/- 0.05
Episode length: 72.62 +/- 11.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.6     |
|    mean_reward      | -0.29    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.43e-05 |
|    n_updates        | 41249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62       |
|    ep_rew_mean      | -0.208   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10172    |
|    fps              | 217      |
|    time_elapsed     | 943      |
|    total_timesteps  | 205184   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.06e-05 |
|    n_updates        | 41295    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.5     |
|    ep_rew_mean      | -0.205   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10176    |
|    fps              | 217      |
|    time_elapsed     | 944      |
|    total_timesteps  | 205383   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.16e-06 |
|    n_updates        | 41345    |
----------------------------------
Eval num_timesteps=205500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 205500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4e-05    |
|    n_updates        | 41374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62       |
|    ep_rew_mean      | -0.208   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10180    |
|    fps              | 217      |
|    time_elapsed     | 946      |
|    total_timesteps  | 205628   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.95e-06 |
|    n_updates        | 41406    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.5     |
|    ep_rew_mean      | -0.214   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10184    |
|    fps              | 217      |
|    time_elapsed     | 947      |
|    total_timesteps  | 205887   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.63e-05 |
|    n_updates        | 41471    |
----------------------------------
Eval num_timesteps=206000, episode_reward=-0.05 +/- 0.35
Episode length: 46.76 +/- 24.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | -0.0464  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.46e-06 |
|    n_updates        | 41499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.8     |
|    ep_rew_mean      | -0.221   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10188    |
|    fps              | 217      |
|    time_elapsed     | 948      |
|    total_timesteps  | 206046   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 41511    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63       |
|    ep_rew_mean      | -0.222   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10192    |
|    fps              | 217      |
|    time_elapsed     | 949      |
|    total_timesteps  | 206232   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 41557    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.8     |
|    ep_rew_mean      | -0.207   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10196    |
|    fps              | 217      |
|    time_elapsed     | 949      |
|    total_timesteps  | 206387   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00673  |
|    n_updates        | 41596    |
----------------------------------
Eval num_timesteps=206500, episode_reward=-0.30 +/- 0.00
Episode length: 74.98 +/- 0.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 206500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.38e-05 |
|    n_updates        | 41624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.3     |
|    ep_rew_mean      | -0.205   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10200    |
|    fps              | 217      |
|    time_elapsed     | 952      |
|    total_timesteps  | 206634   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.02e-06 |
|    n_updates        | 41658    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59       |
|    ep_rew_mean      | -0.196   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10204    |
|    fps              | 217      |
|    time_elapsed     | 952      |
|    total_timesteps  | 206713   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.75e-05 |
|    n_updates        | 41678    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.5     |
|    ep_rew_mean      | -0.19    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10208    |
|    fps              | 217      |
|    time_elapsed     | 952      |
|    total_timesteps  | 206787   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.57e-05 |
|    n_updates        | 41696    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.4     |
|    ep_rew_mean      | -0.205   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10212    |
|    fps              | 217      |
|    time_elapsed     | 952      |
|    total_timesteps  | 206890   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00659  |
|    n_updates        | 41722    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.2     |
|    ep_rew_mean      | -0.196   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10216    |
|    fps              | 217      |
|    time_elapsed     | 952      |
|    total_timesteps  | 206972   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.93e-05 |
|    n_updates        | 41742    |
----------------------------------
Eval num_timesteps=207000, episode_reward=0.03 +/- 0.30
Episode length: 16.88 +/- 0.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | 0.0335   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.27e-06 |
|    n_updates        | 41749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.4     |
|    ep_rew_mean      | -0.179   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10220    |
|    fps              | 217      |
|    time_elapsed     | 953      |
|    total_timesteps  | 207039   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.81e-05 |
|    n_updates        | 41759    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.171   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10224    |
|    fps              | 217      |
|    time_elapsed     | 953      |
|    total_timesteps  | 207107   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.14e-06 |
|    n_updates        | 41776    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.9     |
|    ep_rew_mean      | -0.151   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10228    |
|    fps              | 217      |
|    time_elapsed     | 953      |
|    total_timesteps  | 207173   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.69e-05 |
|    n_updates        | 41793    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.9     |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10232    |
|    fps              | 217      |
|    time_elapsed     | 953      |
|    total_timesteps  | 207275   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 41818    |
----------------------------------
Eval num_timesteps=207500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.86e-06 |
|    n_updates        | 41874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.2     |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10236    |
|    fps              | 217      |
|    time_elapsed     | 956      |
|    total_timesteps  | 207575   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000253 |
|    n_updates        | 41893    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.5     |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10240    |
|    fps              | 217      |
|    time_elapsed     | 956      |
|    total_timesteps  | 207656   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.24e-05 |
|    n_updates        | 41913    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.4     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10244    |
|    fps              | 217      |
|    time_elapsed     | 956      |
|    total_timesteps  | 207743   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.05e-05 |
|    n_updates        | 41935    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.9     |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10248    |
|    fps              | 217      |
|    time_elapsed     | 956      |
|    total_timesteps  | 207950   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.25e-05 |
|    n_updates        | 41987    |
----------------------------------
Eval num_timesteps=208000, episode_reward=-0.12 +/- 0.05
Episode length: 29.42 +/- 13.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 29.4     |
|    mean_reward      | -0.117   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 208000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000139 |
|    n_updates        | 41999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.7     |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10252    |
|    fps              | 217      |
|    time_elapsed     | 957      |
|    total_timesteps  | 208127   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00658  |
|    n_updates        | 42031    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.5     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10256    |
|    fps              | 217      |
|    time_elapsed     | 957      |
|    total_timesteps  | 208272   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.5e-05  |
|    n_updates        | 42067    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.4     |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10260    |
|    fps              | 217      |
|    time_elapsed     | 958      |
|    total_timesteps  | 208358   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.46e-05 |
|    n_updates        | 42089    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38       |
|    ep_rew_mean      | -0.0814  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10264    |
|    fps              | 217      |
|    time_elapsed     | 958      |
|    total_timesteps  | 208428   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.54e-05 |
|    n_updates        | 42106    |
----------------------------------
Eval num_timesteps=208500, episode_reward=0.11 +/- 0.45
Episode length: 38.88 +/- 22.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.9     |
|    mean_reward      | 0.106    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 208500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.38e-05 |
|    n_updates        | 42124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.2     |
|    ep_rew_mean      | -0.0842  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10268    |
|    fps              | 217      |
|    time_elapsed     | 959      |
|    total_timesteps  | 208508   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000171 |
|    n_updates        | 42126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.9     |
|    ep_rew_mean      | -0.0748  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10272    |
|    fps              | 217      |
|    time_elapsed     | 959      |
|    total_timesteps  | 208576   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.13e-06 |
|    n_updates        | 42143    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.6     |
|    ep_rew_mean      | -0.0696  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10276    |
|    fps              | 217      |
|    time_elapsed     | 959      |
|    total_timesteps  | 208644   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.51e-05 |
|    n_updates        | 42160    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.9     |
|    ep_rew_mean      | -0.0627  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10280    |
|    fps              | 217      |
|    time_elapsed     | 959      |
|    total_timesteps  | 208718   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.94e-06 |
|    n_updates        | 42179    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.4     |
|    ep_rew_mean      | -0.0566  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10284    |
|    fps              | 217      |
|    time_elapsed     | 959      |
|    total_timesteps  | 208826   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.2e-06  |
|    n_updates        | 42206    |
----------------------------------
Eval num_timesteps=209000, episode_reward=-0.25 +/- 0.16
Episode length: 67.30 +/- 18.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.3     |
|    mean_reward      | -0.249   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 209000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.74e-05 |
|    n_updates        | 42249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.2     |
|    ep_rew_mean      | -0.06    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10288    |
|    fps              | 217      |
|    time_elapsed     | 962      |
|    total_timesteps  | 209068   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.69e-05 |
|    n_updates        | 42266    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.2     |
|    ep_rew_mean      | -0.064   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10292    |
|    fps              | 217      |
|    time_elapsed     | 962      |
|    total_timesteps  | 209355   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 42338    |
----------------------------------
Eval num_timesteps=209500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75       |
|    mean_reward      | -0.3     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 209500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0133   |
|    n_updates        | 42374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.4     |
|    ep_rew_mean      | -0.0788  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10296    |
|    fps              | 217      |
|    time_elapsed     | 965      |
|    total_timesteps  | 209628   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.15e-05 |
|    n_updates        | 42406    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.8     |
|    ep_rew_mean      | -0.0722  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10300    |
|    fps              | 217      |
|    time_elapsed     | 965      |
|    total_timesteps  | 209711   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.67e-05 |
|    n_updates        | 42427    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31       |
|    ep_rew_mean      | -0.073   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10304    |
|    fps              | 217      |
|    time_elapsed     | 965      |
|    total_timesteps  | 209809   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.06e-06 |
|    n_updates        | 42452    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.9     |
|    ep_rew_mean      | -0.0525  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10308    |
|    fps              | 217      |
|    time_elapsed     | 965      |
|    total_timesteps  | 209873   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.37e-05 |
|    n_updates        | 42468    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.6     |
|    ep_rew_mean      | -0.0415  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10312    |
|    fps              | 217      |
|    time_elapsed     | 965      |
|    total_timesteps  | 209950   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.23e-05 |
|    n_updates        | 42487    |
----------------------------------
Eval num_timesteps=210000, episode_reward=-0.27 +/- 0.05
Episode length: 67.98 +/- 12.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68       |
|    mean_reward      | -0.272   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.62e-05 |
|    n_updates        | 42499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.2     |
|    ep_rew_mean      | -0.044   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10316    |
|    fps              | 216      |
|    time_elapsed     | 968      |
|    total_timesteps  | 210094   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.83e-05 |
|    n_updates        | 42523    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32       |
|    ep_rew_mean      | -0.047   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10320    |
|    fps              | 217      |
|    time_elapsed     | 968      |
|    total_timesteps  | 210235   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00651  |
|    n_updates        | 42558    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.1     |
|    ep_rew_mean      | -0.0475  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10324    |
|    fps              | 217      |
|    time_elapsed     | 968      |
|    total_timesteps  | 210316   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.43e-05 |
|    n_updates        | 42578    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.1     |
|    ep_rew_mean      | -0.0475  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10328    |
|    fps              | 217      |
|    time_elapsed     | 968      |
|    total_timesteps  | 210383   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.48e-06 |
|    n_updates        | 42595    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32       |
|    ep_rew_mean      | -0.0471  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10332    |
|    fps              | 217      |
|    time_elapsed     | 968      |
|    total_timesteps  | 210474   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 42618    |
----------------------------------
Eval num_timesteps=210500, episode_reward=-0.13 +/- 0.15
Episode length: 36.66 +/- 17.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.7     |
|    mean_reward      | -0.126   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 210500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.19e-05 |
|    n_updates        | 42624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.1     |
|    ep_rew_mean      | -0.0393  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10336    |
|    fps              | 217      |
|    time_elapsed     | 970      |
|    total_timesteps  | 210580   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.2e-05  |
|    n_updates        | 42644    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31       |
|    ep_rew_mean      | -0.0433  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10340    |
|    fps              | 217      |
|    time_elapsed     | 970      |
|    total_timesteps  | 210760   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.66e-05 |
|    n_updates        | 42689    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.1     |
|    ep_rew_mean      | -0.0434  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10344    |
|    fps              | 217      |
|    time_elapsed     | 970      |
|    total_timesteps  | 210850   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.02e-06 |
|    n_updates        | 42712    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.7     |
|    ep_rew_mean      | -0.0278  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10348    |
|    fps              | 217      |
|    time_elapsed     | 970      |
|    total_timesteps  | 210918   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.68e-05 |
|    n_updates        | 42729    |
----------------------------------
Eval num_timesteps=211000, episode_reward=-0.17 +/- 0.07
Episode length: 43.80 +/- 18.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | -0.174   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 211000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.28e-05 |
|    n_updates        | 42749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.3     |
|    ep_rew_mean      | -0.0263  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10352    |
|    fps              | 217      |
|    time_elapsed     | 972      |
|    total_timesteps  | 211058   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.76e-05 |
|    n_updates        | 42764    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.6     |
|    ep_rew_mean      | -0.0233  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10356    |
|    fps              | 217      |
|    time_elapsed     | 972      |
|    total_timesteps  | 211129   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.75e-05 |
|    n_updates        | 42782    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.8     |
|    ep_rew_mean      | -0.0241  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10360    |
|    fps              | 217      |
|    time_elapsed     | 972      |
|    total_timesteps  | 211235   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000202 |
|    n_updates        | 42808    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.8     |
|    ep_rew_mean      | -0.0242  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10364    |
|    fps              | 217      |
|    time_elapsed     | 972      |
|    total_timesteps  | 211308   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.97e-05 |
|    n_updates        | 42826    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.6     |
|    ep_rew_mean      | -0.0136  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10368    |
|    fps              | 217      |
|    time_elapsed     | 972      |
|    total_timesteps  | 211373   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.8e-06  |
|    n_updates        | 42843    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.7     |
|    ep_rew_mean      | -0.0137  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10372    |
|    fps              | 217      |
|    time_elapsed     | 972      |
|    total_timesteps  | 211442   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.25e-05 |
|    n_updates        | 42860    |
----------------------------------
Eval num_timesteps=211500, episode_reward=0.07 +/- 0.35
Episode length: 16.76 +/- 0.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.8     |
|    mean_reward      | 0.0741   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 42874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.7     |
|    ep_rew_mean      | -0.0137  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10376    |
|    fps              | 217      |
|    time_elapsed     | 973      |
|    total_timesteps  | 211510   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.86e-05 |
|    n_updates        | 42877    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.9     |
|    ep_rew_mean      | -0.00451 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10380    |
|    fps              | 217      |
|    time_elapsed     | 973      |
|    total_timesteps  | 211605   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.91e-05 |
|    n_updates        | 42901    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.5     |
|    ep_rew_mean      | -0.00291 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10384    |
|    fps              | 217      |
|    time_elapsed     | 973      |
|    total_timesteps  | 211673   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.73e-05 |
|    n_updates        | 42918    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.8     |
|    ep_rew_mean      | 0.00376  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10388    |
|    fps              | 217      |
|    time_elapsed     | 973      |
|    total_timesteps  | 211749   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.82e-05 |
|    n_updates        | 42937    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25       |
|    ep_rew_mean      | 0.0112   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10392    |
|    fps              | 217      |
|    time_elapsed     | 973      |
|    total_timesteps  | 211851   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 42962    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 0.0194   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10396    |
|    fps              | 217      |
|    time_elapsed     | 973      |
|    total_timesteps  | 211919   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00019  |
|    n_updates        | 42979    |
----------------------------------
Eval num_timesteps=212000, episode_reward=-0.20 +/- 0.16
Episode length: 54.78 +/- 20.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.8     |
|    mean_reward      | -0.199   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 212000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.26e-05 |
|    n_updates        | 42999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.2     |
|    ep_rew_mean      | 0.0184   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10400    |
|    fps              | 217      |
|    time_elapsed     | 975      |
|    total_timesteps  | 212028   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.93e-05 |
|    n_updates        | 43006    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 0.0164   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10404    |
|    fps              | 217      |
|    time_elapsed     | 975      |
|    total_timesteps  | 212176   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.12e-06 |
|    n_updates        | 43043    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.7     |
|    ep_rew_mean      | 0.00606  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10408    |
|    fps              | 217      |
|    time_elapsed     | 975      |
|    total_timesteps  | 212247   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.97e-05 |
|    n_updates        | 43061    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.6     |
|    ep_rew_mean      | -0.00357 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10412    |
|    fps              | 217      |
|    time_elapsed     | 976      |
|    total_timesteps  | 212315   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.24e-05 |
|    n_updates        | 43078    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 0.0195   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10416    |
|    fps              | 217      |
|    time_elapsed     | 976      |
|    total_timesteps  | 212383   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.12e-05 |
|    n_updates        | 43095    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.2     |
|    ep_rew_mean      | 0.0124   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10420    |
|    fps              | 217      |
|    time_elapsed     | 976      |
|    total_timesteps  | 212452   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.82e-05 |
|    n_updates        | 43112    |
----------------------------------
Eval num_timesteps=212500, episode_reward=0.05 +/- 0.33
Episode length: 16.96 +/- 0.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17       |
|    mean_reward      | 0.0532   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 212500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.89e-05 |
|    n_updates        | 43124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 0.023    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10424    |
|    fps              | 217      |
|    time_elapsed     | 976      |
|    total_timesteps  | 212517   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.04e-06 |
|    n_updates        | 43129    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 0.0331   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10428    |
|    fps              | 217      |
|    time_elapsed     | 976      |
|    total_timesteps  | 212582   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.63e-06 |
|    n_updates        | 43145    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | 0.0339   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10432    |
|    fps              | 217      |
|    time_elapsed     | 977      |
|    total_timesteps  | 212653   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.47e-05 |
|    n_updates        | 43163    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.4     |
|    ep_rew_mean      | 0.0354   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10436    |
|    fps              | 217      |
|    time_elapsed     | 977      |
|    total_timesteps  | 212722   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 43180    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.4     |
|    ep_rew_mean      | 0.0397   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10440    |
|    fps              | 217      |
|    time_elapsed     | 977      |
|    total_timesteps  | 212795   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.21e-05 |
|    n_updates        | 43198    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.0305   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10444    |
|    fps              | 217      |
|    time_elapsed     | 977      |
|    total_timesteps  | 212864   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 43215    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.1     |
|    ep_rew_mean      | 0.0205   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10448    |
|    fps              | 217      |
|    time_elapsed     | 977      |
|    total_timesteps  | 212933   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 43233    |
----------------------------------
Eval num_timesteps=213000, episode_reward=-0.01 +/- 0.24
Episode length: 16.86 +/- 0.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | -0.00646 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 213000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000242 |
|    n_updates        | 43249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.4     |
|    ep_rew_mean      | 0.0233   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10452    |
|    fps              | 217      |
|    time_elapsed     | 978      |
|    total_timesteps  | 213003   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.3e-05  |
|    n_updates        | 43250    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.5     |
|    ep_rew_mean      | 0.033    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10456    |
|    fps              | 217      |
|    time_elapsed     | 978      |
|    total_timesteps  | 213081   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.32e-05 |
|    n_updates        | 43270    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0446   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10460    |
|    fps              | 217      |
|    time_elapsed     | 978      |
|    total_timesteps  | 213147   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 43286    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.1     |
|    ep_rew_mean      | 0.0449   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10464    |
|    fps              | 217      |
|    time_elapsed     | 978      |
|    total_timesteps  | 213213   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.41e-06 |
|    n_updates        | 43303    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.8     |
|    ep_rew_mean      | 0.028    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10468    |
|    fps              | 218      |
|    time_elapsed     | 978      |
|    total_timesteps  | 213449   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00019  |
|    n_updates        | 43362    |
----------------------------------
Eval num_timesteps=213500, episode_reward=0.03 +/- 0.30
Episode length: 16.82 +/- 1.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.8     |
|    mean_reward      | 0.0338   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00647  |
|    n_updates        | 43374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.8     |
|    ep_rew_mean      | 0.0277   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10472    |
|    fps              | 218      |
|    time_elapsed     | 979      |
|    total_timesteps  | 213525   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00645  |
|    n_updates        | 43381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.8     |
|    ep_rew_mean      | 0.0378   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10476    |
|    fps              | 218      |
|    time_elapsed     | 979      |
|    total_timesteps  | 213593   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000134 |
|    n_updates        | 43398    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.6     |
|    ep_rew_mean      | 0.0288   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10480    |
|    fps              | 218      |
|    time_elapsed     | 979      |
|    total_timesteps  | 213661   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.62e-05 |
|    n_updates        | 43415    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.6     |
|    ep_rew_mean      | 0.0286   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10484    |
|    fps              | 218      |
|    time_elapsed     | 979      |
|    total_timesteps  | 213736   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.93e-05 |
|    n_updates        | 43433    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | 0.0265   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10488    |
|    fps              | 218      |
|    time_elapsed     | 979      |
|    total_timesteps  | 213862   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00655  |
|    n_updates        | 43465    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.8     |
|    ep_rew_mean      | 0.0479   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10492    |
|    fps              | 218      |
|    time_elapsed     | 979      |
|    total_timesteps  | 213931   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.62e-05 |
|    n_updates        | 43482    |
----------------------------------
Eval num_timesteps=214000, episode_reward=0.09 +/- 0.37
Episode length: 17.14 +/- 1.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.1     |
|    mean_reward      | 0.0926   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 214000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.45e-05 |
|    n_updates        | 43499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.9     |
|    ep_rew_mean      | 0.0476   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10496    |
|    fps              | 218      |
|    time_elapsed     | 980      |
|    total_timesteps  | 214005   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 43501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.4     |
|    ep_rew_mean      | 0.0493   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10500    |
|    fps              | 218      |
|    time_elapsed     | 980      |
|    total_timesteps  | 214073   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0068   |
|    n_updates        | 43518    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0525   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10504    |
|    fps              | 218      |
|    time_elapsed     | 980      |
|    total_timesteps  | 214141   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00671  |
|    n_updates        | 43535    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.6     |
|    ep_rew_mean      | 0.0426   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10508    |
|    fps              | 218      |
|    time_elapsed     | 980      |
|    total_timesteps  | 214209   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.51e-06 |
|    n_updates        | 43552    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 19.7     |
|    ep_rew_mean      | 0.0522   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10512    |
|    fps              | 218      |
|    time_elapsed     | 980      |
|    total_timesteps  | 214287   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 43571    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | 0.0397   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10516    |
|    fps              | 218      |
|    time_elapsed     | 980      |
|    total_timesteps  | 214416   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00758  |
|    n_updates        | 43603    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | 0.0498   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10520    |
|    fps              | 218      |
|    time_elapsed     | 980      |
|    total_timesteps  | 214483   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.8e-05  |
|    n_updates        | 43620    |
----------------------------------
Eval num_timesteps=214500, episode_reward=-0.03 +/- 0.20
Episode length: 18.02 +/- 1.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18       |
|    mean_reward      | -0.031   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 214500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000194 |
|    n_updates        | 43624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.3     |
|    ep_rew_mean      | 0.0397   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10524    |
|    fps              | 218      |
|    time_elapsed     | 981      |
|    total_timesteps  | 214551   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.46e-06 |
|    n_updates        | 43637    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.9     |
|    ep_rew_mean      | 0.0176   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10528    |
|    fps              | 218      |
|    time_elapsed     | 981      |
|    total_timesteps  | 214670   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000195 |
|    n_updates        | 43667    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21       |
|    ep_rew_mean      | 0.0172   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10532    |
|    fps              | 218      |
|    time_elapsed     | 981      |
|    total_timesteps  | 214750   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.62e-05 |
|    n_updates        | 43687    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.5     |
|    ep_rew_mean      | 0.0249   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10536    |
|    fps              | 218      |
|    time_elapsed     | 982      |
|    total_timesteps  | 214876   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.84e-06 |
|    n_updates        | 43718    |
----------------------------------
Eval num_timesteps=215000, episode_reward=-0.05 +/- 0.14
Episode length: 17.40 +/- 1.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.4     |
|    mean_reward      | -0.0486  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 215000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.99e-05 |
|    n_updates        | 43749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 0.0212   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10540    |
|    fps              | 218      |
|    time_elapsed     | 982      |
|    total_timesteps  | 215041   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 43760    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 0.021    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10544    |
|    fps              | 218      |
|    time_elapsed     | 982      |
|    total_timesteps  | 215114   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 43778    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 0.0211   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10548    |
|    fps              | 218      |
|    time_elapsed     | 982      |
|    total_timesteps  | 215182   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000162 |
|    n_updates        | 43795    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 0.0312   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10552    |
|    fps              | 218      |
|    time_elapsed     | 983      |
|    total_timesteps  | 215249   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 43812    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0316   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10556    |
|    fps              | 219      |
|    time_elapsed     | 983      |
|    total_timesteps  | 215316   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000147 |
|    n_updates        | 43828    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0216   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10560    |
|    fps              | 219      |
|    time_elapsed     | 983      |
|    total_timesteps  | 215384   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.55e-05 |
|    n_updates        | 43845    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10564    |
|    fps              | 219      |
|    time_elapsed     | 983      |
|    total_timesteps  | 215452   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000187 |
|    n_updates        | 43862    |
----------------------------------
Eval num_timesteps=215500, episode_reward=-0.15 +/- 0.31
Episode length: 57.88 +/- 20.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.9     |
|    mean_reward      | -0.151   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 215500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.48e-05 |
|    n_updates        | 43874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.6     |
|    ep_rew_mean      | 0.0148   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10568    |
|    fps              | 218      |
|    time_elapsed     | 985      |
|    total_timesteps  | 215606   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.5e-05  |
|    n_updates        | 43901    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 0.0231   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10572    |
|    fps              | 218      |
|    time_elapsed     | 985      |
|    total_timesteps  | 215723   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000299 |
|    n_updates        | 43930    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 0.0104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10576    |
|    fps              | 218      |
|    time_elapsed     | 985      |
|    total_timesteps  | 215859   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000276 |
|    n_updates        | 43964    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 0.0104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10580    |
|    fps              | 219      |
|    time_elapsed     | 985      |
|    total_timesteps  | 215927   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 43981    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 0.0206   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10584    |
|    fps              | 219      |
|    time_elapsed     | 985      |
|    total_timesteps  | 215996   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.12e-06 |
|    n_updates        | 43998    |
----------------------------------
Eval num_timesteps=216000, episode_reward=0.03 +/- 0.30
Episode length: 16.78 +/- 0.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.8     |
|    mean_reward      | 0.0339   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00671  |
|    n_updates        | 43999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0314   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10588    |
|    fps              | 219      |
|    time_elapsed     | 986      |
|    total_timesteps  | 216103   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00663  |
|    n_updates        | 44025    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 0.0111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10592    |
|    fps              | 219      |
|    time_elapsed     | 986      |
|    total_timesteps  | 216180   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00023  |
|    n_updates        | 44044    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10596    |
|    fps              | 219      |
|    time_elapsed     | 986      |
|    total_timesteps  | 216248   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000219 |
|    n_updates        | 44061    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10600    |
|    fps              | 219      |
|    time_elapsed     | 986      |
|    total_timesteps  | 216316   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00653  |
|    n_updates        | 44078    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 0.0195   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10604    |
|    fps              | 219      |
|    time_elapsed     | 986      |
|    total_timesteps  | 216430   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.57e-05 |
|    n_updates        | 44107    |
----------------------------------
Eval num_timesteps=216500, episode_reward=-0.27 +/- 0.05
Episode length: 68.18 +/- 12.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.2     |
|    mean_reward      | -0.272   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 216500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.88e-05 |
|    n_updates        | 44124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23.1     |
|    ep_rew_mean      | 0.0185   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10608    |
|    fps              | 218      |
|    time_elapsed     | 989      |
|    total_timesteps  | 216523   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000176 |
|    n_updates        | 44130    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 23       |
|    ep_rew_mean      | 0.00887  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10612    |
|    fps              | 218      |
|    time_elapsed     | 989      |
|    total_timesteps  | 216591   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.22e-05 |
|    n_updates        | 44147    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.5     |
|    ep_rew_mean      | 0.0212   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10616    |
|    fps              | 218      |
|    time_elapsed     | 989      |
|    total_timesteps  | 216663   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00775  |
|    n_updates        | 44165    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0313   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10620    |
|    fps              | 219      |
|    time_elapsed     | 989      |
|    total_timesteps  | 216727   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.54e-05 |
|    n_updates        | 44181    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.4     |
|    ep_rew_mean      | 0.0313   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10624    |
|    fps              | 219      |
|    time_elapsed     | 989      |
|    total_timesteps  | 216795   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.26e-05 |
|    n_updates        | 44198    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.9     |
|    ep_rew_mean      | 0.0434   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10628    |
|    fps              | 219      |
|    time_elapsed     | 989      |
|    total_timesteps  | 216861   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.68e-05 |
|    n_updates        | 44215    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | 0.0439   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10632    |
|    fps              | 219      |
|    time_elapsed     | 989      |
|    total_timesteps  | 216929   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000199 |
|    n_updates        | 44232    |
----------------------------------
Eval num_timesteps=217000, episode_reward=0.16 +/- 0.46
Episode length: 35.60 +/- 19.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.6     |
|    mean_reward      | 0.159    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 217000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0152   |
|    n_updates        | 44249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.4     |
|    ep_rew_mean      | 0.0355   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10636    |
|    fps              | 218      |
|    time_elapsed     | 991      |
|    total_timesteps  | 217015   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.92e-06 |
|    n_updates        | 44253    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.4     |
|    ep_rew_mean      | 0.0395   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10640    |
|    fps              | 219      |
|    time_elapsed     | 991      |
|    total_timesteps  | 217081   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.58e-06 |
|    n_updates        | 44270    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | 0.0369   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10644    |
|    fps              | 219      |
|    time_elapsed     | 991      |
|    total_timesteps  | 217219   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00016  |
|    n_updates        | 44304    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.8     |
|    ep_rew_mean      | 0.034    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10648    |
|    fps              | 219      |
|    time_elapsed     | 991      |
|    total_timesteps  | 217359   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.71e-05 |
|    n_updates        | 44339    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 0.0231   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10652    |
|    fps              | 219      |
|    time_elapsed     | 991      |
|    total_timesteps  | 217449   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0066   |
|    n_updates        | 44362    |
----------------------------------
Eval num_timesteps=217500, episode_reward=0.07 +/- 0.35
Episode length: 18.16 +/- 3.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 18.2     |
|    mean_reward      | 0.0684   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00019  |
|    n_updates        | 44374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 0.023    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10656    |
|    fps              | 219      |
|    time_elapsed     | 992      |
|    total_timesteps  | 217516   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000179 |
|    n_updates        | 44378    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22       |
|    ep_rew_mean      | 0.023    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10660    |
|    fps              | 219      |
|    time_elapsed     | 992      |
|    total_timesteps  | 217584   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.31e-05 |
|    n_updates        | 44395    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.6     |
|    ep_rew_mean      | 0.0308   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10664    |
|    fps              | 219      |
|    time_elapsed     | 992      |
|    total_timesteps  | 217709   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00655  |
|    n_updates        | 44427    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.7     |
|    ep_rew_mean      | 0.0342   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10668    |
|    fps              | 219      |
|    time_elapsed     | 992      |
|    total_timesteps  | 217777   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00665  |
|    n_updates        | 44444    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.2     |
|    ep_rew_mean      | 0.0363   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10672    |
|    fps              | 219      |
|    time_elapsed     | 992      |
|    total_timesteps  | 217841   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.97e-05 |
|    n_updates        | 44460    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.5     |
|    ep_rew_mean      | 0.039    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10676    |
|    fps              | 219      |
|    time_elapsed     | 992      |
|    total_timesteps  | 217910   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00786  |
|    n_updates        | 44477    |
----------------------------------
Eval num_timesteps=218000, episode_reward=0.03 +/- 0.38
Episode length: 31.52 +/- 17.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 31.5     |
|    mean_reward      | 0.0349   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 218000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00757  |
|    n_updates        | 44499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | 0.0364   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10680    |
|    fps              | 219      |
|    time_elapsed     | 993      |
|    total_timesteps  | 218042   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000283 |
|    n_updates        | 44510    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 21.1     |
|    ep_rew_mean      | 0.0265   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10684    |
|    fps              | 219      |
|    time_elapsed     | 994      |
|    total_timesteps  | 218110   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00638  |
|    n_updates        | 44527    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 20.7     |
|    ep_rew_mean      | 0.0281   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10688    |
|    fps              | 219      |
|    time_elapsed     | 994      |
|    total_timesteps  | 218177   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.6e-05  |
|    n_updates        | 44544    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.3     |
|    ep_rew_mean      | 0.0318   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10692    |
|    fps              | 219      |
|    time_elapsed     | 994      |
|    total_timesteps  | 218409   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00753  |
|    n_updates        | 44602    |
----------------------------------
Eval num_timesteps=218500, episode_reward=0.00 +/- 0.39
Episode length: 44.36 +/- 22.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 0.00352  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 218500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0203   |
|    n_updates        | 44624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 0.0402   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10696    |
|    fps              | 219      |
|    time_elapsed     | 995      |
|    total_timesteps  | 218518   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 44629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.7     |
|    ep_rew_mean      | 0.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10700    |
|    fps              | 219      |
|    time_elapsed     | 996      |
|    total_timesteps  | 218590   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000147 |
|    n_updates        | 44647    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 22.9     |
|    ep_rew_mean      | 0.0392   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10704    |
|    fps              | 219      |
|    time_elapsed     | 996      |
|    total_timesteps  | 218725   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 44681    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.1     |
|    ep_rew_mean      | 0.0347   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10708    |
|    fps              | 219      |
|    time_elapsed     | 996      |
|    total_timesteps  | 218931   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000182 |
|    n_updates        | 44732    |
----------------------------------
Eval num_timesteps=219000, episode_reward=0.08 +/- 0.44
Episode length: 45.06 +/- 23.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 0.0806   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 219000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000249 |
|    n_updates        | 44749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 24.5     |
|    ep_rew_mean      | 0.0429   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10712    |
|    fps              | 219      |
|    time_elapsed     | 998      |
|    total_timesteps  | 219044   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.02e-05 |
|    n_updates        | 44760    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | 0.0185   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10716    |
|    fps              | 219      |
|    time_elapsed     | 998      |
|    total_timesteps  | 219224   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.54e-05 |
|    n_updates        | 44805    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 25.6     |
|    ep_rew_mean      | 0.0184   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10720    |
|    fps              | 219      |
|    time_elapsed     | 998      |
|    total_timesteps  | 219290   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.12e-06 |
|    n_updates        | 44822    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 26.9     |
|    ep_rew_mean      | 0.0135   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10724    |
|    fps              | 219      |
|    time_elapsed     | 998      |
|    total_timesteps  | 219480   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.92e-05 |
|    n_updates        | 44869    |
----------------------------------
Eval num_timesteps=219500, episode_reward=-0.01 +/- 0.28
Episode length: 22.08 +/- 14.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 22.1     |
|    mean_reward      | -0.00734 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 219500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00728  |
|    n_updates        | 44874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 28.1     |
|    ep_rew_mean      | -0.00127 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10728    |
|    fps              | 219      |
|    time_elapsed     | 999      |
|    total_timesteps  | 219666   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 44916    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 29.8     |
|    ep_rew_mean      | -0.00841 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10732    |
|    fps              | 219      |
|    time_elapsed     | 999      |
|    total_timesteps  | 219912   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.23e-05 |
|    n_updates        | 44977    |
----------------------------------
Eval num_timesteps=220000, episode_reward=0.14 +/- 0.51
Episode length: 44.86 +/- 24.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 0.141    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00678  |
|    n_updates        | 44999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.8     |
|    ep_rew_mean      | 0.00787  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10736    |
|    fps              | 219      |
|    time_elapsed     | 1001     |
|    total_timesteps  | 220091   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 45022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.1     |
|    ep_rew_mean      | -0.00351 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10740    |
|    fps              | 219      |
|    time_elapsed     | 1001     |
|    total_timesteps  | 220191   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0078   |
|    n_updates        | 45047    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.9     |
|    ep_rew_mean      | -0.00263 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10744    |
|    fps              | 219      |
|    time_elapsed     | 1001     |
|    total_timesteps  | 220307   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000281 |
|    n_updates        | 45076    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.7     |
|    ep_rew_mean      | -0.00207 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10748    |
|    fps              | 220      |
|    time_elapsed     | 1001     |
|    total_timesteps  | 220433   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00643  |
|    n_updates        | 45108    |
----------------------------------
Eval num_timesteps=220500, episode_reward=0.03 +/- 0.37
Episode length: 37.74 +/- 18.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.7     |
|    mean_reward      | 0.0301   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 220500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00024  |
|    n_updates        | 45124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.7     |
|    ep_rew_mean      | -0.00183 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10752    |
|    fps              | 219      |
|    time_elapsed     | 1003     |
|    total_timesteps  | 220517   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.29e-05 |
|    n_updates        | 45129    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.9     |
|    ep_rew_mean      | 0.00742  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10756    |
|    fps              | 219      |
|    time_elapsed     | 1003     |
|    total_timesteps  | 220603   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.7e-05  |
|    n_updates        | 45150    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | 0.0135   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10760    |
|    fps              | 219      |
|    time_elapsed     | 1003     |
|    total_timesteps  | 220770   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.79e-05 |
|    n_updates        | 45192    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.7     |
|    ep_rew_mean      | 0.00402  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10764    |
|    fps              | 220      |
|    time_elapsed     | 1003     |
|    total_timesteps  | 220881   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000236 |
|    n_updates        | 45220    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.7     |
|    ep_rew_mean      | 0.0041   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10768    |
|    fps              | 220      |
|    time_elapsed     | 1003     |
|    total_timesteps  | 220947   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00773  |
|    n_updates        | 45236    |
----------------------------------
Eval num_timesteps=221000, episode_reward=-0.03 +/- 0.20
Episode length: 17.10 +/- 1.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 17.1     |
|    mean_reward      | -0.0274  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 221000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.15e-05 |
|    n_updates        | 45249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.7     |
|    ep_rew_mean      | -0.00605 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10772    |
|    fps              | 220      |
|    time_elapsed     | 1004     |
|    total_timesteps  | 221015   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000211 |
|    n_updates        | 45253    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.7     |
|    ep_rew_mean      | 0.00411  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10776    |
|    fps              | 220      |
|    time_elapsed     | 1004     |
|    total_timesteps  | 221080   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.69e-05 |
|    n_updates        | 45269    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32       |
|    ep_rew_mean      | 0.00299  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10780    |
|    fps              | 220      |
|    time_elapsed     | 1004     |
|    total_timesteps  | 221240   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00614  |
|    n_updates        | 45309    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.2     |
|    ep_rew_mean      | 0.0122   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10784    |
|    fps              | 220      |
|    time_elapsed     | 1004     |
|    total_timesteps  | 221327   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.96e-05 |
|    n_updates        | 45331    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.2     |
|    ep_rew_mean      | 0.00195  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10788    |
|    fps              | 220      |
|    time_elapsed     | 1004     |
|    total_timesteps  | 221401   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0138   |
|    n_updates        | 45350    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 30.6     |
|    ep_rew_mean      | -0.00146 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10792    |
|    fps              | 220      |
|    time_elapsed     | 1004     |
|    total_timesteps  | 221469   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000225 |
|    n_updates        | 45367    |
----------------------------------
Eval num_timesteps=221500, episode_reward=-0.12 +/- 0.21
Episode length: 40.90 +/- 23.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.9     |
|    mean_reward      | -0.123   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 221500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.94e-05 |
|    n_updates        | 45374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | -0.0168  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10796    |
|    fps              | 220      |
|    time_elapsed     | 1006     |
|    total_timesteps  | 221711   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 45427    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.9     |
|    ep_rew_mean      | -0.0169  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10800    |
|    fps              | 220      |
|    time_elapsed     | 1006     |
|    total_timesteps  | 221785   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 45446    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.1     |
|    ep_rew_mean      | -0.0274  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10804    |
|    fps              | 220      |
|    time_elapsed     | 1006     |
|    total_timesteps  | 221933   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.17e-05 |
|    n_updates        | 45483    |
----------------------------------
Eval num_timesteps=222000, episode_reward=-0.17 +/- 0.19
Episode length: 48.20 +/- 26.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.2     |
|    mean_reward      | -0.172   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 222000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00674  |
|    n_updates        | 45499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.6     |
|    ep_rew_mean      | -0.0155  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10808    |
|    fps              | 220      |
|    time_elapsed     | 1008     |
|    total_timesteps  | 222091   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000136 |
|    n_updates        | 45522    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.6     |
|    ep_rew_mean      | -0.0254  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10812    |
|    fps              | 220      |
|    time_elapsed     | 1008     |
|    total_timesteps  | 222203   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000193 |
|    n_updates        | 45550    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.6     |
|    ep_rew_mean      | -0.0256  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10816    |
|    fps              | 220      |
|    time_elapsed     | 1009     |
|    total_timesteps  | 222387   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000161 |
|    n_updates        | 45596    |
----------------------------------
Eval num_timesteps=222500, episode_reward=-0.23 +/- 0.10
Episode length: 56.90 +/- 25.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.9     |
|    mean_reward      | -0.227   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 222500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.75e-05 |
|    n_updates        | 45624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.5     |
|    ep_rew_mean      | -0.0531  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10820    |
|    fps              | 220      |
|    time_elapsed     | 1011     |
|    total_timesteps  | 222641   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.71e-05 |
|    n_updates        | 45660    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.3     |
|    ep_rew_mean      | -0.0482  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10824    |
|    fps              | 220      |
|    time_elapsed     | 1011     |
|    total_timesteps  | 222709   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000297 |
|    n_updates        | 45677    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.8     |
|    ep_rew_mean      | -0.0362  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10828    |
|    fps              | 220      |
|    time_elapsed     | 1011     |
|    total_timesteps  | 222843   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 45710    |
----------------------------------
Eval num_timesteps=223000, episode_reward=-0.05 +/- 0.36
Episode length: 43.38 +/- 24.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | -0.0528  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 223000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000273 |
|    n_updates        | 45749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.8     |
|    ep_rew_mean      | -0.0362  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10832    |
|    fps              | 220      |
|    time_elapsed     | 1013     |
|    total_timesteps  | 223089   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 45772    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.4     |
|    ep_rew_mean      | -0.0548  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10836    |
|    fps              | 220      |
|    time_elapsed     | 1013     |
|    total_timesteps  | 223234   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000263 |
|    n_updates        | 45808    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 31.5     |
|    ep_rew_mean      | -0.045   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10840    |
|    fps              | 220      |
|    time_elapsed     | 1013     |
|    total_timesteps  | 223340   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00018  |
|    n_updates        | 45834    |
----------------------------------
Eval num_timesteps=223500, episode_reward=-0.17 +/- 0.19
Episode length: 48.28 +/- 25.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | -0.173   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 223500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.05e-05 |
|    n_updates        | 45874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.8     |
|    ep_rew_mean      | -0.0503  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10844    |
|    fps              | 220      |
|    time_elapsed     | 1015     |
|    total_timesteps  | 223587   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 45896    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.5     |
|    ep_rew_mean      | -0.0492  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10848    |
|    fps              | 220      |
|    time_elapsed     | 1015     |
|    total_timesteps  | 223685   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000154 |
|    n_updates        | 45921    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 32.4     |
|    ep_rew_mean      | -0.0488  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10852    |
|    fps              | 220      |
|    time_elapsed     | 1015     |
|    total_timesteps  | 223760   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 45939    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 33.8     |
|    ep_rew_mean      | -0.0541  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10856    |
|    fps              | 220      |
|    time_elapsed     | 1015     |
|    total_timesteps  | 223979   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00585  |
|    n_updates        | 45994    |
----------------------------------
Eval num_timesteps=224000, episode_reward=-0.09 +/- 0.34
Episode length: 48.84 +/- 25.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.8     |
|    mean_reward      | -0.0945  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 224000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000243 |
|    n_updates        | 45999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.4     |
|    ep_rew_mean      | -0.0666  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10860    |
|    fps              | 220      |
|    time_elapsed     | 1017     |
|    total_timesteps  | 224207   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00704  |
|    n_updates        | 46051    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 34.6     |
|    ep_rew_mean      | -0.0674  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10864    |
|    fps              | 220      |
|    time_elapsed     | 1017     |
|    total_timesteps  | 224337   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.44e-05 |
|    n_updates        | 46084    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.4     |
|    ep_rew_mean      | -0.0607  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10868    |
|    fps              | 220      |
|    time_elapsed     | 1017     |
|    total_timesteps  | 224487   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.31e-05 |
|    n_updates        | 46121    |
----------------------------------
Eval num_timesteps=224500, episode_reward=-0.10 +/- 0.30
Episode length: 44.76 +/- 23.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | -0.0982  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 224500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000186 |
|    n_updates        | 46124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.7     |
|    ep_rew_mean      | -0.0558  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10872    |
|    fps              | 220      |
|    time_elapsed     | 1019     |
|    total_timesteps  | 224682   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000134 |
|    n_updates        | 46170    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.5     |
|    ep_rew_mean      | -0.0693  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10876    |
|    fps              | 220      |
|    time_elapsed     | 1019     |
|    total_timesteps  | 224835   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.19e-05 |
|    n_updates        | 46208    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.5     |
|    ep_rew_mean      | -0.0691  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10880    |
|    fps              | 220      |
|    time_elapsed     | 1020     |
|    total_timesteps  | 224989   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00763  |
|    n_updates        | 46247    |
----------------------------------
Eval num_timesteps=225000, episode_reward=-0.05 +/- 0.35
Episode length: 41.64 +/- 22.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | -0.0457  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 46249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.5     |
|    ep_rew_mean      | -0.0731  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10884    |
|    fps              | 220      |
|    time_elapsed     | 1021     |
|    total_timesteps  | 225176   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 46293    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.6     |
|    ep_rew_mean      | -0.0776  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10888    |
|    fps              | 220      |
|    time_elapsed     | 1021     |
|    total_timesteps  | 225362   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000226 |
|    n_updates        | 46340    |
----------------------------------
Eval num_timesteps=225500, episode_reward=-0.11 +/- 0.28
Episode length: 46.58 +/- 25.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.6     |
|    mean_reward      | -0.106   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 225500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00587  |
|    n_updates        | 46374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.8     |
|    ep_rew_mean      | -0.0822  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10892    |
|    fps              | 220      |
|    time_elapsed     | 1023     |
|    total_timesteps  | 225545   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000241 |
|    n_updates        | 46386    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.8     |
|    ep_rew_mean      | -0.0725  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10896    |
|    fps              | 220      |
|    time_elapsed     | 1023     |
|    total_timesteps  | 225795   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0121   |
|    n_updates        | 46448    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.3     |
|    ep_rew_mean      | -0.0644  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10900    |
|    fps              | 220      |
|    time_elapsed     | 1024     |
|    total_timesteps  | 225916   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00712  |
|    n_updates        | 46478    |
----------------------------------
Eval num_timesteps=226000, episode_reward=-0.06 +/- 0.35
Episode length: 45.62 +/- 22.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | -0.0618  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 226000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.77e-05 |
|    n_updates        | 46499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.7     |
|    ep_rew_mean      | -0.0658  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10904    |
|    fps              | 220      |
|    time_elapsed     | 1025     |
|    total_timesteps  | 226099   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000257 |
|    n_updates        | 46524    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.9     |
|    ep_rew_mean      | -0.0768  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10908    |
|    fps              | 220      |
|    time_elapsed     | 1026     |
|    total_timesteps  | 226281   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00748  |
|    n_updates        | 46570    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.3     |
|    ep_rew_mean      | -0.0784  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10912    |
|    fps              | 220      |
|    time_elapsed     | 1026     |
|    total_timesteps  | 226433   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 46608    |
----------------------------------
Eval num_timesteps=226500, episode_reward=-0.10 +/- 0.30
Episode length: 45.54 +/- 24.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | -0.102   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 226500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00523  |
|    n_updates        | 46624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.2     |
|    ep_rew_mean      | -0.0678  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10916    |
|    fps              | 220      |
|    time_elapsed     | 1027     |
|    total_timesteps  | 226603   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000196 |
|    n_updates        | 46650    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.8     |
|    ep_rew_mean      | -0.0563  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10920    |
|    fps              | 220      |
|    time_elapsed     | 1028     |
|    total_timesteps  | 226821   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00754  |
|    n_updates        | 46705    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.6     |
|    ep_rew_mean      | -0.0596  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10924    |
|    fps              | 220      |
|    time_elapsed     | 1028     |
|    total_timesteps  | 226969   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000167 |
|    n_updates        | 46742    |
----------------------------------
Eval num_timesteps=227000, episode_reward=-0.25 +/- 0.10
Episode length: 62.22 +/- 24.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.2     |
|    mean_reward      | -0.249   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 227000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000284 |
|    n_updates        | 46749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.8     |
|    ep_rew_mean      | -0.0642  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10928    |
|    fps              | 220      |
|    time_elapsed     | 1030     |
|    total_timesteps  | 227219   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0075   |
|    n_updates        | 46804    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.4     |
|    ep_rew_mean      | -0.0627  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10932    |
|    fps              | 220      |
|    time_elapsed     | 1030     |
|    total_timesteps  | 227427   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00718  |
|    n_updates        | 46856    |
----------------------------------
Eval num_timesteps=227500, episode_reward=0.01 +/- 0.44
Episode length: 46.80 +/- 25.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | 0.0135   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 227500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00536  |
|    n_updates        | 46874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.4     |
|    ep_rew_mean      | -0.0627  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10936    |
|    fps              | 220      |
|    time_elapsed     | 1032     |
|    total_timesteps  | 227573   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00559  |
|    n_updates        | 46893    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.4     |
|    ep_rew_mean      | -0.0767  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10940    |
|    fps              | 220      |
|    time_elapsed     | 1032     |
|    total_timesteps  | 227778   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.64e-05 |
|    n_updates        | 46944    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.9     |
|    ep_rew_mean      | -0.0745  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10944    |
|    fps              | 220      |
|    time_elapsed     | 1033     |
|    total_timesteps  | 227972   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000325 |
|    n_updates        | 46992    |
----------------------------------
Eval num_timesteps=228000, episode_reward=-0.09 +/- 0.28
Episode length: 42.42 +/- 23.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | -0.0887  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 228000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000246 |
|    n_updates        | 46999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.7     |
|    ep_rew_mean      | -0.0781  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10948    |
|    fps              | 220      |
|    time_elapsed     | 1034     |
|    total_timesteps  | 228159   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.92e-05 |
|    n_updates        | 47039    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46       |
|    ep_rew_mean      | -0.0833  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10952    |
|    fps              | 220      |
|    time_elapsed     | 1034     |
|    total_timesteps  | 228363   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 47090    |
----------------------------------
Eval num_timesteps=228500, episode_reward=-0.10 +/- 0.31
Episode length: 45.90 +/- 24.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | -0.103   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 228500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00656  |
|    n_updates        | 47124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.2     |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10956    |
|    fps              | 220      |
|    time_elapsed     | 1036     |
|    total_timesteps  | 228602   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000156 |
|    n_updates        | 47150    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.6     |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10960    |
|    fps              | 220      |
|    time_elapsed     | 1036     |
|    total_timesteps  | 228770   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 47192    |
----------------------------------
Eval num_timesteps=229000, episode_reward=-0.13 +/- 0.27
Episode length: 46.78 +/- 24.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | -0.126   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 229000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.3e-05  |
|    n_updates        | 47249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.8     |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10964    |
|    fps              | 220      |
|    time_elapsed     | 1038     |
|    total_timesteps  | 229012   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.05e-05 |
|    n_updates        | 47252    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.5     |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10968    |
|    fps              | 220      |
|    time_elapsed     | 1038     |
|    total_timesteps  | 229138   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000187 |
|    n_updates        | 47284    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46       |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10972    |
|    fps              | 220      |
|    time_elapsed     | 1039     |
|    total_timesteps  | 229284   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.05e-05 |
|    n_updates        | 47320    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.4     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10976    |
|    fps              | 220      |
|    time_elapsed     | 1039     |
|    total_timesteps  | 229378   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000172 |
|    n_updates        | 47344    |
----------------------------------
Eval num_timesteps=229500, episode_reward=-0.05 +/- 0.39
Episode length: 47.56 +/- 24.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | -0.0496  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 229500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.1e-05  |
|    n_updates        | 47374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.4     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10980    |
|    fps              | 220      |
|    time_elapsed     | 1040     |
|    total_timesteps  | 229530   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.39e-05 |
|    n_updates        | 47382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45       |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10984    |
|    fps              | 220      |
|    time_elapsed     | 1041     |
|    total_timesteps  | 229671   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.92e-05 |
|    n_updates        | 47417    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.9     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10988    |
|    fps              | 220      |
|    time_elapsed     | 1041     |
|    total_timesteps  | 229953   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000367 |
|    n_updates        | 47488    |
----------------------------------
Eval num_timesteps=230000, episode_reward=-0.09 +/- 0.29
Episode length: 43.30 +/- 24.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | -0.0925  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 47499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.4     |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10992    |
|    fps              | 220      |
|    time_elapsed     | 1043     |
|    total_timesteps  | 230089   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00767  |
|    n_updates        | 47522    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.7     |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 10996    |
|    fps              | 220      |
|    time_elapsed     | 1043     |
|    total_timesteps  | 230162   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 47540    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.8     |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11000    |
|    fps              | 220      |
|    time_elapsed     | 1043     |
|    total_timesteps  | 230296   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0001   |
|    n_updates        | 47573    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.3     |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11004    |
|    fps              | 220      |
|    time_elapsed     | 1043     |
|    total_timesteps  | 230427   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 47606    |
----------------------------------
Eval num_timesteps=230500, episode_reward=-0.04 +/- 0.35
Episode length: 39.80 +/- 23.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.8     |
|    mean_reward      | -0.0383  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 230500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.09e-05 |
|    n_updates        | 47624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.9     |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11008    |
|    fps              | 220      |
|    time_elapsed     | 1045     |
|    total_timesteps  | 230669   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.28e-05 |
|    n_updates        | 47667    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.6     |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11012    |
|    fps              | 220      |
|    time_elapsed     | 1045     |
|    total_timesteps  | 230789   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.52e-05 |
|    n_updates        | 47697    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.9     |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11016    |
|    fps              | 220      |
|    time_elapsed     | 1045     |
|    total_timesteps  | 230991   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 47747    |
----------------------------------
Eval num_timesteps=231000, episode_reward=-0.09 +/- 0.34
Episode length: 47.92 +/- 25.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.9     |
|    mean_reward      | -0.091   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 231000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 47749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.9     |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11020    |
|    fps              | 220      |
|    time_elapsed     | 1047     |
|    total_timesteps  | 231210   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.12e-05 |
|    n_updates        | 47802    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.5     |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11024    |
|    fps              | 220      |
|    time_elapsed     | 1047     |
|    total_timesteps  | 231314   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.44e-05 |
|    n_updates        | 47828    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.8     |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11028    |
|    fps              | 220      |
|    time_elapsed     | 1047     |
|    total_timesteps  | 231496   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00631  |
|    n_updates        | 47873    |
----------------------------------
Eval num_timesteps=231500, episode_reward=-0.05 +/- 0.35
Episode length: 41.50 +/- 25.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | -0.0452  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 231500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000136 |
|    n_updates        | 47874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.5     |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11032    |
|    fps              | 220      |
|    time_elapsed     | 1049     |
|    total_timesteps  | 231678   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.85e-05 |
|    n_updates        | 47919    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.6     |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11036    |
|    fps              | 220      |
|    time_elapsed     | 1049     |
|    total_timesteps  | 231834   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000265 |
|    n_updates        | 47958    |
----------------------------------
Eval num_timesteps=232000, episode_reward=-0.17 +/- 0.26
Episode length: 57.76 +/- 22.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.8     |
|    mean_reward      | -0.171   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.88e-05 |
|    n_updates        | 47999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.9     |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11040    |
|    fps              | 220      |
|    time_elapsed     | 1051     |
|    total_timesteps  | 232070   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000155 |
|    n_updates        | 48017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.8     |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11044    |
|    fps              | 220      |
|    time_elapsed     | 1051     |
|    total_timesteps  | 232347   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000139 |
|    n_updates        | 48086    |
----------------------------------
Eval num_timesteps=232500, episode_reward=-0.10 +/- 0.32
Episode length: 50.64 +/- 22.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.6     |
|    mean_reward      | -0.102   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 232500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 48124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.3     |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11048    |
|    fps              | 220      |
|    time_elapsed     | 1053     |
|    total_timesteps  | 232592   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.89e-05 |
|    n_updates        | 48147    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44       |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11052    |
|    fps              | 220      |
|    time_elapsed     | 1053     |
|    total_timesteps  | 232766   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.86e-05 |
|    n_updates        | 48191    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.9     |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11056    |
|    fps              | 221      |
|    time_elapsed     | 1054     |
|    total_timesteps  | 232992   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.25e-05 |
|    n_updates        | 48247    |
----------------------------------
Eval num_timesteps=233000, episode_reward=-0.06 +/- 0.38
Episode length: 50.22 +/- 24.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.2     |
|    mean_reward      | -0.0602  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 233000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.41e-05 |
|    n_updates        | 48249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.6     |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11060    |
|    fps              | 220      |
|    time_elapsed     | 1056     |
|    total_timesteps  | 233235   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.81e-05 |
|    n_updates        | 48308    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44       |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11064    |
|    fps              | 220      |
|    time_elapsed     | 1056     |
|    total_timesteps  | 233410   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000306 |
|    n_updates        | 48352    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.4     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11068    |
|    fps              | 221      |
|    time_elapsed     | 1056     |
|    total_timesteps  | 233479   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00572  |
|    n_updates        | 48369    |
----------------------------------
Eval num_timesteps=233500, episode_reward=-0.00 +/- 0.28
Episode length: 20.42 +/- 11.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.4     |
|    mean_reward      | -0.00064 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 233500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000173 |
|    n_updates        | 48374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.8     |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11072    |
|    fps              | 220      |
|    time_elapsed     | 1057     |
|    total_timesteps  | 233559   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000134 |
|    n_updates        | 48389    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.2     |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11076    |
|    fps              | 221      |
|    time_elapsed     | 1057     |
|    total_timesteps  | 233696   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 48423    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.4     |
|    ep_rew_mean      | -0.079   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11080    |
|    fps              | 221      |
|    time_elapsed     | 1057     |
|    total_timesteps  | 233774   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000155 |
|    n_updates        | 48443    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42       |
|    ep_rew_mean      | -0.077   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11084    |
|    fps              | 221      |
|    time_elapsed     | 1057     |
|    total_timesteps  | 233867   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.71e-05 |
|    n_updates        | 48466    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.1     |
|    ep_rew_mean      | -0.0695  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11088    |
|    fps              | 221      |
|    time_elapsed     | 1057     |
|    total_timesteps  | 233961   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.95e-05 |
|    n_updates        | 48490    |
----------------------------------
Eval num_timesteps=234000, episode_reward=0.05 +/- 0.33
Episode length: 16.62 +/- 0.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.6     |
|    mean_reward      | 0.0546   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000195 |
|    n_updates        | 48499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.5     |
|    ep_rew_mean      | -0.0671  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11092    |
|    fps              | 221      |
|    time_elapsed     | 1058     |
|    total_timesteps  | 234038   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00577  |
|    n_updates        | 48509    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.7     |
|    ep_rew_mean      | -0.0679  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11096    |
|    fps              | 221      |
|    time_elapsed     | 1058     |
|    total_timesteps  | 234129   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.84e-05 |
|    n_updates        | 48532    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.9     |
|    ep_rew_mean      | -0.0626  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11100    |
|    fps              | 221      |
|    time_elapsed     | 1058     |
|    total_timesteps  | 234382   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 48595    |
----------------------------------
Eval num_timesteps=234500, episode_reward=-0.07 +/- 0.32
Episode length: 43.32 +/- 23.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | -0.0725  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 234500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.86e-05 |
|    n_updates        | 48624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.5     |
|    ep_rew_mean      | -0.065   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11104    |
|    fps              | 221      |
|    time_elapsed     | 1060     |
|    total_timesteps  | 234572   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000126 |
|    n_updates        | 48642    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.5     |
|    ep_rew_mean      | -0.0652  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11108    |
|    fps              | 221      |
|    time_elapsed     | 1060     |
|    total_timesteps  | 234818   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00616  |
|    n_updates        | 48704    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41       |
|    ep_rew_mean      | -0.053   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11112    |
|    fps              | 221      |
|    time_elapsed     | 1060     |
|    total_timesteps  | 234885   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000285 |
|    n_updates        | 48721    |
----------------------------------
Eval num_timesteps=235000, episode_reward=-0.18 +/- 0.17
Episode length: 51.16 +/- 24.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | -0.184   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 235000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.92e-05 |
|    n_updates        | 48749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.2     |
|    ep_rew_mean      | -0.05    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11116    |
|    fps              | 221      |
|    time_elapsed     | 1062     |
|    total_timesteps  | 235013   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.12e-05 |
|    n_updates        | 48753    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.8     |
|    ep_rew_mean      | -0.0382  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11120    |
|    fps              | 221      |
|    time_elapsed     | 1062     |
|    total_timesteps  | 235186   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 48796    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.6     |
|    ep_rew_mean      | -0.0418  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11124    |
|    fps              | 221      |
|    time_elapsed     | 1063     |
|    total_timesteps  | 235378   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00016  |
|    n_updates        | 48844    |
----------------------------------
Eval num_timesteps=235500, episode_reward=-0.14 +/- 0.21
Episode length: 46.42 +/- 23.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | -0.145   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 235500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.85e-05 |
|    n_updates        | 48874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.7     |
|    ep_rew_mean      | -0.052   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11128    |
|    fps              | 221      |
|    time_elapsed     | 1064     |
|    total_timesteps  | 235565   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 48891    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.5     |
|    ep_rew_mean      | -0.051   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11132    |
|    fps              | 221      |
|    time_elapsed     | 1065     |
|    total_timesteps  | 235724   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 48930    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.2     |
|    ep_rew_mean      | -0.0599  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11136    |
|    fps              | 221      |
|    time_elapsed     | 1065     |
|    total_timesteps  | 235853   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.74e-05 |
|    n_updates        | 48963    |
----------------------------------
Eval num_timesteps=236000, episode_reward=-0.16 +/- 0.27
Episode length: 54.12 +/- 24.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.1     |
|    mean_reward      | -0.156   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000189 |
|    n_updates        | 48999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.3     |
|    ep_rew_mean      | -0.0605  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11140    |
|    fps              | 221      |
|    time_elapsed     | 1067     |
|    total_timesteps  | 236104   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00772  |
|    n_updates        | 49025    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.7     |
|    ep_rew_mean      | -0.068   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11144    |
|    fps              | 221      |
|    time_elapsed     | 1067     |
|    total_timesteps  | 236317   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00024  |
|    n_updates        | 49079    |
----------------------------------
Eval num_timesteps=236500, episode_reward=-0.11 +/- 0.27
Episode length: 43.50 +/- 23.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | -0.113   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 236500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.68e-05 |
|    n_updates        | 49124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.6     |
|    ep_rew_mean      | -0.0676  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11148    |
|    fps              | 221      |
|    time_elapsed     | 1069     |
|    total_timesteps  | 236552   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000134 |
|    n_updates        | 49137    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.1     |
|    ep_rew_mean      | -0.0658  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11152    |
|    fps              | 221      |
|    time_elapsed     | 1069     |
|    total_timesteps  | 236681   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00768  |
|    n_updates        | 49170    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.5     |
|    ep_rew_mean      | -0.0673  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11156    |
|    fps              | 221      |
|    time_elapsed     | 1069     |
|    total_timesteps  | 236944   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00712  |
|    n_updates        | 49235    |
----------------------------------
Eval num_timesteps=237000, episode_reward=-0.14 +/- 0.24
Episode length: 45.58 +/- 25.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | -0.142   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 237000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00572  |
|    n_updates        | 49249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.1     |
|    ep_rew_mean      | -0.0594  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11160    |
|    fps              | 221      |
|    time_elapsed     | 1071     |
|    total_timesteps  | 237242   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00017  |
|    n_updates        | 49310    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.8     |
|    ep_rew_mean      | -0.0624  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11164    |
|    fps              | 221      |
|    time_elapsed     | 1071     |
|    total_timesteps  | 237489   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000157 |
|    n_updates        | 49372    |
----------------------------------
Eval num_timesteps=237500, episode_reward=-0.17 +/- 0.27
Episode length: 57.60 +/- 21.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.6     |
|    mean_reward      | -0.17    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 237500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 49374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.8     |
|    ep_rew_mean      | -0.0663  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11168    |
|    fps              | 221      |
|    time_elapsed     | 1073     |
|    total_timesteps  | 237657   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.26e-05 |
|    n_updates        | 49414    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.2     |
|    ep_rew_mean      | -0.0781  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11172    |
|    fps              | 221      |
|    time_elapsed     | 1074     |
|    total_timesteps  | 237780   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 49444    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.5     |
|    ep_rew_mean      | -0.0652  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11176    |
|    fps              | 221      |
|    time_elapsed     | 1074     |
|    total_timesteps  | 237847   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 49461    |
----------------------------------
Eval num_timesteps=238000, episode_reward=-0.07 +/- 0.35
Episode length: 48.38 +/- 24.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.4     |
|    mean_reward      | -0.0728  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 238000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.36e-05 |
|    n_updates        | 49499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.7     |
|    ep_rew_mean      | -0.0802  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11180    |
|    fps              | 221      |
|    time_elapsed     | 1075     |
|    total_timesteps  | 238047   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000128 |
|    n_updates        | 49511    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44       |
|    ep_rew_mean      | -0.0852  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11184    |
|    fps              | 221      |
|    time_elapsed     | 1076     |
|    total_timesteps  | 238264   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000171 |
|    n_updates        | 49565    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.6     |
|    ep_rew_mean      | -0.0778  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11188    |
|    fps              | 221      |
|    time_elapsed     | 1076     |
|    total_timesteps  | 238425   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0003   |
|    n_updates        | 49606    |
----------------------------------
Eval num_timesteps=238500, episode_reward=-0.07 +/- 0.34
Episode length: 41.48 +/- 24.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | -0.0652  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 238500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8e-05    |
|    n_updates        | 49624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.3     |
|    ep_rew_mean      | -0.0843  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11192    |
|    fps              | 221      |
|    time_elapsed     | 1078     |
|    total_timesteps  | 238664   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.92e-05 |
|    n_updates        | 49665    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.0927  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11196    |
|    fps              | 221      |
|    time_elapsed     | 1078     |
|    total_timesteps  | 238964   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000265 |
|    n_updates        | 49740    |
----------------------------------
Eval num_timesteps=239000, episode_reward=-0.16 +/- 0.28
Episode length: 55.58 +/- 24.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.6     |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 239000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00021  |
|    n_updates        | 49749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.8     |
|    ep_rew_mean      | -0.0906  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11200    |
|    fps              | 221      |
|    time_elapsed     | 1080     |
|    total_timesteps  | 239164   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.48e-05 |
|    n_updates        | 49790    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.3     |
|    ep_rew_mean      | -0.0884  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11204    |
|    fps              | 221      |
|    time_elapsed     | 1080     |
|    total_timesteps  | 239301   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.61e-05 |
|    n_updates        | 49825    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.1     |
|    ep_rew_mean      | -0.0836  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11208    |
|    fps              | 221      |
|    time_elapsed     | 1080     |
|    total_timesteps  | 239426   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000166 |
|    n_updates        | 49856    |
----------------------------------
Eval num_timesteps=239500, episode_reward=-0.18 +/- 0.17
Episode length: 49.68 +/- 24.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | -0.178   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 239500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 49874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.8     |
|    ep_rew_mean      | -0.0964  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11212    |
|    fps              | 221      |
|    time_elapsed     | 1082     |
|    total_timesteps  | 239562   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000173 |
|    n_updates        | 49890    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.2     |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11216    |
|    fps              | 221      |
|    time_elapsed     | 1082     |
|    total_timesteps  | 239630   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.29e-05 |
|    n_updates        | 49907    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.5     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11220    |
|    fps              | 221      |
|    time_elapsed     | 1082     |
|    total_timesteps  | 239733   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.85e-05 |
|    n_updates        | 49933    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.6     |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11224    |
|    fps              | 221      |
|    time_elapsed     | 1083     |
|    total_timesteps  | 239934   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000269 |
|    n_updates        | 49983    |
----------------------------------
Eval num_timesteps=240000, episode_reward=-0.18 +/- 0.23
Episode length: 55.58 +/- 21.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.6     |
|    mean_reward      | -0.182   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 240000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.69e-05 |
|    n_updates        | 49999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.1     |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11228    |
|    fps              | 221      |
|    time_elapsed     | 1085     |
|    total_timesteps  | 240075   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.18e-05 |
|    n_updates        | 50018    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.9     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11232    |
|    fps              | 221      |
|    time_elapsed     | 1085     |
|    total_timesteps  | 240212   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.63e-05 |
|    n_updates        | 50052    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.3     |
|    ep_rew_mean      | -0.0965  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11236    |
|    fps              | 221      |
|    time_elapsed     | 1085     |
|    total_timesteps  | 240283   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.97e-05 |
|    n_updates        | 50070    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.2     |
|    ep_rew_mean      | -0.092   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11240    |
|    fps              | 221      |
|    time_elapsed     | 1085     |
|    total_timesteps  | 240422   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.68e-05 |
|    n_updates        | 50105    |
----------------------------------
Eval num_timesteps=240500, episode_reward=-0.17 +/- 0.17
Episode length: 48.54 +/- 25.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.5     |
|    mean_reward      | -0.174   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 240500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.69e-05 |
|    n_updates        | 50124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.3     |
|    ep_rew_mean      | -0.0784  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11244    |
|    fps              | 221      |
|    time_elapsed     | 1087     |
|    total_timesteps  | 240546   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 50136    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42       |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11248    |
|    fps              | 221      |
|    time_elapsed     | 1087     |
|    total_timesteps  | 240753   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000101 |
|    n_updates        | 50188    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.8     |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11252    |
|    fps              | 221      |
|    time_elapsed     | 1087     |
|    total_timesteps  | 240963   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0001   |
|    n_updates        | 50240    |
----------------------------------
Eval num_timesteps=241000, episode_reward=-0.14 +/- 0.22
Episode length: 46.28 +/- 25.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | -0.144   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 241000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.95e-05 |
|    n_updates        | 50249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.4     |
|    ep_rew_mean      | -0.095   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11256    |
|    fps              | 221      |
|    time_elapsed     | 1089     |
|    total_timesteps  | 241087   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.35e-05 |
|    n_updates        | 50271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.2     |
|    ep_rew_mean      | -0.0962  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11260    |
|    fps              | 221      |
|    time_elapsed     | 1089     |
|    total_timesteps  | 241166   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.14e-05 |
|    n_updates        | 50291    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.4     |
|    ep_rew_mean      | -0.0926  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11264    |
|    fps              | 221      |
|    time_elapsed     | 1089     |
|    total_timesteps  | 241324   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.91e-05 |
|    n_updates        | 50330    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.3     |
|    ep_rew_mean      | -0.0924  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11268    |
|    fps              | 221      |
|    time_elapsed     | 1089     |
|    total_timesteps  | 241486   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.37e-05 |
|    n_updates        | 50371    |
----------------------------------
Eval num_timesteps=241500, episode_reward=-0.01 +/- 0.24
Episode length: 16.92 +/- 0.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | -0.0066  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 241500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00606  |
|    n_updates        | 50374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.8     |
|    ep_rew_mean      | -0.0903  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11272    |
|    fps              | 221      |
|    time_elapsed     | 1090     |
|    total_timesteps  | 241557   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000145 |
|    n_updates        | 50389    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.8     |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11276    |
|    fps              | 221      |
|    time_elapsed     | 1090     |
|    total_timesteps  | 241624   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00718  |
|    n_updates        | 50405    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.6     |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11280    |
|    fps              | 221      |
|    time_elapsed     | 1090     |
|    total_timesteps  | 241806   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00619  |
|    n_updates        | 50451    |
----------------------------------
Eval num_timesteps=242000, episode_reward=-0.16 +/- 0.27
Episode length: 56.14 +/- 23.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.1     |
|    mean_reward      | -0.164   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.63e-05 |
|    n_updates        | 50499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.4     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11284    |
|    fps              | 221      |
|    time_elapsed     | 1092     |
|    total_timesteps  | 242000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.2     |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11288    |
|    fps              | 221      |
|    time_elapsed     | 1093     |
|    total_timesteps  | 242249   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 50562    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.9     |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11292    |
|    fps              | 221      |
|    time_elapsed     | 1093     |
|    total_timesteps  | 242358   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 50589    |
----------------------------------
Eval num_timesteps=242500, episode_reward=-0.10 +/- 0.33
Episode length: 49.66 +/- 23.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | -0.098   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 242500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 50624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 35.9     |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11296    |
|    fps              | 221      |
|    time_elapsed     | 1095     |
|    total_timesteps  | 242554   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000245 |
|    n_updates        | 50638    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.3     |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11300    |
|    fps              | 221      |
|    time_elapsed     | 1095     |
|    total_timesteps  | 242794   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 50698    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 36.8     |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11304    |
|    fps              | 221      |
|    time_elapsed     | 1095     |
|    total_timesteps  | 242982   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00541  |
|    n_updates        | 50745    |
----------------------------------
Eval num_timesteps=243000, episode_reward=-0.11 +/- 0.21
Episode length: 38.18 +/- 22.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.2     |
|    mean_reward      | -0.112   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.43e-05 |
|    n_updates        | 50749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.5     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11308    |
|    fps              | 221      |
|    time_elapsed     | 1097     |
|    total_timesteps  | 243181   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.9e-05  |
|    n_updates        | 50795    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 37.6     |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11312    |
|    fps              | 221      |
|    time_elapsed     | 1097     |
|    total_timesteps  | 243319   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.69e-06 |
|    n_updates        | 50829    |
----------------------------------
Eval num_timesteps=243500, episode_reward=-0.16 +/- 0.24
Episode length: 50.40 +/- 24.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.4     |
|    mean_reward      | -0.161   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 243500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.76e-05 |
|    n_updates        | 50874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.7     |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11316    |
|    fps              | 221      |
|    time_elapsed     | 1099     |
|    total_timesteps  | 243503   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.4e-05  |
|    n_updates        | 50875    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.7     |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11320    |
|    fps              | 221      |
|    time_elapsed     | 1099     |
|    total_timesteps  | 243802   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.94e-05 |
|    n_updates        | 50950    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.5     |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11324    |
|    fps              | 221      |
|    time_elapsed     | 1099     |
|    total_timesteps  | 243981   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00016  |
|    n_updates        | 50995    |
----------------------------------
Eval num_timesteps=244000, episode_reward=-0.10 +/- 0.32
Episode length: 45.08 +/- 25.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | -0.0997  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 244000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.34e-05 |
|    n_updates        | 50999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.1     |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11328    |
|    fps              | 221      |
|    time_elapsed     | 1101     |
|    total_timesteps  | 244281   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.03e-05 |
|    n_updates        | 51070    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.3     |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11332    |
|    fps              | 221      |
|    time_elapsed     | 1101     |
|    total_timesteps  | 244438   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0072   |
|    n_updates        | 51109    |
----------------------------------
Eval num_timesteps=244500, episode_reward=-0.04 +/- 0.20
Episode length: 20.26 +/- 11.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 20.3     |
|    mean_reward      | -0.0401  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 244500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.75e-05 |
|    n_updates        | 51124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.1     |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11336    |
|    fps              | 221      |
|    time_elapsed     | 1102     |
|    total_timesteps  | 244590   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.5e-05  |
|    n_updates        | 51147    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.1     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11340    |
|    fps              | 222      |
|    time_elapsed     | 1102     |
|    total_timesteps  | 244830   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.08e-05 |
|    n_updates        | 51207    |
----------------------------------
Eval num_timesteps=245000, episode_reward=-0.15 +/- 0.23
Episode length: 48.12 +/- 23.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | -0.152   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 245000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.45e-05 |
|    n_updates        | 51249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.2     |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11344    |
|    fps              | 221      |
|    time_elapsed     | 1104     |
|    total_timesteps  | 245070   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 51267    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.6     |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11348    |
|    fps              | 221      |
|    time_elapsed     | 1104     |
|    total_timesteps  | 245213   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000286 |
|    n_updates        | 51303    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.2     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11352    |
|    fps              | 222      |
|    time_elapsed     | 1105     |
|    total_timesteps  | 245480   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.97e-05 |
|    n_updates        | 51369    |
----------------------------------
Eval num_timesteps=245500, episode_reward=-0.04 +/- 0.39
Episode length: 49.74 +/- 24.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | -0.0382  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 245500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000114 |
|    n_updates        | 51374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.3     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11356    |
|    fps              | 221      |
|    time_elapsed     | 1107     |
|    total_timesteps  | 245719   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 51429    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.8     |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11360    |
|    fps              | 222      |
|    time_elapsed     | 1107     |
|    total_timesteps  | 245945   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000364 |
|    n_updates        | 51486    |
----------------------------------
Eval num_timesteps=246000, episode_reward=-0.17 +/- 0.25
Episode length: 52.10 +/- 26.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.1     |
|    mean_reward      | -0.168   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 246000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000247 |
|    n_updates        | 51499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.7     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11364    |
|    fps              | 221      |
|    time_elapsed     | 1109     |
|    total_timesteps  | 246091   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.37e-05 |
|    n_updates        | 51522    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.2     |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11368    |
|    fps              | 221      |
|    time_elapsed     | 1109     |
|    total_timesteps  | 246305   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000345 |
|    n_updates        | 51576    |
----------------------------------
Eval num_timesteps=246500, episode_reward=-0.07 +/- 0.36
Episode length: 46.82 +/- 26.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | -0.0665  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 246500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.86e-05 |
|    n_updates        | 51624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.9     |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11372    |
|    fps              | 221      |
|    time_elapsed     | 1111     |
|    total_timesteps  | 246545   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.7e-05  |
|    n_updates        | 51636    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.2     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11376    |
|    fps              | 221      |
|    time_elapsed     | 1111     |
|    total_timesteps  | 246645   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 51661    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.2     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11380    |
|    fps              | 222      |
|    time_elapsed     | 1111     |
|    total_timesteps  | 246825   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 51706    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11384    |
|    fps              | 222      |
|    time_elapsed     | 1111     |
|    total_timesteps  | 246971   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.76e-05 |
|    n_updates        | 51742    |
----------------------------------
Eval num_timesteps=247000, episode_reward=-0.14 +/- 0.28
Episode length: 54.46 +/- 20.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.5     |
|    mean_reward      | -0.137   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 247000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000186 |
|    n_updates        | 51749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.6     |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11388    |
|    fps              | 221      |
|    time_elapsed     | 1113     |
|    total_timesteps  | 247207   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0003   |
|    n_updates        | 51801    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11392    |
|    fps              | 222      |
|    time_elapsed     | 1114     |
|    total_timesteps  | 247390   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 51847    |
----------------------------------
Eval num_timesteps=247500, episode_reward=-0.11 +/- 0.30
Episode length: 47.00 +/- 22.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | -0.107   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 247500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.82e-05 |
|    n_updates        | 51874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11396    |
|    fps              | 221      |
|    time_elapsed     | 1115     |
|    total_timesteps  | 247595   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000255 |
|    n_updates        | 51898    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11400    |
|    fps              | 222      |
|    time_elapsed     | 1116     |
|    total_timesteps  | 247837   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.01e-05 |
|    n_updates        | 51959    |
----------------------------------
Eval num_timesteps=248000, episode_reward=-0.10 +/- 0.29
Episode length: 45.50 +/- 23.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | -0.101   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 248000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.58e-05 |
|    n_updates        | 51999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.1     |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11404    |
|    fps              | 221      |
|    time_elapsed     | 1117     |
|    total_timesteps  | 248095   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.53e-05 |
|    n_updates        | 52023    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.5     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11408    |
|    fps              | 222      |
|    time_elapsed     | 1118     |
|    total_timesteps  | 248336   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00591  |
|    n_updates        | 52083    |
----------------------------------
Eval num_timesteps=248500, episode_reward=-0.19 +/- 0.23
Episode length: 57.58 +/- 24.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.6     |
|    mean_reward      | -0.19    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 248500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00627  |
|    n_updates        | 52124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.8     |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11412    |
|    fps              | 221      |
|    time_elapsed     | 1120     |
|    total_timesteps  | 248502   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.86e-05 |
|    n_updates        | 52125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.8     |
|    ep_rew_mean      | -0.0965  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11416    |
|    fps              | 221      |
|    time_elapsed     | 1120     |
|    total_timesteps  | 248679   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.84e-05 |
|    n_updates        | 52169    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.2     |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11420    |
|    fps              | 222      |
|    time_elapsed     | 1120     |
|    total_timesteps  | 248920   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.76e-05 |
|    n_updates        | 52229    |
----------------------------------
Eval num_timesteps=249000, episode_reward=-0.05 +/- 0.39
Episode length: 46.82 +/- 25.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | -0.0466  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 249000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.74e-05 |
|    n_updates        | 52249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.8     |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11424    |
|    fps              | 221      |
|    time_elapsed     | 1122     |
|    total_timesteps  | 249056   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.51e-05 |
|    n_updates        | 52263    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.0965  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11428    |
|    fps              | 221      |
|    time_elapsed     | 1122     |
|    total_timesteps  | 249208   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.18e-05 |
|    n_updates        | 52301    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.0981  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11432    |
|    fps              | 222      |
|    time_elapsed     | 1122     |
|    total_timesteps  | 249405   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000196 |
|    n_updates        | 52351    |
----------------------------------
Eval num_timesteps=249500, episode_reward=-0.13 +/- 0.28
Episode length: 47.48 +/- 24.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | -0.129   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 249500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0061   |
|    n_updates        | 52374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | -0.0875  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11436    |
|    fps              | 221      |
|    time_elapsed     | 1124     |
|    total_timesteps  | 249543   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.74e-05 |
|    n_updates        | 52385    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | -0.094   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11440    |
|    fps              | 221      |
|    time_elapsed     | 1124     |
|    total_timesteps  | 249696   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000292 |
|    n_updates        | 52423    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.0733  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11444    |
|    fps              | 222      |
|    time_elapsed     | 1125     |
|    total_timesteps  | 249919   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 52479    |
----------------------------------
Eval num_timesteps=250000, episode_reward=0.01 +/- 0.27
Episode length: 16.88 +/- 1.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 16.9     |
|    mean_reward      | 0.0135   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 250000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000177 |
|    n_updates        | 52499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.3     |
|    ep_rew_mean      | -0.0827  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11448    |
|    fps              | 222      |
|    time_elapsed     | 1125     |
|    total_timesteps  | 250045   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.17e-05 |
|    n_updates        | 52511    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.6     |
|    ep_rew_mean      | -0.0758  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11452    |
|    fps              | 222      |
|    time_elapsed     | 1125     |
|    total_timesteps  | 250142   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 52535    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.1     |
|    ep_rew_mean      | -0.0839  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11456    |
|    fps              | 222      |
|    time_elapsed     | 1126     |
|    total_timesteps  | 250332   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.64e-05 |
|    n_updates        | 52582    |
----------------------------------
Eval num_timesteps=250500, episode_reward=-0.04 +/- 0.30
Episode length: 35.34 +/- 24.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.3     |
|    mean_reward      | -0.0405  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 250500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000151 |
|    n_updates        | 52624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.7     |
|    ep_rew_mean      | -0.0823  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11460    |
|    fps              | 222      |
|    time_elapsed     | 1127     |
|    total_timesteps  | 250518   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00045  |
|    n_updates        | 52629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.8     |
|    ep_rew_mean      | -0.0825  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11464    |
|    fps              | 222      |
|    time_elapsed     | 1127     |
|    total_timesteps  | 250670   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.54e-05 |
|    n_updates        | 52667    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.3     |
|    ep_rew_mean      | -0.0767  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11468    |
|    fps              | 222      |
|    time_elapsed     | 1127     |
|    total_timesteps  | 250739   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 52684    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.9     |
|    ep_rew_mean      | -0.0747  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11472    |
|    fps              | 222      |
|    time_elapsed     | 1128     |
|    total_timesteps  | 250930   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000482 |
|    n_updates        | 52732    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.5     |
|    ep_rew_mean      | -0.0835  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11476    |
|    fps              | 222      |
|    time_elapsed     | 1128     |
|    total_timesteps  | 250999   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000219 |
|    n_updates        | 52749    |
----------------------------------
Eval num_timesteps=251000, episode_reward=-0.12 +/- 0.28
Episode length: 44.30 +/- 25.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.3     |
|    mean_reward     | -0.117   |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.9     |
|    ep_rew_mean      | -0.0708  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11480    |
|    fps              | 222      |
|    time_elapsed     | 1129     |
|    total_timesteps  | 251114   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000189 |
|    n_updates        | 52778    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.9     |
|    ep_rew_mean      | -0.0708  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11484    |
|    fps              | 222      |
|    time_elapsed     | 1129     |
|    total_timesteps  | 251260   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000301 |
|    n_updates        | 52814    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.3     |
|    ep_rew_mean      | -0.0686  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11488    |
|    fps              | 222      |
|    time_elapsed     | 1130     |
|    total_timesteps  | 251439   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000178 |
|    n_updates        | 52859    |
----------------------------------
Eval num_timesteps=251500, episode_reward=-0.08 +/- 0.25
Episode length: 35.38 +/- 24.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.4     |
|    mean_reward      | -0.0807  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 251500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000261 |
|    n_updates        | 52874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.6     |
|    ep_rew_mean      | -0.0757  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11492    |
|    fps              | 222      |
|    time_elapsed     | 1131     |
|    total_timesteps  | 251552   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000187 |
|    n_updates        | 52887    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.4     |
|    ep_rew_mean      | -0.0849  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11496    |
|    fps              | 222      |
|    time_elapsed     | 1131     |
|    total_timesteps  | 251735   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00052  |
|    n_updates        | 52933    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.3     |
|    ep_rew_mean      | -0.0904  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11500    |
|    fps              | 222      |
|    time_elapsed     | 1131     |
|    total_timesteps  | 251866   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00023  |
|    n_updates        | 52966    |
----------------------------------
Eval num_timesteps=252000, episode_reward=0.01 +/- 0.35
Episode length: 33.66 +/- 22.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 33.7     |
|    mean_reward      | 0.00638  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 52999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.9     |
|    ep_rew_mean      | -0.0789  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11504    |
|    fps              | 222      |
|    time_elapsed     | 1133     |
|    total_timesteps  | 252088   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 53021    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.1     |
|    ep_rew_mean      | -0.0754  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11508    |
|    fps              | 222      |
|    time_elapsed     | 1133     |
|    total_timesteps  | 252243   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000133 |
|    n_updates        | 53060    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.2     |
|    ep_rew_mean      | -0.0859  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11512    |
|    fps              | 222      |
|    time_elapsed     | 1133     |
|    total_timesteps  | 252420   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00017  |
|    n_updates        | 53104    |
----------------------------------
Eval num_timesteps=252500, episode_reward=-0.06 +/- 0.38
Episode length: 49.08 +/- 23.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.1     |
|    mean_reward      | -0.0557  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 252500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 53124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.2     |
|    ep_rew_mean      | -0.0961  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11516    |
|    fps              | 222      |
|    time_elapsed     | 1135     |
|    total_timesteps  | 252603   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.97e-05 |
|    n_updates        | 53150    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.2     |
|    ep_rew_mean      | -0.0959  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11520    |
|    fps              | 222      |
|    time_elapsed     | 1135     |
|    total_timesteps  | 252838   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.47e-05 |
|    n_updates        | 53209    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.8     |
|    ep_rew_mean      | -0.0942  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11524    |
|    fps              | 222      |
|    time_elapsed     | 1135     |
|    total_timesteps  | 252933   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000196 |
|    n_updates        | 53233    |
----------------------------------
Eval num_timesteps=253000, episode_reward=-0.12 +/- 0.29
Episode length: 50.00 +/- 22.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50       |
|    mean_reward      | -0.119   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 253000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00575  |
|    n_updates        | 53249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.9     |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11528    |
|    fps              | 222      |
|    time_elapsed     | 1137     |
|    total_timesteps  | 253096   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000155 |
|    n_updates        | 53273    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 38.5     |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11532    |
|    fps              | 222      |
|    time_elapsed     | 1137     |
|    total_timesteps  | 253250   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000354 |
|    n_updates        | 53312    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 39.1     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11536    |
|    fps              | 222      |
|    time_elapsed     | 1138     |
|    total_timesteps  | 253457   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000301 |
|    n_updates        | 53364    |
----------------------------------
Eval num_timesteps=253500, episode_reward=-0.15 +/- 0.28
Episode length: 52.88 +/- 24.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | -0.151   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 253500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000324 |
|    n_updates        | 53374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.2     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11540    |
|    fps              | 222      |
|    time_elapsed     | 1140     |
|    total_timesteps  | 253712   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000274 |
|    n_updates        | 53427    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 40.4     |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11544    |
|    fps              | 222      |
|    time_elapsed     | 1140     |
|    total_timesteps  | 253961   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.39e-05 |
|    n_updates        | 53490    |
----------------------------------
Eval num_timesteps=254000, episode_reward=-0.14 +/- 0.27
Episode length: 49.62 +/- 24.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.6     |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 254000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000153 |
|    n_updates        | 53499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.4     |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11548    |
|    fps              | 222      |
|    time_elapsed     | 1142     |
|    total_timesteps  | 254188   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00505  |
|    n_updates        | 53546    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42.4     |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11552    |
|    fps              | 222      |
|    time_elapsed     | 1142     |
|    total_timesteps  | 254385   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00018  |
|    n_updates        | 53596    |
----------------------------------
Eval num_timesteps=254500, episode_reward=-0.12 +/- 0.30
Episode length: 50.00 +/- 25.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50       |
|    mean_reward      | -0.119   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 254500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 53624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.9     |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11556    |
|    fps              | 222      |
|    time_elapsed     | 1144     |
|    total_timesteps  | 254518   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.64e-05 |
|    n_updates        | 53629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 41.9     |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11560    |
|    fps              | 222      |
|    time_elapsed     | 1144     |
|    total_timesteps  | 254707   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 53676    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 42       |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11564    |
|    fps              | 222      |
|    time_elapsed     | 1144     |
|    total_timesteps  | 254866   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000229 |
|    n_updates        | 53716    |
----------------------------------
Eval num_timesteps=255000, episode_reward=-0.08 +/- 0.29
Episode length: 39.66 +/- 21.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.7     |
|    mean_reward      | -0.0778  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 255000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.99e-05 |
|    n_updates        | 53749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.7     |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11568    |
|    fps              | 222      |
|    time_elapsed     | 1146     |
|    total_timesteps  | 255113   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.55e-05 |
|    n_updates        | 53778    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 43.4     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11572    |
|    fps              | 222      |
|    time_elapsed     | 1146     |
|    total_timesteps  | 255272   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000117 |
|    n_updates        | 53817    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.1     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11576    |
|    fps              | 222      |
|    time_elapsed     | 1146     |
|    total_timesteps  | 255413   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000352 |
|    n_updates        | 53853    |
----------------------------------
Eval num_timesteps=255500, episode_reward=-0.03 +/- 0.38
Episode length: 42.02 +/- 24.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | -0.0273  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 255500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000584 |
|    n_updates        | 53874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.7     |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11580    |
|    fps              | 222      |
|    time_elapsed     | 1148     |
|    total_timesteps  | 255584   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.7e-05  |
|    n_updates        | 53895    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 44.9     |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11584    |
|    fps              | 222      |
|    time_elapsed     | 1148     |
|    total_timesteps  | 255752   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000297 |
|    n_updates        | 53937    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.1     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11588    |
|    fps              | 222      |
|    time_elapsed     | 1148     |
|    total_timesteps  | 255952   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 53987    |
----------------------------------
Eval num_timesteps=256000, episode_reward=-0.14 +/- 0.26
Episode length: 49.40 +/- 23.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.4     |
|    mean_reward      | -0.137   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 256000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 53999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46       |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11592    |
|    fps              | 222      |
|    time_elapsed     | 1150     |
|    total_timesteps  | 256155   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 54038    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.5     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11596    |
|    fps              | 222      |
|    time_elapsed     | 1150     |
|    total_timesteps  | 256283   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000209 |
|    n_updates        | 54070    |
----------------------------------
Eval num_timesteps=256500, episode_reward=-0.15 +/- 0.21
Episode length: 48.18 +/- 23.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.2     |
|    mean_reward      | -0.152   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 256500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00541  |
|    n_updates        | 54124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.1     |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11600    |
|    fps              | 222      |
|    time_elapsed     | 1152     |
|    total_timesteps  | 256580   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000162 |
|    n_updates        | 54144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.1     |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11604    |
|    fps              | 222      |
|    time_elapsed     | 1152     |
|    total_timesteps  | 256803   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000223 |
|    n_updates        | 54200    |
----------------------------------
Eval num_timesteps=257000, episode_reward=-0.14 +/- 0.28
Episode length: 50.50 +/- 22.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.5     |
|    mean_reward      | -0.141   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 257000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000139 |
|    n_updates        | 54249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48       |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11608    |
|    fps              | 222      |
|    time_elapsed     | 1154     |
|    total_timesteps  | 257043   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.12e-05 |
|    n_updates        | 54260    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.8     |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11612    |
|    fps              | 222      |
|    time_elapsed     | 1155     |
|    total_timesteps  | 257304   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.73e-05 |
|    n_updates        | 54325    |
----------------------------------
Eval num_timesteps=257500, episode_reward=-0.17 +/- 0.22
Episode length: 51.64 +/- 22.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.6     |
|    mean_reward      | -0.166   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 257500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000313 |
|    n_updates        | 54374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11616    |
|    fps              | 222      |
|    time_elapsed     | 1157     |
|    total_timesteps  | 257526   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000296 |
|    n_updates        | 54381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.9     |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11620    |
|    fps              | 222      |
|    time_elapsed     | 1157     |
|    total_timesteps  | 257731   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 54432    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.8     |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11624    |
|    fps              | 222      |
|    time_elapsed     | 1157     |
|    total_timesteps  | 257913   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.65e-05 |
|    n_updates        | 54478    |
----------------------------------
Eval num_timesteps=258000, episode_reward=-0.15 +/- 0.24
Episode length: 48.32 +/- 23.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | -0.153   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000166 |
|    n_updates        | 54499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.2     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11628    |
|    fps              | 222      |
|    time_elapsed     | 1159     |
|    total_timesteps  | 258115   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.4e-05  |
|    n_updates        | 54528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11632    |
|    fps              | 222      |
|    time_elapsed     | 1159     |
|    total_timesteps  | 258293   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000154 |
|    n_updates        | 54573    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11636    |
|    fps              | 222      |
|    time_elapsed     | 1159     |
|    total_timesteps  | 258457   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000162 |
|    n_updates        | 54614    |
----------------------------------
Eval num_timesteps=258500, episode_reward=-0.13 +/- 0.31
Episode length: 52.80 +/- 22.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.8     |
|    mean_reward      | -0.13    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 258500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 54624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.8     |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11640    |
|    fps              | 222      |
|    time_elapsed     | 1161     |
|    total_timesteps  | 258593   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000174 |
|    n_updates        | 54648    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11644    |
|    fps              | 222      |
|    time_elapsed     | 1161     |
|    total_timesteps  | 258813   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000282 |
|    n_updates        | 54703    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48       |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11648    |
|    fps              | 222      |
|    time_elapsed     | 1162     |
|    total_timesteps  | 258992   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00615  |
|    n_updates        | 54747    |
----------------------------------
Eval num_timesteps=259000, episode_reward=-0.15 +/- 0.29
Episode length: 53.54 +/- 23.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.5     |
|    mean_reward      | -0.153   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 259000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.92e-05 |
|    n_updates        | 54749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49       |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11652    |
|    fps              | 222      |
|    time_elapsed     | 1164     |
|    total_timesteps  | 259285   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00493  |
|    n_updates        | 54821    |
----------------------------------
Eval num_timesteps=259500, episode_reward=-0.10 +/- 0.33
Episode length: 49.42 +/- 23.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.4     |
|    mean_reward      | -0.097   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.91e-05 |
|    n_updates        | 54874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11656    |
|    fps              | 222      |
|    time_elapsed     | 1166     |
|    total_timesteps  | 259515   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.6e-05  |
|    n_updates        | 54878    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11660    |
|    fps              | 222      |
|    time_elapsed     | 1166     |
|    total_timesteps  | 259637   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00581  |
|    n_updates        | 54909    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11664    |
|    fps              | 222      |
|    time_elapsed     | 1166     |
|    total_timesteps  | 259793   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000207 |
|    n_updates        | 54948    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11668    |
|    fps              | 222      |
|    time_elapsed     | 1166     |
|    total_timesteps  | 259953   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000171 |
|    n_updates        | 54988    |
----------------------------------
Eval num_timesteps=260000, episode_reward=-0.14 +/- 0.22
Episode length: 45.70 +/- 23.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | -0.142   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 260000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000202 |
|    n_updates        | 54999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11672    |
|    fps              | 222      |
|    time_elapsed     | 1168     |
|    total_timesteps  | 260140   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.9e-05  |
|    n_updates        | 55034    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11676    |
|    fps              | 222      |
|    time_elapsed     | 1168     |
|    total_timesteps  | 260286   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.45e-05 |
|    n_updates        | 55071    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11680    |
|    fps              | 222      |
|    time_elapsed     | 1168     |
|    total_timesteps  | 260436   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00811  |
|    n_updates        | 55108    |
----------------------------------
Eval num_timesteps=260500, episode_reward=-0.15 +/- 0.23
Episode length: 48.20 +/- 23.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.2     |
|    mean_reward      | -0.152   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 260500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 55124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.8     |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11684    |
|    fps              | 222      |
|    time_elapsed     | 1170     |
|    total_timesteps  | 260530   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000234 |
|    n_updates        | 55132    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.1     |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11688    |
|    fps              | 222      |
|    time_elapsed     | 1170     |
|    total_timesteps  | 260658   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00502  |
|    n_updates        | 55164    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.5     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11692    |
|    fps              | 222      |
|    time_elapsed     | 1170     |
|    total_timesteps  | 260900   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000211 |
|    n_updates        | 55224    |
----------------------------------
Eval num_timesteps=261000, episode_reward=-0.14 +/- 0.29
Episode length: 50.10 +/- 25.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.1     |
|    mean_reward      | -0.14    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 55249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.3     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11696    |
|    fps              | 222      |
|    time_elapsed     | 1172     |
|    total_timesteps  | 261117   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000332 |
|    n_updates        | 55279    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.4     |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11700    |
|    fps              | 222      |
|    time_elapsed     | 1173     |
|    total_timesteps  | 261323   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000486 |
|    n_updates        | 55330    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.9     |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11704    |
|    fps              | 222      |
|    time_elapsed     | 1173     |
|    total_timesteps  | 261493   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000266 |
|    n_updates        | 55373    |
----------------------------------
Eval num_timesteps=261500, episode_reward=-0.20 +/- 0.25
Episode length: 59.78 +/- 23.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.8     |
|    mean_reward      | -0.199   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 261500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000318 |
|    n_updates        | 55374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.2     |
|    ep_rew_mean      | -0.098   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11708    |
|    fps              | 222      |
|    time_elapsed     | 1175     |
|    total_timesteps  | 261761   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000388 |
|    n_updates        | 55440    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46       |
|    ep_rew_mean      | -0.0932  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11712    |
|    fps              | 222      |
|    time_elapsed     | 1175     |
|    total_timesteps  | 261903   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000311 |
|    n_updates        | 55475    |
----------------------------------
Eval num_timesteps=262000, episode_reward=-0.17 +/- 0.22
Episode length: 51.98 +/- 23.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52       |
|    mean_reward      | -0.167   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 262000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00491  |
|    n_updates        | 55499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.9     |
|    ep_rew_mean      | -0.0828  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11716    |
|    fps              | 222      |
|    time_elapsed     | 1177     |
|    total_timesteps  | 262114   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000272 |
|    n_updates        | 55528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.8     |
|    ep_rew_mean      | -0.0865  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11720    |
|    fps              | 222      |
|    time_elapsed     | 1177     |
|    total_timesteps  | 262410   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000712 |
|    n_updates        | 55602    |
----------------------------------
Eval num_timesteps=262500, episode_reward=-0.15 +/- 0.28
Episode length: 53.66 +/- 22.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.7     |
|    mean_reward      | -0.154   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 262500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.79e-05 |
|    n_updates        | 55624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.4     |
|    ep_rew_mean      | -0.089   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11724    |
|    fps              | 222      |
|    time_elapsed     | 1180     |
|    total_timesteps  | 262655   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.42e-05 |
|    n_updates        | 55663    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.9     |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11728    |
|    fps              | 222      |
|    time_elapsed     | 1180     |
|    total_timesteps  | 262805   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.53e-05 |
|    n_updates        | 55701    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.6     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11732    |
|    fps              | 222      |
|    time_elapsed     | 1180     |
|    total_timesteps  | 262953   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.8e-05  |
|    n_updates        | 55738    |
----------------------------------
Eval num_timesteps=263000, episode_reward=-0.16 +/- 0.27
Episode length: 55.58 +/- 22.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.6     |
|    mean_reward      | -0.162   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 263000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00523  |
|    n_updates        | 55749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.4     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11736    |
|    fps              | 222      |
|    time_elapsed     | 1182     |
|    total_timesteps  | 263096   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000138 |
|    n_updates        | 55773    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.1     |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11740    |
|    fps              | 222      |
|    time_elapsed     | 1182     |
|    total_timesteps  | 263300   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.33e-05 |
|    n_updates        | 55824    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.8     |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11744    |
|    fps              | 222      |
|    time_elapsed     | 1182     |
|    total_timesteps  | 263497   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000189 |
|    n_updates        | 55874    |
----------------------------------
Eval num_timesteps=263500, episode_reward=-0.13 +/- 0.30
Episode length: 53.44 +/- 23.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.4     |
|    mean_reward      | -0.133   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 263500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.9     |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11748    |
|    fps              | 222      |
|    time_elapsed     | 1184     |
|    total_timesteps  | 263678   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00523  |
|    n_updates        | 55919    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.8     |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11752    |
|    fps              | 222      |
|    time_elapsed     | 1185     |
|    total_timesteps  | 263869   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000118 |
|    n_updates        | 55967    |
----------------------------------
Eval num_timesteps=264000, episode_reward=-0.21 +/- 0.18
Episode length: 56.94 +/- 22.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.9     |
|    mean_reward      | -0.207   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 264000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.16e-05 |
|    n_updates        | 55999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45       |
|    ep_rew_mean      | -0.0895  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11756    |
|    fps              | 222      |
|    time_elapsed     | 1187     |
|    total_timesteps  | 264020   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000167 |
|    n_updates        | 56004    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 45.9     |
|    ep_rew_mean      | -0.0927  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11760    |
|    fps              | 222      |
|    time_elapsed     | 1187     |
|    total_timesteps  | 264222   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000582 |
|    n_updates        | 56055    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.9     |
|    ep_rew_mean      | -0.0969  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11764    |
|    fps              | 222      |
|    time_elapsed     | 1187     |
|    total_timesteps  | 264482   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.65e-05 |
|    n_updates        | 56120    |
----------------------------------
Eval num_timesteps=264500, episode_reward=-0.17 +/- 0.23
Episode length: 51.42 +/- 24.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | -0.165   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 264500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000166 |
|    n_updates        | 56124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.9     |
|    ep_rew_mean      | -0.097   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11768    |
|    fps              | 222      |
|    time_elapsed     | 1189     |
|    total_timesteps  | 264646   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000154 |
|    n_updates        | 56161    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.7     |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11772    |
|    fps              | 222      |
|    time_elapsed     | 1189     |
|    total_timesteps  | 264806   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000181 |
|    n_updates        | 56201    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.1     |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11776    |
|    fps              | 222      |
|    time_elapsed     | 1189     |
|    total_timesteps  | 264998   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000453 |
|    n_updates        | 56249    |
----------------------------------
Eval num_timesteps=265000, episode_reward=-0.15 +/- 0.31
Episode length: 57.96 +/- 22.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58       |
|    mean_reward      | -0.151   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 265000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.3     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11780    |
|    fps              | 222      |
|    time_elapsed     | 1192     |
|    total_timesteps  | 265265   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000236 |
|    n_updates        | 56316    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.1     |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11784    |
|    fps              | 222      |
|    time_elapsed     | 1192     |
|    total_timesteps  | 265436   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 56358    |
----------------------------------
Eval num_timesteps=265500, episode_reward=-0.17 +/- 0.24
Episode length: 52.32 +/- 24.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.3     |
|    mean_reward      | -0.169   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 265500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00042  |
|    n_updates        | 56374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.8     |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11788    |
|    fps              | 222      |
|    time_elapsed     | 1194     |
|    total_timesteps  | 265637   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000134 |
|    n_updates        | 56409    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11792    |
|    fps              | 222      |
|    time_elapsed     | 1194     |
|    total_timesteps  | 265820   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00024  |
|    n_updates        | 56454    |
----------------------------------
Eval num_timesteps=266000, episode_reward=-0.16 +/- 0.24
Episode length: 50.02 +/- 25.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50       |
|    mean_reward      | -0.16    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 266000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 56499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.6     |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11796    |
|    fps              | 222      |
|    time_elapsed     | 1196     |
|    total_timesteps  | 266080   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000489 |
|    n_updates        | 56519    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.9     |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11800    |
|    fps              | 222      |
|    time_elapsed     | 1196     |
|    total_timesteps  | 266310   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.33e-05 |
|    n_updates        | 56577    |
----------------------------------
Eval num_timesteps=266500, episode_reward=-0.18 +/- 0.22
Episode length: 54.66 +/- 22.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.7     |
|    mean_reward      | -0.178   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 266500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 56624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.1     |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11804    |
|    fps              | 222      |
|    time_elapsed     | 1198     |
|    total_timesteps  | 266508   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00471  |
|    n_updates        | 56626    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.9     |
|    ep_rew_mean      | -0.159   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11808    |
|    fps              | 222      |
|    time_elapsed     | 1199     |
|    total_timesteps  | 266746   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 56686    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11812    |
|    fps              | 222      |
|    time_elapsed     | 1199     |
|    total_timesteps  | 266934   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 56733    |
----------------------------------
Eval num_timesteps=267000, episode_reward=-0.09 +/- 0.32
Episode length: 48.80 +/- 22.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.8     |
|    mean_reward      | -0.0945  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 267000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00452  |
|    n_updates        | 56749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11816    |
|    fps              | 222      |
|    time_elapsed     | 1201     |
|    total_timesteps  | 267141   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 56785    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11820    |
|    fps              | 222      |
|    time_elapsed     | 1201     |
|    total_timesteps  | 267281   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00811  |
|    n_updates        | 56820    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.8     |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11824    |
|    fps              | 222      |
|    time_elapsed     | 1201     |
|    total_timesteps  | 267434   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00105  |
|    n_updates        | 56858    |
----------------------------------
Eval num_timesteps=267500, episode_reward=-0.14 +/- 0.26
Episode length: 49.60 +/- 21.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.6     |
|    mean_reward      | -0.138   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 267500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000161 |
|    n_updates        | 56874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11828    |
|    fps              | 222      |
|    time_elapsed     | 1203     |
|    total_timesteps  | 267641   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000184 |
|    n_updates        | 56910    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11832    |
|    fps              | 222      |
|    time_elapsed     | 1203     |
|    total_timesteps  | 267873   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00491  |
|    n_updates        | 56968    |
----------------------------------
Eval num_timesteps=268000, episode_reward=-0.11 +/- 0.27
Episode length: 42.40 +/- 23.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | -0.109   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 268000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00834  |
|    n_updates        | 56999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.9     |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11836    |
|    fps              | 222      |
|    time_elapsed     | 1205     |
|    total_timesteps  | 268082   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000397 |
|    n_updates        | 57020    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.2     |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11840    |
|    fps              | 222      |
|    time_elapsed     | 1205     |
|    total_timesteps  | 268317   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000174 |
|    n_updates        | 57079    |
----------------------------------
Eval num_timesteps=268500, episode_reward=-0.12 +/- 0.25
Episode length: 45.98 +/- 22.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | -0.123   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00446  |
|    n_updates        | 57124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.5     |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11844    |
|    fps              | 222      |
|    time_elapsed     | 1207     |
|    total_timesteps  | 268543   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.37e-05 |
|    n_updates        | 57135    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.5     |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11848    |
|    fps              | 222      |
|    time_elapsed     | 1207     |
|    total_timesteps  | 268732   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.89e-05 |
|    n_updates        | 57182    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.9     |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11852    |
|    fps              | 222      |
|    time_elapsed     | 1207     |
|    total_timesteps  | 268858   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.52e-05 |
|    n_updates        | 57214    |
----------------------------------
Eval num_timesteps=269000, episode_reward=-0.12 +/- 0.30
Episode length: 51.00 +/- 24.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51       |
|    mean_reward      | -0.123   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 269000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 57249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.159   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11856    |
|    fps              | 222      |
|    time_elapsed     | 1209     |
|    total_timesteps  | 269015   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000446 |
|    n_updates        | 57253    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11860    |
|    fps              | 222      |
|    time_elapsed     | 1209     |
|    total_timesteps  | 269146   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 57286    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.2     |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11864    |
|    fps              | 222      |
|    time_elapsed     | 1209     |
|    total_timesteps  | 269303   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 57325    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11868    |
|    fps              | 222      |
|    time_elapsed     | 1210     |
|    total_timesteps  | 269486   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000225 |
|    n_updates        | 57371    |
----------------------------------
Eval num_timesteps=269500, episode_reward=-0.10 +/- 0.27
Episode length: 44.96 +/- 22.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | -0.0991  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 269500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00017  |
|    n_updates        | 57374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49       |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11872    |
|    fps              | 222      |
|    time_elapsed     | 1211     |
|    total_timesteps  | 269708   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000324 |
|    n_updates        | 57426    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11876    |
|    fps              | 222      |
|    time_elapsed     | 1211     |
|    total_timesteps  | 269847   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000228 |
|    n_updates        | 57461    |
----------------------------------
Eval num_timesteps=270000, episode_reward=-0.13 +/- 0.26
Episode length: 47.34 +/- 22.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | -0.129   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000223 |
|    n_updates        | 57499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11880    |
|    fps              | 222      |
|    time_elapsed     | 1213     |
|    total_timesteps  | 270116   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00025  |
|    n_updates        | 57528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11884    |
|    fps              | 222      |
|    time_elapsed     | 1214     |
|    total_timesteps  | 270308   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.8e-05  |
|    n_updates        | 57576    |
----------------------------------
Eval num_timesteps=270500, episode_reward=-0.15 +/- 0.27
Episode length: 51.64 +/- 21.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.6     |
|    mean_reward      | -0.146   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 270500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 57624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.9     |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11888    |
|    fps              | 222      |
|    time_elapsed     | 1216     |
|    total_timesteps  | 270531   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000182 |
|    n_updates        | 57632    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.1     |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11892    |
|    fps              | 222      |
|    time_elapsed     | 1216     |
|    total_timesteps  | 270729   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.26e-05 |
|    n_updates        | 57682    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11896    |
|    fps              | 222      |
|    time_elapsed     | 1216     |
|    total_timesteps  | 270946   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 57736    |
----------------------------------
Eval num_timesteps=271000, episode_reward=-0.20 +/- 0.08
Episode length: 51.20 +/- 20.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.2     |
|    mean_reward      | -0.204   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 271000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000151 |
|    n_updates        | 57749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.6     |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11900    |
|    fps              | 222      |
|    time_elapsed     | 1218     |
|    total_timesteps  | 271173   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.53e-05 |
|    n_updates        | 57793    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11904    |
|    fps              | 222      |
|    time_elapsed     | 1218     |
|    total_timesteps  | 271346   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000188 |
|    n_updates        | 57836    |
----------------------------------
Eval num_timesteps=271500, episode_reward=-0.11 +/- 0.30
Episode length: 48.32 +/- 23.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | -0.113   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 271500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000171 |
|    n_updates        | 57874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.3     |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11908    |
|    fps              | 222      |
|    time_elapsed     | 1220     |
|    total_timesteps  | 271577   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000492 |
|    n_updates        | 57894    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.1     |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11912    |
|    fps              | 222      |
|    time_elapsed     | 1220     |
|    total_timesteps  | 271843   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.58e-05 |
|    n_updates        | 57960    |
----------------------------------
Eval num_timesteps=272000, episode_reward=-0.04 +/- 0.38
Episode length: 46.26 +/- 23.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | -0.0443  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 272000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.03e-05 |
|    n_updates        | 57999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11916    |
|    fps              | 222      |
|    time_elapsed     | 1222     |
|    total_timesteps  | 272012   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000273 |
|    n_updates        | 58002    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11920    |
|    fps              | 222      |
|    time_elapsed     | 1222     |
|    total_timesteps  | 272136   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00018  |
|    n_updates        | 58033    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.8     |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11924    |
|    fps              | 222      |
|    time_elapsed     | 1222     |
|    total_timesteps  | 272217   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000369 |
|    n_updates        | 58054    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.2     |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11928    |
|    fps              | 222      |
|    time_elapsed     | 1223     |
|    total_timesteps  | 272458   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 58114    |
----------------------------------
Eval num_timesteps=272500, episode_reward=-0.11 +/- 0.26
Episode length: 42.88 +/- 24.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | -0.111   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00535  |
|    n_updates        | 58124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48       |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11932    |
|    fps              | 222      |
|    time_elapsed     | 1224     |
|    total_timesteps  | 272674   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.5e-05  |
|    n_updates        | 58168    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.1     |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11936    |
|    fps              | 222      |
|    time_elapsed     | 1225     |
|    total_timesteps  | 272888   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.31e-05 |
|    n_updates        | 58221    |
----------------------------------
Eval num_timesteps=273000, episode_reward=-0.11 +/- 0.28
Episode length: 47.32 +/- 22.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | -0.108   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 273000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.64e-05 |
|    n_updates        | 58249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.9     |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11940    |
|    fps              | 222      |
|    time_elapsed     | 1226     |
|    total_timesteps  | 273106   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000153 |
|    n_updates        | 58276    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.6     |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11944    |
|    fps              | 222      |
|    time_elapsed     | 1227     |
|    total_timesteps  | 273307   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00461  |
|    n_updates        | 58326    |
----------------------------------
Eval num_timesteps=273500, episode_reward=-0.12 +/- 0.30
Episode length: 49.82 +/- 23.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.8     |
|    mean_reward      | -0.119   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 273500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000299 |
|    n_updates        | 58374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.7     |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11948    |
|    fps              | 222      |
|    time_elapsed     | 1228     |
|    total_timesteps  | 273503   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00456  |
|    n_updates        | 58375    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.7     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11952    |
|    fps              | 222      |
|    time_elapsed     | 1229     |
|    total_timesteps  | 273629   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.96e-05 |
|    n_updates        | 58407    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.4     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11956    |
|    fps              | 222      |
|    time_elapsed     | 1229     |
|    total_timesteps  | 273758   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.1e-05  |
|    n_updates        | 58439    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.5     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11960    |
|    fps              | 222      |
|    time_elapsed     | 1229     |
|    total_timesteps  | 273901   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000296 |
|    n_updates        | 58475    |
----------------------------------
Eval num_timesteps=274000, episode_reward=-0.17 +/- 0.23
Episode length: 53.10 +/- 22.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.1     |
|    mean_reward      | -0.172   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 274000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00031  |
|    n_updates        | 58499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11964    |
|    fps              | 222      |
|    time_elapsed     | 1231     |
|    total_timesteps  | 274143   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00506  |
|    n_updates        | 58535    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11968    |
|    fps              | 222      |
|    time_elapsed     | 1231     |
|    total_timesteps  | 274406   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.81e-05 |
|    n_updates        | 58601    |
----------------------------------
Eval num_timesteps=274500, episode_reward=-0.14 +/- 0.23
Episode length: 45.66 +/- 24.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | -0.142   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 274500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.06e-05 |
|    n_updates        | 58624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49       |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11972    |
|    fps              | 222      |
|    time_elapsed     | 1233     |
|    total_timesteps  | 274608   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.62e-05 |
|    n_updates        | 58651    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11976    |
|    fps              | 222      |
|    time_elapsed     | 1233     |
|    total_timesteps  | 274777   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000303 |
|    n_updates        | 58694    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11980    |
|    fps              | 222      |
|    time_elapsed     | 1233     |
|    total_timesteps  | 274961   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.74e-05 |
|    n_updates        | 58740    |
----------------------------------
Eval num_timesteps=275000, episode_reward=-0.10 +/- 0.34
Episode length: 54.02 +/- 22.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54       |
|    mean_reward      | -0.0954  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 275000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.39e-05 |
|    n_updates        | 58749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11984    |
|    fps              | 222      |
|    time_elapsed     | 1235     |
|    total_timesteps  | 275231   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 58807    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49       |
|    ep_rew_mean      | -0.0953  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11988    |
|    fps              | 222      |
|    time_elapsed     | 1236     |
|    total_timesteps  | 275431   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 58857    |
----------------------------------
Eval num_timesteps=275500, episode_reward=-0.11 +/- 0.34
Episode length: 53.08 +/- 23.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.1     |
|    mean_reward      | -0.112   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 275500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00032  |
|    n_updates        | 58874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49       |
|    ep_rew_mean      | -0.0952  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11992    |
|    fps              | 222      |
|    time_elapsed     | 1238     |
|    total_timesteps  | 275625   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000355 |
|    n_updates        | 58906    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.0829  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 11996    |
|    fps              | 222      |
|    time_elapsed     | 1238     |
|    total_timesteps  | 275784   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000141 |
|    n_updates        | 58945    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.3     |
|    ep_rew_mean      | -0.0686  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12000    |
|    fps              | 222      |
|    time_elapsed     | 1238     |
|    total_timesteps  | 275905   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.67e-05 |
|    n_updates        | 58976    |
----------------------------------
Eval num_timesteps=276000, episode_reward=-0.14 +/- 0.26
Episode length: 49.96 +/- 22.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50       |
|    mean_reward      | -0.139   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 276000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000372 |
|    n_updates        | 58999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.6     |
|    ep_rew_mean      | -0.0799  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12004    |
|    fps              | 222      |
|    time_elapsed     | 1240     |
|    total_timesteps  | 276111   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000206 |
|    n_updates        | 59027    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.8     |
|    ep_rew_mean      | -0.0867  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12008    |
|    fps              | 222      |
|    time_elapsed     | 1240     |
|    total_timesteps  | 276261   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000149 |
|    n_updates        | 59065    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 46.4     |
|    ep_rew_mean      | -0.085   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12012    |
|    fps              | 222      |
|    time_elapsed     | 1240     |
|    total_timesteps  | 276485   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000308 |
|    n_updates        | 59121    |
----------------------------------
Eval num_timesteps=276500, episode_reward=-0.08 +/- 0.29
Episode length: 45.06 +/- 22.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | -0.0794  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 276500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000184 |
|    n_updates        | 59124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47       |
|    ep_rew_mean      | -0.0871  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12016    |
|    fps              | 222      |
|    time_elapsed     | 1242     |
|    total_timesteps  | 276707   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000224 |
|    n_updates        | 59176    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.9     |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12020    |
|    fps              | 222      |
|    time_elapsed     | 1242     |
|    total_timesteps  | 276921   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00486  |
|    n_updates        | 59230    |
----------------------------------
Eval num_timesteps=277000, episode_reward=-0.10 +/- 0.32
Episode length: 51.02 +/- 22.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51       |
|    mean_reward      | -0.103   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.32e-05 |
|    n_updates        | 59249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12024    |
|    fps              | 222      |
|    time_elapsed     | 1244     |
|    total_timesteps  | 277085   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000145 |
|    n_updates        | 59271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.3     |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12028    |
|    fps              | 222      |
|    time_elapsed     | 1244     |
|    total_timesteps  | 277288   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 59321    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.2     |
|    ep_rew_mean      | -0.0921  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12032    |
|    fps              | 222      |
|    time_elapsed     | 1245     |
|    total_timesteps  | 277494   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00653  |
|    n_updates        | 59373    |
----------------------------------
Eval num_timesteps=277500, episode_reward=-0.09 +/- 0.34
Episode length: 51.98 +/- 21.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52       |
|    mean_reward      | -0.0871  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 277500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 59374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | -0.0942  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12036    |
|    fps              | 222      |
|    time_elapsed     | 1247     |
|    total_timesteps  | 277759   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000199 |
|    n_updates        | 59439    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | -0.094   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12040    |
|    fps              | 222      |
|    time_elapsed     | 1247     |
|    total_timesteps  | 277973   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00025  |
|    n_updates        | 59493    |
----------------------------------
Eval num_timesteps=278000, episode_reward=-0.12 +/- 0.31
Episode length: 56.32 +/- 21.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.3     |
|    mean_reward      | -0.125   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 278000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 59499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49       |
|    ep_rew_mean      | -0.0953  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12044    |
|    fps              | 222      |
|    time_elapsed     | 1249     |
|    total_timesteps  | 278206   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000452 |
|    n_updates        | 59551    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.0864  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12048    |
|    fps              | 222      |
|    time_elapsed     | 1249     |
|    total_timesteps  | 278430   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000981 |
|    n_updates        | 59607    |
----------------------------------
Eval num_timesteps=278500, episode_reward=-0.14 +/- 0.27
Episode length: 49.08 +/- 22.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.1     |
|    mean_reward      | -0.136   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 278500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000171 |
|    n_updates        | 59624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.5     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12052    |
|    fps              | 222      |
|    time_elapsed     | 1251     |
|    total_timesteps  | 278681   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000165 |
|    n_updates        | 59670    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.2     |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12056    |
|    fps              | 222      |
|    time_elapsed     | 1252     |
|    total_timesteps  | 278876   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.86e-05 |
|    n_updates        | 59718    |
----------------------------------
Eval num_timesteps=279000, episode_reward=-0.03 +/- 0.43
Episode length: 52.06 +/- 23.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.1     |
|    mean_reward      | -0.0276  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.11e-05 |
|    n_updates        | 59749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.9     |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12060    |
|    fps              | 222      |
|    time_elapsed     | 1254     |
|    total_timesteps  | 279090   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 59772    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.8     |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12064    |
|    fps              | 222      |
|    time_elapsed     | 1254     |
|    total_timesteps  | 279327   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.82e-05 |
|    n_updates        | 59831    |
----------------------------------
Eval num_timesteps=279500, episode_reward=-0.00 +/- 0.40
Episode length: 46.32 +/- 22.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | -0.00436 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 279500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 59874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51       |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12068    |
|    fps              | 222      |
|    time_elapsed     | 1256     |
|    total_timesteps  | 279504   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.22e-05 |
|    n_updates        | 59875    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12072    |
|    fps              | 222      |
|    time_elapsed     | 1256     |
|    total_timesteps  | 279606   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.17e-05 |
|    n_updates        | 59901    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12076    |
|    fps              | 222      |
|    time_elapsed     | 1256     |
|    total_timesteps  | 279808   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 59951    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12080    |
|    fps              | 222      |
|    time_elapsed     | 1256     |
|    total_timesteps  | 279998   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000122 |
|    n_updates        | 59999    |
----------------------------------
Eval num_timesteps=280000, episode_reward=-0.21 +/- 0.09
Episode length: 51.86 +/- 23.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.9     |
|    mean_reward      | -0.207   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 280000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.1     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12084    |
|    fps              | 222      |
|    time_elapsed     | 1258     |
|    total_timesteps  | 280142   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00491  |
|    n_updates        | 60035    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.8     |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12088    |
|    fps              | 222      |
|    time_elapsed     | 1258     |
|    total_timesteps  | 280313   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 60078    |
----------------------------------
Eval num_timesteps=280500, episode_reward=-0.09 +/- 0.32
Episode length: 48.92 +/- 22.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.9     |
|    mean_reward      | -0.0948  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 280500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000252 |
|    n_updates        | 60124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.8     |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12092    |
|    fps              | 222      |
|    time_elapsed     | 1260     |
|    total_timesteps  | 280608   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000318 |
|    n_updates        | 60151    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.148   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12096    |
|    fps              | 222      |
|    time_elapsed     | 1260     |
|    total_timesteps  | 280754   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000334 |
|    n_updates        | 60188    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.1     |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12100    |
|    fps              | 222      |
|    time_elapsed     | 1261     |
|    total_timesteps  | 280912   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.31e-05 |
|    n_updates        | 60227    |
----------------------------------
Eval num_timesteps=281000, episode_reward=-0.15 +/- 0.26
Episode length: 53.66 +/- 23.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.7     |
|    mean_reward      | -0.154   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 281000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00505  |
|    n_updates        | 60249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.7     |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12104    |
|    fps              | 222      |
|    time_elapsed     | 1263     |
|    total_timesteps  | 281180   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 60294    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.7     |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12108    |
|    fps              | 222      |
|    time_elapsed     | 1263     |
|    total_timesteps  | 281430   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 60357    |
----------------------------------
Eval num_timesteps=281500, episode_reward=-0.06 +/- 0.37
Episode length: 49.08 +/- 23.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.1     |
|    mean_reward      | -0.0556  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 281500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000151 |
|    n_updates        | 60374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52       |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12112    |
|    fps              | 222      |
|    time_elapsed     | 1265     |
|    total_timesteps  | 281681   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00993  |
|    n_updates        | 60420    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.2     |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12116    |
|    fps              | 222      |
|    time_elapsed     | 1265     |
|    total_timesteps  | 281823   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 60455    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.8     |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12120    |
|    fps              | 222      |
|    time_elapsed     | 1265     |
|    total_timesteps  | 281998   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000205 |
|    n_updates        | 60499    |
----------------------------------
Eval num_timesteps=282000, episode_reward=-0.07 +/- 0.35
Episode length: 48.88 +/- 23.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.9     |
|    mean_reward      | -0.0748  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 282000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.6     |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12124    |
|    fps              | 222      |
|    time_elapsed     | 1267     |
|    total_timesteps  | 282145   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000142 |
|    n_updates        | 60536    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.1     |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12128    |
|    fps              | 222      |
|    time_elapsed     | 1267     |
|    total_timesteps  | 282398   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00383  |
|    n_updates        | 60599    |
----------------------------------
Eval num_timesteps=282500, episode_reward=-0.09 +/- 0.32
Episode length: 46.54 +/- 23.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | -0.0854  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 282500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000242 |
|    n_updates        | 60624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.9     |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12132    |
|    fps              | 222      |
|    time_elapsed     | 1269     |
|    total_timesteps  | 282587   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.54e-05 |
|    n_updates        | 60646    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.148   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12136    |
|    fps              | 222      |
|    time_elapsed     | 1269     |
|    total_timesteps  | 282729   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000147 |
|    n_updates        | 60682    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12140    |
|    fps              | 222      |
|    time_elapsed     | 1270     |
|    total_timesteps  | 282969   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 60742    |
----------------------------------
Eval num_timesteps=283000, episode_reward=-0.15 +/- 0.26
Episode length: 52.84 +/- 21.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.8     |
|    mean_reward      | -0.151   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 283000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0109   |
|    n_updates        | 60749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.9     |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12144    |
|    fps              | 222      |
|    time_elapsed     | 1272     |
|    total_timesteps  | 283193   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00592  |
|    n_updates        | 60798    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12148    |
|    fps              | 222      |
|    time_elapsed     | 1272     |
|    total_timesteps  | 283355   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.62e-05 |
|    n_updates        | 60838    |
----------------------------------
Eval num_timesteps=283500, episode_reward=-0.07 +/- 0.37
Episode length: 48.68 +/- 23.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | -0.0741  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 283500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000305 |
|    n_updates        | 60874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49       |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12152    |
|    fps              | 222      |
|    time_elapsed     | 1274     |
|    total_timesteps  | 283585   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000395 |
|    n_updates        | 60896    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12156    |
|    fps              | 222      |
|    time_elapsed     | 1274     |
|    total_timesteps  | 283731   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.55e-05 |
|    n_updates        | 60932    |
----------------------------------
Eval num_timesteps=284000, episode_reward=-0.10 +/- 0.33
Episode length: 51.42 +/- 21.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | -0.105   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 284000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00108  |
|    n_updates        | 60999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.4     |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12160    |
|    fps              | 222      |
|    time_elapsed     | 1276     |
|    total_timesteps  | 284029   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000169 |
|    n_updates        | 61007    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.1     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12164    |
|    fps              | 222      |
|    time_elapsed     | 1276     |
|    total_timesteps  | 284239   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.62e-05 |
|    n_updates        | 61059    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.8     |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12168    |
|    fps              | 222      |
|    time_elapsed     | 1276     |
|    total_timesteps  | 284485   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00016  |
|    n_updates        | 61121    |
----------------------------------
Eval num_timesteps=284500, episode_reward=-0.13 +/- 0.31
Episode length: 53.02 +/- 22.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53       |
|    mean_reward      | -0.131   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 284500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00553  |
|    n_updates        | 61124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.7     |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12172    |
|    fps              | 222      |
|    time_elapsed     | 1278     |
|    total_timesteps  | 284680   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000273 |
|    n_updates        | 61169    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.2     |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12176    |
|    fps              | 222      |
|    time_elapsed     | 1279     |
|    total_timesteps  | 284828   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.85e-05 |
|    n_updates        | 61206    |
----------------------------------
Eval num_timesteps=285000, episode_reward=-0.12 +/- 0.30
Episode length: 50.56 +/- 22.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.6     |
|    mean_reward      | -0.121   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 285000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.18e-05 |
|    n_updates        | 61249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.6     |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12180    |
|    fps              | 222      |
|    time_elapsed     | 1280     |
|    total_timesteps  | 285058   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00433  |
|    n_updates        | 61264    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.7     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12184    |
|    fps              | 222      |
|    time_elapsed     | 1281     |
|    total_timesteps  | 285313   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000256 |
|    n_updates        | 61328    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.9     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12188    |
|    fps              | 222      |
|    time_elapsed     | 1281     |
|    total_timesteps  | 285401   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.03e-05 |
|    n_updates        | 61350    |
----------------------------------
Eval num_timesteps=285500, episode_reward=-0.11 +/- 0.34
Episode length: 52.70 +/- 23.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.7     |
|    mean_reward      | -0.11    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 285500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.02e-05 |
|    n_updates        | 61374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12192    |
|    fps              | 222      |
|    time_elapsed     | 1283     |
|    total_timesteps  | 285574   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.55e-05 |
|    n_updates        | 61393    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12196    |
|    fps              | 222      |
|    time_elapsed     | 1283     |
|    total_timesteps  | 285727   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000276 |
|    n_updates        | 61431    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12200    |
|    fps              | 222      |
|    time_elapsed     | 1283     |
|    total_timesteps  | 285881   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000278 |
|    n_updates        | 61470    |
----------------------------------
Eval num_timesteps=286000, episode_reward=-0.09 +/- 0.36
Episode length: 51.56 +/- 23.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.6     |
|    mean_reward      | -0.0856  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 286000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.78e-05 |
|    n_updates        | 61499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12204    |
|    fps              | 222      |
|    time_elapsed     | 1285     |
|    total_timesteps  | 286025   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00735  |
|    n_updates        | 61506    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.6     |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12208    |
|    fps              | 222      |
|    time_elapsed     | 1285     |
|    total_timesteps  | 286287   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000145 |
|    n_updates        | 61571    |
----------------------------------
Eval num_timesteps=286500, episode_reward=-0.16 +/- 0.18
Episode length: 46.10 +/- 24.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | -0.164   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 286500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.57e-05 |
|    n_updates        | 61624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12212    |
|    fps              | 222      |
|    time_elapsed     | 1287     |
|    total_timesteps  | 286517   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000218 |
|    n_updates        | 61629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12216    |
|    fps              | 222      |
|    time_elapsed     | 1287     |
|    total_timesteps  | 286750   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.87e-05 |
|    n_updates        | 61687    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12220    |
|    fps              | 222      |
|    time_elapsed     | 1288     |
|    total_timesteps  | 286945   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.36e-06 |
|    n_updates        | 61736    |
----------------------------------
Eval num_timesteps=287000, episode_reward=-0.12 +/- 0.34
Episode length: 59.74 +/- 19.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.7     |
|    mean_reward      | -0.118   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 287000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.37e-05 |
|    n_updates        | 61749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12224    |
|    fps              | 222      |
|    time_elapsed     | 1290     |
|    total_timesteps  | 287076   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.91e-05 |
|    n_updates        | 61768    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.1     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12228    |
|    fps              | 222      |
|    time_elapsed     | 1290     |
|    total_timesteps  | 287305   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.73e-05 |
|    n_updates        | 61826    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12232    |
|    fps              | 222      |
|    time_elapsed     | 1290     |
|    total_timesteps  | 287437   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000218 |
|    n_updates        | 61859    |
----------------------------------
Eval num_timesteps=287500, episode_reward=-0.08 +/- 0.34
Episode length: 45.44 +/- 23.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | -0.081   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 287500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.27e-05 |
|    n_updates        | 61874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49       |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12236    |
|    fps              | 222      |
|    time_elapsed     | 1292     |
|    total_timesteps  | 287634   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.13e-05 |
|    n_updates        | 61908    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.6     |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12240    |
|    fps              | 222      |
|    time_elapsed     | 1292     |
|    total_timesteps  | 287931   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 61982    |
----------------------------------
Eval num_timesteps=288000, episode_reward=-0.17 +/- 0.18
Episode length: 47.32 +/- 21.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | -0.169   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.44e-05 |
|    n_updates        | 61999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12244    |
|    fps              | 222      |
|    time_elapsed     | 1294     |
|    total_timesteps  | 288123   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000321 |
|    n_updates        | 62030    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12248    |
|    fps              | 222      |
|    time_elapsed     | 1294     |
|    total_timesteps  | 288281   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00349  |
|    n_updates        | 62070    |
----------------------------------
Eval num_timesteps=288500, episode_reward=-0.05 +/- 0.41
Episode length: 52.54 +/- 23.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.5     |
|    mean_reward      | -0.0495  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 288500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000157 |
|    n_updates        | 62124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.9     |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12252    |
|    fps              | 222      |
|    time_elapsed     | 1296     |
|    total_timesteps  | 288575   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00013  |
|    n_updates        | 62143    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.3     |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12256    |
|    fps              | 222      |
|    time_elapsed     | 1297     |
|    total_timesteps  | 288860   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000189 |
|    n_updates        | 62214    |
----------------------------------
Eval num_timesteps=289000, episode_reward=-0.13 +/- 0.30
Episode length: 51.74 +/- 21.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.7     |
|    mean_reward      | -0.126   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 289000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.82e-05 |
|    n_updates        | 62249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12260    |
|    fps              | 222      |
|    time_elapsed     | 1299     |
|    total_timesteps  | 289072   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.85e-05 |
|    n_updates        | 62267    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.2     |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12264    |
|    fps              | 222      |
|    time_elapsed     | 1299     |
|    total_timesteps  | 289255   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5e-05    |
|    n_updates        | 62313    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12268    |
|    fps              | 222      |
|    time_elapsed     | 1299     |
|    total_timesteps  | 289455   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.4e-05  |
|    n_updates        | 62363    |
----------------------------------
Eval num_timesteps=289500, episode_reward=-0.06 +/- 0.40
Episode length: 54.96 +/- 20.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55       |
|    mean_reward      | -0.0592  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 289500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000114 |
|    n_updates        | 62374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.1     |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12272    |
|    fps              | 222      |
|    time_elapsed     | 1301     |
|    total_timesteps  | 289590   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 62397    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12276    |
|    fps              | 222      |
|    time_elapsed     | 1301     |
|    total_timesteps  | 289823   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 62455    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49       |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12280    |
|    fps              | 222      |
|    time_elapsed     | 1301     |
|    total_timesteps  | 289961   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 62490    |
----------------------------------
Eval num_timesteps=290000, episode_reward=-0.15 +/- 0.26
Episode length: 51.66 +/- 22.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.7     |
|    mean_reward      | -0.146   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000545 |
|    n_updates        | 62499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12284    |
|    fps              | 222      |
|    time_elapsed     | 1303     |
|    total_timesteps  | 290165   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.87e-05 |
|    n_updates        | 62541    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12288    |
|    fps              | 222      |
|    time_elapsed     | 1304     |
|    total_timesteps  | 290347   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00022  |
|    n_updates        | 62586    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.8     |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12292    |
|    fps              | 222      |
|    time_elapsed     | 1304     |
|    total_timesteps  | 290457   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000358 |
|    n_updates        | 62614    |
----------------------------------
Eval num_timesteps=290500, episode_reward=-0.11 +/- 0.32
Episode length: 52.52 +/- 22.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.5     |
|    mean_reward      | -0.109   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 290500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 62624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12296    |
|    fps              | 222      |
|    time_elapsed     | 1306     |
|    total_timesteps  | 290676   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000378 |
|    n_updates        | 62668    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12300    |
|    fps              | 222      |
|    time_elapsed     | 1306     |
|    total_timesteps  | 290807   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.77e-05 |
|    n_updates        | 62701    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12304    |
|    fps              | 222      |
|    time_elapsed     | 1306     |
|    total_timesteps  | 290995   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.12e-05 |
|    n_updates        | 62748    |
----------------------------------
Eval num_timesteps=291000, episode_reward=-0.14 +/- 0.29
Episode length: 55.12 +/- 22.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.1     |
|    mean_reward      | -0.14    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 291000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.89e-05 |
|    n_updates        | 62749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12308    |
|    fps              | 222      |
|    time_elapsed     | 1308     |
|    total_timesteps  | 291242   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.78e-05 |
|    n_updates        | 62810    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12312    |
|    fps              | 222      |
|    time_elapsed     | 1308     |
|    total_timesteps  | 291462   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000162 |
|    n_updates        | 62865    |
----------------------------------
Eval num_timesteps=291500, episode_reward=-0.12 +/- 0.31
Episode length: 50.06 +/- 24.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.1     |
|    mean_reward      | -0.12    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 291500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.55e-05 |
|    n_updates        | 62874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.4     |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12316    |
|    fps              | 222      |
|    time_elapsed     | 1310     |
|    total_timesteps  | 291687   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.38e-05 |
|    n_updates        | 62921    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12320    |
|    fps              | 222      |
|    time_elapsed     | 1311     |
|    total_timesteps  | 291900   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000742 |
|    n_updates        | 62974    |
----------------------------------
Eval num_timesteps=292000, episode_reward=-0.11 +/- 0.30
Episode length: 48.34 +/- 22.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | -0.113   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 292000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00038  |
|    n_updates        | 62999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.6     |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12324    |
|    fps              | 222      |
|    time_elapsed     | 1313     |
|    total_timesteps  | 292141   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000314 |
|    n_updates        | 63035    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12328    |
|    fps              | 222      |
|    time_elapsed     | 1313     |
|    total_timesteps  | 292335   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.52e-05 |
|    n_updates        | 63083    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12332    |
|    fps              | 222      |
|    time_elapsed     | 1313     |
|    total_timesteps  | 292477   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.25e-05 |
|    n_updates        | 63119    |
----------------------------------
Eval num_timesteps=292500, episode_reward=-0.12 +/- 0.30
Episode length: 49.12 +/- 22.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.1     |
|    mean_reward      | -0.116   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 292500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000193 |
|    n_updates        | 63124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.5     |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12336    |
|    fps              | 222      |
|    time_elapsed     | 1315     |
|    total_timesteps  | 292684   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.24e-05 |
|    n_updates        | 63170    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.1     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12340    |
|    fps              | 222      |
|    time_elapsed     | 1315     |
|    total_timesteps  | 292937   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.12e-05 |
|    n_updates        | 63234    |
----------------------------------
Eval num_timesteps=293000, episode_reward=-0.15 +/- 0.27
Episode length: 52.06 +/- 22.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.1     |
|    mean_reward      | -0.148   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.08e-05 |
|    n_updates        | 63249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.6     |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12344    |
|    fps              | 222      |
|    time_elapsed     | 1317     |
|    total_timesteps  | 293086   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000498 |
|    n_updates        | 63271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12348    |
|    fps              | 222      |
|    time_elapsed     | 1317     |
|    total_timesteps  | 293313   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.05e-05 |
|    n_updates        | 63328    |
----------------------------------
Eval num_timesteps=293500, episode_reward=-0.14 +/- 0.29
Episode length: 54.02 +/- 21.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54       |
|    mean_reward      | -0.135   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00672  |
|    n_updates        | 63374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.6     |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12352    |
|    fps              | 222      |
|    time_elapsed     | 1319     |
|    total_timesteps  | 293532   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000276 |
|    n_updates        | 63382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12356    |
|    fps              | 222      |
|    time_elapsed     | 1319     |
|    total_timesteps  | 293700   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.46e-05 |
|    n_updates        | 63424    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12360    |
|    fps              | 222      |
|    time_elapsed     | 1320     |
|    total_timesteps  | 293909   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.83e-05 |
|    n_updates        | 63477    |
----------------------------------
Eval num_timesteps=294000, episode_reward=-0.09 +/- 0.34
Episode length: 47.84 +/- 23.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.8     |
|    mean_reward      | -0.0906  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 294000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.6e-05  |
|    n_updates        | 63499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12364    |
|    fps              | 222      |
|    time_elapsed     | 1322     |
|    total_timesteps  | 294181   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000336 |
|    n_updates        | 63545    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.1     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12368    |
|    fps              | 222      |
|    time_elapsed     | 1322     |
|    total_timesteps  | 294367   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 63591    |
----------------------------------
Eval num_timesteps=294500, episode_reward=-0.15 +/- 0.17
Episode length: 42.22 +/- 22.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | -0.148   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 294500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000329 |
|    n_updates        | 63624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12372    |
|    fps              | 222      |
|    time_elapsed     | 1323     |
|    total_timesteps  | 294593   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000178 |
|    n_updates        | 63648    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12376    |
|    fps              | 222      |
|    time_elapsed     | 1324     |
|    total_timesteps  | 294852   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.07e-05 |
|    n_updates        | 63712    |
----------------------------------
Eval num_timesteps=295000, episode_reward=-0.12 +/- 0.31
Episode length: 50.64 +/- 22.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.6     |
|    mean_reward      | -0.122   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 295000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000156 |
|    n_updates        | 63749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.9     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12380    |
|    fps              | 222      |
|    time_elapsed     | 1326     |
|    total_timesteps  | 295047   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.91e-05 |
|    n_updates        | 63761    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.8     |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12384    |
|    fps              | 222      |
|    time_elapsed     | 1326     |
|    total_timesteps  | 295242   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 63810    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12388    |
|    fps              | 222      |
|    time_elapsed     | 1326     |
|    total_timesteps  | 295381   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000215 |
|    n_updates        | 63845    |
----------------------------------
Eval num_timesteps=295500, episode_reward=-0.06 +/- 0.40
Episode length: 54.06 +/- 19.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.1     |
|    mean_reward      | -0.0555  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 295500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000157 |
|    n_updates        | 63874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.1     |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12392    |
|    fps              | 222      |
|    time_elapsed     | 1328     |
|    total_timesteps  | 295572   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.87e-05 |
|    n_updates        | 63892    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.6     |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12396    |
|    fps              | 222      |
|    time_elapsed     | 1328     |
|    total_timesteps  | 295736   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000187 |
|    n_updates        | 63933    |
----------------------------------
Eval num_timesteps=296000, episode_reward=-0.12 +/- 0.28
Episode length: 50.12 +/- 22.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.1     |
|    mean_reward      | -0.12    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 296000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000133 |
|    n_updates        | 63999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52       |
|    ep_rew_mean      | -0.147   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12400    |
|    fps              | 222      |
|    time_elapsed     | 1330     |
|    total_timesteps  | 296003   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.81e-05 |
|    n_updates        | 64000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12404    |
|    fps              | 222      |
|    time_elapsed     | 1330     |
|    total_timesteps  | 296160   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.1e-05  |
|    n_updates        | 64039    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.8     |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12408    |
|    fps              | 222      |
|    time_elapsed     | 1331     |
|    total_timesteps  | 296420   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 64104    |
----------------------------------
Eval num_timesteps=296500, episode_reward=-0.11 +/- 0.33
Episode length: 53.60 +/- 22.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.6     |
|    mean_reward      | -0.114   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 296500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00708  |
|    n_updates        | 64124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.3     |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12412    |
|    fps              | 222      |
|    time_elapsed     | 1333     |
|    total_timesteps  | 296589   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.83e-05 |
|    n_updates        | 64147    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12416    |
|    fps              | 222      |
|    time_elapsed     | 1333     |
|    total_timesteps  | 296846   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 64211    |
----------------------------------
Eval num_timesteps=297000, episode_reward=-0.08 +/- 0.36
Episode length: 50.68 +/- 22.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.7     |
|    mean_reward      | -0.082   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 64249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.7     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12420    |
|    fps              | 222      |
|    time_elapsed     | 1335     |
|    total_timesteps  | 297070   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000509 |
|    n_updates        | 64267    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.2     |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12424    |
|    fps              | 222      |
|    time_elapsed     | 1335     |
|    total_timesteps  | 297257   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000147 |
|    n_updates        | 64314    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12428    |
|    fps              | 222      |
|    time_elapsed     | 1335     |
|    total_timesteps  | 297498   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00032  |
|    n_updates        | 64374    |
----------------------------------
Eval num_timesteps=297500, episode_reward=-0.09 +/- 0.35
Episode length: 52.28 +/- 21.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.3     |
|    mean_reward      | -0.0883  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 297500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.7     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12432    |
|    fps              | 222      |
|    time_elapsed     | 1337     |
|    total_timesteps  | 297647   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.39e-05 |
|    n_updates        | 64411    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12436    |
|    fps              | 222      |
|    time_elapsed     | 1337     |
|    total_timesteps  | 297843   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.43e-05 |
|    n_updates        | 64460    |
----------------------------------
Eval num_timesteps=298000, episode_reward=-0.11 +/- 0.29
Episode length: 48.56 +/- 23.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.6     |
|    mean_reward      | -0.114   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 298000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000185 |
|    n_updates        | 64499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.9     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12440    |
|    fps              | 222      |
|    time_elapsed     | 1339     |
|    total_timesteps  | 298027   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00959  |
|    n_updates        | 64506    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52       |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12444    |
|    fps              | 222      |
|    time_elapsed     | 1340     |
|    total_timesteps  | 298281   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.25e-05 |
|    n_updates        | 64570    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12448    |
|    fps              | 222      |
|    time_elapsed     | 1340     |
|    total_timesteps  | 298470   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 64617    |
----------------------------------
Eval num_timesteps=298500, episode_reward=-0.16 +/- 0.23
Episode length: 50.46 +/- 23.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.5     |
|    mean_reward      | -0.161   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 298500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.12e-05 |
|    n_updates        | 64624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.9     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12452    |
|    fps              | 222      |
|    time_elapsed     | 1342     |
|    total_timesteps  | 298622   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.43e-05 |
|    n_updates        | 64655    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.9     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12456    |
|    fps              | 222      |
|    time_elapsed     | 1342     |
|    total_timesteps  | 298789   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 64697    |
----------------------------------
Eval num_timesteps=299000, episode_reward=-0.08 +/- 0.33
Episode length: 46.06 +/- 24.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | -0.0834  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 299000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000303 |
|    n_updates        | 64749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.5     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12460    |
|    fps              | 222      |
|    time_elapsed     | 1344     |
|    total_timesteps  | 299054   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000293 |
|    n_updates        | 64763    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12464    |
|    fps              | 222      |
|    time_elapsed     | 1344     |
|    total_timesteps  | 299220   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00517  |
|    n_updates        | 64804    |
----------------------------------
Eval num_timesteps=299500, episode_reward=-0.06 +/- 0.38
Episode length: 49.90 +/- 22.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.9     |
|    mean_reward      | -0.0588  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 299500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000311 |
|    n_updates        | 64874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.5     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12468    |
|    fps              | 222      |
|    time_elapsed     | 1346     |
|    total_timesteps  | 299516   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000132 |
|    n_updates        | 64878    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12472    |
|    fps              | 222      |
|    time_elapsed     | 1346     |
|    total_timesteps  | 299631   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000293 |
|    n_updates        | 64907    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.4     |
|    ep_rew_mean      | -0.0967  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12476    |
|    fps              | 222      |
|    time_elapsed     | 1346     |
|    total_timesteps  | 299790   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000265 |
|    n_updates        | 64947    |
----------------------------------
Eval num_timesteps=300000, episode_reward=-0.06 +/- 0.40
Episode length: 54.60 +/- 22.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.6     |
|    mean_reward      | -0.0577  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 300000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.65e-05 |
|    n_updates        | 64999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12480    |
|    fps              | 222      |
|    time_elapsed     | 1348     |
|    total_timesteps  | 300043   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00112  |
|    n_updates        | 65010    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12484    |
|    fps              | 222      |
|    time_elapsed     | 1349     |
|    total_timesteps  | 300244   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00359  |
|    n_updates        | 65060    |
----------------------------------
Eval num_timesteps=300500, episode_reward=-0.18 +/- 0.18
Episode length: 50.50 +/- 23.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.5     |
|    mean_reward      | -0.181   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 300500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000354 |
|    n_updates        | 65124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12488    |
|    fps              | 222      |
|    time_elapsed     | 1351     |
|    total_timesteps  | 300544   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.03e-05 |
|    n_updates        | 65135    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.2     |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12492    |
|    fps              | 222      |
|    time_elapsed     | 1351     |
|    total_timesteps  | 300691   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.14e-05 |
|    n_updates        | 65172    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.5     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12496    |
|    fps              | 222      |
|    time_elapsed     | 1351     |
|    total_timesteps  | 300891   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00424  |
|    n_updates        | 65222    |
----------------------------------
Eval num_timesteps=301000, episode_reward=-0.21 +/- 0.18
Episode length: 58.86 +/- 23.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.9     |
|    mean_reward      | -0.215   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 301000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000647 |
|    n_updates        | 65249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.8     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12500    |
|    fps              | 222      |
|    time_elapsed     | 1353     |
|    total_timesteps  | 301086   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000378 |
|    n_updates        | 65271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.8     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12504    |
|    fps              | 222      |
|    time_elapsed     | 1353     |
|    total_timesteps  | 301335   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00425  |
|    n_updates        | 65333    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.5     |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12508    |
|    fps              | 222      |
|    time_elapsed     | 1353     |
|    total_timesteps  | 301473   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0002   |
|    n_updates        | 65368    |
----------------------------------
Eval num_timesteps=301500, episode_reward=-0.01 +/- 0.44
Episode length: 53.92 +/- 20.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.9     |
|    mean_reward      | -0.015   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 301500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000781 |
|    n_updates        | 65374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.5     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12512    |
|    fps              | 222      |
|    time_elapsed     | 1356     |
|    total_timesteps  | 301742   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.23e-05 |
|    n_updates        | 65435    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.1     |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12516    |
|    fps              | 222      |
|    time_elapsed     | 1356     |
|    total_timesteps  | 301954   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000198 |
|    n_updates        | 65488    |
----------------------------------
Eval num_timesteps=302000, episode_reward=-0.08 +/- 0.33
Episode length: 44.00 +/- 21.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | -0.0752  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 302000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000234 |
|    n_updates        | 65499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.7     |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12520    |
|    fps              | 222      |
|    time_elapsed     | 1358     |
|    total_timesteps  | 302140   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.55e-05 |
|    n_updates        | 65534    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.3     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12524    |
|    fps              | 222      |
|    time_elapsed     | 1358     |
|    total_timesteps  | 302389   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.2e-05  |
|    n_updates        | 65597    |
----------------------------------
Eval num_timesteps=302500, episode_reward=-0.18 +/- 0.22
Episode length: 54.46 +/- 23.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.5     |
|    mean_reward      | -0.177   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 302500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000284 |
|    n_updates        | 65624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.4     |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12528    |
|    fps              | 222      |
|    time_elapsed     | 1360     |
|    total_timesteps  | 302637   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.63e-05 |
|    n_updates        | 65659    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.4     |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12532    |
|    fps              | 222      |
|    time_elapsed     | 1360     |
|    total_timesteps  | 302890   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00661  |
|    n_updates        | 65722    |
----------------------------------
Eval num_timesteps=303000, episode_reward=-0.06 +/- 0.38
Episode length: 51.04 +/- 21.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51       |
|    mean_reward      | -0.0635  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 303000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000309 |
|    n_updates        | 65749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12536    |
|    fps              | 222      |
|    time_elapsed     | 1362     |
|    total_timesteps  | 303000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.8     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12540    |
|    fps              | 222      |
|    time_elapsed     | 1362     |
|    total_timesteps  | 303203   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.01e-05 |
|    n_updates        | 65800    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.7     |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12544    |
|    fps              | 222      |
|    time_elapsed     | 1362     |
|    total_timesteps  | 303355   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.21e-05 |
|    n_updates        | 65838    |
----------------------------------
Eval num_timesteps=303500, episode_reward=-0.05 +/- 0.39
Episode length: 48.06 +/- 23.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | -0.0515  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 303500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000299 |
|    n_updates        | 65874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.7     |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12548    |
|    fps              | 222      |
|    time_elapsed     | 1364     |
|    total_timesteps  | 303543   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.79e-05 |
|    n_updates        | 65885    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.3     |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12552    |
|    fps              | 222      |
|    time_elapsed     | 1364     |
|    total_timesteps  | 303750   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000334 |
|    n_updates        | 65937    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51       |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12556    |
|    fps              | 222      |
|    time_elapsed     | 1365     |
|    total_timesteps  | 303893   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 65973    |
----------------------------------
Eval num_timesteps=304000, episode_reward=-0.14 +/- 0.30
Episode length: 55.20 +/- 23.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.2     |
|    mean_reward      | -0.14    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 304000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.42e-05 |
|    n_updates        | 65999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.0909  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12560    |
|    fps              | 222      |
|    time_elapsed     | 1367     |
|    total_timesteps  | 304094   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00423  |
|    n_updates        | 66023    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.4     |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12564    |
|    fps              | 222      |
|    time_elapsed     | 1367     |
|    total_timesteps  | 304356   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.24e-05 |
|    n_updates        | 66088    |
----------------------------------
Eval num_timesteps=304500, episode_reward=-0.09 +/- 0.32
Episode length: 48.06 +/- 23.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | -0.0915  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 304500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.14e-05 |
|    n_updates        | 66124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51       |
|    ep_rew_mean      | -0.0934  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12568    |
|    fps              | 222      |
|    time_elapsed     | 1369     |
|    total_timesteps  | 304618   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 66154    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.7     |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12572    |
|    fps              | 222      |
|    time_elapsed     | 1369     |
|    total_timesteps  | 304803   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.99e-05 |
|    n_updates        | 66200    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.4     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12576    |
|    fps              | 222      |
|    time_elapsed     | 1369     |
|    total_timesteps  | 304930   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.04e-05 |
|    n_updates        | 66232    |
----------------------------------
Eval num_timesteps=305000, episode_reward=-0.06 +/- 0.37
Episode length: 49.66 +/- 22.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | -0.0579  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 305000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000146 |
|    n_updates        | 66249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.4     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12580    |
|    fps              | 222      |
|    time_elapsed     | 1371     |
|    total_timesteps  | 305180   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00538  |
|    n_updates        | 66294    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51       |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12584    |
|    fps              | 222      |
|    time_elapsed     | 1371     |
|    total_timesteps  | 305342   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000267 |
|    n_updates        | 66335    |
----------------------------------
Eval num_timesteps=305500, episode_reward=-0.14 +/- 0.29
Episode length: 50.98 +/- 24.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51       |
|    mean_reward      | -0.143   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 305500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000157 |
|    n_updates        | 66374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.0881  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12588    |
|    fps              | 222      |
|    time_elapsed     | 1373     |
|    total_timesteps  | 305515   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000191 |
|    n_updates        | 66378    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.9     |
|    ep_rew_mean      | -0.0789  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12592    |
|    fps              | 222      |
|    time_elapsed     | 1373     |
|    total_timesteps  | 305681   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000749 |
|    n_updates        | 66420    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.8     |
|    ep_rew_mean      | -0.0684  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12596    |
|    fps              | 222      |
|    time_elapsed     | 1374     |
|    total_timesteps  | 305869   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000714 |
|    n_updates        | 66467    |
----------------------------------
Eval num_timesteps=306000, episode_reward=-0.17 +/- 0.23
Episode length: 51.74 +/- 22.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.7     |
|    mean_reward      | -0.166   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 306000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000112 |
|    n_updates        | 66499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.0665  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12600    |
|    fps              | 222      |
|    time_elapsed     | 1376     |
|    total_timesteps  | 306017   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.92e-05 |
|    n_updates        | 66504    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.0763  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12604    |
|    fps              | 222      |
|    time_elapsed     | 1376     |
|    total_timesteps  | 306259   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.77e-05 |
|    n_updates        | 66564    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.4     |
|    ep_rew_mean      | -0.077   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12608    |
|    fps              | 222      |
|    time_elapsed     | 1376     |
|    total_timesteps  | 306416   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000208 |
|    n_updates        | 66603    |
----------------------------------
Eval num_timesteps=306500, episode_reward=-0.11 +/- 0.33
Episode length: 52.94 +/- 22.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | -0.111   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 306500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.88e-05 |
|    n_updates        | 66624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.6     |
|    ep_rew_mean      | -0.0738  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12612    |
|    fps              | 222      |
|    time_elapsed     | 1378     |
|    total_timesteps  | 306603   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000116 |
|    n_updates        | 66650    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48       |
|    ep_rew_mean      | -0.0715  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12616    |
|    fps              | 222      |
|    time_elapsed     | 1378     |
|    total_timesteps  | 306759   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00046  |
|    n_updates        | 66689    |
----------------------------------
Eval num_timesteps=307000, episode_reward=-0.13 +/- 0.25
Episode length: 46.48 +/- 22.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | -0.125   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 307000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.78e-05 |
|    n_updates        | 66749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.0861  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12620    |
|    fps              | 222      |
|    time_elapsed     | 1380     |
|    total_timesteps  | 307059   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00544  |
|    n_updates        | 66764    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.8     |
|    ep_rew_mean      | -0.0844  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12624    |
|    fps              | 222      |
|    time_elapsed     | 1380     |
|    total_timesteps  | 307265   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000377 |
|    n_updates        | 66816    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.2     |
|    ep_rew_mean      | -0.0721  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12628    |
|    fps              | 222      |
|    time_elapsed     | 1381     |
|    total_timesteps  | 307457   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00454  |
|    n_updates        | 66864    |
----------------------------------
Eval num_timesteps=307500, episode_reward=-0.12 +/- 0.30
Episode length: 50.42 +/- 20.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.4     |
|    mean_reward      | -0.121   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 307500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000105 |
|    n_updates        | 66874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.1     |
|    ep_rew_mean      | -0.0717  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12632    |
|    fps              | 222      |
|    time_elapsed     | 1383     |
|    total_timesteps  | 307698   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000191 |
|    n_updates        | 66924    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.0763  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12636    |
|    fps              | 222      |
|    time_elapsed     | 1383     |
|    total_timesteps  | 307924   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000237 |
|    n_updates        | 66980    |
----------------------------------
Eval num_timesteps=308000, episode_reward=-0.13 +/- 0.27
Episode length: 47.12 +/- 23.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.1     |
|    mean_reward      | -0.128   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 308000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 66999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.0762  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12640    |
|    fps              | 222      |
|    time_elapsed     | 1385     |
|    total_timesteps  | 308124   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.15e-05 |
|    n_updates        | 67030    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.6     |
|    ep_rew_mean      | -0.082   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12644    |
|    fps              | 222      |
|    time_elapsed     | 1385     |
|    total_timesteps  | 308420   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0003   |
|    n_updates        | 67104    |
----------------------------------
Eval num_timesteps=308500, episode_reward=-0.01 +/- 0.42
Episode length: 47.86 +/- 22.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.9     |
|    mean_reward      | -0.0105  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 308500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00011  |
|    n_updates        | 67124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.0911  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12648    |
|    fps              | 222      |
|    time_elapsed     | 1387     |
|    total_timesteps  | 308585   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000237 |
|    n_updates        | 67146    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.0609  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12652    |
|    fps              | 222      |
|    time_elapsed     | 1387     |
|    total_timesteps  | 308787   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 67196    |
----------------------------------
Eval num_timesteps=309000, episode_reward=-0.09 +/- 0.34
Episode length: 53.78 +/- 20.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.8     |
|    mean_reward      | -0.0943  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 309000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000171 |
|    n_updates        | 67249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.3     |
|    ep_rew_mean      | -0.0545  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12656    |
|    fps              | 222      |
|    time_elapsed     | 1389     |
|    total_timesteps  | 309021   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000255 |
|    n_updates        | 67255    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.9     |
|    ep_rew_mean      | -0.0768  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12660    |
|    fps              | 222      |
|    time_elapsed     | 1389     |
|    total_timesteps  | 309279   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00535  |
|    n_updates        | 67319    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.0694  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12664    |
|    fps              | 222      |
|    time_elapsed     | 1389     |
|    total_timesteps  | 309355   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000178 |
|    n_updates        | 67338    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.6     |
|    ep_rew_mean      | -0.0636  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12668    |
|    fps              | 222      |
|    time_elapsed     | 1390     |
|    total_timesteps  | 309475   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 67368    |
----------------------------------
Eval num_timesteps=309500, episode_reward=-0.13 +/- 0.33
Episode length: 57.98 +/- 21.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58       |
|    mean_reward      | -0.131   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 309500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00023  |
|    n_updates        | 67374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.1     |
|    ep_rew_mean      | -0.0557  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12672    |
|    fps              | 222      |
|    time_elapsed     | 1392     |
|    total_timesteps  | 309711   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000583 |
|    n_updates        | 67427    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.0462  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12676    |
|    fps              | 222      |
|    time_elapsed     | 1392     |
|    total_timesteps  | 309852   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000624 |
|    n_updates        | 67462    |
----------------------------------
Eval num_timesteps=310000, episode_reward=-0.15 +/- 0.30
Episode length: 57.46 +/- 22.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.5     |
|    mean_reward      | -0.149   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 310000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00602  |
|    n_updates        | 67499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.0361  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12680    |
|    fps              | 222      |
|    time_elapsed     | 1394     |
|    total_timesteps  | 310100   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 67524    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.4     |
|    ep_rew_mean      | -0.0371  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12684    |
|    fps              | 222      |
|    time_elapsed     | 1394     |
|    total_timesteps  | 310286   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000236 |
|    n_updates        | 67571    |
----------------------------------
Eval num_timesteps=310500, episode_reward=-0.11 +/- 0.32
Episode length: 52.22 +/- 22.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.2     |
|    mean_reward      | -0.108   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 310500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000904 |
|    n_updates        | 67624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.2     |
|    ep_rew_mean      | -0.04    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12688    |
|    fps              | 222      |
|    time_elapsed     | 1396     |
|    total_timesteps  | 310531   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00046  |
|    n_updates        | 67632    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.0393  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12692    |
|    fps              | 222      |
|    time_elapsed     | 1396     |
|    total_timesteps  | 310679   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 67669    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.0461  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12696    |
|    fps              | 222      |
|    time_elapsed     | 1397     |
|    total_timesteps  | 310788   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000469 |
|    n_updates        | 67696    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.3     |
|    ep_rew_mean      | -0.0267  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12700    |
|    fps              | 222      |
|    time_elapsed     | 1397     |
|    total_timesteps  | 310950   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000256 |
|    n_updates        | 67737    |
----------------------------------
Eval num_timesteps=311000, episode_reward=-0.11 +/- 0.34
Episode length: 51.78 +/- 22.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.8     |
|    mean_reward      | -0.106   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 311000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.54e-05 |
|    n_updates        | 67749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.2     |
|    ep_rew_mean      | -0.0262  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12704    |
|    fps              | 222      |
|    time_elapsed     | 1399     |
|    total_timesteps  | 311181   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000325 |
|    n_updates        | 67795    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.0305  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12708    |
|    fps              | 222      |
|    time_elapsed     | 1399     |
|    total_timesteps  | 311446   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.82e-05 |
|    n_updates        | 67861    |
----------------------------------
Eval num_timesteps=311500, episode_reward=-0.06 +/- 0.38
Episode length: 50.34 +/- 22.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.3     |
|    mean_reward      | -0.0606  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 311500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000349 |
|    n_updates        | 67874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.6     |
|    ep_rew_mean      | -0.0419  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12712    |
|    fps              | 222      |
|    time_elapsed     | 1401     |
|    total_timesteps  | 311667   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000252 |
|    n_updates        | 67916    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.3     |
|    ep_rew_mean      | -0.0246  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12716    |
|    fps              | 222      |
|    time_elapsed     | 1401     |
|    total_timesteps  | 311891   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0002   |
|    n_updates        | 67972    |
----------------------------------
Eval num_timesteps=312000, episode_reward=-0.18 +/- 0.23
Episode length: 55.34 +/- 21.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.3     |
|    mean_reward      | -0.181   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 312000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00552  |
|    n_updates        | 67999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.8     |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12720    |
|    fps              | 222      |
|    time_elapsed     | 1403     |
|    total_timesteps  | 312141   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00399  |
|    n_updates        | 68035    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.4     |
|    ep_rew_mean      | -0.0011  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12724    |
|    fps              | 222      |
|    time_elapsed     | 1404     |
|    total_timesteps  | 312309   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000141 |
|    n_updates        | 68077    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | 0.00187  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12728    |
|    fps              | 222      |
|    time_elapsed     | 1404     |
|    total_timesteps  | 312427   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00102  |
|    n_updates        | 68106    |
----------------------------------
Eval num_timesteps=312500, episode_reward=-0.07 +/- 0.39
Episode length: 52.34 +/- 20.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.3     |
|    mean_reward      | -0.0686  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 312500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.31e-05 |
|    n_updates        | 68124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.9     |
|    ep_rew_mean      | 0.0252   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12732    |
|    fps              | 222      |
|    time_elapsed     | 1406     |
|    total_timesteps  | 312585   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00062  |
|    n_updates        | 68146    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | 0.0166   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12736    |
|    fps              | 222      |
|    time_elapsed     | 1406     |
|    total_timesteps  | 312778   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000294 |
|    n_updates        | 68194    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | 0.00661  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12740    |
|    fps              | 222      |
|    time_elapsed     | 1406     |
|    total_timesteps  | 312977   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000277 |
|    n_updates        | 68244    |
----------------------------------
Eval num_timesteps=313000, episode_reward=-0.00 +/- 0.45
Episode length: 51.08 +/- 23.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.1     |
|    mean_reward      | -0.00356 |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 313000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000267 |
|    n_updates        | 68249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.2     |
|    ep_rew_mean      | 0.00789  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12744    |
|    fps              | 222      |
|    time_elapsed     | 1408     |
|    total_timesteps  | 313241   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000102 |
|    n_updates        | 68310    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.7     |
|    ep_rew_mean      | 0.0159   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12748    |
|    fps              | 222      |
|    time_elapsed     | 1408     |
|    total_timesteps  | 313456   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000166 |
|    n_updates        | 68363    |
----------------------------------
Eval num_timesteps=313500, episode_reward=-0.14 +/- 0.27
Episode length: 50.06 +/- 24.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.1     |
|    mean_reward      | -0.14    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 313500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00454  |
|    n_updates        | 68374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.5     |
|    ep_rew_mean      | -0.00322 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12752    |
|    fps              | 222      |
|    time_elapsed     | 1410     |
|    total_timesteps  | 313636   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000516 |
|    n_updates        | 68408    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.7     |
|    ep_rew_mean      | -0.0102  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12756    |
|    fps              | 222      |
|    time_elapsed     | 1410     |
|    total_timesteps  | 313795   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8e-05    |
|    n_updates        | 68448    |
----------------------------------
Eval num_timesteps=314000, episode_reward=-0.03 +/- 0.39
Episode length: 46.70 +/- 21.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | -0.0259  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 314000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00341  |
|    n_updates        | 68499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.3     |
|    ep_rew_mean      | 0.00175  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12760    |
|    fps              | 222      |
|    time_elapsed     | 1412     |
|    total_timesteps  | 314005   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0036   |
|    n_updates        | 68501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.8     |
|    ep_rew_mean      | -0.00454 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12764    |
|    fps              | 222      |
|    time_elapsed     | 1412     |
|    total_timesteps  | 314238   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 68559    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | 0.00212  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12768    |
|    fps              | 222      |
|    time_elapsed     | 1413     |
|    total_timesteps  | 314441   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000188 |
|    n_updates        | 68610    |
----------------------------------
Eval num_timesteps=314500, episode_reward=-0.20 +/- 0.17
Episode length: 54.82 +/- 23.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.8     |
|    mean_reward      | -0.199   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 314500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.08e-05 |
|    n_updates        | 68624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | -0.00736 |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12772    |
|    fps              | 222      |
|    time_elapsed     | 1415     |
|    total_timesteps  | 314664   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.48e-05 |
|    n_updates        | 68665    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.0192  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12776    |
|    fps              | 222      |
|    time_elapsed     | 1415     |
|    total_timesteps  | 314851   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000262 |
|    n_updates        | 68712    |
----------------------------------
Eval num_timesteps=315000, episode_reward=-0.06 +/- 0.40
Episode length: 55.18 +/- 20.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.2     |
|    mean_reward      | -0.0601  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 315000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000147 |
|    n_updates        | 68749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.6     |
|    ep_rew_mean      | -0.0277  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12780    |
|    fps              | 222      |
|    time_elapsed     | 1417     |
|    total_timesteps  | 315063   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00425  |
|    n_updates        | 68765    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | -0.037   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12784    |
|    fps              | 222      |
|    time_elapsed     | 1417     |
|    total_timesteps  | 315232   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00294  |
|    n_updates        | 68807    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49       |
|    ep_rew_mean      | -0.0452  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12788    |
|    fps              | 222      |
|    time_elapsed     | 1418     |
|    total_timesteps  | 315431   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000394 |
|    n_updates        | 68857    |
----------------------------------
Eval num_timesteps=315500, episode_reward=-0.10 +/- 0.29
Episode length: 45.20 +/- 22.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | -0.1     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 315500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000164 |
|    n_updates        | 68874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.0527  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12792    |
|    fps              | 222      |
|    time_elapsed     | 1419     |
|    total_timesteps  | 315516   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000165 |
|    n_updates        | 68878    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.058   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12796    |
|    fps              | 222      |
|    time_elapsed     | 1419     |
|    total_timesteps  | 315758   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00031  |
|    n_updates        | 68939    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.6     |
|    ep_rew_mean      | -0.0577  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12800    |
|    fps              | 222      |
|    time_elapsed     | 1420     |
|    total_timesteps  | 315911   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00288  |
|    n_updates        | 68977    |
----------------------------------
Eval num_timesteps=316000, episode_reward=-0.11 +/- 0.34
Episode length: 58.12 +/- 19.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.1     |
|    mean_reward      | -0.112   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 316000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000281 |
|    n_updates        | 68999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.8     |
|    ep_rew_mean      | -0.0582  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12804    |
|    fps              | 222      |
|    time_elapsed     | 1422     |
|    total_timesteps  | 316156   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00411  |
|    n_updates        | 69038    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.5     |
|    ep_rew_mean      | -0.0472  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12808    |
|    fps              | 222      |
|    time_elapsed     | 1422     |
|    total_timesteps  | 316397   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000427 |
|    n_updates        | 69099    |
----------------------------------
Eval num_timesteps=316500, episode_reward=-0.11 +/- 0.33
Episode length: 52.20 +/- 22.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.2     |
|    mean_reward      | -0.108   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 316500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000388 |
|    n_updates        | 69124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.8     |
|    ep_rew_mean      | -0.0444  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12812    |
|    fps              | 222      |
|    time_elapsed     | 1424     |
|    total_timesteps  | 316548   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000289 |
|    n_updates        | 69136    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.4     |
|    ep_rew_mean      | -0.063   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12816    |
|    fps              | 222      |
|    time_elapsed     | 1424     |
|    total_timesteps  | 316735   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00401  |
|    n_updates        | 69183    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 47.9     |
|    ep_rew_mean      | -0.0409  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12820    |
|    fps              | 222      |
|    time_elapsed     | 1424     |
|    total_timesteps  | 316935   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000308 |
|    n_updates        | 69233    |
----------------------------------
Eval num_timesteps=317000, episode_reward=-0.05 +/- 0.40
Episode length: 53.10 +/- 22.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.1     |
|    mean_reward      | -0.0517  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 317000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000317 |
|    n_updates        | 69249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 48.1     |
|    ep_rew_mean      | -0.0617  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12824    |
|    fps              | 222      |
|    time_elapsed     | 1426     |
|    total_timesteps  | 317124   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.9e-05  |
|    n_updates        | 69280    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50       |
|    ep_rew_mean      | -0.079   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12828    |
|    fps              | 222      |
|    time_elapsed     | 1427     |
|    total_timesteps  | 317424   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00094  |
|    n_updates        | 69355    |
----------------------------------
Eval num_timesteps=317500, episode_reward=0.01 +/- 0.41
Episode length: 42.72 +/- 22.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 0.0101   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 317500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000212 |
|    n_updates        | 69374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.8     |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12832    |
|    fps              | 222      |
|    time_elapsed     | 1429     |
|    total_timesteps  | 317660   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000202 |
|    n_updates        | 69414    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.8     |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12836    |
|    fps              | 222      |
|    time_elapsed     | 1429     |
|    total_timesteps  | 317960   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000896 |
|    n_updates        | 69489    |
----------------------------------
Eval num_timesteps=318000, episode_reward=-0.04 +/- 0.40
Episode length: 51.36 +/- 23.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.4     |
|    mean_reward      | -0.0447  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 318000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00041  |
|    n_updates        | 69499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | -0.0956  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12840    |
|    fps              | 222      |
|    time_elapsed     | 1431     |
|    total_timesteps  | 318137   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000256 |
|    n_updates        | 69534    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.5     |
|    ep_rew_mean      | -0.0951  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12844    |
|    fps              | 222      |
|    time_elapsed     | 1431     |
|    total_timesteps  | 318388   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000149 |
|    n_updates        | 69596    |
----------------------------------
Eval num_timesteps=318500, episode_reward=-0.07 +/- 0.35
Episode length: 48.32 +/- 22.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | -0.0726  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 318500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000137 |
|    n_updates        | 69624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.3     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12848    |
|    fps              | 222      |
|    time_elapsed     | 1433     |
|    total_timesteps  | 318688   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000361 |
|    n_updates        | 69671    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.2     |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12852    |
|    fps              | 222      |
|    time_elapsed     | 1433     |
|    total_timesteps  | 318958   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 69739    |
----------------------------------
Eval num_timesteps=319000, episode_reward=-0.14 +/- 0.27
Episode length: 50.20 +/- 22.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.2     |
|    mean_reward      | -0.14    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 319000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000499 |
|    n_updates        | 69749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.5     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12856    |
|    fps              | 222      |
|    time_elapsed     | 1435     |
|    total_timesteps  | 319146   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000419 |
|    n_updates        | 69786    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.4     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12860    |
|    fps              | 222      |
|    time_elapsed     | 1435     |
|    total_timesteps  | 319348   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000284 |
|    n_updates        | 69836    |
----------------------------------
Eval num_timesteps=319500, episode_reward=-0.16 +/- 0.27
Episode length: 54.38 +/- 22.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.4     |
|    mean_reward      | -0.157   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 319500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.95e-05 |
|    n_updates        | 69874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53       |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12864    |
|    fps              | 222      |
|    time_elapsed     | 1437     |
|    total_timesteps  | 319534   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00019  |
|    n_updates        | 69883    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.4     |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12868    |
|    fps              | 222      |
|    time_elapsed     | 1438     |
|    total_timesteps  | 319776   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000691 |
|    n_updates        | 69943    |
----------------------------------
Eval num_timesteps=320000, episode_reward=-0.11 +/- 0.34
Episode length: 52.78 +/- 23.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.8     |
|    mean_reward      | -0.111   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 320000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000527 |
|    n_updates        | 69999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.9     |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12872    |
|    fps              | 222      |
|    time_elapsed     | 1440     |
|    total_timesteps  | 320050   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000203 |
|    n_updates        | 70012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.4     |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12876    |
|    fps              | 222      |
|    time_elapsed     | 1440     |
|    total_timesteps  | 320195   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000227 |
|    n_updates        | 70048    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.3     |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12880    |
|    fps              | 222      |
|    time_elapsed     | 1440     |
|    total_timesteps  | 320393   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00469  |
|    n_updates        | 70098    |
----------------------------------
Eval num_timesteps=320500, episode_reward=-0.07 +/- 0.36
Episode length: 46.70 +/- 22.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | -0.0661  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 320500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000167 |
|    n_updates        | 70124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54       |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12884    |
|    fps              | 222      |
|    time_elapsed     | 1442     |
|    total_timesteps  | 320634   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000279 |
|    n_updates        | 70158    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54       |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12888    |
|    fps              | 222      |
|    time_elapsed     | 1442     |
|    total_timesteps  | 320833   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000375 |
|    n_updates        | 70208    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.5     |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12892    |
|    fps              | 222      |
|    time_elapsed     | 1442     |
|    total_timesteps  | 320970   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000221 |
|    n_updates        | 70242    |
----------------------------------
Eval num_timesteps=321000, episode_reward=-0.19 +/- 0.18
Episode length: 53.78 +/- 21.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.8     |
|    mean_reward      | -0.195   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 321000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000523 |
|    n_updates        | 70249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.9     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12896    |
|    fps              | 222      |
|    time_elapsed     | 1445     |
|    total_timesteps  | 321244   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.28e-05 |
|    n_updates        | 70310    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55       |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12900    |
|    fps              | 222      |
|    time_elapsed     | 1445     |
|    total_timesteps  | 321407   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000176 |
|    n_updates        | 70351    |
----------------------------------
Eval num_timesteps=321500, episode_reward=-0.09 +/- 0.36
Episode length: 52.90 +/- 22.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | -0.091   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 321500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000267 |
|    n_updates        | 70374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.1     |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12904    |
|    fps              | 222      |
|    time_elapsed     | 1447     |
|    total_timesteps  | 321670   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00052  |
|    n_updates        | 70417    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.5     |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12908    |
|    fps              | 222      |
|    time_elapsed     | 1447     |
|    total_timesteps  | 321844   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0012   |
|    n_updates        | 70460    |
----------------------------------
Eval num_timesteps=322000, episode_reward=-0.07 +/- 0.40
Episode length: 53.28 +/- 22.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.3     |
|    mean_reward      | -0.0725  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 322000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.94e-05 |
|    n_updates        | 70499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.1     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12912    |
|    fps              | 222      |
|    time_elapsed     | 1449     |
|    total_timesteps  | 322059   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000355 |
|    n_updates        | 70514    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.5     |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12916    |
|    fps              | 222      |
|    time_elapsed     | 1449     |
|    total_timesteps  | 322188   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00019  |
|    n_updates        | 70546    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.8     |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12920    |
|    fps              | 222      |
|    time_elapsed     | 1449     |
|    total_timesteps  | 322417   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000461 |
|    n_updates        | 70604    |
----------------------------------
Eval num_timesteps=322500, episode_reward=-0.15 +/- 0.27
Episode length: 52.88 +/- 23.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.9     |
|    mean_reward      | -0.151   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 322500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000274 |
|    n_updates        | 70624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55       |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12924    |
|    fps              | 222      |
|    time_elapsed     | 1451     |
|    total_timesteps  | 322623   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00364  |
|    n_updates        | 70655    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.7     |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12928    |
|    fps              | 222      |
|    time_elapsed     | 1452     |
|    total_timesteps  | 322890   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000246 |
|    n_updates        | 70722    |
----------------------------------
Eval num_timesteps=323000, episode_reward=-0.08 +/- 0.35
Episode length: 51.00 +/- 22.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51       |
|    mean_reward      | -0.0834  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 323000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000202 |
|    n_updates        | 70749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.2     |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12932    |
|    fps              | 222      |
|    time_elapsed     | 1454     |
|    total_timesteps  | 323181   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000264 |
|    n_updates        | 70795    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.3     |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12936    |
|    fps              | 222      |
|    time_elapsed     | 1454     |
|    total_timesteps  | 323393   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.91e-05 |
|    n_updates        | 70848    |
----------------------------------
Eval num_timesteps=323500, episode_reward=-0.17 +/- 0.21
Episode length: 51.56 +/- 22.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.6     |
|    mean_reward      | -0.166   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 323500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00702  |
|    n_updates        | 70874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.1     |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12940    |
|    fps              | 222      |
|    time_elapsed     | 1456     |
|    total_timesteps  | 323546   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00658  |
|    n_updates        | 70886    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.7     |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12944    |
|    fps              | 222      |
|    time_elapsed     | 1456     |
|    total_timesteps  | 323756   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000574 |
|    n_updates        | 70938    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53       |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12948    |
|    fps              | 222      |
|    time_elapsed     | 1456     |
|    total_timesteps  | 323988   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000188 |
|    n_updates        | 70996    |
----------------------------------
Eval num_timesteps=324000, episode_reward=-0.16 +/- 0.26
Episode length: 55.32 +/- 23.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.3     |
|    mean_reward      | -0.161   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 324000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00731  |
|    n_updates        | 70999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.4     |
|    ep_rew_mean      | -0.0889  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12952    |
|    fps              | 222      |
|    time_elapsed     | 1458     |
|    total_timesteps  | 324199   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000901 |
|    n_updates        | 71049    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.7     |
|    ep_rew_mean      | -0.0861  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12956    |
|    fps              | 222      |
|    time_elapsed     | 1459     |
|    total_timesteps  | 324316   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000195 |
|    n_updates        | 71078    |
----------------------------------
Eval num_timesteps=324500, episode_reward=-0.12 +/- 0.36
Episode length: 61.04 +/- 20.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61       |
|    mean_reward      | -0.124   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 324500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000516 |
|    n_updates        | 71124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | -0.0955  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12960    |
|    fps              | 222      |
|    time_elapsed     | 1461     |
|    total_timesteps  | 324504   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000198 |
|    n_updates        | 71125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.4     |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12964    |
|    fps              | 222      |
|    time_elapsed     | 1461     |
|    total_timesteps  | 324672   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.57e-05 |
|    n_updates        | 71167    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.3     |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12968    |
|    fps              | 222      |
|    time_elapsed     | 1461     |
|    total_timesteps  | 324903   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000169 |
|    n_updates        | 71225    |
----------------------------------
Eval num_timesteps=325000, episode_reward=-0.12 +/- 0.33
Episode length: 54.16 +/- 20.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.2     |
|    mean_reward      | -0.116   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 325000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000326 |
|    n_updates        | 71249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 49.7     |
|    ep_rew_mean      | -0.0979  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12972    |
|    fps              | 222      |
|    time_elapsed     | 1463     |
|    total_timesteps  | 325017   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 71254    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.2     |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12976    |
|    fps              | 222      |
|    time_elapsed     | 1464     |
|    total_timesteps  | 325317   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000629 |
|    n_updates        | 71329    |
----------------------------------
Eval num_timesteps=325500, episode_reward=-0.04 +/- 0.40
Episode length: 54.50 +/- 20.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.5     |
|    mean_reward      | -0.0372  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 325500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00359  |
|    n_updates        | 71374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.6     |
|    ep_rew_mean      | -0.0957  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12980    |
|    fps              | 222      |
|    time_elapsed     | 1466     |
|    total_timesteps  | 325555   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00477  |
|    n_updates        | 71388    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.7     |
|    ep_rew_mean      | -0.0822  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12984    |
|    fps              | 222      |
|    time_elapsed     | 1466     |
|    total_timesteps  | 325707   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000228 |
|    n_updates        | 71426    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.0604  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12988    |
|    fps              | 222      |
|    time_elapsed     | 1466     |
|    total_timesteps  | 325863   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.72e-05 |
|    n_updates        | 71465    |
----------------------------------
Eval num_timesteps=326000, episode_reward=-0.14 +/- 0.31
Episode length: 56.20 +/- 22.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.2     |
|    mean_reward      | -0.144   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 326000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.68e-05 |
|    n_updates        | 71499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.7     |
|    ep_rew_mean      | -0.0621  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12992    |
|    fps              | 222      |
|    time_elapsed     | 1468     |
|    total_timesteps  | 326041   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0048   |
|    n_updates        | 71510    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.7     |
|    ep_rew_mean      | -0.0622  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 12996    |
|    fps              | 222      |
|    time_elapsed     | 1468     |
|    total_timesteps  | 326317   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000192 |
|    n_updates        | 71579    |
----------------------------------
Eval num_timesteps=326500, episode_reward=-0.17 +/- 0.26
Episode length: 58.44 +/- 20.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.4     |
|    mean_reward      | -0.173   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 326500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000333 |
|    n_updates        | 71624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.1     |
|    ep_rew_mean      | -0.0536  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13000    |
|    fps              | 221      |
|    time_elapsed     | 1471     |
|    total_timesteps  | 326516   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00261  |
|    n_updates        | 71628    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.3     |
|    ep_rew_mean      | -0.0504  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13004    |
|    fps              | 222      |
|    time_elapsed     | 1471     |
|    total_timesteps  | 326699   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000268 |
|    n_updates        | 71674    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 50.8     |
|    ep_rew_mean      | -0.0525  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13008    |
|    fps              | 222      |
|    time_elapsed     | 1471     |
|    total_timesteps  | 326926   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000291 |
|    n_updates        | 71731    |
----------------------------------
Eval num_timesteps=327000, episode_reward=-0.17 +/- 0.27
Episode length: 56.92 +/- 21.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.9     |
|    mean_reward      | -0.167   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 327000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000306 |
|    n_updates        | 71749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51       |
|    ep_rew_mean      | -0.0631  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13012    |
|    fps              | 221      |
|    time_elapsed     | 1473     |
|    total_timesteps  | 327156   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000439 |
|    n_updates        | 71788    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.2     |
|    ep_rew_mean      | -0.068   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13016    |
|    fps              | 222      |
|    time_elapsed     | 1473     |
|    total_timesteps  | 327406   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000466 |
|    n_updates        | 71851    |
----------------------------------
Eval num_timesteps=327500, episode_reward=-0.07 +/- 0.39
Episode length: 52.20 +/- 23.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 52.2     |
|    mean_reward      | -0.0682  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 327500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000491 |
|    n_updates        | 71874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52       |
|    ep_rew_mean      | -0.0571  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13020    |
|    fps              | 221      |
|    time_elapsed     | 1475     |
|    total_timesteps  | 327613   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000302 |
|    n_updates        | 71903    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.5     |
|    ep_rew_mean      | -0.0795  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13024    |
|    fps              | 222      |
|    time_elapsed     | 1476     |
|    total_timesteps  | 327878   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00014  |
|    n_updates        | 71969    |
----------------------------------
Eval num_timesteps=328000, episode_reward=-0.22 +/- 0.09
Episode length: 54.76 +/- 22.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.8     |
|    mean_reward      | -0.219   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 328000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000624 |
|    n_updates        | 71999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.7     |
|    ep_rew_mean      | -0.09    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13028    |
|    fps              | 221      |
|    time_elapsed     | 1478     |
|    total_timesteps  | 328156   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000106 |
|    n_updates        | 72038    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.8     |
|    ep_rew_mean      | -0.0765  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13032    |
|    fps              | 222      |
|    time_elapsed     | 1478     |
|    total_timesteps  | 328359   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.64e-05 |
|    n_updates        | 72089    |
----------------------------------
Eval num_timesteps=328500, episode_reward=-0.05 +/- 0.39
Episode length: 53.02 +/- 21.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53       |
|    mean_reward      | -0.0513  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 328500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000312 |
|    n_updates        | 72124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 51.9     |
|    ep_rew_mean      | -0.0668  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13036    |
|    fps              | 221      |
|    time_elapsed     | 1480     |
|    total_timesteps  | 328580   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000199 |
|    n_updates        | 72144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 52.9     |
|    ep_rew_mean      | -0.0809  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13040    |
|    fps              | 222      |
|    time_elapsed     | 1480     |
|    total_timesteps  | 328835   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00665  |
|    n_updates        | 72208    |
----------------------------------
Eval num_timesteps=329000, episode_reward=-0.23 +/- 0.18
Episode length: 62.98 +/- 17.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63       |
|    mean_reward      | -0.231   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 329000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00049  |
|    n_updates        | 72249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.4     |
|    ep_rew_mean      | -0.083   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13044    |
|    fps              | 221      |
|    time_elapsed     | 1483     |
|    total_timesteps  | 329096   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.35e-05 |
|    n_updates        | 72273    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53.2     |
|    ep_rew_mean      | -0.0722  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13048    |
|    fps              | 221      |
|    time_elapsed     | 1483     |
|    total_timesteps  | 329309   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00022  |
|    n_updates        | 72327    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 53       |
|    ep_rew_mean      | -0.0614  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13052    |
|    fps              | 222      |
|    time_elapsed     | 1483     |
|    total_timesteps  | 329499   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000285 |
|    n_updates        | 72374    |
----------------------------------
Eval num_timesteps=329500, episode_reward=-0.17 +/- 0.23
Episode length: 53.48 +/- 23.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -0.173   |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 54.4     |
|    ep_rew_mean      | -0.0672  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13056    |
|    fps              | 221      |
|    time_elapsed     | 1485     |
|    total_timesteps  | 329760   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000187 |
|    n_updates        | 72439    |
----------------------------------
Eval num_timesteps=330000, episode_reward=-0.14 +/- 0.31
Episode length: 59.42 +/- 19.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.4     |
|    mean_reward      | -0.137   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 330000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000583 |
|    n_updates        | 72499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55       |
|    ep_rew_mean      | -0.0695  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13060    |
|    fps              | 221      |
|    time_elapsed     | 1488     |
|    total_timesteps  | 330007   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000414 |
|    n_updates        | 72501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.8     |
|    ep_rew_mean      | -0.0625  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13064    |
|    fps              | 221      |
|    time_elapsed     | 1488     |
|    total_timesteps  | 330249   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00525  |
|    n_updates        | 72562    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.4     |
|    ep_rew_mean      | -0.061   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13068    |
|    fps              | 221      |
|    time_elapsed     | 1488     |
|    total_timesteps  | 330443   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.3e-05  |
|    n_updates        | 72610    |
----------------------------------
Eval num_timesteps=330500, episode_reward=-0.17 +/- 0.26
Episode length: 56.54 +/- 21.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.5     |
|    mean_reward      | -0.166   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 330500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.38e-05 |
|    n_updates        | 72624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.2     |
|    ep_rew_mean      | -0.0784  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13072    |
|    fps              | 221      |
|    time_elapsed     | 1490     |
|    total_timesteps  | 330740   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.64e-05 |
|    n_updates        | 72684    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.6     |
|    ep_rew_mean      | -0.076   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13076    |
|    fps              | 221      |
|    time_elapsed     | 1491     |
|    total_timesteps  | 330981   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00018  |
|    n_updates        | 72745    |
----------------------------------
Eval num_timesteps=331000, episode_reward=-0.10 +/- 0.37
Episode length: 59.02 +/- 20.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59       |
|    mean_reward      | -0.0954  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 331000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.37e-05 |
|    n_updates        | 72749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.3     |
|    ep_rew_mean      | -0.0948  |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13080    |
|    fps              | 221      |
|    time_elapsed     | 1493     |
|    total_timesteps  | 331187   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.95e-05 |
|    n_updates        | 72796    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.4     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13084    |
|    fps              | 221      |
|    time_elapsed     | 1493     |
|    total_timesteps  | 331446   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000183 |
|    n_updates        | 72861    |
----------------------------------
Eval num_timesteps=331500, episode_reward=-0.13 +/- 0.34
Episode length: 56.56 +/- 20.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.6     |
|    mean_reward      | -0.126   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 331500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0103   |
|    n_updates        | 72874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.2     |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13088    |
|    fps              | 221      |
|    time_elapsed     | 1495     |
|    total_timesteps  | 331688   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.27e-05 |
|    n_updates        | 72921    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.7     |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13092    |
|    fps              | 221      |
|    time_elapsed     | 1495     |
|    total_timesteps  | 331912   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000135 |
|    n_updates        | 72977    |
----------------------------------
Eval num_timesteps=332000, episode_reward=-0.07 +/- 0.38
Episode length: 53.20 +/- 21.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53.2     |
|    mean_reward      | -0.072   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 332000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00116  |
|    n_updates        | 72999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58       |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13096    |
|    fps              | 221      |
|    time_elapsed     | 1497     |
|    total_timesteps  | 332114   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000206 |
|    n_updates        | 73028    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.1     |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13100    |
|    fps              | 221      |
|    time_elapsed     | 1498     |
|    total_timesteps  | 332224   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000275 |
|    n_updates        | 73055    |
----------------------------------
Eval num_timesteps=332500, episode_reward=-0.14 +/- 0.32
Episode length: 59.78 +/- 20.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.8     |
|    mean_reward      | -0.139   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 332500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000108 |
|    n_updates        | 73124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.2     |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13104    |
|    fps              | 221      |
|    time_elapsed     | 1500     |
|    total_timesteps  | 332524   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000213 |
|    n_updates        | 73130    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.9     |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13108    |
|    fps              | 221      |
|    time_elapsed     | 1500     |
|    total_timesteps  | 332715   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000171 |
|    n_updates        | 73178    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.2     |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13112    |
|    fps              | 221      |
|    time_elapsed     | 1500     |
|    total_timesteps  | 332977   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000422 |
|    n_updates        | 73244    |
----------------------------------
Eval num_timesteps=333000, episode_reward=-0.11 +/- 0.34
Episode length: 51.88 +/- 21.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.9     |
|    mean_reward      | -0.107   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 333000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00864  |
|    n_updates        | 73249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.2     |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13116    |
|    fps              | 221      |
|    time_elapsed     | 1502     |
|    total_timesteps  | 333223   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000358 |
|    n_updates        | 73305    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58       |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13120    |
|    fps              | 221      |
|    time_elapsed     | 1503     |
|    total_timesteps  | 333411   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.3e-05  |
|    n_updates        | 73352    |
----------------------------------
Eval num_timesteps=333500, episode_reward=-0.07 +/- 0.41
Episode length: 57.36 +/- 20.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.4     |
|    mean_reward      | -0.0688  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 333500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000134 |
|    n_updates        | 73374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.3     |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13124    |
|    fps              | 221      |
|    time_elapsed     | 1505     |
|    total_timesteps  | 333711   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000196 |
|    n_updates        | 73427    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.1     |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13128    |
|    fps              | 221      |
|    time_elapsed     | 1505     |
|    total_timesteps  | 333969   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000361 |
|    n_updates        | 73492    |
----------------------------------
Eval num_timesteps=334000, episode_reward=-0.13 +/- 0.32
Episode length: 58.32 +/- 18.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.3     |
|    mean_reward      | -0.133   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 334000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000529 |
|    n_updates        | 73499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.1     |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13132    |
|    fps              | 221      |
|    time_elapsed     | 1507     |
|    total_timesteps  | 334269   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00471  |
|    n_updates        | 73567    |
----------------------------------
Eval num_timesteps=334500, episode_reward=-0.10 +/- 0.34
Episode length: 54.26 +/- 22.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.3     |
|    mean_reward      | -0.0963  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 334500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.59e-05 |
|    n_updates        | 73624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.3     |
|    ep_rew_mean      | -0.147   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13136    |
|    fps              | 221      |
|    time_elapsed     | 1510     |
|    total_timesteps  | 334510   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000303 |
|    n_updates        | 73627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.4     |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13140    |
|    fps              | 221      |
|    time_elapsed     | 1510     |
|    total_timesteps  | 334773   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000246 |
|    n_updates        | 73693    |
----------------------------------
Eval num_timesteps=335000, episode_reward=-0.17 +/- 0.32
Episode length: 62.32 +/- 18.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.3     |
|    mean_reward      | -0.169   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 335000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.64e-05 |
|    n_updates        | 73749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.2     |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13144    |
|    fps              | 221      |
|    time_elapsed     | 1512     |
|    total_timesteps  | 335015   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00691  |
|    n_updates        | 73753    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.6     |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13148    |
|    fps              | 221      |
|    time_elapsed     | 1512     |
|    total_timesteps  | 335269   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00439  |
|    n_updates        | 73817    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.5     |
|    ep_rew_mean      | -0.148   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13152    |
|    fps              | 221      |
|    time_elapsed     | 1513     |
|    total_timesteps  | 335453   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000391 |
|    n_updates        | 73863    |
----------------------------------
Eval num_timesteps=335500, episode_reward=-0.21 +/- 0.22
Episode length: 63.36 +/- 19.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.4     |
|    mean_reward      | -0.213   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 335500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.31e-05 |
|    n_updates        | 73874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.7     |
|    ep_rew_mean      | -0.148   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13156    |
|    fps              | 221      |
|    time_elapsed     | 1515     |
|    total_timesteps  | 335726   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000239 |
|    n_updates        | 73931    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59       |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13160    |
|    fps              | 221      |
|    time_elapsed     | 1515     |
|    total_timesteps  | 335902   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000156 |
|    n_updates        | 73975    |
----------------------------------
Eval num_timesteps=336000, episode_reward=-0.06 +/- 0.40
Episode length: 56.12 +/- 20.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.1     |
|    mean_reward      | -0.0639  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 336000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.3e-05  |
|    n_updates        | 73999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.4     |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13164    |
|    fps              | 221      |
|    time_elapsed     | 1517     |
|    total_timesteps  | 336089   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.91e-05 |
|    n_updates        | 74022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.8     |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13168    |
|    fps              | 221      |
|    time_elapsed     | 1518     |
|    total_timesteps  | 336318   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000163 |
|    n_updates        | 74079    |
----------------------------------
Eval num_timesteps=336500, episode_reward=-0.13 +/- 0.30
Episode length: 53.02 +/- 22.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 53       |
|    mean_reward      | -0.132   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 336500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 74124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.7     |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13172    |
|    fps              | 221      |
|    time_elapsed     | 1520     |
|    total_timesteps  | 336509   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000298 |
|    n_updates        | 74127    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.2     |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13176    |
|    fps              | 221      |
|    time_elapsed     | 1520     |
|    total_timesteps  | 336803   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.73e-05 |
|    n_updates        | 74200    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58       |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13180    |
|    fps              | 221      |
|    time_elapsed     | 1520     |
|    total_timesteps  | 336987   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000168 |
|    n_updates        | 74246    |
----------------------------------
Eval num_timesteps=337000, episode_reward=-0.11 +/- 0.35
Episode length: 56.92 +/- 20.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.9     |
|    mean_reward      | -0.107   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 337000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00654  |
|    n_updates        | 74249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.6     |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13184    |
|    fps              | 221      |
|    time_elapsed     | 1522     |
|    total_timesteps  | 337209   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0111   |
|    n_updates        | 74302    |
----------------------------------
Eval num_timesteps=337500, episode_reward=-0.20 +/- 0.22
Episode length: 59.98 +/- 20.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60       |
|    mean_reward      | -0.199   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 337500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.48e-05 |
|    n_updates        | 74374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.2     |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13188    |
|    fps              | 221      |
|    time_elapsed     | 1525     |
|    total_timesteps  | 337509   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.68e-05 |
|    n_updates        | 74377    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59       |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13192    |
|    fps              | 221      |
|    time_elapsed     | 1525     |
|    total_timesteps  | 337809   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.28e-05 |
|    n_updates        | 74452    |
----------------------------------
Eval num_timesteps=338000, episode_reward=-0.06 +/- 0.40
Episode length: 54.80 +/- 23.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.8     |
|    mean_reward      | -0.0587  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 338000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 74499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.9     |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13196    |
|    fps              | 221      |
|    time_elapsed     | 1527     |
|    total_timesteps  | 338103   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000761 |
|    n_updates        | 74525    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.3     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13200    |
|    fps              | 221      |
|    time_elapsed     | 1527     |
|    total_timesteps  | 338350   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000367 |
|    n_updates        | 74587    |
----------------------------------
Eval num_timesteps=338500, episode_reward=-0.14 +/- 0.31
Episode length: 55.32 +/- 21.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.3     |
|    mean_reward      | -0.141   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 338500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000512 |
|    n_updates        | 74624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.2     |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13204    |
|    fps              | 221      |
|    time_elapsed     | 1529     |
|    total_timesteps  | 338544   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000466 |
|    n_updates        | 74635    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.1     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13208    |
|    fps              | 221      |
|    time_elapsed     | 1530     |
|    total_timesteps  | 338721   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.33e-05 |
|    n_updates        | 74680    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.1     |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13212    |
|    fps              | 221      |
|    time_elapsed     | 1530     |
|    total_timesteps  | 338984   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.2e-05  |
|    n_updates        | 74745    |
----------------------------------
Eval num_timesteps=339000, episode_reward=-0.16 +/- 0.31
Episode length: 59.74 +/- 20.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.7     |
|    mean_reward      | -0.158   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 339000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000313 |
|    n_updates        | 74749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.8     |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13216    |
|    fps              | 221      |
|    time_elapsed     | 1532     |
|    total_timesteps  | 339205   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00354  |
|    n_updates        | 74801    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.5     |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13220    |
|    fps              | 221      |
|    time_elapsed     | 1532     |
|    total_timesteps  | 339366   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00138  |
|    n_updates        | 74841    |
----------------------------------
Eval num_timesteps=339500, episode_reward=-0.17 +/- 0.27
Episode length: 57.00 +/- 21.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57       |
|    mean_reward      | -0.167   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 339500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.63e-05 |
|    n_updates        | 74874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.5     |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13224    |
|    fps              | 221      |
|    time_elapsed     | 1534     |
|    total_timesteps  | 339564   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000179 |
|    n_updates        | 74890    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.5     |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13228    |
|    fps              | 221      |
|    time_elapsed     | 1535     |
|    total_timesteps  | 339714   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000202 |
|    n_updates        | 74928    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57       |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13232    |
|    fps              | 221      |
|    time_elapsed     | 1535     |
|    total_timesteps  | 339971   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.6e-05  |
|    n_updates        | 74992    |
----------------------------------
Eval num_timesteps=340000, episode_reward=-0.17 +/- 0.29
Episode length: 58.72 +/- 21.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.7     |
|    mean_reward      | -0.174   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 340000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000738 |
|    n_updates        | 74999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.8     |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13236    |
|    fps              | 221      |
|    time_elapsed     | 1537     |
|    total_timesteps  | 340189   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.74e-05 |
|    n_updates        | 75047    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.2     |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13240    |
|    fps              | 221      |
|    time_elapsed     | 1537     |
|    total_timesteps  | 340393   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.67e-05 |
|    n_updates        | 75098    |
----------------------------------
Eval num_timesteps=340500, episode_reward=-0.18 +/- 0.26
Episode length: 59.68 +/- 19.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.7     |
|    mean_reward      | -0.178   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 340500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000317 |
|    n_updates        | 75124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.7     |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13244    |
|    fps              | 221      |
|    time_elapsed     | 1540     |
|    total_timesteps  | 340686   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000464 |
|    n_updates        | 75171    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.8     |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13248    |
|    fps              | 221      |
|    time_elapsed     | 1540     |
|    total_timesteps  | 340853   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000433 |
|    n_updates        | 75213    |
----------------------------------
Eval num_timesteps=341000, episode_reward=-0.19 +/- 0.26
Episode length: 62.70 +/- 18.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.7     |
|    mean_reward      | -0.19    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 341000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 75249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.7     |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13252    |
|    fps              | 221      |
|    time_elapsed     | 1542     |
|    total_timesteps  | 341127   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00433  |
|    n_updates        | 75281    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.1     |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13256    |
|    fps              | 221      |
|    time_elapsed     | 1542     |
|    total_timesteps  | 341336   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000123 |
|    n_updates        | 75333    |
----------------------------------
Eval num_timesteps=341500, episode_reward=-0.04 +/- 0.43
Episode length: 54.68 +/- 22.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.7     |
|    mean_reward      | -0.0381  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 341500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000104 |
|    n_updates        | 75374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.3     |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13260    |
|    fps              | 221      |
|    time_elapsed     | 1545     |
|    total_timesteps  | 341532   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000307 |
|    n_updates        | 75382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57       |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13264    |
|    fps              | 221      |
|    time_elapsed     | 1545     |
|    total_timesteps  | 341791   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000531 |
|    n_updates        | 75447    |
----------------------------------
Eval num_timesteps=342000, episode_reward=-0.18 +/- 0.26
Episode length: 60.06 +/- 20.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.1     |
|    mean_reward      | -0.18    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 342000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.59e-05 |
|    n_updates        | 75499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.2     |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13268    |
|    fps              | 221      |
|    time_elapsed     | 1547     |
|    total_timesteps  | 342034   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000376 |
|    n_updates        | 75508    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58       |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13272    |
|    fps              | 221      |
|    time_elapsed     | 1547     |
|    total_timesteps  | 342310   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.85e-05 |
|    n_updates        | 75577    |
----------------------------------
Eval num_timesteps=342500, episode_reward=-0.20 +/- 0.23
Episode length: 59.04 +/- 19.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59       |
|    mean_reward      | -0.196   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 342500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000591 |
|    n_updates        | 75624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.4     |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13276    |
|    fps              | 220      |
|    time_elapsed     | 1550     |
|    total_timesteps  | 342539   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.34e-05 |
|    n_updates        | 75634    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.8     |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13280    |
|    fps              | 221      |
|    time_elapsed     | 1550     |
|    total_timesteps  | 342765   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00049  |
|    n_updates        | 75691    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.7     |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13284    |
|    fps              | 221      |
|    time_elapsed     | 1550     |
|    total_timesteps  | 342979   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.2e-05  |
|    n_updates        | 75744    |
----------------------------------
Eval num_timesteps=343000, episode_reward=-0.19 +/- 0.26
Episode length: 63.74 +/- 17.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.7     |
|    mean_reward      | -0.195   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 343000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000151 |
|    n_updates        | 75749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.2     |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13288    |
|    fps              | 221      |
|    time_elapsed     | 1552     |
|    total_timesteps  | 343229   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000322 |
|    n_updates        | 75807    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.5     |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13292    |
|    fps              | 221      |
|    time_elapsed     | 1553     |
|    total_timesteps  | 343464   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.88e-05 |
|    n_updates        | 75865    |
----------------------------------
Eval num_timesteps=343500, episode_reward=-0.10 +/- 0.35
Episode length: 56.12 +/- 20.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.1     |
|    mean_reward      | -0.104   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 343500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00355  |
|    n_updates        | 75874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.6     |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13296    |
|    fps              | 220      |
|    time_elapsed     | 1555     |
|    total_timesteps  | 343663   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000241 |
|    n_updates        | 75915    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 55.8     |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13300    |
|    fps              | 221      |
|    time_elapsed     | 1555     |
|    total_timesteps  | 343932   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000316 |
|    n_updates        | 75982    |
----------------------------------
Eval num_timesteps=344000, episode_reward=-0.15 +/- 0.31
Episode length: 56.48 +/- 21.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.5     |
|    mean_reward      | -0.145   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 344000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000348 |
|    n_updates        | 75999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 56.5     |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13304    |
|    fps              | 220      |
|    time_elapsed     | 1557     |
|    total_timesteps  | 344198   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00365  |
|    n_updates        | 76049    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.5     |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13308    |
|    fps              | 221      |
|    time_elapsed     | 1558     |
|    total_timesteps  | 344475   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000193 |
|    n_updates        | 76118    |
----------------------------------
Eval num_timesteps=344500, episode_reward=-0.18 +/- 0.28
Episode length: 60.88 +/- 19.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.9     |
|    mean_reward      | -0.183   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 344500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000242 |
|    n_updates        | 76124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.3     |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13312    |
|    fps              | 220      |
|    time_elapsed     | 1560     |
|    total_timesteps  | 344715   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000197 |
|    n_updates        | 76178    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 57.7     |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13316    |
|    fps              | 221      |
|    time_elapsed     | 1560     |
|    total_timesteps  | 344972   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000398 |
|    n_updates        | 76242    |
----------------------------------
Eval num_timesteps=345000, episode_reward=-0.06 +/- 0.44
Episode length: 64.14 +/- 17.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.1     |
|    mean_reward      | -0.0559  |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 345000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000638 |
|    n_updates        | 76249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.6     |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13320    |
|    fps              | 220      |
|    time_elapsed     | 1563     |
|    total_timesteps  | 345226   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000361 |
|    n_updates        | 76306    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 58.9     |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13324    |
|    fps              | 220      |
|    time_elapsed     | 1563     |
|    total_timesteps  | 345455   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00843  |
|    n_updates        | 76363    |
----------------------------------
Eval num_timesteps=345500, episode_reward=-0.16 +/- 0.27
Episode length: 54.80 +/- 24.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.8     |
|    mean_reward      | -0.159   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 345500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000438 |
|    n_updates        | 76374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.9     |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13328    |
|    fps              | 220      |
|    time_elapsed     | 1565     |
|    total_timesteps  | 345706   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000473 |
|    n_updates        | 76426    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.9     |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13332    |
|    fps              | 220      |
|    time_elapsed     | 1565     |
|    total_timesteps  | 345961   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000161 |
|    n_updates        | 76490    |
----------------------------------
Eval num_timesteps=346000, episode_reward=-0.20 +/- 0.24
Episode length: 59.02 +/- 22.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59       |
|    mean_reward      | -0.196   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 346000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000153 |
|    n_updates        | 76499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.6     |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13336    |
|    fps              | 220      |
|    time_elapsed     | 1568     |
|    total_timesteps  | 346251   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.81e-06 |
|    n_updates        | 76562    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.1     |
|    ep_rew_mean      | -0.16    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13340    |
|    fps              | 220      |
|    time_elapsed     | 1568     |
|    total_timesteps  | 346407   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.26e-05 |
|    n_updates        | 76601    |
----------------------------------
Eval num_timesteps=346500, episode_reward=-0.25 +/- 0.17
Episode length: 68.44 +/- 15.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.4     |
|    mean_reward      | -0.254   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 346500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.4e-05  |
|    n_updates        | 76624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 59.5     |
|    ep_rew_mean      | -0.168   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13344    |
|    fps              | 220      |
|    time_elapsed     | 1570     |
|    total_timesteps  | 346637   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 76659    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.8     |
|    ep_rew_mean      | -0.183   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13348    |
|    fps              | 220      |
|    time_elapsed     | 1571     |
|    total_timesteps  | 346937   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 76734    |
----------------------------------
Eval num_timesteps=347000, episode_reward=-0.25 +/- 0.17
Episode length: 68.64 +/- 15.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.6     |
|    mean_reward      | -0.254   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 347000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000197 |
|    n_updates        | 76749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 60.6     |
|    ep_rew_mean      | -0.182   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13352    |
|    fps              | 220      |
|    time_elapsed     | 1573     |
|    total_timesteps  | 347187   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0002   |
|    n_updates        | 76796    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.5     |
|    ep_rew_mean      | -0.196   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13356    |
|    fps              | 220      |
|    time_elapsed     | 1573     |
|    total_timesteps  | 347487   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.07e-05 |
|    n_updates        | 76871    |
----------------------------------
Eval num_timesteps=347500, episode_reward=-0.21 +/- 0.23
Episode length: 62.78 +/- 19.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.8     |
|    mean_reward      | -0.211   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 347500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.0012   |
|    n_updates        | 76874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.2     |
|    ep_rew_mean      | -0.174   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13360    |
|    fps              | 220      |
|    time_elapsed     | 1576     |
|    total_timesteps  | 347654   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000119 |
|    n_updates        | 76913    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.6     |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13364    |
|    fps              | 220      |
|    time_elapsed     | 1576     |
|    total_timesteps  | 347951   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000184 |
|    n_updates        | 76987    |
----------------------------------
Eval num_timesteps=348000, episode_reward=-0.15 +/- 0.29
Episode length: 58.74 +/- 21.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 58.7     |
|    mean_reward      | -0.154   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 348000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00306  |
|    n_updates        | 76999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.2     |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13368    |
|    fps              | 220      |
|    time_elapsed     | 1578     |
|    total_timesteps  | 348251   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000304 |
|    n_updates        | 77062    |
----------------------------------
Eval num_timesteps=348500, episode_reward=-0.16 +/- 0.32
Episode length: 60.46 +/- 21.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.5     |
|    mean_reward      | -0.161   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 348500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.67e-05 |
|    n_updates        | 77124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62       |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13372    |
|    fps              | 220      |
|    time_elapsed     | 1581     |
|    total_timesteps  | 348508   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000683 |
|    n_updates        | 77126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.7     |
|    ep_rew_mean      | -0.19    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13376    |
|    fps              | 220      |
|    time_elapsed     | 1581     |
|    total_timesteps  | 348808   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00654  |
|    n_updates        | 77201    |
----------------------------------
Eval num_timesteps=349000, episode_reward=-0.18 +/- 0.23
Episode length: 55.68 +/- 21.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.7     |
|    mean_reward      | -0.182   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 349000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.93e-05 |
|    n_updates        | 77249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.9     |
|    ep_rew_mean      | -0.191   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13380    |
|    fps              | 220      |
|    time_elapsed     | 1583     |
|    total_timesteps  | 349058   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00035  |
|    n_updates        | 77264    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.8     |
|    ep_rew_mean      | -0.191   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13384    |
|    fps              | 220      |
|    time_elapsed     | 1583     |
|    total_timesteps  | 349255   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000341 |
|    n_updates        | 77313    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.5     |
|    ep_rew_mean      | -0.19    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13388    |
|    fps              | 220      |
|    time_elapsed     | 1583     |
|    total_timesteps  | 349483   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000145 |
|    n_updates        | 77370    |
----------------------------------
Eval num_timesteps=349500, episode_reward=-0.24 +/- 0.16
Episode length: 65.38 +/- 18.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.4     |
|    mean_reward      | -0.241   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 349500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.04e-05 |
|    n_updates        | 77374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.2     |
|    ep_rew_mean      | -0.192   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13392    |
|    fps              | 220      |
|    time_elapsed     | 1586     |
|    total_timesteps  | 349783   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000767 |
|    n_updates        | 77445    |
----------------------------------
Eval num_timesteps=350000, episode_reward=-0.21 +/- 0.26
Episode length: 66.52 +/- 15.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.5     |
|    mean_reward      | -0.206   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 350000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.36e-05 |
|    n_updates        | 77499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.5     |
|    ep_rew_mean      | -0.193   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13396    |
|    fps              | 220      |
|    time_elapsed     | 1588     |
|    total_timesteps  | 350008   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000335 |
|    n_updates        | 77501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.3     |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13400    |
|    fps              | 220      |
|    time_elapsed     | 1589     |
|    total_timesteps  | 350165   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000295 |
|    n_updates        | 77541    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.8     |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13404    |
|    fps              | 220      |
|    time_elapsed     | 1589     |
|    total_timesteps  | 350382   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000152 |
|    n_updates        | 77595    |
----------------------------------
Eval num_timesteps=350500, episode_reward=-0.20 +/- 0.27
Episode length: 65.16 +/- 15.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.2     |
|    mean_reward      | -0.2     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 350500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.04e-05 |
|    n_updates        | 77624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.1     |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13408    |
|    fps              | 220      |
|    time_elapsed     | 1591     |
|    total_timesteps  | 350682   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000214 |
|    n_updates        | 77670    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.5     |
|    ep_rew_mean      | -0.186   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13412    |
|    fps              | 220      |
|    time_elapsed     | 1591     |
|    total_timesteps  | 350865   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000207 |
|    n_updates        | 77716    |
----------------------------------
Eval num_timesteps=351000, episode_reward=-0.14 +/- 0.32
Episode length: 54.00 +/- 22.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54       |
|    mean_reward      | -0.135   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 351000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9e-05    |
|    n_updates        | 77749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.6     |
|    ep_rew_mean      | -0.196   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13416    |
|    fps              | 220      |
|    time_elapsed     | 1594     |
|    total_timesteps  | 351128   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00211  |
|    n_updates        | 77781    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.7     |
|    ep_rew_mean      | -0.196   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13420    |
|    fps              | 220      |
|    time_elapsed     | 1594     |
|    total_timesteps  | 351393   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.59e-05 |
|    n_updates        | 77848    |
----------------------------------
Eval num_timesteps=351500, episode_reward=-0.19 +/- 0.28
Episode length: 63.34 +/- 19.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.3     |
|    mean_reward      | -0.193   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 351500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.47e-05 |
|    n_updates        | 77874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.4     |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13424    |
|    fps              | 220      |
|    time_elapsed     | 1596     |
|    total_timesteps  | 351693   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000153 |
|    n_updates        | 77923    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.7     |
|    ep_rew_mean      | -0.2     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13428    |
|    fps              | 220      |
|    time_elapsed     | 1597     |
|    total_timesteps  | 351974   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00054  |
|    n_updates        | 77993    |
----------------------------------
Eval num_timesteps=352000, episode_reward=-0.24 +/- 0.24
Episode length: 68.98 +/- 15.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69       |
|    mean_reward      | -0.236   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 352000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.84e-05 |
|    n_updates        | 77999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.7     |
|    ep_rew_mean      | -0.196   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13432    |
|    fps              | 220      |
|    time_elapsed     | 1599     |
|    total_timesteps  | 352130   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00155  |
|    n_updates        | 78032    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.5     |
|    ep_rew_mean      | -0.196   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13436    |
|    fps              | 220      |
|    time_elapsed     | 1599     |
|    total_timesteps  | 352403   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000127 |
|    n_updates        | 78100    |
----------------------------------
Eval num_timesteps=352500, episode_reward=-0.27 +/- 0.06
Episode length: 68.68 +/- 15.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.7     |
|    mean_reward      | -0.275   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 352500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.85e-05 |
|    n_updates        | 78124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.5     |
|    ep_rew_mean      | -0.2     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13440    |
|    fps              | 220      |
|    time_elapsed     | 1602     |
|    total_timesteps  | 352659   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.98e-05 |
|    n_updates        | 78164    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.2     |
|    ep_rew_mean      | -0.203   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13444    |
|    fps              | 220      |
|    time_elapsed     | 1602     |
|    total_timesteps  | 352959   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.87e-05 |
|    n_updates        | 78239    |
----------------------------------
Eval num_timesteps=353000, episode_reward=-0.23 +/- 0.22
Episode length: 66.72 +/- 17.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.7     |
|    mean_reward      | -0.227   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 353000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8e-05    |
|    n_updates        | 78249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.4     |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13448    |
|    fps              | 220      |
|    time_elapsed     | 1605     |
|    total_timesteps  | 353177   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000163 |
|    n_updates        | 78294    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.3     |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13452    |
|    fps              | 220      |
|    time_elapsed     | 1605     |
|    total_timesteps  | 353421   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.85e-05 |
|    n_updates        | 78355    |
----------------------------------
Eval num_timesteps=353500, episode_reward=-0.26 +/- 0.18
Episode length: 69.88 +/- 13.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.9     |
|    mean_reward      | -0.259   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 353500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000211 |
|    n_updates        | 78374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.8     |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13456    |
|    fps              | 219      |
|    time_elapsed     | 1608     |
|    total_timesteps  | 353662   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000241 |
|    n_updates        | 78415    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.4     |
|    ep_rew_mean      | -0.209   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13460    |
|    fps              | 220      |
|    time_elapsed     | 1608     |
|    total_timesteps  | 353895   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000338 |
|    n_updates        | 78473    |
----------------------------------
Eval num_timesteps=354000, episode_reward=-0.28 +/- 0.06
Episode length: 69.46 +/- 14.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.5     |
|    mean_reward      | -0.278   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 354000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000335 |
|    n_updates        | 78499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.4     |
|    ep_rew_mean      | -0.209   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13464    |
|    fps              | 219      |
|    time_elapsed     | 1611     |
|    total_timesteps  | 354195   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00012  |
|    n_updates        | 78548    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.8     |
|    ep_rew_mean      | -0.207   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13468    |
|    fps              | 219      |
|    time_elapsed     | 1611     |
|    total_timesteps  | 354433   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000165 |
|    n_updates        | 78608    |
----------------------------------
Eval num_timesteps=354500, episode_reward=-0.22 +/- 0.24
Episode length: 64.84 +/- 19.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.8     |
|    mean_reward      | -0.219   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 354500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.34e-05 |
|    n_updates        | 78624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.6     |
|    ep_rew_mean      | -0.216   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13472    |
|    fps              | 219      |
|    time_elapsed     | 1613     |
|    total_timesteps  | 354673   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000144 |
|    n_updates        | 78668    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.6     |
|    ep_rew_mean      | -0.216   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13476    |
|    fps              | 219      |
|    time_elapsed     | 1614     |
|    total_timesteps  | 354973   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 78743    |
----------------------------------
Eval num_timesteps=355000, episode_reward=-0.28 +/- 0.04
Episode length: 70.70 +/- 10.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.7     |
|    mean_reward      | -0.283   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 355000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.37e-05 |
|    n_updates        | 78749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.5     |
|    ep_rew_mean      | -0.216   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13480    |
|    fps              | 219      |
|    time_elapsed     | 1616     |
|    total_timesteps  | 355213   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000156 |
|    n_updates        | 78803    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62       |
|    ep_rew_mean      | -0.218   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13484    |
|    fps              | 219      |
|    time_elapsed     | 1616     |
|    total_timesteps  | 355453   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00487  |
|    n_updates        | 78863    |
----------------------------------
Eval num_timesteps=355500, episode_reward=-0.24 +/- 0.18
Episode length: 65.88 +/- 19.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.9     |
|    mean_reward      | -0.243   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 355500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.28e-05 |
|    n_updates        | 78874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.7     |
|    ep_rew_mean      | -0.22    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13488    |
|    fps              | 219      |
|    time_elapsed     | 1619     |
|    total_timesteps  | 355753   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.27e-05 |
|    n_updates        | 78938    |
----------------------------------
Eval num_timesteps=356000, episode_reward=-0.24 +/- 0.18
Episode length: 65.34 +/- 17.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.3     |
|    mean_reward      | -0.241   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 356000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000223 |
|    n_updates        | 78999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.7     |
|    ep_rew_mean      | -0.22    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13492    |
|    fps              | 219      |
|    time_elapsed     | 1622     |
|    total_timesteps  | 356053   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.99e-05 |
|    n_updates        | 79013    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.5     |
|    ep_rew_mean      | -0.209   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13496    |
|    fps              | 219      |
|    time_elapsed     | 1622     |
|    total_timesteps  | 356253   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.5e-05  |
|    n_updates        | 79063    |
----------------------------------
Eval num_timesteps=356500, episode_reward=-0.22 +/- 0.18
Episode length: 60.68 +/- 21.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 60.7     |
|    mean_reward      | -0.222   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 356500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00278  |
|    n_updates        | 79124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.4     |
|    ep_rew_mean      | -0.213   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13500    |
|    fps              | 219      |
|    time_elapsed     | 1624     |
|    total_timesteps  | 356502   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000259 |
|    n_updates        | 79125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.2     |
|    ep_rew_mean      | -0.226   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13504    |
|    fps              | 219      |
|    time_elapsed     | 1624     |
|    total_timesteps  | 356802   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.49e-05 |
|    n_updates        | 79200    |
----------------------------------
Eval num_timesteps=357000, episode_reward=-0.25 +/- 0.23
Episode length: 71.46 +/- 11.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.5     |
|    mean_reward      | -0.246   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 357000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.75e-05 |
|    n_updates        | 79249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.6     |
|    ep_rew_mean      | -0.214   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13508    |
|    fps              | 219      |
|    time_elapsed     | 1627     |
|    total_timesteps  | 357040   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.7e-05  |
|    n_updates        | 79259    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.3     |
|    ep_rew_mean      | -0.207   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13512    |
|    fps              | 219      |
|    time_elapsed     | 1627     |
|    total_timesteps  | 357299   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.03e-05 |
|    n_updates        | 79324    |
----------------------------------
Eval num_timesteps=357500, episode_reward=-0.21 +/- 0.28
Episode length: 66.70 +/- 16.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.7     |
|    mean_reward      | -0.206   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 357500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.28e-05 |
|    n_updates        | 79374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.1     |
|    ep_rew_mean      | -0.206   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13516    |
|    fps              | 219      |
|    time_elapsed     | 1630     |
|    total_timesteps  | 357534   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.44e-05 |
|    n_updates        | 79383    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.3     |
|    ep_rew_mean      | -0.203   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13520    |
|    fps              | 219      |
|    time_elapsed     | 1630     |
|    total_timesteps  | 357726   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.76e-05 |
|    n_updates        | 79431    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.5     |
|    ep_rew_mean      | -0.199   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13524    |
|    fps              | 219      |
|    time_elapsed     | 1630     |
|    total_timesteps  | 357938   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.55e-05 |
|    n_updates        | 79484    |
----------------------------------
Eval num_timesteps=358000, episode_reward=-0.25 +/- 0.17
Episode length: 66.38 +/- 16.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.4     |
|    mean_reward      | -0.245   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 358000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.23e-05 |
|    n_updates        | 79499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.4     |
|    ep_rew_mean      | -0.209   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13528    |
|    fps              | 219      |
|    time_elapsed     | 1633     |
|    total_timesteps  | 358215   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000719 |
|    n_updates        | 79553    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.3     |
|    ep_rew_mean      | -0.213   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13532    |
|    fps              | 219      |
|    time_elapsed     | 1633     |
|    total_timesteps  | 358464   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00343  |
|    n_updates        | 79615    |
----------------------------------
Eval num_timesteps=358500, episode_reward=-0.24 +/- 0.18
Episode length: 66.14 +/- 17.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.1     |
|    mean_reward      | -0.244   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 358500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000191 |
|    n_updates        | 79624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.6     |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13536    |
|    fps              | 219      |
|    time_elapsed     | 1636     |
|    total_timesteps  | 358666   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.67e-05 |
|    n_updates        | 79666    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.1     |
|    ep_rew_mean      | -0.212   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13540    |
|    fps              | 219      |
|    time_elapsed     | 1636     |
|    total_timesteps  | 358966   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000148 |
|    n_updates        | 79741    |
----------------------------------
Eval num_timesteps=359000, episode_reward=-0.26 +/- 0.16
Episode length: 68.86 +/- 12.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.9     |
|    mean_reward      | -0.255   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 359000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.003    |
|    n_updates        | 79749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63       |
|    ep_rew_mean      | -0.212   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13544    |
|    fps              | 219      |
|    time_elapsed     | 1639     |
|    total_timesteps  | 359258   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000598 |
|    n_updates        | 79814    |
----------------------------------
Eval num_timesteps=359500, episode_reward=-0.22 +/- 0.16
Episode length: 61.34 +/- 19.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.3     |
|    mean_reward      | -0.225   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 359500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 79874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.3     |
|    ep_rew_mean      | -0.213   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13548    |
|    fps              | 219      |
|    time_elapsed     | 1641     |
|    total_timesteps  | 359511   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.96e-05 |
|    n_updates        | 79877    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.7     |
|    ep_rew_mean      | -0.214   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13552    |
|    fps              | 219      |
|    time_elapsed     | 1641     |
|    total_timesteps  | 359789   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000145 |
|    n_updates        | 79947    |
----------------------------------
Eval num_timesteps=360000, episode_reward=-0.26 +/- 0.15
Episode length: 70.06 +/- 13.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.1     |
|    mean_reward      | -0.26    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 360000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000224 |
|    n_updates        | 79999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.8     |
|    ep_rew_mean      | -0.225   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13556    |
|    fps              | 218      |
|    time_elapsed     | 1644     |
|    total_timesteps  | 360037   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.89e-05 |
|    n_updates        | 80009    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64       |
|    ep_rew_mean      | -0.226   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13560    |
|    fps              | 219      |
|    time_elapsed     | 1644     |
|    total_timesteps  | 360294   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.31e-05 |
|    n_updates        | 80073    |
----------------------------------
Eval num_timesteps=360500, episode_reward=-0.23 +/- 0.18
Episode length: 61.58 +/- 19.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.6     |
|    mean_reward      | -0.226   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 360500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.78e-05 |
|    n_updates        | 80124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.8     |
|    ep_rew_mean      | -0.225   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13564    |
|    fps              | 218      |
|    time_elapsed     | 1646     |
|    total_timesteps  | 360571   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.29e-05 |
|    n_updates        | 80142    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.6     |
|    ep_rew_mean      | -0.224   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13568    |
|    fps              | 219      |
|    time_elapsed     | 1647     |
|    total_timesteps  | 360793   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.84e-05 |
|    n_updates        | 80198    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63       |
|    ep_rew_mean      | -0.222   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13572    |
|    fps              | 219      |
|    time_elapsed     | 1647     |
|    total_timesteps  | 360974   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000209 |
|    n_updates        | 80243    |
----------------------------------
Eval num_timesteps=361000, episode_reward=-0.22 +/- 0.24
Episode length: 66.00 +/- 18.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66       |
|    mean_reward      | -0.224   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 361000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00162  |
|    n_updates        | 80249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.8     |
|    ep_rew_mean      | -0.221   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13576    |
|    fps              | 218      |
|    time_elapsed     | 1649     |
|    total_timesteps  | 361256   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.49e-05 |
|    n_updates        | 80313    |
----------------------------------
Eval num_timesteps=361500, episode_reward=-0.24 +/- 0.18
Episode length: 64.82 +/- 18.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.8     |
|    mean_reward      | -0.239   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 361500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00335  |
|    n_updates        | 80374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63       |
|    ep_rew_mean      | -0.222   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13580    |
|    fps              | 218      |
|    time_elapsed     | 1652     |
|    total_timesteps  | 361513   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.2e-05  |
|    n_updates        | 80378    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.2     |
|    ep_rew_mean      | -0.223   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13584    |
|    fps              | 218      |
|    time_elapsed     | 1652     |
|    total_timesteps  | 361777   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.24e-05 |
|    n_updates        | 80444    |
----------------------------------
Eval num_timesteps=362000, episode_reward=-0.26 +/- 0.17
Episode length: 69.98 +/- 11.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70       |
|    mean_reward      | -0.26    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 362000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.49e-05 |
|    n_updates        | 80499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.2     |
|    ep_rew_mean      | -0.223   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13588    |
|    fps              | 218      |
|    time_elapsed     | 1655     |
|    total_timesteps  | 362077   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000223 |
|    n_updates        | 80519    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62       |
|    ep_rew_mean      | -0.208   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13592    |
|    fps              | 218      |
|    time_elapsed     | 1655     |
|    total_timesteps  | 362253   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.53e-05 |
|    n_updates        | 80563    |
----------------------------------
Eval num_timesteps=362500, episode_reward=-0.28 +/- 0.05
Episode length: 70.68 +/- 12.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.7     |
|    mean_reward      | -0.283   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 362500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.23e-05 |
|    n_updates        | 80624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63       |
|    ep_rew_mean      | -0.222   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13596    |
|    fps              | 218      |
|    time_elapsed     | 1658     |
|    total_timesteps  | 362553   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000103 |
|    n_updates        | 80638    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.5     |
|    ep_rew_mean      | -0.224   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13600    |
|    fps              | 218      |
|    time_elapsed     | 1658     |
|    total_timesteps  | 362853   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.82e-05 |
|    n_updates        | 80713    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.5     |
|    ep_rew_mean      | -0.206   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13604    |
|    fps              | 218      |
|    time_elapsed     | 1658     |
|    total_timesteps  | 362950   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000203 |
|    n_updates        | 80737    |
----------------------------------
Eval num_timesteps=363000, episode_reward=-0.21 +/- 0.18
Episode length: 57.60 +/- 21.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.6     |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 363000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.02e-05 |
|    n_updates        | 80749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.7     |
|    ep_rew_mean      | -0.216   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13608    |
|    fps              | 218      |
|    time_elapsed     | 1660     |
|    total_timesteps  | 363209   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000169 |
|    n_updates        | 80802    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.7     |
|    ep_rew_mean      | -0.226   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13612    |
|    fps              | 218      |
|    time_elapsed     | 1661     |
|    total_timesteps  | 363465   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.87e-05 |
|    n_updates        | 80866    |
----------------------------------
Eval num_timesteps=363500, episode_reward=-0.26 +/- 0.16
Episode length: 70.12 +/- 13.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.1     |
|    mean_reward      | -0.26    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 363500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.03e-06 |
|    n_updates        | 80874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 61.7     |
|    ep_rew_mean      | -0.226   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13616    |
|    fps              | 218      |
|    time_elapsed     | 1663     |
|    total_timesteps  | 363701   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.49e-05 |
|    n_updates        | 80925    |
----------------------------------
Eval num_timesteps=364000, episode_reward=-0.24 +/- 0.17
Episode length: 64.66 +/- 17.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.7     |
|    mean_reward      | -0.238   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 364000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3e-05    |
|    n_updates        | 80999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.8     |
|    ep_rew_mean      | -0.231   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13620    |
|    fps              | 218      |
|    time_elapsed     | 1666     |
|    total_timesteps  | 364001   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.41e-05 |
|    n_updates        | 81000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.6     |
|    ep_rew_mean      | -0.234   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13624    |
|    fps              | 218      |
|    time_elapsed     | 1666     |
|    total_timesteps  | 364301   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000358 |
|    n_updates        | 81075    |
----------------------------------
Eval num_timesteps=364500, episode_reward=-0.21 +/- 0.22
Episode length: 61.82 +/- 16.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.8     |
|    mean_reward      | -0.207   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 364500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.83e-05 |
|    n_updates        | 81124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.6     |
|    ep_rew_mean      | -0.234   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13628    |
|    fps              | 218      |
|    time_elapsed     | 1668     |
|    total_timesteps  | 364578   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.73e-05 |
|    n_updates        | 81144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64       |
|    ep_rew_mean      | -0.236   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13632    |
|    fps              | 218      |
|    time_elapsed     | 1669     |
|    total_timesteps  | 364868   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.24e-05 |
|    n_updates        | 81216    |
----------------------------------
Eval num_timesteps=365000, episode_reward=-0.21 +/- 0.28
Episode length: 66.90 +/- 16.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.9     |
|    mean_reward      | -0.207   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 365000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000225 |
|    n_updates        | 81249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.4     |
|    ep_rew_mean      | -0.237   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13636    |
|    fps              | 218      |
|    time_elapsed     | 1671     |
|    total_timesteps  | 365103   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00547  |
|    n_updates        | 81275    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.2     |
|    ep_rew_mean      | -0.236   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13640    |
|    fps              | 218      |
|    time_elapsed     | 1671     |
|    total_timesteps  | 365384   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.31e-05 |
|    n_updates        | 81345    |
----------------------------------
Eval num_timesteps=365500, episode_reward=-0.24 +/- 0.23
Episode length: 71.02 +/- 12.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71       |
|    mean_reward      | -0.244   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 365500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.67e-05 |
|    n_updates        | 81374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.8     |
|    ep_rew_mean      | -0.235   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13644    |
|    fps              | 218      |
|    time_elapsed     | 1674     |
|    total_timesteps  | 365637   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.73e-05 |
|    n_updates        | 81409    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.2     |
|    ep_rew_mean      | -0.223   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13648    |
|    fps              | 218      |
|    time_elapsed     | 1674     |
|    total_timesteps  | 365835   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.83e-05 |
|    n_updates        | 81458    |
----------------------------------
Eval num_timesteps=366000, episode_reward=-0.25 +/- 0.16
Episode length: 68.00 +/- 14.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68       |
|    mean_reward      | -0.252   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 366000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.87e-05 |
|    n_updates        | 81499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 62.9     |
|    ep_rew_mean      | -0.211   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13652    |
|    fps              | 218      |
|    time_elapsed     | 1677     |
|    total_timesteps  | 366078   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.02e-05 |
|    n_updates        | 81519    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.4     |
|    ep_rew_mean      | -0.213   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13656    |
|    fps              | 218      |
|    time_elapsed     | 1677     |
|    total_timesteps  | 366378   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.02e-05 |
|    n_updates        | 81594    |
----------------------------------
Eval num_timesteps=366500, episode_reward=-0.21 +/- 0.27
Episode length: 67.36 +/- 15.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.4     |
|    mean_reward      | -0.209   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 366500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000311 |
|    n_updates        | 81624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.8     |
|    ep_rew_mean      | -0.215   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13660    |
|    fps              | 218      |
|    time_elapsed     | 1680     |
|    total_timesteps  | 366678   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.97e-05 |
|    n_updates        | 81669    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 63.8     |
|    ep_rew_mean      | -0.215   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13664    |
|    fps              | 218      |
|    time_elapsed     | 1680     |
|    total_timesteps  | 366953   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 81738    |
----------------------------------
Eval num_timesteps=367000, episode_reward=-0.28 +/- 0.05
Episode length: 70.50 +/- 11.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.5     |
|    mean_reward      | -0.282   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 367000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.15e-05 |
|    n_updates        | 81749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.3     |
|    ep_rew_mean      | -0.217   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13668    |
|    fps              | 218      |
|    time_elapsed     | 1683     |
|    total_timesteps  | 367228   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.74e-05 |
|    n_updates        | 81806    |
----------------------------------
Eval num_timesteps=367500, episode_reward=-0.27 +/- 0.18
Episode length: 71.64 +/- 11.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.6     |
|    mean_reward      | -0.266   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 367500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000261 |
|    n_updates        | 81874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.5     |
|    ep_rew_mean      | -0.222   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13672    |
|    fps              | 218      |
|    time_elapsed     | 1685     |
|    total_timesteps  | 367528   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00286  |
|    n_updates        | 81881    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.9     |
|    ep_rew_mean      | -0.219   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13676    |
|    fps              | 218      |
|    time_elapsed     | 1686     |
|    total_timesteps  | 367747   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.16e-05 |
|    n_updates        | 81936    |
----------------------------------
Eval num_timesteps=368000, episode_reward=-0.21 +/- 0.27
Episode length: 68.48 +/- 13.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.5     |
|    mean_reward      | -0.214   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 368000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.28e-05 |
|    n_updates        | 81999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.3     |
|    ep_rew_mean      | -0.221   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13680    |
|    fps              | 217      |
|    time_elapsed     | 1688     |
|    total_timesteps  | 368047   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.67e-05 |
|    n_updates        | 82011    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.4     |
|    ep_rew_mean      | -0.221   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13684    |
|    fps              | 218      |
|    time_elapsed     | 1688     |
|    total_timesteps  | 368318   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.22e-05 |
|    n_updates        | 82079    |
----------------------------------
Eval num_timesteps=368500, episode_reward=-0.22 +/- 0.24
Episode length: 65.14 +/- 17.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.1     |
|    mean_reward      | -0.22    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 368500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.27e-05 |
|    n_updates        | 82124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65       |
|    ep_rew_mean      | -0.22    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13688    |
|    fps              | 217      |
|    time_elapsed     | 1691     |
|    total_timesteps  | 368581   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.88e-05 |
|    n_updates        | 82145    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.8     |
|    ep_rew_mean      | -0.233   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13692    |
|    fps              | 218      |
|    time_elapsed     | 1691     |
|    total_timesteps  | 368828   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.3e-05  |
|    n_updates        | 82206    |
----------------------------------
Eval num_timesteps=369000, episode_reward=-0.16 +/- 0.35
Episode length: 64.62 +/- 17.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.6     |
|    mean_reward      | -0.158   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 369000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.04e-05 |
|    n_updates        | 82249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.8     |
|    ep_rew_mean      | -0.233   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13696    |
|    fps              | 217      |
|    time_elapsed     | 1694     |
|    total_timesteps  | 369128   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.04e-05 |
|    n_updates        | 82281    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.3     |
|    ep_rew_mean      | -0.231   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13700    |
|    fps              | 218      |
|    time_elapsed     | 1694     |
|    total_timesteps  | 369388   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000149 |
|    n_updates        | 82346    |
----------------------------------
Eval num_timesteps=369500, episode_reward=-0.24 +/- 0.15
Episode length: 64.78 +/- 18.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.8     |
|    mean_reward      | -0.239   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 369500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.97e-05 |
|    n_updates        | 82374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.2     |
|    ep_rew_mean      | -0.249   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13704    |
|    fps              | 217      |
|    time_elapsed     | 1696     |
|    total_timesteps  | 369673   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.49e-05 |
|    n_updates        | 82418    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.6     |
|    ep_rew_mean      | -0.25    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13708    |
|    fps              | 217      |
|    time_elapsed     | 1697     |
|    total_timesteps  | 369973   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.93e-05 |
|    n_updates        | 82493    |
----------------------------------
Eval num_timesteps=370000, episode_reward=-0.24 +/- 0.21
Episode length: 70.06 +/- 13.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.1     |
|    mean_reward      | -0.24    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 370000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.52e-05 |
|    n_updates        | 82499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.1     |
|    ep_rew_mean      | -0.252   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13712    |
|    fps              | 217      |
|    time_elapsed     | 1699     |
|    total_timesteps  | 370273   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.47e-05 |
|    n_updates        | 82568    |
----------------------------------
Eval num_timesteps=370500, episode_reward=-0.20 +/- 0.27
Episode length: 66.04 +/- 16.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66       |
|    mean_reward      | -0.204   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 370500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.91e-05 |
|    n_updates        | 82624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.7     |
|    ep_rew_mean      | -0.255   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13716    |
|    fps              | 217      |
|    time_elapsed     | 1702     |
|    total_timesteps  | 370571   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.77e-05 |
|    n_updates        | 82642    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.2     |
|    ep_rew_mean      | -0.242   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13720    |
|    fps              | 217      |
|    time_elapsed     | 1702     |
|    total_timesteps  | 370819   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.19e-05 |
|    n_updates        | 82704    |
----------------------------------
Eval num_timesteps=371000, episode_reward=-0.28 +/- 0.05
Episode length: 70.24 +/- 11.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.2     |
|    mean_reward      | -0.281   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 371000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.16e-05 |
|    n_updates        | 82749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.7     |
|    ep_rew_mean      | -0.231   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13724    |
|    fps              | 217      |
|    time_elapsed     | 1705     |
|    total_timesteps  | 371072   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.57e-05 |
|    n_updates        | 82767    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.5     |
|    ep_rew_mean      | -0.23    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13728    |
|    fps              | 217      |
|    time_elapsed     | 1705     |
|    total_timesteps  | 371323   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.51e-05 |
|    n_updates        | 82830    |
----------------------------------
Eval num_timesteps=371500, episode_reward=-0.27 +/- 0.06
Episode length: 67.04 +/- 15.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67       |
|    mean_reward      | -0.268   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 371500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.85e-06 |
|    n_updates        | 82874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.8     |
|    ep_rew_mean      | -0.227   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13732    |
|    fps              | 217      |
|    time_elapsed     | 1708     |
|    total_timesteps  | 371544   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.73e-06 |
|    n_updates        | 82885    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.4     |
|    ep_rew_mean      | -0.229   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13736    |
|    fps              | 217      |
|    time_elapsed     | 1708     |
|    total_timesteps  | 371844   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.64e-05 |
|    n_updates        | 82960    |
----------------------------------
Eval num_timesteps=372000, episode_reward=-0.27 +/- 0.06
Episode length: 67.20 +/- 15.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.2     |
|    mean_reward      | -0.269   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 372000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.7e-05  |
|    n_updates        | 82999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.8     |
|    ep_rew_mean      | -0.227   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13740    |
|    fps              | 217      |
|    time_elapsed     | 1710     |
|    total_timesteps  | 372064   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.16e-05 |
|    n_updates        | 83015    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | -0.227   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13744    |
|    fps              | 217      |
|    time_elapsed     | 1711     |
|    total_timesteps  | 372307   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.73e-05 |
|    n_updates        | 83076    |
----------------------------------
Eval num_timesteps=372500, episode_reward=-0.28 +/- 0.05
Episode length: 70.20 +/- 11.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.2     |
|    mean_reward      | -0.281   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 372500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.86e-05 |
|    n_updates        | 83124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.8     |
|    ep_rew_mean      | -0.237   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13748    |
|    fps              | 217      |
|    time_elapsed     | 1713     |
|    total_timesteps  | 372513   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.36e-05 |
|    n_updates        | 83128    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | -0.248   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13752    |
|    fps              | 217      |
|    time_elapsed     | 1714     |
|    total_timesteps  | 372781   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.17e-05 |
|    n_updates        | 83195    |
----------------------------------
Eval num_timesteps=373000, episode_reward=-0.25 +/- 0.15
Episode length: 68.48 +/- 14.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.5     |
|    mean_reward      | -0.254   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 373000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.72e-05 |
|    n_updates        | 83249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | -0.248   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13756    |
|    fps              | 217      |
|    time_elapsed     | 1716     |
|    total_timesteps  | 373081   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.7e-05  |
|    n_updates        | 83270    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | -0.248   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13760    |
|    fps              | 217      |
|    time_elapsed     | 1716     |
|    total_timesteps  | 373381   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.72e-05 |
|    n_updates        | 83345    |
----------------------------------
Eval num_timesteps=373500, episode_reward=-0.25 +/- 0.23
Episode length: 71.76 +/- 10.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.8     |
|    mean_reward      | -0.247   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 373500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.16e-05 |
|    n_updates        | 83374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.1     |
|    ep_rew_mean      | -0.248   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13764    |
|    fps              | 217      |
|    time_elapsed     | 1719     |
|    total_timesteps  | 373667   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.56e-05 |
|    n_updates        | 83416    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.3     |
|    ep_rew_mean      | -0.249   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13768    |
|    fps              | 217      |
|    time_elapsed     | 1719     |
|    total_timesteps  | 373956   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.76e-05 |
|    n_updates        | 83488    |
----------------------------------
Eval num_timesteps=374000, episode_reward=-0.29 +/- 0.04
Episode length: 71.40 +/- 10.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.4     |
|    mean_reward      | -0.285   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 374000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.2e-05  |
|    n_updates        | 83499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | -0.248   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13772    |
|    fps              | 217      |
|    time_elapsed     | 1722     |
|    total_timesteps  | 374223   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.14e-05 |
|    n_updates        | 83555    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.4     |
|    ep_rew_mean      | -0.249   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13776    |
|    fps              | 217      |
|    time_elapsed     | 1722     |
|    total_timesteps  | 374489   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.71e-05 |
|    n_updates        | 83622    |
----------------------------------
Eval num_timesteps=374500, episode_reward=-0.24 +/- 0.17
Episode length: 66.12 +/- 15.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.1     |
|    mean_reward      | -0.244   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 374500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.5e-05  |
|    n_updates        | 83624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.4     |
|    ep_rew_mean      | -0.249   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13780    |
|    fps              | 217      |
|    time_elapsed     | 1725     |
|    total_timesteps  | 374789   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.57e-06 |
|    n_updates        | 83697    |
----------------------------------
Eval num_timesteps=375000, episode_reward=-0.24 +/- 0.17
Episode length: 65.44 +/- 17.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.4     |
|    mean_reward      | -0.241   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 375000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.25e-06 |
|    n_updates        | 83749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.7     |
|    ep_rew_mean      | -0.25    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13784    |
|    fps              | 217      |
|    time_elapsed     | 1727     |
|    total_timesteps  | 375085   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.88e-06 |
|    n_updates        | 83771    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.5     |
|    ep_rew_mean      | -0.25    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13788    |
|    fps              | 217      |
|    time_elapsed     | 1728     |
|    total_timesteps  | 375329   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.48e-06 |
|    n_updates        | 83832    |
----------------------------------
Eval num_timesteps=375500, episode_reward=-0.27 +/- 0.17
Episode length: 73.22 +/- 7.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.2     |
|    mean_reward      | -0.273   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 375500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.36e-05 |
|    n_updates        | 83874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68       |
|    ep_rew_mean      | -0.252   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13792    |
|    fps              | 217      |
|    time_elapsed     | 1730     |
|    total_timesteps  | 375629   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.56e-05 |
|    n_updates        | 83907    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.8     |
|    ep_rew_mean      | -0.251   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13796    |
|    fps              | 217      |
|    time_elapsed     | 1731     |
|    total_timesteps  | 375910   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.32e-06 |
|    n_updates        | 83977    |
----------------------------------
Eval num_timesteps=376000, episode_reward=-0.25 +/- 0.22
Episode length: 73.06 +/- 7.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.1     |
|    mean_reward      | -0.252   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 376000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.3e-05  |
|    n_updates        | 83999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.2     |
|    ep_rew_mean      | -0.239   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13800    |
|    fps              | 216      |
|    time_elapsed     | 1733     |
|    total_timesteps  | 376111   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.89e-05 |
|    n_updates        | 84027    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.4     |
|    ep_rew_mean      | -0.239   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13804    |
|    fps              | 217      |
|    time_elapsed     | 1734     |
|    total_timesteps  | 376411   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.54e-05 |
|    n_updates        | 84102    |
----------------------------------
Eval num_timesteps=376500, episode_reward=-0.21 +/- 0.23
Episode length: 63.46 +/- 17.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.5     |
|    mean_reward      | -0.213   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 376500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.48e-05 |
|    n_updates        | 84124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.3     |
|    ep_rew_mean      | -0.239   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13808    |
|    fps              | 216      |
|    time_elapsed     | 1736     |
|    total_timesteps  | 376704   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.37e-05 |
|    n_updates        | 84175    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.8     |
|    ep_rew_mean      | -0.237   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13812    |
|    fps              | 217      |
|    time_elapsed     | 1736     |
|    total_timesteps  | 376951   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.26e-05 |
|    n_updates        | 84237    |
----------------------------------
Eval num_timesteps=377000, episode_reward=-0.17 +/- 0.28
Episode length: 57.10 +/- 23.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.1     |
|    mean_reward      | -0.168   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 377000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.93e-05 |
|    n_updates        | 84249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.5     |
|    ep_rew_mean      | -0.232   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13816    |
|    fps              | 216      |
|    time_elapsed     | 1739     |
|    total_timesteps  | 377117   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000223 |
|    n_updates        | 84279    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.6     |
|    ep_rew_mean      | -0.242   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13820    |
|    fps              | 216      |
|    time_elapsed     | 1739     |
|    total_timesteps  | 377376   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.46e-05 |
|    n_updates        | 84343    |
----------------------------------
Eval num_timesteps=377500, episode_reward=-0.27 +/- 0.06
Episode length: 67.96 +/- 16.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68       |
|    mean_reward      | -0.272   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 377500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.32e-05 |
|    n_updates        | 84374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.9     |
|    ep_rew_mean      | -0.253   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13824    |
|    fps              | 216      |
|    time_elapsed     | 1741     |
|    total_timesteps  | 377665   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.42e-05 |
|    n_updates        | 84416    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.4     |
|    ep_rew_mean      | -0.255   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13828    |
|    fps              | 216      |
|    time_elapsed     | 1742     |
|    total_timesteps  | 377965   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.5e-05  |
|    n_updates        | 84491    |
----------------------------------
Eval num_timesteps=378000, episode_reward=-0.29 +/- 0.04
Episode length: 71.96 +/- 9.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72       |
|    mean_reward      | -0.288   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 378000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000207 |
|    n_updates        | 84499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | -0.256   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13832    |
|    fps              | 216      |
|    time_elapsed     | 1744     |
|    total_timesteps  | 378210   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000185 |
|    n_updates        | 84552    |
----------------------------------
Eval num_timesteps=378500, episode_reward=-0.23 +/- 0.24
Episode length: 67.46 +/- 16.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.5     |
|    mean_reward      | -0.23    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 378500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.09e-05 |
|    n_updates        | 84624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | -0.256   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13836    |
|    fps              | 216      |
|    time_elapsed     | 1747     |
|    total_timesteps  | 378510   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.31e-05 |
|    n_updates        | 84627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.4     |
|    ep_rew_mean      | -0.259   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13840    |
|    fps              | 216      |
|    time_elapsed     | 1747     |
|    total_timesteps  | 378805   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.7e-05  |
|    n_updates        | 84701    |
----------------------------------
Eval num_timesteps=379000, episode_reward=-0.25 +/- 0.07
Episode length: 63.70 +/- 18.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.7     |
|    mean_reward      | -0.254   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 379000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.17e-06 |
|    n_updates        | 84749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.6     |
|    ep_rew_mean      | -0.26    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13844    |
|    fps              | 216      |
|    time_elapsed     | 1750     |
|    total_timesteps  | 379070   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.42e-05 |
|    n_updates        | 84767    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.2     |
|    ep_rew_mean      | -0.263   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13848    |
|    fps              | 216      |
|    time_elapsed     | 1750     |
|    total_timesteps  | 379338   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.57e-05 |
|    n_updates        | 84834    |
----------------------------------
Eval num_timesteps=379500, episode_reward=-0.25 +/- 0.07
Episode length: 61.62 +/- 18.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.6     |
|    mean_reward      | -0.246   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 379500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.14e-05 |
|    n_updates        | 84874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.7     |
|    ep_rew_mean      | -0.25    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13852    |
|    fps              | 216      |
|    time_elapsed     | 1752     |
|    total_timesteps  | 379547   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 84886    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.2     |
|    ep_rew_mean      | -0.239   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13856    |
|    fps              | 216      |
|    time_elapsed     | 1753     |
|    total_timesteps  | 379805   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000433 |
|    n_updates        | 84951    |
----------------------------------
Eval num_timesteps=380000, episode_reward=-0.19 +/- 0.28
Episode length: 61.50 +/- 21.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.5     |
|    mean_reward      | -0.186   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 380000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.32e-05 |
|    n_updates        | 84999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.2     |
|    ep_rew_mean      | -0.238   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13860    |
|    fps              | 216      |
|    time_elapsed     | 1755     |
|    total_timesteps  | 380101   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.88e-05 |
|    n_updates        | 85025    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.8     |
|    ep_rew_mean      | -0.227   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13864    |
|    fps              | 216      |
|    time_elapsed     | 1755     |
|    total_timesteps  | 380343   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.22e-05 |
|    n_updates        | 85085    |
----------------------------------
Eval num_timesteps=380500, episode_reward=-0.18 +/- 0.26
Episode length: 59.14 +/- 21.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 59.1     |
|    mean_reward      | -0.176   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 380500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.68e-06 |
|    n_updates        | 85124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.4     |
|    ep_rew_mean      | -0.215   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13868    |
|    fps              | 216      |
|    time_elapsed     | 1757     |
|    total_timesteps  | 380593   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000129 |
|    n_updates        | 85148    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | -0.216   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13872    |
|    fps              | 216      |
|    time_elapsed     | 1758     |
|    total_timesteps  | 380893   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.77e-06 |
|    n_updates        | 85223    |
----------------------------------
Eval num_timesteps=381000, episode_reward=-0.26 +/- 0.16
Episode length: 69.64 +/- 13.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.6     |
|    mean_reward      | -0.258   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 381000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.95e-05 |
|    n_updates        | 85249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | -0.218   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13876    |
|    fps              | 216      |
|    time_elapsed     | 1760     |
|    total_timesteps  | 381193   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.29e-05 |
|    n_updates        | 85298    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | -0.217   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13880    |
|    fps              | 216      |
|    time_elapsed     | 1761     |
|    total_timesteps  | 381461   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.82e-06 |
|    n_updates        | 85365    |
----------------------------------
Eval num_timesteps=381500, episode_reward=-0.19 +/- 0.27
Episode length: 63.76 +/- 18.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.8     |
|    mean_reward      | -0.195   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 381500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.56e-05 |
|    n_updates        | 85374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.8     |
|    ep_rew_mean      | -0.213   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13884    |
|    fps              | 216      |
|    time_elapsed     | 1763     |
|    total_timesteps  | 381665   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.26e-05 |
|    n_updates        | 85416    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.2     |
|    ep_rew_mean      | -0.211   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13888    |
|    fps              | 216      |
|    time_elapsed     | 1763     |
|    total_timesteps  | 381854   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000232 |
|    n_updates        | 85463    |
----------------------------------
Eval num_timesteps=382000, episode_reward=-0.26 +/- 0.17
Episode length: 70.58 +/- 11.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.6     |
|    mean_reward      | -0.262   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 382000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.51e-05 |
|    n_updates        | 85499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.7     |
|    ep_rew_mean      | -0.209   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13892    |
|    fps              | 216      |
|    time_elapsed     | 1766     |
|    total_timesteps  | 382102   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.43e-05 |
|    n_updates        | 85525    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.9     |
|    ep_rew_mean      | -0.209   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13896    |
|    fps              | 216      |
|    time_elapsed     | 1766     |
|    total_timesteps  | 382402   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00478  |
|    n_updates        | 85600    |
----------------------------------
Eval num_timesteps=382500, episode_reward=-0.23 +/- 0.16
Episode length: 61.82 +/- 20.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 61.8     |
|    mean_reward      | -0.227   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 382500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.07e-05 |
|    n_updates        | 85624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.9     |
|    ep_rew_mean      | -0.223   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13900    |
|    fps              | 216      |
|    time_elapsed     | 1769     |
|    total_timesteps  | 382702   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.78e-05 |
|    n_updates        | 85675    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.4     |
|    ep_rew_mean      | -0.221   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13904    |
|    fps              | 216      |
|    time_elapsed     | 1769     |
|    total_timesteps  | 382947   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00022  |
|    n_updates        | 85736    |
----------------------------------
Eval num_timesteps=383000, episode_reward=-0.20 +/- 0.26
Episode length: 65.80 +/- 14.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.8     |
|    mean_reward      | -0.203   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 383000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.94e-05 |
|    n_updates        | 85749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.7     |
|    ep_rew_mean      | -0.208   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13908    |
|    fps              | 216      |
|    time_elapsed     | 1771     |
|    total_timesteps  | 383173   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 85793    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.2     |
|    ep_rew_mean      | -0.211   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13912    |
|    fps              | 216      |
|    time_elapsed     | 1772     |
|    total_timesteps  | 383473   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000283 |
|    n_updates        | 85868    |
----------------------------------
Eval num_timesteps=383500, episode_reward=-0.21 +/- 0.23
Episode length: 62.16 +/- 20.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.2     |
|    mean_reward      | -0.208   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 383500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.69e-05 |
|    n_updates        | 85874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.5     |
|    ep_rew_mean      | -0.212   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13916    |
|    fps              | 216      |
|    time_elapsed     | 1774     |
|    total_timesteps  | 383665   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.64e-05 |
|    n_updates        | 85916    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.1     |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13920    |
|    fps              | 216      |
|    time_elapsed     | 1774     |
|    total_timesteps  | 383889   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.49e-05 |
|    n_updates        | 85972    |
----------------------------------
Eval num_timesteps=384000, episode_reward=-0.25 +/- 0.17
Episode length: 68.06 +/- 14.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.1     |
|    mean_reward      | -0.252   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 384000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.05e-05 |
|    n_updates        | 85999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.2     |
|    ep_rew_mean      | -0.186   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13924    |
|    fps              | 216      |
|    time_elapsed     | 1777     |
|    total_timesteps  | 384086   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000617 |
|    n_updates        | 86021    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.2     |
|    ep_rew_mean      | -0.186   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13928    |
|    fps              | 216      |
|    time_elapsed     | 1777     |
|    total_timesteps  | 384386   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.59e-05 |
|    n_updates        | 86096    |
----------------------------------
Eval num_timesteps=384500, episode_reward=-0.23 +/- 0.24
Episode length: 67.46 +/- 16.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.5     |
|    mean_reward      | -0.23    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 384500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000162 |
|    n_updates        | 86124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.8     |
|    ep_rew_mean      | -0.189   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13932    |
|    fps              | 216      |
|    time_elapsed     | 1779     |
|    total_timesteps  | 384686   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.6e-05  |
|    n_updates        | 86171    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.2     |
|    ep_rew_mean      | -0.186   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13936    |
|    fps              | 216      |
|    time_elapsed     | 1780     |
|    total_timesteps  | 384929   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.55e-05 |
|    n_updates        | 86232    |
----------------------------------
Eval num_timesteps=385000, episode_reward=-0.24 +/- 0.17
Episode length: 63.94 +/- 17.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.9     |
|    mean_reward      | -0.235   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 385000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.69e-05 |
|    n_updates        | 86249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.2     |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13940    |
|    fps              | 216      |
|    time_elapsed     | 1782     |
|    total_timesteps  | 385229   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.61e-05 |
|    n_updates        | 86307    |
----------------------------------
Eval num_timesteps=385500, episode_reward=-0.25 +/- 0.16
Episode length: 68.00 +/- 14.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68       |
|    mean_reward      | -0.252   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 385500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000113 |
|    n_updates        | 86374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.5     |
|    ep_rew_mean      | -0.188   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13944    |
|    fps              | 215      |
|    time_elapsed     | 1785     |
|    total_timesteps  | 385524   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000109 |
|    n_updates        | 86380    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.3     |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13948    |
|    fps              | 216      |
|    time_elapsed     | 1785     |
|    total_timesteps  | 385767   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000292 |
|    n_updates        | 86441    |
----------------------------------
Eval num_timesteps=386000, episode_reward=-0.27 +/- 0.15
Episode length: 72.08 +/- 9.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.1     |
|    mean_reward      | -0.268   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 386000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.18e-05 |
|    n_updates        | 86499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.8     |
|    ep_rew_mean      | -0.189   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13952    |
|    fps              | 215      |
|    time_elapsed     | 1788     |
|    total_timesteps  | 386030   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00114  |
|    n_updates        | 86507    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.2     |
|    ep_rew_mean      | -0.201   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13956    |
|    fps              | 215      |
|    time_elapsed     | 1788     |
|    total_timesteps  | 386330   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000107 |
|    n_updates        | 86582    |
----------------------------------
Eval num_timesteps=386500, episode_reward=-0.28 +/- 0.06
Episode length: 69.58 +/- 14.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.6     |
|    mean_reward      | -0.278   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 386500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000742 |
|    n_updates        | 86624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.3     |
|    ep_rew_mean      | -0.201   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13960    |
|    fps              | 215      |
|    time_elapsed     | 1791     |
|    total_timesteps  | 386630   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.83e-05 |
|    n_updates        | 86657    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.5     |
|    ep_rew_mean      | -0.212   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13964    |
|    fps              | 215      |
|    time_elapsed     | 1791     |
|    total_timesteps  | 386892   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.06e-05 |
|    n_updates        | 86722    |
----------------------------------
Eval num_timesteps=387000, episode_reward=-0.23 +/- 0.17
Episode length: 63.40 +/- 16.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.4     |
|    mean_reward      | -0.233   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 387000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.77e-05 |
|    n_updates        | 86749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.8     |
|    ep_rew_mean      | -0.219   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13968    |
|    fps              | 215      |
|    time_elapsed     | 1793     |
|    total_timesteps  | 387071   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.63e-05 |
|    n_updates        | 86767    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.2     |
|    ep_rew_mean      | -0.217   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13972    |
|    fps              | 215      |
|    time_elapsed     | 1794     |
|    total_timesteps  | 387314   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 86828    |
----------------------------------
Eval num_timesteps=387500, episode_reward=-0.16 +/- 0.32
Episode length: 65.76 +/- 14.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.8     |
|    mean_reward      | -0.163   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 387500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00127  |
|    n_updates        | 86874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.1     |
|    ep_rew_mean      | -0.216   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13976    |
|    fps              | 215      |
|    time_elapsed     | 1796     |
|    total_timesteps  | 387599   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000111 |
|    n_updates        | 86899    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.4     |
|    ep_rew_mean      | -0.217   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13980    |
|    fps              | 215      |
|    time_elapsed     | 1796     |
|    total_timesteps  | 387899   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.89e-05 |
|    n_updates        | 86974    |
----------------------------------
Eval num_timesteps=388000, episode_reward=-0.20 +/- 0.30
Episode length: 71.12 +/- 10.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.1     |
|    mean_reward      | -0.204   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 388000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.62e-05 |
|    n_updates        | 86999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.3     |
|    ep_rew_mean      | -0.221   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13984    |
|    fps              | 215      |
|    time_elapsed     | 1799     |
|    total_timesteps  | 388199   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.66e-05 |
|    n_updates        | 87049    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.5     |
|    ep_rew_mean      | -0.226   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13988    |
|    fps              | 215      |
|    time_elapsed     | 1799     |
|    total_timesteps  | 388499   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.44e-05 |
|    n_updates        | 87124    |
----------------------------------
Eval num_timesteps=388500, episode_reward=-0.22 +/- 0.26
Episode length: 68.98 +/- 12.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69       |
|    mean_reward     | -0.216   |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | -0.228   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13992    |
|    fps              | 215      |
|    time_elapsed     | 1802     |
|    total_timesteps  | 388799   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000133 |
|    n_updates        | 87199    |
----------------------------------
Eval num_timesteps=389000, episode_reward=-0.21 +/- 0.27
Episode length: 67.90 +/- 14.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.9     |
|    mean_reward      | -0.211   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 389000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.37e-05 |
|    n_updates        | 87249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | -0.228   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 13996    |
|    fps              | 215      |
|    time_elapsed     | 1805     |
|    total_timesteps  | 389099   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.83e-05 |
|    n_updates        | 87274    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | -0.228   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14000    |
|    fps              | 215      |
|    time_elapsed     | 1805     |
|    total_timesteps  | 389399   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.66e-05 |
|    n_updates        | 87349    |
----------------------------------
Eval num_timesteps=389500, episode_reward=-0.25 +/- 0.23
Episode length: 72.14 +/- 8.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.1     |
|    mean_reward      | -0.248   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 389500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000246 |
|    n_updates        | 87374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.5     |
|    ep_rew_mean      | -0.23    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14004    |
|    fps              | 215      |
|    time_elapsed     | 1808     |
|    total_timesteps  | 389699   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000115 |
|    n_updates        | 87424    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.1     |
|    ep_rew_mean      | -0.232   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14008    |
|    fps              | 215      |
|    time_elapsed     | 1808     |
|    total_timesteps  | 389984   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000125 |
|    n_updates        | 87495    |
----------------------------------
Eval num_timesteps=390000, episode_reward=-0.20 +/- 0.27
Episode length: 66.20 +/- 14.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.2     |
|    mean_reward      | -0.204   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 390000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000248 |
|    n_updates        | 87499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.2     |
|    ep_rew_mean      | -0.228   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14012    |
|    fps              | 215      |
|    time_elapsed     | 1810     |
|    total_timesteps  | 390190   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000291 |
|    n_updates        | 87547    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.7     |
|    ep_rew_mean      | -0.231   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14016    |
|    fps              | 215      |
|    time_elapsed     | 1811     |
|    total_timesteps  | 390437   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.42e-05 |
|    n_updates        | 87609    |
----------------------------------
Eval num_timesteps=390500, episode_reward=-0.22 +/- 0.28
Episode length: 69.30 +/- 14.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.3     |
|    mean_reward      | -0.217   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 390500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.58e-05 |
|    n_updates        | 87624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.2     |
|    ep_rew_mean      | -0.233   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14020    |
|    fps              | 215      |
|    time_elapsed     | 1813     |
|    total_timesteps  | 390708   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.94e-05 |
|    n_updates        | 87676    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69       |
|    ep_rew_mean      | -0.256   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14024    |
|    fps              | 215      |
|    time_elapsed     | 1814     |
|    total_timesteps  | 390990   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.96e-05 |
|    n_updates        | 87747    |
----------------------------------
Eval num_timesteps=391000, episode_reward=-0.26 +/- 0.14
Episode length: 69.46 +/- 11.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.5     |
|    mean_reward      | -0.258   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 391000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.74e-06 |
|    n_updates        | 87749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69       |
|    ep_rew_mean      | -0.256   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14028    |
|    fps              | 215      |
|    time_elapsed     | 1816     |
|    total_timesteps  | 391290   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000161 |
|    n_updates        | 87822    |
----------------------------------
Eval num_timesteps=391500, episode_reward=-0.22 +/- 0.23
Episode length: 65.66 +/- 18.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.7     |
|    mean_reward      | -0.222   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 391500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.59e-05 |
|    n_updates        | 87874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69       |
|    ep_rew_mean      | -0.256   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14032    |
|    fps              | 215      |
|    time_elapsed     | 1819     |
|    total_timesteps  | 391583   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.11e-05 |
|    n_updates        | 87895    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.5     |
|    ep_rew_mean      | -0.258   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14036    |
|    fps              | 215      |
|    time_elapsed     | 1819     |
|    total_timesteps  | 391883   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.19e-06 |
|    n_updates        | 87970    |
----------------------------------
Eval num_timesteps=392000, episode_reward=-0.24 +/- 0.21
Episode length: 69.12 +/- 14.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.1     |
|    mean_reward      | -0.236   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 392000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00025  |
|    n_updates        | 87999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69       |
|    ep_rew_mean      | -0.256   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14040    |
|    fps              | 215      |
|    time_elapsed     | 1822     |
|    total_timesteps  | 392126   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.44e-05 |
|    n_updates        | 88031    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.4     |
|    ep_rew_mean      | -0.253   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14044    |
|    fps              | 215      |
|    time_elapsed     | 1822     |
|    total_timesteps  | 392365   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.61e-06 |
|    n_updates        | 88091    |
----------------------------------
Eval num_timesteps=392500, episode_reward=-0.13 +/- 0.33
Episode length: 56.82 +/- 21.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.8     |
|    mean_reward      | -0.127   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 392500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000325 |
|    n_updates        | 88124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.4     |
|    ep_rew_mean      | -0.263   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14048    |
|    fps              | 215      |
|    time_elapsed     | 1824     |
|    total_timesteps  | 392606   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.25e-05 |
|    n_updates        | 88151    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68       |
|    ep_rew_mean      | -0.262   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14052    |
|    fps              | 215      |
|    time_elapsed     | 1824     |
|    total_timesteps  | 392829   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.19e-05 |
|    n_updates        | 88207    |
----------------------------------
Eval num_timesteps=393000, episode_reward=-0.21 +/- 0.24
Episode length: 63.62 +/- 20.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.6     |
|    mean_reward      | -0.214   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 393000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.78e-06 |
|    n_updates        | 88249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.5     |
|    ep_rew_mean      | -0.25    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14056    |
|    fps              | 215      |
|    time_elapsed     | 1827     |
|    total_timesteps  | 393083   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.35e-05 |
|    n_updates        | 88270    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.5     |
|    ep_rew_mean      | -0.236   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14060    |
|    fps              | 215      |
|    time_elapsed     | 1827     |
|    total_timesteps  | 393278   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.07e-05 |
|    n_updates        | 88319    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.9     |
|    ep_rew_mean      | -0.223   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14064    |
|    fps              | 215      |
|    time_elapsed     | 1827     |
|    total_timesteps  | 393485   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.41e-05 |
|    n_updates        | 88371    |
----------------------------------
Eval num_timesteps=393500, episode_reward=-0.25 +/- 0.17
Episode length: 67.52 +/- 14.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.5     |
|    mean_reward      | -0.25    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 393500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.82e-05 |
|    n_updates        | 88374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | -0.227   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14068    |
|    fps              | 215      |
|    time_elapsed     | 1830     |
|    total_timesteps  | 393744   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.19e-05 |
|    n_updates        | 88435    |
----------------------------------
Eval num_timesteps=394000, episode_reward=-0.14 +/- 0.32
Episode length: 56.30 +/- 22.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.3     |
|    mean_reward      | -0.145   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 394000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000138 |
|    n_updates        | 88499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67       |
|    ep_rew_mean      | -0.208   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14072    |
|    fps              | 215      |
|    time_elapsed     | 1832     |
|    total_timesteps  | 394017   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000407 |
|    n_updates        | 88504    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.2     |
|    ep_rew_mean      | -0.208   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14076    |
|    fps              | 215      |
|    time_elapsed     | 1832     |
|    total_timesteps  | 394317   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00027  |
|    n_updates        | 88579    |
----------------------------------
Eval num_timesteps=394500, episode_reward=-0.19 +/- 0.28
Episode length: 63.62 +/- 17.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63.6     |
|    mean_reward      | -0.194   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 394500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000158 |
|    n_updates        | 88624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.9     |
|    ep_rew_mean      | -0.207   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14080    |
|    fps              | 215      |
|    time_elapsed     | 1835     |
|    total_timesteps  | 394592   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.74e-05 |
|    n_updates        | 88647    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.6     |
|    ep_rew_mean      | -0.206   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14084    |
|    fps              | 215      |
|    time_elapsed     | 1835     |
|    total_timesteps  | 394860   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.89e-05 |
|    n_updates        | 88714    |
----------------------------------
Eval num_timesteps=395000, episode_reward=-0.21 +/- 0.22
Episode length: 62.48 +/- 19.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.5     |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 395000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000243 |
|    n_updates        | 88749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.6     |
|    ep_rew_mean      | -0.206   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14088    |
|    fps              | 214      |
|    time_elapsed     | 1837     |
|    total_timesteps  | 395160   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 88789    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.2     |
|    ep_rew_mean      | -0.195   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14092    |
|    fps              | 215      |
|    time_elapsed     | 1838     |
|    total_timesteps  | 395420   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.82e-05 |
|    n_updates        | 88854    |
----------------------------------
Eval num_timesteps=395500, episode_reward=-0.21 +/- 0.26
Episode length: 67.02 +/- 14.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67       |
|    mean_reward      | -0.208   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 395500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.4e-05  |
|    n_updates        | 88874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.8     |
|    ep_rew_mean      | -0.183   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14096    |
|    fps              | 214      |
|    time_elapsed     | 1840     |
|    total_timesteps  | 395682   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000216 |
|    n_updates        | 88920    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.7     |
|    ep_rew_mean      | -0.183   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14100    |
|    fps              | 215      |
|    time_elapsed     | 1841     |
|    total_timesteps  | 395972   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.59e-05 |
|    n_updates        | 88992    |
----------------------------------
Eval num_timesteps=396000, episode_reward=-0.24 +/- 0.17
Episode length: 65.96 +/- 16.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66       |
|    mean_reward      | -0.243   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 396000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000331 |
|    n_updates        | 88999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.6     |
|    ep_rew_mean      | -0.182   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14104    |
|    fps              | 214      |
|    time_elapsed     | 1843     |
|    total_timesteps  | 396256   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.45e-05 |
|    n_updates        | 89063    |
----------------------------------
Eval num_timesteps=396500, episode_reward=-0.21 +/- 0.23
Episode length: 63.02 +/- 17.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 63       |
|    mean_reward      | -0.212   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 396500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000131 |
|    n_updates        | 89124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.6     |
|    ep_rew_mean      | -0.192   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14108    |
|    fps              | 214      |
|    time_elapsed     | 1846     |
|    total_timesteps  | 396546   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.99e-05 |
|    n_updates        | 89136    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.6     |
|    ep_rew_mean      | -0.196   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14112    |
|    fps              | 214      |
|    time_elapsed     | 1846     |
|    total_timesteps  | 396846   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.73e-05 |
|    n_updates        | 89211    |
----------------------------------
Eval num_timesteps=397000, episode_reward=-0.22 +/- 0.24
Episode length: 65.84 +/- 17.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.8     |
|    mean_reward      | -0.223   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 397000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.59e-05 |
|    n_updates        | 89249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.5     |
|    ep_rew_mean      | -0.196   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14116    |
|    fps              | 214      |
|    time_elapsed     | 1848     |
|    total_timesteps  | 397088   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.47e-05 |
|    n_updates        | 89271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.6     |
|    ep_rew_mean      | -0.196   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14120    |
|    fps              | 214      |
|    time_elapsed     | 1849     |
|    total_timesteps  | 397364   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000121 |
|    n_updates        | 89340    |
----------------------------------
Eval num_timesteps=397500, episode_reward=-0.29 +/- 0.04
Episode length: 71.40 +/- 10.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.4     |
|    mean_reward      | -0.285   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 397500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000169 |
|    n_updates        | 89374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.2     |
|    ep_rew_mean      | -0.195   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14124    |
|    fps              | 214      |
|    time_elapsed     | 1851     |
|    total_timesteps  | 397612   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.23e-05 |
|    n_updates        | 89402    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.5     |
|    ep_rew_mean      | -0.182   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14128    |
|    fps              | 214      |
|    time_elapsed     | 1851     |
|    total_timesteps  | 397841   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.85e-05 |
|    n_updates        | 89460    |
----------------------------------
Eval num_timesteps=398000, episode_reward=-0.18 +/- 0.31
Episode length: 64.50 +/- 18.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 64.5     |
|    mean_reward      | -0.178   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 398000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.25e-05 |
|    n_updates        | 89499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.1     |
|    ep_rew_mean      | -0.18    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14132    |
|    fps              | 214      |
|    time_elapsed     | 1854     |
|    total_timesteps  | 398090   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00015  |
|    n_updates        | 89522    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 64.5     |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14136    |
|    fps              | 214      |
|    time_elapsed     | 1854     |
|    total_timesteps  | 398338   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.4e-05  |
|    n_updates        | 89584    |
----------------------------------
Eval num_timesteps=398500, episode_reward=-0.21 +/- 0.27
Episode length: 67.90 +/- 14.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.9     |
|    mean_reward      | -0.211   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 398500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 7.15e-05 |
|    n_updates        | 89624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.1     |
|    ep_rew_mean      | -0.18    |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14140    |
|    fps              | 214      |
|    time_elapsed     | 1857     |
|    total_timesteps  | 398638   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.000124 |
|    n_updates        | 89659    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.4     |
|    ep_rew_mean      | -0.181   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14144    |
|    fps              | 214      |
|    time_elapsed     | 1857     |
|    total_timesteps  | 398903   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.13e-05 |
|    n_updates        | 89725    |
----------------------------------
Eval num_timesteps=399000, episode_reward=-0.27 +/- 0.05
Episode length: 67.92 +/- 13.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.9     |
|    mean_reward      | -0.271   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 399000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.52e-05 |
|    n_updates        | 89749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66       |
|    ep_rew_mean      | -0.184   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14148    |
|    fps              | 214      |
|    time_elapsed     | 1860     |
|    total_timesteps  | 399203   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.02e-05 |
|    n_updates        | 89800    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.3     |
|    ep_rew_mean      | -0.185   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14152    |
|    fps              | 214      |
|    time_elapsed     | 1860     |
|    total_timesteps  | 399461   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.57e-05 |
|    n_updates        | 89865    |
----------------------------------
Eval num_timesteps=399500, episode_reward=-0.19 +/- 0.22
Episode length: 57.28 +/- 20.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 57.3     |
|    mean_reward      | -0.189   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 399500   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 0.00288  |
|    n_updates        | 89874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.8     |
|    ep_rew_mean      | -0.197   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14156    |
|    fps              | 214      |
|    time_elapsed     | 1862     |
|    total_timesteps  | 399760   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.36e-05 |
|    n_updates        | 89939    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | -0.206   |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 14160    |
|    fps              | 214      |
|    time_elapsed     | 1862     |
|    total_timesteps  | 399944   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.7e-06  |
|    n_updates        | 89985    |
----------------------------------
Eval num_timesteps=400000, episode_reward=-0.17 +/- 0.30
Episode length: 62.26 +/- 18.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.3     |
|    mean_reward      | -0.169   |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 400000   |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.48e-05 |
|    n_updates        | 89999    |
----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/predict-position/dqn-4/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Parameters: {'batch_size': 64, 'learning_rate': 1e-05, 'buffer_size': 15000, 'gamma': 0.99, 'exploration_fraction': 0.4, 'exploration_final_eps': 0.001, 'learning_starts': 40000.0, 'decay_start_steps': 40000.0, 'decay_end_steps': 200000.0}
Training steps: 400000
Frame skip: 4
Using cuda device
Eval num_timesteps=500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | -0.0207  |
| time/              |          |
|    fps             | 130      |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 2500          |
| train/                  |               |
|    approx_kl            | 0.00040370759 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.39         |
|    explained_variance   | -0.0991       |
|    learning_rate        | 1e-05         |
|    loss                 | -0.00151      |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.000573     |
|    value_loss           | 0.0198        |
-------------------------------------------
Eval num_timesteps=3000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.8     |
|    ep_rew_mean     | 0.00977  |
| time/              |          |
|    fps             | 130      |
|    iterations      | 2        |
|    time_elapsed    | 31       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 75             |
|    mean_reward          | -0.3           |
| time/                   |                |
|    total_timesteps      | 4500           |
| train/                  |                |
|    approx_kl            | 0.000100567384 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.39          |
|    explained_variance   | -0.0583        |
|    learning_rate        | 1e-05          |
|    loss                 | 0.0107         |
|    n_updates            | 20             |
|    policy_gradient_loss | -0.000213      |
|    value_loss           | 0.0315         |
--------------------------------------------
Eval num_timesteps=5000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.5     |
|    ep_rew_mean     | 0.0168   |
| time/              |          |
|    fps             | 130      |
|    iterations      | 3        |
|    time_elapsed    | 46       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 6500          |
| train/                  |               |
|    approx_kl            | 0.00010786348 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.39         |
|    explained_variance   | -0.0575       |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0124        |
|    n_updates            | 30            |
|    policy_gradient_loss | -0.000184     |
|    value_loss           | 0.0316        |
-------------------------------------------
Eval num_timesteps=7000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | -0.0112  |
| time/              |          |
|    fps             | 129      |
|    iterations      | 4        |
|    time_elapsed    | 63       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 8500         |
| train/                  |              |
|    approx_kl            | 0.0021616996 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.0125      |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0204       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 0.0246       |
------------------------------------------
Eval num_timesteps=9000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | 0.00862  |
| time/              |          |
|    fps             | 127      |
|    iterations      | 5        |
|    time_elapsed    | 80       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0047196965 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.015       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.007        |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 0.0322       |
------------------------------------------
Eval num_timesteps=11000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.8     |
|    ep_rew_mean     | 0.01     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 6        |
|    time_elapsed    | 97       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 12500        |
| train/                  |              |
|    approx_kl            | 0.0012181685 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | -0.0338      |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0198       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.000771    |
|    value_loss           | 0.0286       |
------------------------------------------
Eval num_timesteps=13000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 0.0129   |
| time/              |          |
|    fps             | 125      |
|    iterations      | 7        |
|    time_elapsed    | 114      |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 14500        |
| train/                  |              |
|    approx_kl            | 0.0011042876 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | -0.0119      |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0152       |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000337    |
|    value_loss           | 0.0288       |
------------------------------------------
Eval num_timesteps=15000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 0.0135   |
| time/              |          |
|    fps             | 124      |
|    iterations      | 8        |
|    time_elapsed    | 131      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 75         |
|    mean_reward          | -0.3       |
| time/                   |            |
|    total_timesteps      | 16500      |
| train/                  |            |
|    approx_kl            | 0.01071987 |
|    clip_fraction        | 0.0162     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | -0.0139    |
|    learning_rate        | 1e-05      |
|    loss                 | -0.0152    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.00258   |
|    value_loss           | 0.0302     |
----------------------------------------
Eval num_timesteps=17000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 0.00401  |
| time/              |          |
|    fps             | 124      |
|    iterations      | 9        |
|    time_elapsed    | 148      |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.013831189 |
|    clip_fraction        | 0.00762     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | -0.0175     |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00959    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00154    |
|    value_loss           | 0.0283      |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 0.00478  |
| time/              |          |
|    fps             | 123      |
|    iterations      | 10       |
|    time_elapsed    | 165      |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0035076225 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | -0.0218      |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0115       |
|    n_updates            | 100          |
|    policy_gradient_loss | -3.43e-05    |
|    value_loss           | 0.0256       |
------------------------------------------
Eval num_timesteps=21000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -0.00496 |
| time/              |          |
|    fps             | 121      |
|    iterations      | 11       |
|    time_elapsed    | 185      |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.004553102 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | -0.0277     |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0101      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00167    |
|    value_loss           | 0.0198      |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | 0.0243   |
| time/              |          |
|    fps             | 121      |
|    iterations      | 12       |
|    time_elapsed    | 202      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0043865493 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | -0.0137      |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0131       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 0.0423       |
------------------------------------------
Eval num_timesteps=25500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 0.0565   |
| time/              |          |
|    fps             | 121      |
|    iterations      | 13       |
|    time_elapsed    | 219      |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0014015386 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | -0.0307      |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0264       |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000273    |
|    value_loss           | 0.0448       |
------------------------------------------
Eval num_timesteps=27500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 0.0545   |
| time/              |          |
|    fps             | 121      |
|    iterations      | 14       |
|    time_elapsed    | 236      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.013236357 |
|    clip_fraction        | 0.00786     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.00519     |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0126      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00357    |
|    value_loss           | 0.0416      |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -0.00685 |
| time/              |          |
|    fps             | 121      |
|    iterations      | 15       |
|    time_elapsed    | 253      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.001667722 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | -0.0563     |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0094      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.000345   |
|    value_loss           | 0.0198      |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 0.0151   |
| time/              |          |
|    fps             | 121      |
|    iterations      | 16       |
|    time_elapsed    | 270      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 75         |
|    mean_reward          | -0.3       |
| time/                   |            |
|    total_timesteps      | 33000      |
| train/                  |            |
|    approx_kl            | 0.01546682 |
|    clip_fraction        | 0.0909     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 5.12e-05   |
|    learning_rate        | 1e-05      |
|    loss                 | 0.0175     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.00548   |
|    value_loss           | 0.0313     |
----------------------------------------
Eval num_timesteps=33500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 0.014    |
| time/              |          |
|    fps             | 120      |
|    iterations      | 17       |
|    time_elapsed    | 287      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014654358 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | -0.0249     |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00177    |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00172    |
|    value_loss           | 0.0255      |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 0.0432   |
| time/              |          |
|    fps             | 120      |
|    iterations      | 18       |
|    time_elapsed    | 304      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.008824783 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.00904     |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0221      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 0.0401      |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 0.064    |
| time/              |          |
|    fps             | 120      |
|    iterations      | 19       |
|    time_elapsed    | 321      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0073812758 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.962       |
|    explained_variance   | 0.0273       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0165       |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 0.0446       |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 0.0236   |
| time/              |          |
|    fps             | 120      |
|    iterations      | 20       |
|    time_elapsed    | 338      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.006552733 |
|    clip_fraction        | 0.0377      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.846      |
|    explained_variance   | -0.00116    |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0355      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 0.0402      |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.2     |
|    ep_rew_mean     | -0.00191 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 21       |
|    time_elapsed    | 359      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 43500        |
| train/                  |              |
|    approx_kl            | 0.0074704643 |
|    clip_fraction        | 0.0794       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.721       |
|    explained_variance   | 0.0191       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0192       |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00186     |
|    value_loss           | 0.0258       |
------------------------------------------
Eval num_timesteps=44000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | -0.00358 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 22       |
|    time_elapsed    | 376      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 0.0033958089 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.717       |
|    explained_variance   | 0.0115       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000745     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00263     |
|    value_loss           | 0.0259       |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 0.0154   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 23       |
|    time_elapsed    | 393      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 47500        |
| train/                  |              |
|    approx_kl            | 0.0030593728 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.644       |
|    explained_variance   | 0.0263       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.018        |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 0.0287       |
------------------------------------------
Eval num_timesteps=48000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.7     |
|    ep_rew_mean     | 0.0163   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 24       |
|    time_elapsed    | 410      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.003528106 |
|    clip_fraction        | 0.00542     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.602      |
|    explained_variance   | 0.024       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0121      |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.000997    |
|    value_loss           | 0.0283      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 0.0281   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 25       |
|    time_elapsed    | 427      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 51500        |
| train/                  |              |
|    approx_kl            | 0.0028056605 |
|    clip_fraction        | 0.00278      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.527       |
|    explained_variance   | 0.0383       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0196       |
|    n_updates            | 250          |
|    policy_gradient_loss | 0.00158      |
|    value_loss           | 0.0294       |
------------------------------------------
Eval num_timesteps=52000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | -0.0537  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 26       |
|    time_elapsed    | 444      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 53500        |
| train/                  |              |
|    approx_kl            | 0.0029574041 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.456       |
|    explained_variance   | -0.0333      |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000722    |
|    n_updates            | 260          |
|    policy_gradient_loss | 0.000258     |
|    value_loss           | 0.0114       |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -0.00192 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 27       |
|    time_elapsed    | 461      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 0.0024090335 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.391       |
|    explained_variance   | 0.0732       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0125       |
|    n_updates            | 270          |
|    policy_gradient_loss | 0.000277     |
|    value_loss           | 0.0295       |
------------------------------------------
Eval num_timesteps=56000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | -0.00465 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 28       |
|    time_elapsed    | 478      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 0.0011041407 |
|    clip_fraction        | 0.00874      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.34        |
|    explained_variance   | 0.0514       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00918      |
|    n_updates            | 280          |
|    policy_gradient_loss | 0.000914     |
|    value_loss           | 0.0281       |
------------------------------------------
Eval num_timesteps=58000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | -0.0174  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 29       |
|    time_elapsed    | 495      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0015791641 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.306       |
|    explained_variance   | 0.0483       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0236       |
|    n_updates            | 290          |
|    policy_gradient_loss | 0.000551     |
|    value_loss           | 0.0285       |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.3     |
|    ep_rew_mean     | -0.0424  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 30       |
|    time_elapsed    | 512      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | 0.0010168307 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.313       |
|    explained_variance   | -0.0513      |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0167       |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00279     |
|    value_loss           | 0.0084       |
------------------------------------------
Eval num_timesteps=62000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -0.069   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 31       |
|    time_elapsed    | 529      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 63500        |
| train/                  |              |
|    approx_kl            | 0.0008702419 |
|    clip_fraction        | 0.00947      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.334       |
|    explained_variance   | 0.0615       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00526      |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 0.0155       |
------------------------------------------
Eval num_timesteps=64000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | -0.0473  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 32       |
|    time_elapsed    | 549      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0013336416 |
|    clip_fraction        | 0.0359       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.381       |
|    explained_variance   | 0.0738       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0034       |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00519     |
|    value_loss           | 0.0147       |
------------------------------------------
Eval num_timesteps=66500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | 0.00797  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 33       |
|    time_elapsed    | 566      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 0.0014361888 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.354       |
|    explained_variance   | 0.06         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.014        |
|    n_updates            | 330          |
|    policy_gradient_loss | 0.000896     |
|    value_loss           | 0.0339       |
------------------------------------------
Eval num_timesteps=68500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27       |
|    ep_rew_mean     | -0.00699 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 34       |
|    time_elapsed    | 583      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 70000         |
| train/                  |               |
|    approx_kl            | 0.00093394273 |
|    clip_fraction        | 0.00649       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.316        |
|    explained_variance   | 0.033         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0183        |
|    n_updates            | 340           |
|    policy_gradient_loss | 0.000664      |
|    value_loss           | 0.0226        |
-------------------------------------------
Eval num_timesteps=70500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.1     |
|    ep_rew_mean     | -0.0413  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 35       |
|    time_elapsed    | 600      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 72000         |
| train/                  |               |
|    approx_kl            | 0.00011472049 |
|    clip_fraction        | 0.00366       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.315        |
|    explained_variance   | 0.0517        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00284       |
|    n_updates            | 350           |
|    policy_gradient_loss | -0.00107      |
|    value_loss           | 0.018         |
-------------------------------------------
Eval num_timesteps=72500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -0.0496  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 36       |
|    time_elapsed    | 616      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0010058789 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.336       |
|    explained_variance   | 0.0644       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00508     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00304     |
|    value_loss           | 0.0104       |
------------------------------------------
Eval num_timesteps=74500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.2     |
|    ep_rew_mean     | -0.0879  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 37       |
|    time_elapsed    | 633      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 0.0015202455 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.37        |
|    explained_variance   | -0.497       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0128       |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.0058      |
|    value_loss           | 0.000813     |
------------------------------------------
Eval num_timesteps=76500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -0.022   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 38       |
|    time_elapsed    | 650      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0010836024 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.333       |
|    explained_variance   | 0.0324       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00811      |
|    n_updates            | 380          |
|    policy_gradient_loss | 0.00102      |
|    value_loss           | 0.0246       |
------------------------------------------
Eval num_timesteps=78500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -0.0417  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 39       |
|    time_elapsed    | 667      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0010891573 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.366       |
|    explained_variance   | 0.0257       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00453      |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00166     |
|    value_loss           | 0.012        |
------------------------------------------
Eval num_timesteps=80500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -0.0525  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 40       |
|    time_elapsed    | 684      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0006417026 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.394       |
|    explained_variance   | 0.0274       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00822      |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00215     |
|    value_loss           | 0.0148       |
------------------------------------------
Eval num_timesteps=82500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | -0.036   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 41       |
|    time_elapsed    | 701      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 84000         |
| train/                  |               |
|    approx_kl            | 0.00085419114 |
|    clip_fraction        | 0.00249       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.345        |
|    explained_variance   | -0.00711      |
|    learning_rate        | 1e-05         |
|    loss                 | 0.006         |
|    n_updates            | 410           |
|    policy_gradient_loss | 0.00129       |
|    value_loss           | 0.0147        |
-------------------------------------------
Eval num_timesteps=84500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -0.0375  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 42       |
|    time_elapsed    | 721      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 86500        |
| train/                  |              |
|    approx_kl            | 0.0012334706 |
|    clip_fraction        | 0.00542      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.344       |
|    explained_variance   | 0.0705       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00964      |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 0.0214       |
------------------------------------------
Eval num_timesteps=87000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -0.0452  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 43       |
|    time_elapsed    | 738      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 88500        |
| train/                  |              |
|    approx_kl            | 0.0006897543 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.369       |
|    explained_variance   | 0.058        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00262     |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.0025      |
|    value_loss           | 0.0139       |
------------------------------------------
Eval num_timesteps=89000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -0.0136  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 44       |
|    time_elapsed    | 755      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0006811345 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.404       |
|    explained_variance   | 0.0538       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0149       |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00341     |
|    value_loss           | 0.0246       |
------------------------------------------
Eval num_timesteps=91000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | -0.0444  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 45       |
|    time_elapsed    | 772      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 92500        |
| train/                  |              |
|    approx_kl            | 0.0010637782 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.367       |
|    explained_variance   | -0.0146      |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00601      |
|    n_updates            | 450          |
|    policy_gradient_loss | 0.00045      |
|    value_loss           | 0.0152       |
------------------------------------------
Eval num_timesteps=93000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | -0.00938 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 46       |
|    time_elapsed    | 788      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 94500         |
| train/                  |               |
|    approx_kl            | 0.00058146054 |
|    clip_fraction        | 0.00391       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.369        |
|    explained_variance   | 0.0275        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.000506      |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.0013       |
|    value_loss           | 0.0241        |
-------------------------------------------
Eval num_timesteps=95000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | -0.0414  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 47       |
|    time_elapsed    | 805      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0010095139 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.337       |
|    explained_variance   | 0.0266       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0154       |
|    n_updates            | 470          |
|    policy_gradient_loss | 0.00134      |
|    value_loss           | 0.0179       |
------------------------------------------
Eval num_timesteps=97000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -0.0336  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 48       |
|    time_elapsed    | 822      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 98500        |
| train/                  |              |
|    approx_kl            | 0.0006433863 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.327       |
|    explained_variance   | 0.0707       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00894     |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 0.02         |
------------------------------------------
Eval num_timesteps=99000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | -0.0281  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 49       |
|    time_elapsed    | 839      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 100500        |
| train/                  |               |
|    approx_kl            | 0.00047690145 |
|    clip_fraction        | 0.00459       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.309        |
|    explained_variance   | 0.0365        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0123        |
|    n_updates            | 490           |
|    policy_gradient_loss | 0.000609      |
|    value_loss           | 0.0211        |
-------------------------------------------
Eval num_timesteps=101000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | -0.0488  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 50       |
|    time_elapsed    | 856      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 102500       |
| train/                  |              |
|    approx_kl            | 0.0009992372 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.351       |
|    explained_variance   | 0.0803       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0137      |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00288     |
|    value_loss           | 0.014        |
------------------------------------------
Eval num_timesteps=103000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | -0.0325  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 51       |
|    time_elapsed    | 873      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 75         |
|    mean_reward          | -0.3       |
| time/                   |            |
|    total_timesteps      | 104500     |
| train/                  |            |
|    approx_kl            | 0.00212237 |
|    clip_fraction        | 0.0105     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.295     |
|    explained_variance   | -0.0284    |
|    learning_rate        | 1e-05      |
|    loss                 | 0.0114     |
|    n_updates            | 510        |
|    policy_gradient_loss | 0.00108    |
|    value_loss           | 0.0179     |
----------------------------------------
Eval num_timesteps=105000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 0.0128   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 52       |
|    time_elapsed    | 890      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0011681214 |
|    clip_fraction        | 0.00933      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.242       |
|    explained_variance   | 0.0468       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0163       |
|    n_updates            | 520          |
|    policy_gradient_loss | 0.000795     |
|    value_loss           | 0.0279       |
------------------------------------------
Eval num_timesteps=107000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 0.0135   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 53       |
|    time_elapsed    | 910      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0004473351 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.279       |
|    explained_variance   | 0.0696       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0114       |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.000595    |
|    value_loss           | 0.0323       |
------------------------------------------
Eval num_timesteps=109500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 0.00605  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 54       |
|    time_elapsed    | 927      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 111000        |
| train/                  |               |
|    approx_kl            | 0.00070774707 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.284        |
|    explained_variance   | 0.0481        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0049        |
|    n_updates            | 540           |
|    policy_gradient_loss | -0.000733     |
|    value_loss           | 0.0266        |
-------------------------------------------
Eval num_timesteps=111500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 0.00268  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 55       |
|    time_elapsed    | 944      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 0.0010242823 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.229       |
|    explained_variance   | 0.0706       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0181       |
|    n_updates            | 550          |
|    policy_gradient_loss | 0.000393     |
|    value_loss           | 0.0269       |
------------------------------------------
Eval num_timesteps=113500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -0.0175  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 56       |
|    time_elapsed    | 961      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 0.0003704719 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.218       |
|    explained_variance   | 0.0148       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00874      |
|    n_updates            | 560          |
|    policy_gradient_loss | 0.000296     |
|    value_loss           | 0.0221       |
------------------------------------------
Eval num_timesteps=115500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | -0.0311  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 57       |
|    time_elapsed    | 978      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 117000       |
| train/                  |              |
|    approx_kl            | 0.0008041504 |
|    clip_fraction        | 0.00269      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0.0507       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00405      |
|    n_updates            | 570          |
|    policy_gradient_loss | 0.000689     |
|    value_loss           | 0.0184       |
------------------------------------------
Eval num_timesteps=117500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -0.0635  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 58       |
|    time_elapsed    | 994      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0016138048 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.0572       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0117       |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00421     |
|    value_loss           | 0.0079       |
------------------------------------------
Eval num_timesteps=119500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -0.0324  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 59       |
|    time_elapsed    | 1011     |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 75         |
|    mean_reward          | -0.3       |
| time/                   |            |
|    total_timesteps      | 121000     |
| train/                  |            |
|    approx_kl            | 0.04927281 |
|    clip_fraction        | 0.0536     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.345     |
|    explained_variance   | 0.0892     |
|    learning_rate        | 1e-05      |
|    loss                 | 0.00865    |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.00594   |
|    value_loss           | 0.0189     |
----------------------------------------
Eval num_timesteps=121500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 0.0847   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 60       |
|    time_elapsed    | 1028     |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0022960126 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.323       |
|    explained_variance   | 0.0318       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0417       |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.000757    |
|    value_loss           | 0.0515       |
------------------------------------------
Eval num_timesteps=123500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | -0.00076 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 61       |
|    time_elapsed    | 1045     |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0026994855 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.301       |
|    explained_variance   | 0.0324       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00569      |
|    n_updates            | 610          |
|    policy_gradient_loss | 0.000582     |
|    value_loss           | 0.0222       |
------------------------------------------
Eval num_timesteps=125500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.5     |
|    ep_rew_mean     | -0.023   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 62       |
|    time_elapsed    | 1062     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 0.0011064657 |
|    clip_fraction        | 0.00864      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.267       |
|    explained_variance   | 0.044        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0227       |
|    n_updates            | 620          |
|    policy_gradient_loss | 0.000229     |
|    value_loss           | 0.0197       |
------------------------------------------
Eval num_timesteps=127500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | -0.015   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 63       |
|    time_elapsed    | 1082     |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 129500        |
| train/                  |               |
|    approx_kl            | 0.00092647143 |
|    clip_fraction        | 0.00791       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.28         |
|    explained_variance   | 0.0481        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0179        |
|    n_updates            | 630           |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 0.0198        |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 0.0549   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 64       |
|    time_elapsed    | 1099     |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 0.0010006328 |
|    clip_fraction        | 0.00845      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.256       |
|    explained_variance   | 0.0667       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0298       |
|    n_updates            | 640          |
|    policy_gradient_loss | 0.000121     |
|    value_loss           | 0.0438       |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | -0.041   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 65       |
|    time_elapsed    | 1116     |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 133500       |
| train/                  |              |
|    approx_kl            | 0.0020119864 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.232       |
|    explained_variance   | -0.023       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00501     |
|    n_updates            | 650          |
|    policy_gradient_loss | 0.000933     |
|    value_loss           | 0.0146       |
------------------------------------------
Eval num_timesteps=134000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 0.00539  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 66       |
|    time_elapsed    | 1134     |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 135500        |
| train/                  |               |
|    approx_kl            | 0.00059856195 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.233        |
|    explained_variance   | 0.0781        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0257        |
|    n_updates            | 660           |
|    policy_gradient_loss | 0.000361      |
|    value_loss           | 0.0288        |
-------------------------------------------
Eval num_timesteps=136000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | -0.0379  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 67       |
|    time_elapsed    | 1150     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 137500       |
| train/                  |              |
|    approx_kl            | 0.0018791455 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.204       |
|    explained_variance   | 0.0373       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0209       |
|    n_updates            | 670          |
|    policy_gradient_loss | 0.000612     |
|    value_loss           | 0.0186       |
------------------------------------------
Eval num_timesteps=138000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | 0.0152   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 68       |
|    time_elapsed    | 1167     |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 139500       |
| train/                  |              |
|    approx_kl            | 0.0011413691 |
|    clip_fraction        | 0.0285       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.248       |
|    explained_variance   | 0.0724       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0197       |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.0027      |
|    value_loss           | 0.0294       |
------------------------------------------
Eval num_timesteps=140000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | -0.00135 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 69       |
|    time_elapsed    | 1184     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 141500       |
| train/                  |              |
|    approx_kl            | 0.0007308128 |
|    clip_fraction        | 0.00728      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0.0805       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00467      |
|    n_updates            | 690          |
|    policy_gradient_loss | -8.88e-05    |
|    value_loss           | 0.0208       |
------------------------------------------
Eval num_timesteps=142000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | -0.0465  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 70       |
|    time_elapsed    | 1201     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 143500       |
| train/                  |              |
|    approx_kl            | 0.0010484672 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.228       |
|    explained_variance   | 0.0281       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00681      |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00253     |
|    value_loss           | 0.0136       |
------------------------------------------
Eval num_timesteps=144000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | -0.0116  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 71       |
|    time_elapsed    | 1218     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 0.0008681313 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.199       |
|    explained_variance   | 0.049        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00636      |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.000454    |
|    value_loss           | 0.0228       |
------------------------------------------
Eval num_timesteps=146000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | -0.0079  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 72       |
|    time_elapsed    | 1235     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 147500      |
| train/                  |             |
|    approx_kl            | 0.001668345 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.234      |
|    explained_variance   | 0.0948      |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00116     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 0.0258      |
-----------------------------------------
Eval num_timesteps=148000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | -0.031   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 73       |
|    time_elapsed    | 1256     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.001271938 |
|    clip_fraction        | 0.0119      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.242      |
|    explained_variance   | 0.025       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00171    |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00264    |
|    value_loss           | 0.0171      |
-----------------------------------------
Eval num_timesteps=150500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | -0.00823 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 74       |
|    time_elapsed    | 1273     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 152000        |
| train/                  |               |
|    approx_kl            | 0.00087711023 |
|    clip_fraction        | 0.0201        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.246        |
|    explained_variance   | 0.0813        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.023         |
|    n_updates            | 740           |
|    policy_gradient_loss | -0.00191      |
|    value_loss           | 0.0254        |
-------------------------------------------
Eval num_timesteps=152500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 0.0245   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 75       |
|    time_elapsed    | 1289     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 154000        |
| train/                  |               |
|    approx_kl            | 0.00073306245 |
|    clip_fraction        | 0.014         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.262        |
|    explained_variance   | 0.0891        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0213        |
|    n_updates            | 750           |
|    policy_gradient_loss | -0.00166      |
|    value_loss           | 0.0303        |
-------------------------------------------
Eval num_timesteps=154500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.2     |
|    ep_rew_mean     | 0.0281   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 76       |
|    time_elapsed    | 1306     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0009640913 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.202       |
|    explained_variance   | 0.0613       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0098       |
|    n_updates            | 760          |
|    policy_gradient_loss | 0.000616     |
|    value_loss           | 0.0302       |
------------------------------------------
Eval num_timesteps=156500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | -0.0375  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 77       |
|    time_elapsed    | 1323     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.000514962 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.243      |
|    explained_variance   | 0.0603      |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0025      |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.000837   |
|    value_loss           | 0.0168      |
-----------------------------------------
Eval num_timesteps=158500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 0.0894   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 78       |
|    time_elapsed    | 1340     |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0011963133 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.252       |
|    explained_variance   | 0.1          |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0272       |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00166     |
|    value_loss           | 0.0496       |
------------------------------------------
Eval num_timesteps=160500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | -0.00252 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 79       |
|    time_elapsed    | 1357     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 162000       |
| train/                  |              |
|    approx_kl            | 0.0006929649 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.257       |
|    explained_variance   | 0.0819       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0068       |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.000691    |
|    value_loss           | 0.0286       |
------------------------------------------
Eval num_timesteps=162500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 0.00766  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 80       |
|    time_elapsed    | 1374     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0013144473 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | 0.0949       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0285       |
|    n_updates            | 800          |
|    policy_gradient_loss | 8.18e-05     |
|    value_loss           | 0.0257       |
------------------------------------------
Eval num_timesteps=164500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.5     |
|    ep_rew_mean     | 0.0472   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 81       |
|    time_elapsed    | 1391     |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.002272006 |
|    clip_fraction        | 0.0448      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.201      |
|    explained_variance   | 0.0391      |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0224      |
|    n_updates            | 810         |
|    policy_gradient_loss | 1.95e-05    |
|    value_loss           | 0.0367      |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.5     |
|    ep_rew_mean     | 0.0371   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 82       |
|    time_elapsed    | 1408     |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 0.0009064707 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.218       |
|    explained_variance   | 0.0929       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0207       |
|    n_updates            | 820          |
|    policy_gradient_loss | -2.71e-06    |
|    value_loss           | 0.0401       |
------------------------------------------
Eval num_timesteps=168500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | -0.00939 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 83       |
|    time_elapsed    | 1424     |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.031108556 |
|    clip_fraction        | 0.053       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.371      |
|    explained_variance   | 0.0586      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0178     |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 0.0171      |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | -0.0304  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 84       |
|    time_elapsed    | 1445     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0008269071 |
|    clip_fraction        | 0.00747      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.324       |
|    explained_variance   | 0.0567       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.011        |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.000555    |
|    value_loss           | 0.0135       |
------------------------------------------
Eval num_timesteps=173000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 0.0129   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 85       |
|    time_elapsed    | 1462     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 174500        |
| train/                  |               |
|    approx_kl            | 0.00011639035 |
|    clip_fraction        | 0.00322       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.338        |
|    explained_variance   | 0.0698        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0159        |
|    n_updates            | 850           |
|    policy_gradient_loss | 0.000698      |
|    value_loss           | 0.0279        |
-------------------------------------------
Eval num_timesteps=175000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 0.0036   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 86       |
|    time_elapsed    | 1479     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.002202072 |
|    clip_fraction        | 0.0225      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.342      |
|    explained_variance   | 0.0975      |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0117      |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00308    |
|    value_loss           | 0.0353      |
-----------------------------------------
Eval num_timesteps=177000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 0.023    |
| time/              |          |
|    fps             | 119      |
|    iterations      | 87       |
|    time_elapsed    | 1495     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0019557923 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.318       |
|    explained_variance   | 0.0905       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0188       |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 0.0379       |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 0.0347   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 88       |
|    time_elapsed    | 1512     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.002189822 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.29       |
|    explained_variance   | 0.0723      |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0266      |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 0.0331      |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 0.013    |
| time/              |          |
|    fps             | 119      |
|    iterations      | 89       |
|    time_elapsed    | 1529     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 0.0017863822 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.249       |
|    explained_variance   | 0.0608       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0164       |
|    n_updates            | 890          |
|    policy_gradient_loss | 0.00107      |
|    value_loss           | 0.0266       |
------------------------------------------
Eval num_timesteps=183000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 0.0114   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 90       |
|    time_elapsed    | 1546     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 184500      |
| train/                  |             |
|    approx_kl            | 0.002162484 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.284      |
|    explained_variance   | 0.0698      |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00954     |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00105    |
|    value_loss           | 0.0274      |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | 0.0119   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 91       |
|    time_elapsed    | 1564     |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 75            |
|    mean_reward          | -0.3          |
| time/                   |               |
|    total_timesteps      | 186500        |
| train/                  |               |
|    approx_kl            | 0.00080490136 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.286        |
|    explained_variance   | 0.0956        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.025         |
|    n_updates            | 910           |
|    policy_gradient_loss | -0.0013       |
|    value_loss           | 0.0336        |
-------------------------------------------
Eval num_timesteps=187000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -0.00599 |
| time/              |          |
|    fps             | 119      |
|    iterations      | 92       |
|    time_elapsed    | 1580     |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 0.0015896041 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.238       |
|    explained_variance   | 0.0827       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0167       |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.000469    |
|    value_loss           | 0.022        |
------------------------------------------
Eval num_timesteps=189000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 0.0249   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 93       |
|    time_elapsed    | 1597     |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 0.0011828416 |
|    clip_fraction        | 0.00996      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.211       |
|    explained_variance   | 0.0695       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00565      |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.000168    |
|    value_loss           | 0.0337       |
------------------------------------------
Eval num_timesteps=191000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | 0.0823   |
| time/              |          |
|    fps             | 118      |
|    iterations      | 94       |
|    time_elapsed    | 1618     |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0010056768 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.242       |
|    explained_variance   | 0.137        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.019        |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.000868    |
|    value_loss           | 0.049        |
------------------------------------------
Eval num_timesteps=193500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 0.0251   |
| time/              |          |
|    fps             | 118      |
|    iterations      | 95       |
|    time_elapsed    | 1635     |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.002760748 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.147       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00482     |
|    n_updates            | 950         |
|    policy_gradient_loss | -0.00051    |
|    value_loss           | 0.033       |
-----------------------------------------
Eval num_timesteps=195500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18       |
|    ep_rew_mean     | -0.001   |
| time/              |          |
|    fps             | 118      |
|    iterations      | 96       |
|    time_elapsed    | 1652     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 0.0014252684 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.231       |
|    explained_variance   | 0.121        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00422      |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 0.0272       |
------------------------------------------
Eval num_timesteps=197500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 0.0333   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 97       |
|    time_elapsed    | 1669     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.001504463 |
|    clip_fraction        | 0.011       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.197      |
|    explained_variance   | 0.0892      |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0125      |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00194    |
|    value_loss           | 0.037       |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 0.0666   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 98       |
|    time_elapsed    | 1686     |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 75           |
|    mean_reward          | -0.3         |
| time/                   |              |
|    total_timesteps      | 201000       |
| train/                  |              |
|    approx_kl            | 0.0016070998 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.179       |
|    explained_variance   | 0.127        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0215       |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00117     |
|    value_loss           | 0.0459       |
------------------------------------------
Eval num_timesteps=201500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-0.30 +/- 0.00
Episode length: 75.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -0.3     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 0.0266   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 99       |
|    time_elapsed    | 1703     |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=-0.17 +/- 0.34
Episode length: 68.16 +/- 15.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 68.2         |
|    mean_reward          | -0.172       |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 0.0032917017 |
|    clip_fraction        | 0.0464       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.192       |
|    explained_variance   | 0.119        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0156       |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00268     |
|    value_loss           | 0.0354       |
------------------------------------------
New best mean reward!
Eval num_timesteps=203500, episode_reward=-0.22 +/- 0.26
Episode length: 70.92 +/- 11.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.9     |
|    mean_reward     | -0.224   |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-0.20 +/- 0.30
Episode length: 70.14 +/- 12.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.1     |
|    mean_reward     | -0.2     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-0.25 +/- 0.22
Episode length: 71.74 +/- 8.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.7     |
|    mean_reward     | -0.247   |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -0.0143  |
| time/              |          |
|    fps             | 119      |
|    iterations      | 100      |
|    time_elapsed    | 1719     |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 0.00184       |
| time/                   |               |
|    total_timesteps      | 205000        |
| train/                  |               |
|    approx_kl            | 0.00063445425 |
|    clip_fraction        | 0.00391       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.167        |
|    explained_variance   | 0.118         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0209        |
|    n_updates            | 1000          |
|    policy_gradient_loss | -0.000574     |
|    value_loss           | 0.0186        |
-------------------------------------------
New best mean reward!
Eval num_timesteps=205500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00204  |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
New best mean reward!
Eval num_timesteps=206000, episode_reward=-0.02 +/- 0.20
Episode length: 14.84 +/- 0.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0184  |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | 0.0193   |
| time/              |          |
|    fps             | 119      |
|    iterations      | 101      |
|    time_elapsed    | 1726     |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 0.0222       |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0021462352 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.137       |
|    explained_variance   | 0.147        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0199       |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 0.0253       |
------------------------------------------
Eval num_timesteps=207500, episode_reward=-0.02 +/- 0.20
Episode length: 14.88 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0185  |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | 0.0197   |
| time/              |          |
|    fps             | 120      |
|    iterations      | 102      |
|    time_elapsed    | 1732     |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=-0.04 +/- 0.14
Episode length: 14.92 +/- 0.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.9         |
|    mean_reward          | -0.0386      |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0012301728 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 0.0664       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0213       |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.000445    |
|    value_loss           | 0.0274       |
------------------------------------------
Eval num_timesteps=209500, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00192  |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=0.08 +/- 0.35
Episode length: 14.46 +/- 1.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0832   |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | 0.0186   |
| time/              |          |
|    fps             | 121      |
|    iterations      | 103      |
|    time_elapsed    | 1738     |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-0.27 +/- 0.16
Episode length: 73.02 +/- 9.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73           |
|    mean_reward          | -0.272       |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 0.0019749142 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.125        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0213       |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 0.0397       |
------------------------------------------
Eval num_timesteps=211500, episode_reward=-0.30 +/- 0.01
Episode length: 74.26 +/- 3.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.3     |
|    mean_reward     | -0.297   |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-0.26 +/- 0.21
Episode length: 74.08 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.1     |
|    mean_reward     | -0.256   |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-0.25 +/- 0.22
Episode length: 73.74 +/- 5.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.7     |
|    mean_reward     | -0.255   |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | 0.0185   |
| time/              |          |
|    fps             | 121      |
|    iterations      | 104      |
|    time_elapsed    | 1755     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=0.00 +/- 0.24
Episode length: 14.74 +/- 1.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 0.00206      |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 0.0017855952 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.132       |
|    explained_variance   | 0.128        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.01         |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.00109     |
|    value_loss           | 0.031        |
------------------------------------------
Eval num_timesteps=213500, episode_reward=0.10 +/- 0.37
Episode length: 14.44 +/- 1.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.103    |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
New best mean reward!
Eval num_timesteps=214000, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00182  |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=0.02 +/- 0.27
Episode length: 14.72 +/- 0.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0221   |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | 0.0393   |
| time/              |          |
|    fps             | 122      |
|    iterations      | 105      |
|    time_elapsed    | 1762     |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 0.0426       |
| time/                   |              |
|    total_timesteps      | 215500       |
| train/                  |              |
|    approx_kl            | 0.0017950055 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.131       |
|    explained_variance   | 0.139        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00689      |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.000585    |
|    value_loss           | 0.0382       |
------------------------------------------
Eval num_timesteps=216000, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=0.08 +/- 0.35
Episode length: 14.52 +/- 1.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.083    |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-0.02 +/- 0.20
Episode length: 14.88 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0185  |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | 0.0406   |
| time/              |          |
|    fps             | 122      |
|    iterations      | 106      |
|    time_elapsed    | 1768     |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=0.04 +/- 0.30
Episode length: 14.68 +/- 1.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0424        |
| time/                   |               |
|    total_timesteps      | 217500        |
| train/                  |               |
|    approx_kl            | 0.00026422457 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.112        |
|    explained_variance   | 0.125         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0171        |
|    n_updates            | 1060          |
|    policy_gradient_loss | -0.00028      |
|    value_loss           | 0.0349        |
-------------------------------------------
Eval num_timesteps=218000, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=0.10 +/- 0.37
Episode length: 14.38 +/- 1.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.104    |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
New best mean reward!
Eval num_timesteps=219000, episode_reward=0.02 +/- 0.27
Episode length: 14.76 +/- 0.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.022    |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | 0.0405   |
| time/              |          |
|    fps             | 123      |
|    iterations      | 107      |
|    time_elapsed    | 1775     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-0.02 +/- 0.20
Episode length: 14.84 +/- 0.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | -0.0183       |
| time/                   |               |
|    total_timesteps      | 219500        |
| train/                  |               |
|    approx_kl            | 0.00036448237 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.104        |
|    explained_variance   | 0.127         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0227        |
|    n_updates            | 1070          |
|    policy_gradient_loss | -0.000394     |
|    value_loss           | 0.0359        |
-------------------------------------------
Eval num_timesteps=220000, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0388  |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=0.06 +/- 0.33
Episode length: 14.54 +/- 1.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.063    |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00192  |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -0.00011 |
| time/              |          |
|    fps             | 124      |
|    iterations      | 108      |
|    time_elapsed    | 1781     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 0.0222       |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 0.0018003652 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0933      |
|    explained_variance   | 0.107        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000894    |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.000925    |
|    value_loss           | 0.0203       |
------------------------------------------
Eval num_timesteps=222000, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00186  |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | 0.0305   |
| time/              |          |
|    fps             | 124      |
|    iterations      | 109      |
|    time_elapsed    | 1788     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=-0.02 +/- 0.20
Episode length: 14.86 +/- 0.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -0.0185       |
| time/                   |               |
|    total_timesteps      | 223500        |
| train/                  |               |
|    approx_kl            | 0.00014022179 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0836       |
|    explained_variance   | 0.139         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0232        |
|    n_updates            | 1090          |
|    policy_gradient_loss | -6.06e-05     |
|    value_loss           | 0.0383        |
-------------------------------------------
Eval num_timesteps=224000, episode_reward=-0.04 +/- 0.14
Episode length: 14.90 +/- 0.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0386  |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=0.10 +/- 0.37
Episode length: 14.38 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.104    |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | 0.02     |
| time/              |          |
|    fps             | 125      |
|    iterations      | 110      |
|    time_elapsed    | 1794     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=0.08 +/- 0.35
Episode length: 14.46 +/- 1.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.5         |
|    mean_reward          | 0.0832       |
| time/                   |              |
|    total_timesteps      | 225500       |
| train/                  |              |
|    approx_kl            | 0.0006841213 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0943      |
|    explained_variance   | 0.134        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0335       |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.000698    |
|    value_loss           | 0.0342       |
------------------------------------------
Eval num_timesteps=226000, episode_reward=0.04 +/- 0.30
Episode length: 14.68 +/- 1.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0423   |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0221   |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=0.08 +/- 0.35
Episode length: 14.48 +/- 1.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0831   |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 0.0311   |
| time/              |          |
|    fps             | 126      |
|    iterations      | 111      |
|    time_elapsed    | 1800     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=0.06 +/- 0.33
Episode length: 14.58 +/- 1.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0628        |
| time/                   |               |
|    total_timesteps      | 227500        |
| train/                  |               |
|    approx_kl            | 8.6408865e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.085        |
|    explained_variance   | 0.14          |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00212       |
|    n_updates            | 1110          |
|    policy_gradient_loss | -0.000165     |
|    value_loss           | 0.0286        |
-------------------------------------------
Eval num_timesteps=228000, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00198  |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=0.06 +/- 0.33
Episode length: 14.54 +/- 1.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0628   |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | 0.00026  |
| time/              |          |
|    fps             | 126      |
|    iterations      | 112      |
|    time_elapsed    | 1807     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=0.08 +/- 0.35
Episode length: 14.44 +/- 1.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 0.0833       |
| time/                   |              |
|    total_timesteps      | 229500       |
| train/                  |              |
|    approx_kl            | 0.0009348094 |
|    clip_fraction        | 0.00806      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0765      |
|    explained_variance   | 0.114        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0134       |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.000518    |
|    value_loss           | 0.0207       |
------------------------------------------
Eval num_timesteps=230000, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0425   |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-0.02 +/- 0.20
Episode length: 14.82 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0183  |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | 0.0813   |
| time/              |          |
|    fps             | 127      |
|    iterations      | 113      |
|    time_elapsed    | 1813     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=0.06 +/- 0.33
Episode length: 14.54 +/- 1.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.5         |
|    mean_reward          | 0.0629       |
| time/                   |              |
|    total_timesteps      | 231500       |
| train/                  |              |
|    approx_kl            | 0.0006001749 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0775      |
|    explained_variance   | 0.146        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0279       |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.000944    |
|    value_loss           | 0.0425       |
------------------------------------------
Eval num_timesteps=232000, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00198  |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=0.04 +/- 0.30
Episode length: 14.64 +/- 1.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0425   |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=0.06 +/- 0.33
Episode length: 14.50 +/- 1.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0631   |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | 0.00025  |
| time/              |          |
|    fps             | 128      |
|    iterations      | 114      |
|    time_elapsed    | 1819     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=0.06 +/- 0.33
Episode length: 14.46 +/- 1.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.5         |
|    mean_reward          | 0.0633       |
| time/                   |              |
|    total_timesteps      | 233500       |
| train/                  |              |
|    approx_kl            | 0.0038136146 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0715      |
|    explained_variance   | 0.0856       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00971      |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 0.0211       |
------------------------------------------
Eval num_timesteps=234000, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=0.04 +/- 0.30
Episode length: 14.66 +/- 1.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0425   |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=0.08 +/- 0.35
Episode length: 14.44 +/- 1.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.0832   |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0425   |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 0.00106  |
| time/              |          |
|    fps             | 128      |
|    iterations      | 115      |
|    time_elapsed    | 1826     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=0.08 +/- 0.35
Episode length: 14.56 +/- 1.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0829        |
| time/                   |               |
|    total_timesteps      | 236000        |
| train/                  |               |
|    approx_kl            | 0.00053733477 |
|    clip_fraction        | 0.0111        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0689       |
|    explained_variance   | 0.0891        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0133        |
|    n_updates            | 1150          |
|    policy_gradient_loss | -0.00078      |
|    value_loss           | 0.0236        |
-------------------------------------------
Eval num_timesteps=236500, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.0019   |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=0.08 +/- 0.35
Episode length: 14.42 +/- 1.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.0833   |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=0.00 +/- 0.24
Episode length: 14.72 +/- 1.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.00212  |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | 0.0213   |
| time/              |          |
|    fps             | 129      |
|    iterations      | 116      |
|    time_elapsed    | 1833     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=-0.02 +/- 0.20
Episode length: 14.86 +/- 0.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -0.0184       |
| time/                   |               |
|    total_timesteps      | 238000        |
| train/                  |               |
|    approx_kl            | 0.00067665434 |
|    clip_fraction        | 0.00859       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0704       |
|    explained_variance   | 0.144         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0248        |
|    n_updates            | 1160          |
|    policy_gradient_loss | -0.000598     |
|    value_loss           | 0.0346        |
-------------------------------------------
Eval num_timesteps=238500, episode_reward=0.04 +/- 0.30
Episode length: 14.70 +/- 0.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0424   |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00194  |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.0319   |
| time/              |          |
|    fps             | 130      |
|    iterations      | 117      |
|    time_elapsed    | 1839     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=0.08 +/- 0.35
Episode length: 14.44 +/- 1.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | 0.0833        |
| time/                   |               |
|    total_timesteps      | 240000        |
| train/                  |               |
|    approx_kl            | 0.00047203145 |
|    clip_fraction        | 0.00889       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0635       |
|    explained_variance   | 0.162         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0133        |
|    n_updates            | 1170          |
|    policy_gradient_loss | -0.000455     |
|    value_loss           | 0.0268        |
-------------------------------------------
Eval num_timesteps=240500, episode_reward=0.02 +/- 0.27
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0221   |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=-0.04 +/- 0.14
Episode length: 14.92 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | 0.00119  |
| time/              |          |
|    fps             | 130      |
|    iterations      | 118      |
|    time_elapsed    | 1845     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-0.06 +/- 0.00
Episode length: 15.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | -0.059        |
| time/                   |               |
|    total_timesteps      | 242000        |
| train/                  |               |
|    approx_kl            | 0.00034699964 |
|    clip_fraction        | 0.00474       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0577       |
|    explained_variance   | 0.12          |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0144        |
|    n_updates            | 1180          |
|    policy_gradient_loss | -0.000226     |
|    value_loss           | 0.0287        |
-------------------------------------------
Eval num_timesteps=242500, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=0.02 +/- 0.27
Episode length: 14.78 +/- 0.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.022    |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=0.08 +/- 0.35
Episode length: 14.56 +/- 1.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0829   |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.0521   |
| time/              |          |
|    fps             | 131      |
|    iterations      | 119      |
|    time_elapsed    | 1852     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=0.08 +/- 0.35
Episode length: 14.54 +/- 1.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.5         |
|    mean_reward          | 0.0829       |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0006136971 |
|    clip_fraction        | 0.00542      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.054       |
|    explained_variance   | 0.149        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0206       |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.000551    |
|    value_loss           | 0.0344       |
------------------------------------------
Eval num_timesteps=244500, episode_reward=-0.02 +/- 0.20
Episode length: 14.88 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0185  |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=0.04 +/- 0.30
Episode length: 14.68 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0424   |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0424   |
| time/              |          |
|    fps             | 132      |
|    iterations      | 120      |
|    time_elapsed    | 1858     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=0.04 +/- 0.30
Episode length: 14.64 +/- 1.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0425        |
| time/                   |               |
|    total_timesteps      | 246000        |
| train/                  |               |
|    approx_kl            | 0.00022679058 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0451       |
|    explained_variance   | 0.16          |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0143        |
|    n_updates            | 1200          |
|    policy_gradient_loss | -0.000342     |
|    value_loss           | 0.0372        |
-------------------------------------------
Eval num_timesteps=246500, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0221   |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=0.10 +/- 0.37
Episode length: 14.42 +/- 1.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.104    |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0626   |
| time/              |          |
|    fps             | 132      |
|    iterations      | 121      |
|    time_elapsed    | 1864     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 0.00196       |
| time/                   |               |
|    total_timesteps      | 248000        |
| train/                  |               |
|    approx_kl            | 3.3227552e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0358       |
|    explained_variance   | 0.194         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0258        |
|    n_updates            | 1210          |
|    policy_gradient_loss | -0.000135     |
|    value_loss           | 0.0358        |
-------------------------------------------
Eval num_timesteps=248500, episode_reward=0.06 +/- 0.33
Episode length: 14.54 +/- 1.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0629   |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00186  |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.00165  |
| time/              |          |
|    fps             | 133      |
|    iterations      | 122      |
|    time_elapsed    | 1870     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=0.08 +/- 0.35
Episode length: 14.50 +/- 1.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.5          |
|    mean_reward          | 0.0831        |
| time/                   |               |
|    total_timesteps      | 250000        |
| train/                  |               |
|    approx_kl            | 0.00019577789 |
|    clip_fraction        | 0.00283       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0429       |
|    explained_variance   | 0.119         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00418       |
|    n_updates            | 1220          |
|    policy_gradient_loss | -0.000309     |
|    value_loss           | 0.0202        |
-------------------------------------------
Eval num_timesteps=250500, episode_reward=0.06 +/- 0.33
Episode length: 14.52 +/- 1.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0629   |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-0.02 +/- 0.20
Episode length: 14.82 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0183  |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=0.04 +/- 0.30
Episode length: 14.66 +/- 1.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0424   |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0525   |
| time/              |          |
|    fps             | 134      |
|    iterations      | 123      |
|    time_elapsed    | 1877     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=0.04 +/- 0.31
Episode length: 14.60 +/- 1.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0427        |
| time/                   |               |
|    total_timesteps      | 252000        |
| train/                  |               |
|    approx_kl            | 0.00013636591 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0357       |
|    explained_variance   | 0.147         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00883       |
|    n_updates            | 1230          |
|    policy_gradient_loss | -0.000468     |
|    value_loss           | 0.0304        |
-------------------------------------------
Eval num_timesteps=252500, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 1.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00186  |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=-0.02 +/- 0.20
Episode length: 14.86 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0184  |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -0.00847 |
| time/              |          |
|    fps             | 134      |
|    iterations      | 124      |
|    time_elapsed    | 1883     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=0.02 +/- 0.28
Episode length: 14.74 +/- 0.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 0.0221       |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 0.0003062674 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0349      |
|    explained_variance   | 0.144        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0062       |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.000354    |
|    value_loss           | 0.0232       |
------------------------------------------
Eval num_timesteps=254500, episode_reward=0.08 +/- 0.35
Episode length: 14.52 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0831   |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=-0.04 +/- 0.14
Episode length: 14.92 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.032    |
| time/              |          |
|    fps             | 135      |
|    iterations      | 125      |
|    time_elapsed    | 1890     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 0.00182       |
| time/                   |               |
|    total_timesteps      | 256500        |
| train/                  |               |
|    approx_kl            | 3.2296317e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0279       |
|    explained_variance   | 0.158         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0149        |
|    n_updates            | 1250          |
|    policy_gradient_loss | -9.26e-05     |
|    value_loss           | 0.0376        |
-------------------------------------------
Eval num_timesteps=257000, episode_reward=0.06 +/- 0.33
Episode length: 14.56 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0628   |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-0.04 +/- 0.14
Episode length: 14.92 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0321   |
| time/              |          |
|    fps             | 136      |
|    iterations      | 126      |
|    time_elapsed    | 1897     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=-0.02 +/- 0.20
Episode length: 14.86 +/- 0.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -0.0184       |
| time/                   |               |
|    total_timesteps      | 258500        |
| train/                  |               |
|    approx_kl            | 0.00014002193 |
|    clip_fraction        | 0.00347       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0272       |
|    explained_variance   | 0.181         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0152        |
|    n_updates            | 1260          |
|    policy_gradient_loss | -0.000193     |
|    value_loss           | 0.0321        |
-------------------------------------------
Eval num_timesteps=259000, episode_reward=0.06 +/- 0.33
Episode length: 14.56 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0627   |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=0.08 +/- 0.35
Episode length: 14.38 +/- 1.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.0835   |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=0.06 +/- 0.33
Episode length: 14.60 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0626   |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | -0.0189  |
| time/              |          |
|    fps             | 136      |
|    iterations      | 127      |
|    time_elapsed    | 1903     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=0.00 +/- 0.24
Episode length: 14.74 +/- 1.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.00204       |
| time/                   |               |
|    total_timesteps      | 260500        |
| train/                  |               |
|    approx_kl            | 0.00017921181 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0276       |
|    explained_variance   | 0.0958        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00445       |
|    n_updates            | 1270          |
|    policy_gradient_loss | -0.000315     |
|    value_loss           | 0.0178        |
-------------------------------------------
Eval num_timesteps=261000, episode_reward=0.08 +/- 0.35
Episode length: 14.50 +/- 1.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0831   |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00186  |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00198  |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0323   |
| time/              |          |
|    fps             | 137      |
|    iterations      | 128      |
|    time_elapsed    | 1909     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=0.02 +/- 0.28
Episode length: 14.74 +/- 0.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0221        |
| time/                   |               |
|    total_timesteps      | 262500        |
| train/                  |               |
|    approx_kl            | 4.8464513e-05 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.03         |
|    explained_variance   | 0.197         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00821       |
|    n_updates            | 1280          |
|    policy_gradient_loss | -7.75e-05     |
|    value_loss           | 0.0308        |
-------------------------------------------
Eval num_timesteps=263000, episode_reward=0.06 +/- 0.33
Episode length: 14.52 +/- 1.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0629   |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=0.08 +/- 0.35
Episode length: 14.50 +/- 1.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.083    |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00184  |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -0.00858 |
| time/              |          |
|    fps             | 137      |
|    iterations      | 129      |
|    time_elapsed    | 1916     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0426        |
| time/                   |               |
|    total_timesteps      | 264500        |
| train/                  |               |
|    approx_kl            | 3.9385195e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0295       |
|    explained_variance   | 0.15          |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00623       |
|    n_updates            | 1290          |
|    policy_gradient_loss | -0.000277     |
|    value_loss           | 0.0275        |
-------------------------------------------
Eval num_timesteps=265000, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-0.02 +/- 0.20
Episode length: 14.86 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0184  |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -0.0186  |
| time/              |          |
|    fps             | 138      |
|    iterations      | 130      |
|    time_elapsed    | 1922     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=-0.06 +/- 0.00
Episode length: 15.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | -0.059       |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 9.321631e-05 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0245      |
|    explained_variance   | 0.17         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0156       |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.000203    |
|    value_loss           | 0.0276       |
------------------------------------------
Eval num_timesteps=267000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00188  |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=0.08 +/- 0.35
Episode length: 14.48 +/- 1.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0832   |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=0.06 +/- 0.33
Episode length: 14.50 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0631   |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.00192  |
| time/              |          |
|    fps             | 139      |
|    iterations      | 131      |
|    time_elapsed    | 1928     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 0.0426       |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 9.434257e-05 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0233      |
|    explained_variance   | 0.12         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0073       |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.000134    |
|    value_loss           | 0.0222       |
------------------------------------------
Eval num_timesteps=269000, episode_reward=0.06 +/- 0.33
Episode length: 14.56 +/- 1.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0629   |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=0.06 +/- 0.33
Episode length: 14.64 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0625   |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-0.02 +/- 0.20
Episode length: 14.86 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0184  |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.0321   |
| time/              |          |
|    fps             | 139      |
|    iterations      | 132      |
|    time_elapsed    | 1935     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=-0.04 +/- 0.14
Episode length: 14.90 +/- 0.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -0.0386       |
| time/                   |               |
|    total_timesteps      | 270500        |
| train/                  |               |
|    approx_kl            | 0.00012075485 |
|    clip_fraction        | 0.002         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0.17          |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0235        |
|    n_updates            | 1320          |
|    policy_gradient_loss | -0.000188     |
|    value_loss           | 0.0363        |
-------------------------------------------
Eval num_timesteps=271000, episode_reward=-0.02 +/- 0.20
Episode length: 14.82 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0183  |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00186  |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.00192  |
| time/              |          |
|    fps             | 140      |
|    iterations      | 133      |
|    time_elapsed    | 1941     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=0.08 +/- 0.35
Episode length: 14.46 +/- 1.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.5         |
|    mean_reward          | 0.0832       |
| time/                   |              |
|    total_timesteps      | 272500       |
| train/                  |              |
|    approx_kl            | 9.223283e-05 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0167      |
|    explained_variance   | 0.141        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00703      |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.000539    |
|    value_loss           | 0.0223       |
------------------------------------------
Eval num_timesteps=273000, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00184  |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-0.02 +/- 0.20
Episode length: 14.80 +/- 0.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0182  |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00194  |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.5     |
|    ep_rew_mean     | 0.0931   |
| time/              |          |
|    fps             | 140      |
|    iterations      | 134      |
|    time_elapsed    | 1948     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=0.08 +/- 0.35
Episode length: 14.46 +/- 1.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.5         |
|    mean_reward          | 0.0833       |
| time/                   |              |
|    total_timesteps      | 274500       |
| train/                  |              |
|    approx_kl            | 5.682683e-05 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0187      |
|    explained_variance   | 0.204        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0138       |
|    n_updates            | 1340         |
|    policy_gradient_loss | -6.52e-05    |
|    value_loss           | 0.0433       |
------------------------------------------
Eval num_timesteps=275000, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00198  |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0221   |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0223   |
| time/              |          |
|    fps             | 141      |
|    iterations      | 135      |
|    time_elapsed    | 1954     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 0.0018        |
| time/                   |               |
|    total_timesteps      | 276500        |
| train/                  |               |
|    approx_kl            | 0.00016219323 |
|    clip_fraction        | 0.00186       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0184       |
|    explained_variance   | 0.22          |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0141        |
|    n_updates            | 1350          |
|    policy_gradient_loss | -7.79e-05     |
|    value_loss           | 0.03          |
-------------------------------------------
Eval num_timesteps=277000, episode_reward=-0.04 +/- 0.14
Episode length: 14.92 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=0.04 +/- 0.30
Episode length: 14.64 +/- 1.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0425   |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.0018   |
| time/              |          |
|    fps             | 142      |
|    iterations      | 136      |
|    time_elapsed    | 1961     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=0.04 +/- 0.31
Episode length: 14.56 +/- 1.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0427        |
| time/                   |               |
|    total_timesteps      | 279000        |
| train/                  |               |
|    approx_kl            | 5.4820615e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.014        |
|    explained_variance   | 0.137         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00891       |
|    n_updates            | 1360          |
|    policy_gradient_loss | -9.49e-06     |
|    value_loss           | 0.0202        |
-------------------------------------------
Eval num_timesteps=279500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=0.08 +/- 0.35
Episode length: 14.44 +/- 1.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.0833   |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 1.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -0.0184  |
| time/              |          |
|    fps             | 142      |
|    iterations      | 137      |
|    time_elapsed    | 1967     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=0.04 +/- 0.30
Episode length: 14.64 +/- 1.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0425        |
| time/                   |               |
|    total_timesteps      | 281000        |
| train/                  |               |
|    approx_kl            | 3.5448465e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0126       |
|    explained_variance   | 0.0941        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0157        |
|    n_updates            | 1370          |
|    policy_gradient_loss | 2.65e-06      |
|    value_loss           | 0.0207        |
-------------------------------------------
Eval num_timesteps=281500, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00196  |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=0.04 +/- 0.31
Episode length: 14.58 +/- 1.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0427   |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0527   |
| time/              |          |
|    fps             | 143      |
|    iterations      | 138      |
|    time_elapsed    | 1974     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=0.08 +/- 0.35
Episode length: 14.50 +/- 1.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.5         |
|    mean_reward          | 0.083        |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 9.693613e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0131      |
|    explained_variance   | 0.173        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0302       |
|    n_updates            | 1380         |
|    policy_gradient_loss | 1.15e-06     |
|    value_loss           | 0.037        |
------------------------------------------
Eval num_timesteps=283500, episode_reward=0.06 +/- 0.33
Episode length: 14.62 +/- 1.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0626   |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=0.04 +/- 0.30
Episode length: 14.68 +/- 0.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0424   |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0121   |
| time/              |          |
|    fps             | 143      |
|    iterations      | 139      |
|    time_elapsed    | 1980     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 0.00202       |
| time/                   |               |
|    total_timesteps      | 285000        |
| train/                  |               |
|    approx_kl            | 2.5764253e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0.207         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0133        |
|    n_updates            | 1390          |
|    policy_gradient_loss | -4.18e-05     |
|    value_loss           | 0.0253        |
-------------------------------------------
Eval num_timesteps=285500, episode_reward=0.06 +/- 0.33
Episode length: 14.52 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.063    |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00192  |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0122   |
| time/              |          |
|    fps             | 144      |
|    iterations      | 140      |
|    time_elapsed    | 1986     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 0.00198       |
| time/                   |               |
|    total_timesteps      | 287000        |
| train/                  |               |
|    approx_kl            | 2.0755368e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0126       |
|    explained_variance   | 0.219         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0063        |
|    n_updates            | 1400          |
|    policy_gradient_loss | -9.18e-05     |
|    value_loss           | 0.0321        |
-------------------------------------------
Eval num_timesteps=287500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=0.06 +/- 0.33
Episode length: 14.48 +/- 1.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0631   |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-0.02 +/- 0.20
Episode length: 14.82 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0183  |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -0.00833 |
| time/              |          |
|    fps             | 144      |
|    iterations      | 141      |
|    time_elapsed    | 1993     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0426        |
| time/                   |               |
|    total_timesteps      | 289000        |
| train/                  |               |
|    approx_kl            | 1.2007367e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00981      |
|    explained_variance   | 0.109         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00542       |
|    n_updates            | 1410          |
|    policy_gradient_loss | -1.55e-06     |
|    value_loss           | 0.0181        |
-------------------------------------------
Eval num_timesteps=289500, episode_reward=-0.02 +/- 0.20
Episode length: 14.86 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0184  |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=0.08 +/- 0.35
Episode length: 14.46 +/- 1.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0833   |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -0.0286  |
| time/              |          |
|    fps             | 145      |
|    iterations      | 142      |
|    time_elapsed    | 1999     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0425        |
| time/                   |               |
|    total_timesteps      | 291000        |
| train/                  |               |
|    approx_kl            | 1.8269755e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0108       |
|    explained_variance   | 0.0821        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00179       |
|    n_updates            | 1420          |
|    policy_gradient_loss | -2.37e-05     |
|    value_loss           | 0.0128        |
-------------------------------------------
Eval num_timesteps=291500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00204  |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=0.00 +/- 0.24
Episode length: 14.72 +/- 1.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.00214  |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0628   |
| time/              |          |
|    fps             | 145      |
|    iterations      | 143      |
|    time_elapsed    | 2006     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=-0.02 +/- 0.20
Episode length: 14.82 +/- 0.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -0.0183      |
| time/                   |              |
|    total_timesteps      | 293000       |
| train/                  |              |
|    approx_kl            | 4.434155e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0127      |
|    explained_variance   | 0.197        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00743      |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.000326    |
|    value_loss           | 0.0431       |
------------------------------------------
Eval num_timesteps=293500, episode_reward=0.08 +/- 0.35
Episode length: 14.50 +/- 1.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0831   |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00192  |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=0.06 +/- 0.33
Episode length: 14.54 +/- 1.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.063    |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.0119   |
| time/              |          |
|    fps             | 146      |
|    iterations      | 144      |
|    time_elapsed    | 2012     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0426        |
| time/                   |               |
|    total_timesteps      | 295000        |
| train/                  |               |
|    approx_kl            | 0.00010384436 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0141       |
|    explained_variance   | 0.203         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0203        |
|    n_updates            | 1440          |
|    policy_gradient_loss | -0.000271     |
|    value_loss           | 0.0334        |
-------------------------------------------
Eval num_timesteps=295500, episode_reward=0.08 +/- 0.35
Episode length: 14.46 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0832   |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=0.06 +/- 0.33
Episode length: 14.48 +/- 1.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0631   |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=0.10 +/- 0.37
Episode length: 14.38 +/- 1.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.104    |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.0118   |
| time/              |          |
|    fps             | 147      |
|    iterations      | 145      |
|    time_elapsed    | 2018     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=0.16 +/- 0.42
Episode length: 14.32 +/- 1.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.3          |
|    mean_reward          | 0.164         |
| time/                   |               |
|    total_timesteps      | 297000        |
| train/                  |               |
|    approx_kl            | 3.9081933e-05 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0129       |
|    explained_variance   | 0.133         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0171        |
|    n_updates            | 1450          |
|    policy_gradient_loss | -7.73e-05     |
|    value_loss           | 0.0209        |
-------------------------------------------
New best mean reward!
Eval num_timesteps=297500, episode_reward=0.14 +/- 0.41
Episode length: 14.28 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.3     |
|    mean_reward     | 0.144    |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.0019   |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00194  |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=0.04 +/- 0.31
Episode length: 14.58 +/- 1.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0427   |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0121   |
| time/              |          |
|    fps             | 147      |
|    iterations      | 146      |
|    time_elapsed    | 2025     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 0.0223       |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 0.0001358341 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0152      |
|    explained_variance   | 0.176        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00777      |
|    n_updates            | 1460         |
|    policy_gradient_loss | -6.3e-05     |
|    value_loss           | 0.0272       |
------------------------------------------
Eval num_timesteps=300000, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00202  |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-0.02 +/- 0.20
Episode length: 14.82 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0183  |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=0.00 +/- 0.24
Episode length: 14.74 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.002    |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.00183  |
| time/              |          |
|    fps             | 148      |
|    iterations      | 147      |
|    time_elapsed    | 2031     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=0.04 +/- 0.31
Episode length: 14.60 +/- 1.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 0.0427       |
| time/                   |              |
|    total_timesteps      | 301500       |
| train/                  |              |
|    approx_kl            | 3.197242e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0.184        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00875      |
|    n_updates            | 1470         |
|    policy_gradient_loss | -2.83e-05    |
|    value_loss           | 0.0232       |
------------------------------------------
Eval num_timesteps=302000, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00192  |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 1.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.0119   |
| time/              |          |
|    fps             | 148      |
|    iterations      | 148      |
|    time_elapsed    | 2038     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0224        |
| time/                   |               |
|    total_timesteps      | 303500        |
| train/                  |               |
|    approx_kl            | 0.00016590604 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | 0.229         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0166        |
|    n_updates            | 1480          |
|    policy_gradient_loss | -0.000167     |
|    value_loss           | 0.0259        |
-------------------------------------------
Eval num_timesteps=304000, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=-0.04 +/- 0.14
Episode length: 14.90 +/- 0.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0386  |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -0.0185  |
| time/              |          |
|    fps             | 149      |
|    iterations      | 149      |
|    time_elapsed    | 2044     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.9         |
|    mean_reward          | -0.0388      |
| time/                   |              |
|    total_timesteps      | 305500       |
| train/                  |              |
|    approx_kl            | 2.365699e-05 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0117      |
|    explained_variance   | 0.159        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0151       |
|    n_updates            | 1490         |
|    policy_gradient_loss | -1.63e-05    |
|    value_loss           | 0.0281       |
------------------------------------------
Eval num_timesteps=306000, episode_reward=0.02 +/- 0.28
Episode length: 14.74 +/- 0.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0221   |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=0.04 +/- 0.30
Episode length: 14.64 +/- 1.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0425   |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.00177  |
| time/              |          |
|    fps             | 149      |
|    iterations      | 150      |
|    time_elapsed    | 2050     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=0.10 +/- 0.37
Episode length: 14.32 +/- 1.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.3          |
|    mean_reward          | 0.104         |
| time/                   |               |
|    total_timesteps      | 307500        |
| train/                  |               |
|    approx_kl            | 3.6059064e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0109       |
|    explained_variance   | 0.182         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0171        |
|    n_updates            | 1500          |
|    policy_gradient_loss | -2.93e-06     |
|    value_loss           | 0.0275        |
-------------------------------------------
Eval num_timesteps=308000, episode_reward=0.08 +/- 0.35
Episode length: 14.48 +/- 1.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0832   |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=0.04 +/- 0.31
Episode length: 14.58 +/- 1.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0428   |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -0.00819 |
| time/              |          |
|    fps             | 150      |
|    iterations      | 151      |
|    time_elapsed    | 2057     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=0.12 +/- 0.39
Episode length: 14.30 +/- 1.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.3         |
|    mean_reward          | 0.124        |
| time/                   |              |
|    total_timesteps      | 309500       |
| train/                  |              |
|    approx_kl            | 6.528851e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0118      |
|    explained_variance   | 0.205        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00637      |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.000144    |
|    value_loss           | 0.0188       |
------------------------------------------
Eval num_timesteps=310000, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.0019   |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0424   |
| time/              |          |
|    fps             | 150      |
|    iterations      | 152      |
|    time_elapsed    | 2063     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 0.0222       |
| time/                   |              |
|    total_timesteps      | 311500       |
| train/                  |              |
|    approx_kl            | 6.517637e-05 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0104      |
|    explained_variance   | 0.179        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0106       |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.000124    |
|    value_loss           | 0.0294       |
------------------------------------------
Eval num_timesteps=312000, episode_reward=0.06 +/- 0.33
Episode length: 14.58 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0627   |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=0.02 +/- 0.28
Episode length: 14.74 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0221   |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=0.08 +/- 0.35
Episode length: 14.52 +/- 1.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.083    |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.5     |
|    ep_rew_mean     | 0.0629   |
| time/              |          |
|    fps             | 151      |
|    iterations      | 153      |
|    time_elapsed    | 2069     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0223        |
| time/                   |               |
|    total_timesteps      | 313500        |
| train/                  |               |
|    approx_kl            | 1.2858509e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0102       |
|    explained_variance   | 0.217         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0214        |
|    n_updates            | 1530          |
|    policy_gradient_loss | -8.77e-06     |
|    value_loss           | 0.0375        |
-------------------------------------------
Eval num_timesteps=314000, episode_reward=0.04 +/- 0.30
Episode length: 14.68 +/- 0.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0424   |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=0.04 +/- 0.31
Episode length: 14.56 +/- 1.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0428   |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00188  |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -0.0184  |
| time/              |          |
|    fps             | 151      |
|    iterations      | 154      |
|    time_elapsed    | 2076     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -0.0387       |
| time/                   |               |
|    total_timesteps      | 315500        |
| train/                  |               |
|    approx_kl            | 1.4812249e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00771      |
|    explained_variance   | 0.173         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.028         |
|    n_updates            | 1540          |
|    policy_gradient_loss | -6.45e-05     |
|    value_loss           | 0.025         |
-------------------------------------------
Eval num_timesteps=316000, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=0.06 +/- 0.33
Episode length: 14.48 +/- 1.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.063    |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=-0.02 +/- 0.20
Episode length: 14.84 +/- 0.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0184  |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -0.0286  |
| time/              |          |
|    fps             | 152      |
|    iterations      | 155      |
|    time_elapsed    | 2082     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 0.002         |
| time/                   |               |
|    total_timesteps      | 317500        |
| train/                  |               |
|    approx_kl            | 2.6336347e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00817      |
|    explained_variance   | 0.118         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00736       |
|    n_updates            | 1550          |
|    policy_gradient_loss | -1.57e-06     |
|    value_loss           | 0.0151        |
-------------------------------------------
Eval num_timesteps=318000, episode_reward=0.06 +/- 0.33
Episode length: 14.50 +/- 1.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.063    |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00186  |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0222   |
| time/              |          |
|    fps             | 152      |
|    iterations      | 156      |
|    time_elapsed    | 2088     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=-0.02 +/- 0.20
Episode length: 14.90 +/- 0.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -0.0186       |
| time/                   |               |
|    total_timesteps      | 319500        |
| train/                  |               |
|    approx_kl            | 7.5844786e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0107       |
|    explained_variance   | 0.179         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0159        |
|    n_updates            | 1560          |
|    policy_gradient_loss | -7.7e-05      |
|    value_loss           | 0.0284        |
-------------------------------------------
Eval num_timesteps=320000, episode_reward=0.16 +/- 0.42
Episode length: 14.20 +/- 1.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 0.164    |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
New best mean reward!
Eval num_timesteps=320500, episode_reward=0.00 +/- 0.24
Episode length: 14.82 +/- 0.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00176  |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=0.06 +/- 0.33
Episode length: 14.48 +/- 1.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0631   |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0325   |
| time/              |          |
|    fps             | 153      |
|    iterations      | 157      |
|    time_elapsed    | 2095     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=0.06 +/- 0.33
Episode length: 14.54 +/- 1.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 0.0629      |
| time/                   |             |
|    total_timesteps      | 322000      |
| train/                  |             |
|    approx_kl            | 1.56569e-05 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00897    |
|    explained_variance   | 0.251       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0128      |
|    n_updates            | 1570        |
|    policy_gradient_loss | -1.74e-05   |
|    value_loss           | 0.0314      |
-----------------------------------------
Eval num_timesteps=322500, episode_reward=0.06 +/- 0.33
Episode length: 14.60 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0627   |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.0019   |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0527   |
| time/              |          |
|    fps             | 153      |
|    iterations      | 158      |
|    time_elapsed    | 2102     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=0.10 +/- 0.37
Episode length: 14.32 +/- 1.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.3         |
|    mean_reward          | 0.104        |
| time/                   |              |
|    total_timesteps      | 324000       |
| train/                  |              |
|    approx_kl            | 4.298819e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00683     |
|    explained_variance   | 0.199        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0165       |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.00022     |
|    value_loss           | 0.0338       |
------------------------------------------
Eval num_timesteps=324500, episode_reward=0.04 +/- 0.30
Episode length: 14.64 +/- 1.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0425   |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=0.14 +/- 0.41
Episode length: 14.24 +/- 1.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | 0.144    |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00182  |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.012    |
| time/              |          |
|    fps             | 154      |
|    iterations      | 159      |
|    time_elapsed    | 2108     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0222        |
| time/                   |               |
|    total_timesteps      | 326000        |
| train/                  |               |
|    approx_kl            | 2.8225128e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00712      |
|    explained_variance   | 0.161         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.011         |
|    n_updates            | 1590          |
|    policy_gradient_loss | -1.16e-05     |
|    value_loss           | 0.0226        |
-------------------------------------------
Eval num_timesteps=326500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.002    |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=0.06 +/- 0.33
Episode length: 14.56 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0628   |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=0.04 +/- 0.30
Episode length: 14.64 +/- 1.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0425   |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.00192  |
| time/              |          |
|    fps             | 154      |
|    iterations      | 160      |
|    time_elapsed    | 2114     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0426        |
| time/                   |               |
|    total_timesteps      | 328000        |
| train/                  |               |
|    approx_kl            | 6.1448576e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00871      |
|    explained_variance   | 0.222         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0095        |
|    n_updates            | 1600          |
|    policy_gradient_loss | -8.9e-05      |
|    value_loss           | 0.025         |
-------------------------------------------
Eval num_timesteps=328500, episode_reward=0.06 +/- 0.33
Episode length: 14.56 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0628   |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=-0.02 +/- 0.20
Episode length: 14.88 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0185  |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=0.12 +/- 0.39
Episode length: 14.32 +/- 1.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.3     |
|    mean_reward     | 0.124    |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0628   |
| time/              |          |
|    fps             | 155      |
|    iterations      | 161      |
|    time_elapsed    | 2120     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=0.04 +/- 0.31
Episode length: 14.60 +/- 1.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 0.0427       |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 4.045645e-05 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00586     |
|    explained_variance   | 0.19         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0339       |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.000128    |
|    value_loss           | 0.0407       |
------------------------------------------
Eval num_timesteps=330500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=0.02 +/- 0.27
Episode length: 14.76 +/- 0.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.022    |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0527   |
| time/              |          |
|    fps             | 155      |
|    iterations      | 162      |
|    time_elapsed    | 2127     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=0.06 +/- 0.33
Episode length: 14.58 +/- 1.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0628        |
| time/                   |               |
|    total_timesteps      | 332000        |
| train/                  |               |
|    approx_kl            | 4.8748916e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00581      |
|    explained_variance   | 0.244         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0124        |
|    n_updates            | 1620          |
|    policy_gradient_loss | -1.2e-07      |
|    value_loss           | 0.0357        |
-------------------------------------------
Eval num_timesteps=332500, episode_reward=-0.02 +/- 0.20
Episode length: 14.90 +/- 0.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0186  |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00198  |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00194  |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.00205  |
| time/              |          |
|    fps             | 156      |
|    iterations      | 163      |
|    time_elapsed    | 2133     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=-0.02 +/- 0.20
Episode length: 14.84 +/- 0.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | -0.0183       |
| time/                   |               |
|    total_timesteps      | 334000        |
| train/                  |               |
|    approx_kl            | 1.4071062e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00581      |
|    explained_variance   | 0.196         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0159        |
|    n_updates            | 1630          |
|    policy_gradient_loss | -0.0001       |
|    value_loss           | 0.0211        |
-------------------------------------------
Eval num_timesteps=334500, episode_reward=-0.04 +/- 0.14
Episode length: 14.92 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=0.06 +/- 0.33
Episode length: 14.54 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0628   |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.002    |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0628   |
| time/              |          |
|    fps             | 156      |
|    iterations      | 164      |
|    time_elapsed    | 2140     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=0.06 +/- 0.33
Episode length: 14.54 +/- 1.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.5          |
|    mean_reward          | 0.0629        |
| time/                   |               |
|    total_timesteps      | 336000        |
| train/                  |               |
|    approx_kl            | 1.8480932e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00575      |
|    explained_variance   | 0.242         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0165        |
|    n_updates            | 1640          |
|    policy_gradient_loss | -3.64e-06     |
|    value_loss           | 0.0343        |
-------------------------------------------
Eval num_timesteps=336500, episode_reward=0.06 +/- 0.33
Episode length: 14.62 +/- 1.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0626   |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=0.04 +/- 0.30
Episode length: 14.68 +/- 0.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0424   |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0122   |
| time/              |          |
|    fps             | 157      |
|    iterations      | 165      |
|    time_elapsed    | 2146     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=0.00 +/- 0.24
Episode length: 14.72 +/- 1.11
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 14.7           |
|    mean_reward          | 0.00214        |
| time/                   |                |
|    total_timesteps      | 338000         |
| train/                  |                |
|    approx_kl            | 0.000120356766 |
|    clip_fraction        | 0.00103        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00682       |
|    explained_variance   | 0.192          |
|    learning_rate        | 1e-05          |
|    loss                 | 0.0163         |
|    n_updates            | 1650           |
|    policy_gradient_loss | -2.9e-05       |
|    value_loss           | 0.0219         |
--------------------------------------------
Eval num_timesteps=338500, episode_reward=0.10 +/- 0.37
Episode length: 14.34 +/- 1.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.3     |
|    mean_reward     | 0.104    |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=0.00 +/- 0.24
Episode length: 14.74 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.00204  |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00194  |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.5     |
|    ep_rew_mean     | 0.073    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 166      |
|    time_elapsed    | 2152     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=0.04 +/- 0.30
Episode length: 14.66 +/- 1.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0424        |
| time/                   |               |
|    total_timesteps      | 340000        |
| train/                  |               |
|    approx_kl            | 4.7146226e-05 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00764      |
|    explained_variance   | 0.238         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0156        |
|    n_updates            | 1660          |
|    policy_gradient_loss | -0.000343     |
|    value_loss           | 0.0344        |
-------------------------------------------
Eval num_timesteps=340500, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=-0.02 +/- 0.20
Episode length: 14.86 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0184  |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00188  |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.012    |
| time/              |          |
|    fps             | 158      |
|    iterations      | 167      |
|    time_elapsed    | 2159     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0223        |
| time/                   |               |
|    total_timesteps      | 342500        |
| train/                  |               |
|    approx_kl            | 4.3506006e-05 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00765      |
|    explained_variance   | 0.149         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0109        |
|    n_updates            | 1670          |
|    policy_gradient_loss | -4.09e-05     |
|    value_loss           | 0.0255        |
-------------------------------------------
Eval num_timesteps=343000, episode_reward=-0.02 +/- 0.20
Episode length: 14.86 +/- 0.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0184  |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=0.06 +/- 0.33
Episode length: 14.54 +/- 1.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0629   |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-0.04 +/- 0.14
Episode length: 14.92 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -0.00817 |
| time/              |          |
|    fps             | 158      |
|    iterations      | 168      |
|    time_elapsed    | 2166     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 0.0222       |
| time/                   |              |
|    total_timesteps      | 344500       |
| train/                  |              |
|    approx_kl            | 4.455968e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0075      |
|    explained_variance   | 0.205        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00783      |
|    n_updates            | 1680         |
|    policy_gradient_loss | -4.63e-05    |
|    value_loss           | 0.0236       |
------------------------------------------
Eval num_timesteps=345000, episode_reward=0.04 +/- 0.30
Episode length: 14.66 +/- 1.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0424   |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=0.04 +/- 0.31
Episode length: 14.60 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0427   |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.5     |
|    ep_rew_mean     | 0.063    |
| time/              |          |
|    fps             | 159      |
|    iterations      | 169      |
|    time_elapsed    | 2172     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 0.0426       |
| time/                   |              |
|    total_timesteps      | 346500       |
| train/                  |              |
|    approx_kl            | 4.727929e-05 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00814     |
|    explained_variance   | 0.243        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.017        |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.0001      |
|    value_loss           | 0.0363       |
------------------------------------------
Eval num_timesteps=347000, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=-0.02 +/- 0.20
Episode length: 14.82 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0183  |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=0.12 +/- 0.39
Episode length: 14.36 +/- 1.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.124    |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0426   |
| time/              |          |
|    fps             | 159      |
|    iterations      | 170      |
|    time_elapsed    | 2178     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=0.02 +/- 0.27
Episode length: 14.74 +/- 0.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0221        |
| time/                   |               |
|    total_timesteps      | 348500        |
| train/                  |               |
|    approx_kl            | 3.7278514e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00875      |
|    explained_variance   | 0.191         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0121        |
|    n_updates            | 1700          |
|    policy_gradient_loss | -1.42e-05     |
|    value_loss           | 0.0319        |
-------------------------------------------
Eval num_timesteps=349000, episode_reward=0.06 +/- 0.33
Episode length: 14.62 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0627   |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=0.06 +/- 0.33
Episode length: 14.62 +/- 1.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0626   |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00186  |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.012    |
| time/              |          |
|    fps             | 160      |
|    iterations      | 171      |
|    time_elapsed    | 2185     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=0.06 +/- 0.33
Episode length: 14.46 +/- 1.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.5          |
|    mean_reward          | 0.0632        |
| time/                   |               |
|    total_timesteps      | 350500        |
| train/                  |               |
|    approx_kl            | 1.0814663e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00841      |
|    explained_variance   | 0.221         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0234        |
|    n_updates            | 1710          |
|    policy_gradient_loss | -2.01e-05     |
|    value_loss           | 0.0263        |
-------------------------------------------
Eval num_timesteps=351000, episode_reward=-0.04 +/- 0.14
Episode length: 14.90 +/- 0.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0386  |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=-0.02 +/- 0.20
Episode length: 14.82 +/- 0.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0182  |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0628   |
| time/              |          |
|    fps             | 160      |
|    iterations      | 172      |
|    time_elapsed    | 2191     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=0.08 +/- 0.35
Episode length: 14.46 +/- 1.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.5          |
|    mean_reward          | 0.0832        |
| time/                   |               |
|    total_timesteps      | 352500        |
| train/                  |               |
|    approx_kl            | 2.7823262e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00814      |
|    explained_variance   | 0.285         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00915       |
|    n_updates            | 1720          |
|    policy_gradient_loss | -2.3e-06      |
|    value_loss           | 0.0388        |
-------------------------------------------
Eval num_timesteps=353000, episode_reward=0.10 +/- 0.37
Episode length: 14.42 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.103    |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00194  |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00196  |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0627   |
| time/              |          |
|    fps             | 161      |
|    iterations      | 173      |
|    time_elapsed    | 2197     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=0.04 +/- 0.30
Episode length: 14.68 +/- 0.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0424        |
| time/                   |               |
|    total_timesteps      | 354500        |
| train/                  |               |
|    approx_kl            | 1.9829487e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00892      |
|    explained_variance   | 0.293         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0189        |
|    n_updates            | 1730          |
|    policy_gradient_loss | -2.44e-05     |
|    value_loss           | 0.0348        |
-------------------------------------------
Eval num_timesteps=355000, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=0.04 +/- 0.30
Episode length: 14.66 +/- 1.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0425   |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00192  |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0425   |
| time/              |          |
|    fps             | 161      |
|    iterations      | 174      |
|    time_elapsed    | 2203     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 0.00188      |
| time/                   |              |
|    total_timesteps      | 356500       |
| train/                  |              |
|    approx_kl            | 4.031675e-05 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00891     |
|    explained_variance   | 0.188        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00324      |
|    n_updates            | 1740         |
|    policy_gradient_loss | -5.4e-05     |
|    value_loss           | 0.0289       |
------------------------------------------
Eval num_timesteps=357000, episode_reward=0.10 +/- 0.37
Episode length: 14.36 +/- 1.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.104    |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0388  |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=0.08 +/- 0.35
Episode length: 14.46 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0833   |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0528   |
| time/              |          |
|    fps             | 162      |
|    iterations      | 175      |
|    time_elapsed    | 2210     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=-0.04 +/- 0.14
Episode length: 14.90 +/- 0.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -0.0386       |
| time/                   |               |
|    total_timesteps      | 358500        |
| train/                  |               |
|    approx_kl            | 1.5173835e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00863      |
|    explained_variance   | 0.24          |
|    learning_rate        | 1e-05         |
|    loss                 | 0.021         |
|    n_updates            | 1750          |
|    policy_gradient_loss | -0.00014      |
|    value_loss           | 0.0359        |
-------------------------------------------
Eval num_timesteps=359000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.0019   |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=0.06 +/- 0.33
Episode length: 14.60 +/- 1.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0627   |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.002    |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0426   |
| time/              |          |
|    fps             | 162      |
|    iterations      | 176      |
|    time_elapsed    | 2216     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 1.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 0.0221       |
| time/                   |              |
|    total_timesteps      | 360500       |
| train/                  |              |
|    approx_kl            | 4.006221e-05 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00653     |
|    explained_variance   | 0.286        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00955      |
|    n_updates            | 1760         |
|    policy_gradient_loss | -1.85e-05    |
|    value_loss           | 0.0368       |
------------------------------------------
Eval num_timesteps=361000, episode_reward=-0.02 +/- 0.20
Episode length: 14.88 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0185  |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00196  |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00192  |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0527   |
| time/              |          |
|    fps             | 163      |
|    iterations      | 177      |
|    time_elapsed    | 2223     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=0.04 +/- 0.31
Episode length: 14.58 +/- 1.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 0.0427       |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 7.686263e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00689     |
|    explained_variance   | 0.289        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0107       |
|    n_updates            | 1770         |
|    policy_gradient_loss | -5.76e-06    |
|    value_loss           | 0.041        |
------------------------------------------
Eval num_timesteps=363000, episode_reward=0.00 +/- 0.24
Episode length: 14.74 +/- 1.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.00206  |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=0.00 +/- 0.24
Episode length: 14.74 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.00202  |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0425   |
| time/              |          |
|    fps             | 163      |
|    iterations      | 178      |
|    time_elapsed    | 2230     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | 0.00194      |
| time/                   |              |
|    total_timesteps      | 365000       |
| train/                  |              |
|    approx_kl            | 6.165006e-06 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0102      |
|    explained_variance   | 0.294        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0163       |
|    n_updates            | 1780         |
|    policy_gradient_loss | -7.28e-05    |
|    value_loss           | 0.0352       |
------------------------------------------
Eval num_timesteps=365500, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=0.06 +/- 0.33
Episode length: 14.60 +/- 1.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0627   |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=0.00 +/- 0.24
Episode length: 14.72 +/- 1.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.00216  |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -0.00834 |
| time/              |          |
|    fps             | 163      |
|    iterations      | 179      |
|    time_elapsed    | 2236     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -0.0388       |
| time/                   |               |
|    total_timesteps      | 367000        |
| train/                  |               |
|    approx_kl            | 4.6682544e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00765      |
|    explained_variance   | 0.204         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0192        |
|    n_updates            | 1790          |
|    policy_gradient_loss | -5.77e-06     |
|    value_loss           | 0.0276        |
-------------------------------------------
Eval num_timesteps=367500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=0.08 +/- 0.35
Episode length: 14.56 +/- 1.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0829   |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0527   |
| time/              |          |
|    fps             | 164      |
|    iterations      | 180      |
|    time_elapsed    | 2242     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 0.0223       |
| time/                   |              |
|    total_timesteps      | 369000       |
| train/                  |              |
|    approx_kl            | 2.614045e-05 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0093      |
|    explained_variance   | 0.271        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00716      |
|    n_updates            | 1800         |
|    policy_gradient_loss | -1.73e-05    |
|    value_loss           | 0.0377       |
------------------------------------------
Eval num_timesteps=369500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 1.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.0019   |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=0.06 +/- 0.33
Episode length: 14.58 +/- 1.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0628   |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=0.00 +/- 0.24
Episode length: 14.82 +/- 0.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00176  |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.5     |
|    ep_rew_mean     | 0.063    |
| time/              |          |
|    fps             | 164      |
|    iterations      | 181      |
|    time_elapsed    | 2249     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=0.06 +/- 0.33
Episode length: 14.58 +/- 1.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0627        |
| time/                   |               |
|    total_timesteps      | 371000        |
| train/                  |               |
|    approx_kl            | 3.5194855e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00759      |
|    explained_variance   | 0.308         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0165        |
|    n_updates            | 1810          |
|    policy_gradient_loss | -3.2e-05      |
|    value_loss           | 0.0379        |
-------------------------------------------
Eval num_timesteps=371500, episode_reward=0.00 +/- 0.24
Episode length: 14.82 +/- 0.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.0018   |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=-0.02 +/- 0.20
Episode length: 14.84 +/- 0.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0183  |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=-0.02 +/- 0.20
Episode length: 14.84 +/- 0.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0183  |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0121   |
| time/              |          |
|    fps             | 165      |
|    iterations      | 182      |
|    time_elapsed    | 2255     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=-0.02 +/- 0.20
Episode length: 14.88 +/- 0.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -0.0184       |
| time/                   |               |
|    total_timesteps      | 373000        |
| train/                  |               |
|    approx_kl            | 1.2828386e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00645      |
|    explained_variance   | 0.246         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0162        |
|    n_updates            | 1820          |
|    policy_gradient_loss | -3.47e-06     |
|    value_loss           | 0.0277        |
-------------------------------------------
Eval num_timesteps=373500, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=0.10 +/- 0.37
Episode length: 14.34 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.3     |
|    mean_reward     | 0.104    |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0425   |
| time/              |          |
|    fps             | 165      |
|    iterations      | 183      |
|    time_elapsed    | 2261     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=0.04 +/- 0.30
Episode length: 14.64 +/- 1.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0425        |
| time/                   |               |
|    total_timesteps      | 375000        |
| train/                  |               |
|    approx_kl            | 1.6169099e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00728      |
|    explained_variance   | 0.272         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0162        |
|    n_updates            | 1830          |
|    policy_gradient_loss | -5.06e-05     |
|    value_loss           | 0.0365        |
-------------------------------------------
Eval num_timesteps=375500, episode_reward=-0.02 +/- 0.20
Episode length: 14.84 +/- 0.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0184  |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=0.08 +/- 0.35
Episode length: 14.50 +/- 1.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0831   |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=0.08 +/- 0.35
Episode length: 14.54 +/- 1.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.083    |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0324   |
| time/              |          |
|    fps             | 166      |
|    iterations      | 184      |
|    time_elapsed    | 2268     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=0.04 +/- 0.30
Episode length: 14.66 +/- 1.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0424        |
| time/                   |               |
|    total_timesteps      | 377000        |
| train/                  |               |
|    approx_kl            | 2.2426218e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00847      |
|    explained_variance   | 0.234         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0169        |
|    n_updates            | 1840          |
|    policy_gradient_loss | -4.69e-05     |
|    value_loss           | 0.0329        |
-------------------------------------------
Eval num_timesteps=377500, episode_reward=0.06 +/- 0.33
Episode length: 14.50 +/- 1.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0631   |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=0.02 +/- 0.28
Episode length: 14.74 +/- 0.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0221   |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0323   |
| time/              |          |
|    fps             | 166      |
|    iterations      | 185      |
|    time_elapsed    | 2274     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 0.00186       |
| time/                   |               |
|    total_timesteps      | 379000        |
| train/                  |               |
|    approx_kl            | 0.00023237118 |
|    clip_fraction        | 0.00249       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0109       |
|    explained_variance   | 0.242         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0239        |
|    n_updates            | 1850          |
|    policy_gradient_loss | -9.88e-05     |
|    value_loss           | 0.0282        |
-------------------------------------------
Eval num_timesteps=379500, episode_reward=0.02 +/- 0.28
Episode length: 14.68 +/- 1.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=0.02 +/- 0.27
Episode length: 14.74 +/- 0.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0221   |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -0.0185  |
| time/              |          |
|    fps             | 167      |
|    iterations      | 186      |
|    time_elapsed    | 2280     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=0.04 +/- 0.30
Episode length: 14.64 +/- 1.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.6          |
|    mean_reward          | 0.0425        |
| time/                   |               |
|    total_timesteps      | 381000        |
| train/                  |               |
|    approx_kl            | 0.00012251068 |
|    clip_fraction        | 0.00327       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | 0.175         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0107        |
|    n_updates            | 1860          |
|    policy_gradient_loss | -0.000234     |
|    value_loss           | 0.0187        |
-------------------------------------------
Eval num_timesteps=381500, episode_reward=0.06 +/- 0.33
Episode length: 14.48 +/- 1.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0631   |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=0.04 +/- 0.31
Episode length: 14.54 +/- 1.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0428   |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0526   |
| time/              |          |
|    fps             | 167      |
|    iterations      | 187      |
|    time_elapsed    | 2287     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=-0.02 +/- 0.20
Episode length: 14.88 +/- 0.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -0.0185       |
| time/                   |               |
|    total_timesteps      | 383000        |
| train/                  |               |
|    approx_kl            | 2.9992632e-05 |
|    clip_fraction        | 0.00322       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0128       |
|    explained_variance   | 0.246         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00652       |
|    n_updates            | 1870          |
|    policy_gradient_loss | 0.000193      |
|    value_loss           | 0.0329        |
-------------------------------------------
Eval num_timesteps=383500, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00194  |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.0019   |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -0.00816 |
| time/              |          |
|    fps             | 167      |
|    iterations      | 188      |
|    time_elapsed    | 2294     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=-0.02 +/- 0.20
Episode length: 14.80 +/- 0.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | -0.0182       |
| time/                   |               |
|    total_timesteps      | 385500        |
| train/                  |               |
|    approx_kl            | 3.2551761e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00891      |
|    explained_variance   | 0.255         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0102        |
|    n_updates            | 1880          |
|    policy_gradient_loss | 4.92e-07      |
|    value_loss           | 0.0222        |
-------------------------------------------
Eval num_timesteps=386000, episode_reward=0.00 +/- 0.24
Episode length: 14.80 +/- 0.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.0019   |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=0.00 +/- 0.24
Episode length: 14.74 +/- 1.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.00204  |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=-0.04 +/- 0.14
Episode length: 14.92 +/- 0.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0323   |
| time/              |          |
|    fps             | 168      |
|    iterations      | 189      |
|    time_elapsed    | 2300     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=0.00 +/- 0.24
Episode length: 14.74 +/- 1.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 0.00208      |
| time/                   |              |
|    total_timesteps      | 387500       |
| train/                  |              |
|    approx_kl            | 5.704636e-05 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0108      |
|    explained_variance   | 0.213        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0111       |
|    n_updates            | 1890         |
|    policy_gradient_loss | -2.81e-05    |
|    value_loss           | 0.0286       |
------------------------------------------
Eval num_timesteps=388000, episode_reward=0.02 +/- 0.28
Episode length: 14.62 +/- 1.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0225   |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=-0.02 +/- 0.20
Episode length: 14.88 +/- 0.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0185  |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=0.04 +/- 0.31
Episode length: 14.60 +/- 1.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0428   |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0424   |
| time/              |          |
|    fps             | 168      |
|    iterations      | 190      |
|    time_elapsed    | 2307     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=0.12 +/- 0.39
Episode length: 14.28 +/- 1.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.3         |
|    mean_reward          | 0.124        |
| time/                   |              |
|    total_timesteps      | 389500       |
| train/                  |              |
|    approx_kl            | 0.0009702755 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0262      |
|    explained_variance   | 0.241        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0167       |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.000584    |
|    value_loss           | 0.0323       |
------------------------------------------
Eval num_timesteps=390000, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0224   |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=0.04 +/- 0.30
Episode length: 14.58 +/- 1.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0427   |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0424   |
| time/              |          |
|    fps             | 169      |
|    iterations      | 191      |
|    time_elapsed    | 2313     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=0.02 +/- 0.28
Episode length: 14.74 +/- 0.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0221        |
| time/                   |               |
|    total_timesteps      | 391500        |
| train/                  |               |
|    approx_kl            | 3.1858537e-05 |
|    clip_fraction        | 0.00142       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0215       |
|    explained_variance   | 0.193         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00841       |
|    n_updates            | 1910          |
|    policy_gradient_loss | -4.96e-05     |
|    value_loss           | 0.0294        |
-------------------------------------------
Eval num_timesteps=392000, episode_reward=0.04 +/- 0.30
Episode length: 14.60 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=0.02 +/- 0.28
Episode length: 14.66 +/- 1.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=0.00 +/- 0.24
Episode length: 14.78 +/- 0.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 0.00184  |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | 0.0224   |
| time/              |          |
|    fps             | 169      |
|    iterations      | 192      |
|    time_elapsed    | 2320     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=-0.02 +/- 0.20
Episode length: 14.84 +/- 0.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | -0.0184       |
| time/                   |               |
|    total_timesteps      | 393500        |
| train/                  |               |
|    approx_kl            | 0.00026848624 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0166       |
|    explained_variance   | 0.206         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0154        |
|    n_updates            | 1920          |
|    policy_gradient_loss | -4.39e-05     |
|    value_loss           | 0.021         |
-------------------------------------------
Eval num_timesteps=394000, episode_reward=0.10 +/- 0.37
Episode length: 14.40 +/- 1.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.103    |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=0.02 +/- 0.28
Episode length: 14.70 +/- 1.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0223   |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=0.02 +/- 0.28
Episode length: 14.72 +/- 0.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.0222   |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0627   |
| time/              |          |
|    fps             | 169      |
|    iterations      | 193      |
|    time_elapsed    | 2326     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=0.00 +/- 0.24
Episode length: 14.76 +/- 0.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | 0.002         |
| time/                   |               |
|    total_timesteps      | 395500        |
| train/                  |               |
|    approx_kl            | 0.00016835291 |
|    clip_fraction        | 0.00449       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0203       |
|    explained_variance   | 0.282         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00989       |
|    n_updates            | 1930          |
|    policy_gradient_loss | -0.000249     |
|    value_loss           | 0.0361        |
-------------------------------------------
Eval num_timesteps=396000, episode_reward=-0.02 +/- 0.20
Episode length: 14.84 +/- 0.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -0.0184  |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=0.00 +/- 0.24
Episode length: 14.74 +/- 1.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | 0.00208  |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=0.06 +/- 0.33
Episode length: 14.62 +/- 1.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0626   |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | 0.0628   |
| time/              |          |
|    fps             | 170      |
|    iterations      | 194      |
|    time_elapsed    | 2332     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=0.02 +/- 0.28
Episode length: 14.74 +/- 0.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | 0.0221        |
| time/                   |               |
|    total_timesteps      | 397500        |
| train/                  |               |
|    approx_kl            | 5.1594718e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0142       |
|    explained_variance   | 0.273         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0106        |
|    n_updates            | 1940          |
|    policy_gradient_loss | -6.64e-05     |
|    value_loss           | 0.0356        |
-------------------------------------------
Eval num_timesteps=398000, episode_reward=0.08 +/- 0.35
Episode length: 14.44 +/- 1.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | 0.0833   |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=-0.04 +/- 0.14
Episode length: 14.94 +/- 0.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0387  |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=-0.02 +/- 0.20
Episode length: 14.88 +/- 0.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -0.0185  |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -0.00814 |
| time/              |          |
|    fps             | 170      |
|    iterations      | 195      |
|    time_elapsed    | 2339     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=-0.02 +/- 0.20
Episode length: 14.86 +/- 0.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.9         |
|    mean_reward          | -0.0184      |
| time/                   |              |
|    total_timesteps      | 399500       |
| train/                  |              |
|    approx_kl            | 0.0006130774 |
|    clip_fraction        | 0.00693      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0156      |
|    explained_variance   | 0.231        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0023       |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 0.0256       |
------------------------------------------
Eval num_timesteps=400000, episode_reward=0.04 +/- 0.30
Episode length: 14.62 +/- 1.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 0.0426   |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=0.06 +/- 0.33
Episode length: 14.50 +/- 1.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.063    |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=0.08 +/- 0.35
Episode length: 14.48 +/- 1.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 0.0831   |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 0.00179  |
| time/              |          |
|    fps             | 171      |
|    iterations      | 196      |
|    time_elapsed    | 2345     |
|    total_timesteps | 401408   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/predict-position/ppo-4/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 1e-05, 'gamma': 0.94, 'gae_lambda': 0.93}
Training steps: 400000
Frame skip: 4
