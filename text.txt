Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.8     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 425      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-1.00 +/- 0.00
Episode length: 75.30 +/- 8.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.3        |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012693025 |
|    clip_fraction        | 0.0906      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0128     |
|    learning_rate        | 0.01        |
|    loss                 | 0.0595      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00374    |
|    value_loss           | 1.56        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85       |
|    ep_rew_mean     | 0.438    |
| time/              |          |
|    fps             | 341      |
|    iterations      | 2        |
|    time_elapsed    | 23       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-1.00 +/- 0.00
Episode length: 69.20 +/- 7.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.2      |
|    mean_reward          | -1        |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.0145115 |
|    clip_fraction        | 0.178     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.34     |
|    explained_variance   | 0.214     |
|    learning_rate        | 0.01      |
|    loss                 | 0.0247    |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.00745  |
|    value_loss           | 0.182     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.4     |
|    ep_rew_mean     | 0.92     |
| time/              |          |
|    fps             | 324      |
|    iterations      | 3        |
|    time_elapsed    | 37       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.90 +/- 0.30
Episode length: 68.40 +/- 12.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.4        |
|    mean_reward          | -0.9        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.017324444 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0845      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 0.211       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.2     |
|    ep_rew_mean     | 1.35     |
| time/              |          |
|    fps             | 316      |
|    iterations      | 4        |
|    time_elapsed    | 51       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.80 +/- 0.40
Episode length: 68.90 +/- 10.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.9        |
|    mean_reward          | -0.8        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.013199104 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0819      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00234    |
|    value_loss           | 0.251       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 1.58     |
| time/              |          |
|    fps             | 313      |
|    iterations      | 5        |
|    time_elapsed    | 65       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | 1.9         |
| time/                   |             |
|    fps                  | 312         |
|    iterations           | 6           |
|    time_elapsed         | 78          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014083947 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0933      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00881    |
|    value_loss           | 0.248       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.20 +/- 0.60
Episode length: 73.00 +/- 10.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73          |
|    mean_reward          | -0.2        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.014770243 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.01        |
|    loss                 | 0.139       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00536    |
|    value_loss           | 0.305       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 2.06     |
| time/              |          |
|    fps             | 308      |
|    iterations      | 7        |
|    time_elapsed    | 92       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.10 +/- 0.54
Episode length: 81.10 +/- 7.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.1        |
|    mean_reward          | -0.1        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.016499953 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.503       |
|    learning_rate        | 0.01        |
|    loss                 | 0.147       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 0.305       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.03     |
| time/              |          |
|    fps             | 305      |
|    iterations      | 8        |
|    time_elapsed    | 107      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=1.60 +/- 1.02
Episode length: 87.60 +/- 15.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.6        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.019845042 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.01        |
|    loss                 | 0.112       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00396    |
|    value_loss           | 0.3         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.08     |
| time/              |          |
|    fps             | 304      |
|    iterations      | 9        |
|    time_elapsed    | 121      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=1.60 +/- 1.11
Episode length: 90.40 +/- 23.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.4        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.012065472 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.01        |
|    loss                 | 0.0739      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 0.279       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.25     |
| time/              |          |
|    fps             | 302      |
|    iterations      | 10       |
|    time_elapsed    | 135      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=1.20 +/- 1.33
Episode length: 79.80 +/- 21.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.8        |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.015970577 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.01        |
|    loss                 | 0.154       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.000828   |
|    value_loss           | 0.341       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.16     |
| time/              |          |
|    fps             | 300      |
|    iterations      | 11       |
|    time_elapsed    | 149      |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 107         |
|    ep_rew_mean          | 2.28        |
| time/                   |             |
|    fps                  | 301         |
|    iterations           | 12          |
|    time_elapsed         | 162         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.018360853 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.01        |
|    loss                 | 0.118       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0035     |
|    value_loss           | 0.321       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=1.20 +/- 0.75
Episode length: 92.90 +/- 16.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.9        |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.019767988 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.01        |
|    loss                 | 0.15        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 0.342       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.3      |
| time/              |          |
|    fps             | 300      |
|    iterations      | 13       |
|    time_elapsed    | 177      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=1.50 +/- 1.02
Episode length: 90.10 +/- 24.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 90.1      |
|    mean_reward          | 1.5       |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0171634 |
|    clip_fraction        | 0.169     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.05     |
|    explained_variance   | 0.51      |
|    learning_rate        | 0.01      |
|    loss                 | 0.239     |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.00236  |
|    value_loss           | 0.324     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 299      |
|    iterations      | 14       |
|    time_elapsed    | 191      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=2.10 +/- 1.22
Episode length: 90.90 +/- 23.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.9        |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.019179132 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.586       |
|    learning_rate        | 0.01        |
|    loss                 | 0.117       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00446    |
|    value_loss           | 0.291       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 15       |
|    time_elapsed    | 205      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=65000, episode_reward=1.50 +/- 0.92
Episode length: 94.60 +/- 19.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.6        |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 65000       |
| train/                  |             |
|    approx_kl            | 0.014832625 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.01        |
|    loss                 | 0.119       |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00127    |
|    value_loss           | 0.287       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 16       |
|    time_elapsed    | 220      |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 2.76        |
| time/                   |             |
|    fps                  | 298         |
|    iterations           | 17          |
|    time_elapsed         | 233         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.016310561 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.01        |
|    loss                 | 0.189       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00091    |
|    value_loss           | 0.329       |
-----------------------------------------
Eval num_timesteps=70000, episode_reward=1.20 +/- 0.87
Episode length: 85.40 +/- 18.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85.4        |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.039444637 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.498       |
|    learning_rate        | 0.01        |
|    loss                 | 0.232       |
|    n_updates            | 170         |
|    policy_gradient_loss | 0.00255     |
|    value_loss           | 0.331       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 298      |
|    iterations      | 18       |
|    time_elapsed    | 247      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=75000, episode_reward=0.90 +/- 1.14
Episode length: 75.40 +/- 19.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 75.4       |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 75000      |
| train/                  |            |
|    approx_kl            | 0.03203462 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.991     |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.01       |
|    loss                 | 0.0962     |
|    n_updates            | 180        |
|    policy_gradient_loss | 0.00271    |
|    value_loss           | 0.296      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 19       |
|    time_elapsed    | 261      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=80000, episode_reward=0.90 +/- 0.70
Episode length: 81.90 +/- 12.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.9        |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.043251872 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.958      |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.01        |
|    loss                 | 0.106       |
|    n_updates            | 190         |
|    policy_gradient_loss | 0.00417     |
|    value_loss           | 0.297       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.16     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 20       |
|    time_elapsed    | 275      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=85000, episode_reward=0.40 +/- 0.92
Episode length: 71.50 +/- 17.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 71.5        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 85000       |
| train/                  |             |
|    approx_kl            | 0.029096607 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.941      |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.01        |
|    loss                 | 0.158       |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.00535     |
|    value_loss           | 0.295       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.13     |
| time/              |          |
|    fps             | 297      |
|    iterations      | 21       |
|    time_elapsed    | 289      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=90000, episode_reward=1.60 +/- 1.28
Episode length: 82.40 +/- 19.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.4        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.030659366 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.937      |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.01        |
|    loss                 | 0.207       |
|    n_updates            | 210         |
|    policy_gradient_loss | 0.0042      |
|    value_loss           | 0.312       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.28     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 22       |
|    time_elapsed    | 303      |
|    total_timesteps | 90112    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 2.26       |
| time/                   |            |
|    fps                  | 298        |
|    iterations           | 23         |
|    time_elapsed         | 316        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.02822128 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.954     |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.01       |
|    loss                 | 0.145      |
|    n_updates            | 220        |
|    policy_gradient_loss | 0.00255    |
|    value_loss           | 0.261      |
----------------------------------------
Eval num_timesteps=95000, episode_reward=2.00 +/- 1.00
Episode length: 98.80 +/- 21.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.8        |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.024552047 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.929      |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.01        |
|    loss                 | 0.179       |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.00401     |
|    value_loss           | 0.281       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.4      |
| time/              |          |
|    fps             | 297      |
|    iterations      | 24       |
|    time_elapsed    | 330      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=100000, episode_reward=1.80 +/- 0.98
Episode length: 99.80 +/- 17.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.8        |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.035884522 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.934      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.01        |
|    loss                 | 0.126       |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.00112     |
|    value_loss           | 0.329       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 296      |
|    iterations      | 25       |
|    time_elapsed    | 345      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=105000, episode_reward=1.50 +/- 1.12
Episode length: 87.90 +/- 27.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.9        |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 105000      |
| train/                  |             |
|    approx_kl            | 0.016907658 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.958      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.01        |
|    loss                 | 0.108       |
|    n_updates            | 250         |
|    policy_gradient_loss | 0.000743    |
|    value_loss           | 0.262       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.54     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 26       |
|    time_elapsed    | 360      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=110000, episode_reward=1.20 +/- 0.87
Episode length: 77.50 +/- 20.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 77.5       |
|    mean_reward          | 1.2        |
| time/                   |            |
|    total_timesteps      | 110000     |
| train/                  |            |
|    approx_kl            | 0.03400964 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.962     |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.01       |
|    loss                 | 0.127      |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.00453   |
|    value_loss           | 0.297      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 27       |
|    time_elapsed    | 374      |
|    total_timesteps | 110592   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 2.47        |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 28          |
|    time_elapsed         | 387         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.032742154 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.922      |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.01        |
|    loss                 | 0.184       |
|    n_updates            | 270         |
|    policy_gradient_loss | -3.27e-05   |
|    value_loss           | 0.319       |
-----------------------------------------
Eval num_timesteps=115000, episode_reward=1.10 +/- 0.83
Episode length: 89.80 +/- 22.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 89.8       |
|    mean_reward          | 1.1        |
| time/                   |            |
|    total_timesteps      | 115000     |
| train/                  |            |
|    approx_kl            | 0.02118367 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.899     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.01       |
|    loss                 | 0.192      |
|    n_updates            | 280        |
|    policy_gradient_loss | 0.00202    |
|    value_loss           | 0.32       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 29       |
|    time_elapsed    | 402      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=120000, episode_reward=1.60 +/- 0.66
Episode length: 87.40 +/- 22.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.4        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.026308466 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.921      |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.01        |
|    loss                 | 0.203       |
|    n_updates            | 290         |
|    policy_gradient_loss | 0.00242     |
|    value_loss           | 0.342       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 30       |
|    time_elapsed    | 416      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=125000, episode_reward=1.60 +/- 1.36
Episode length: 95.40 +/- 16.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.4        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.021701239 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.924      |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.01        |
|    loss                 | 0.122       |
|    n_updates            | 300         |
|    policy_gradient_loss | -2.38e-05   |
|    value_loss           | 0.307       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 31       |
|    time_elapsed    | 430      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=130000, episode_reward=1.40 +/- 0.92
Episode length: 92.10 +/- 18.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.1        |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.030967386 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.947      |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.01        |
|    loss                 | 0.16        |
|    n_updates            | 310         |
|    policy_gradient_loss | 0.00388     |
|    value_loss           | 0.356       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 294      |
|    iterations      | 32       |
|    time_elapsed    | 445      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=135000, episode_reward=1.10 +/- 1.04
Episode length: 81.60 +/- 22.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 81.6       |
|    mean_reward          | 1.1        |
| time/                   |            |
|    total_timesteps      | 135000     |
| train/                  |            |
|    approx_kl            | 0.02716824 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.926     |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.01       |
|    loss                 | 0.217      |
|    n_updates            | 320        |
|    policy_gradient_loss | 0.00489    |
|    value_loss           | 0.357      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 33       |
|    time_elapsed    | 459      |
|    total_timesteps | 135168   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 111        |
|    ep_rew_mean          | 2.76       |
| time/                   |            |
|    fps                  | 294        |
|    iterations           | 34         |
|    time_elapsed         | 472        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.02937889 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.952     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.01       |
|    loss                 | 0.114      |
|    n_updates            | 330        |
|    policy_gradient_loss | 0.00379    |
|    value_loss           | 0.318      |
----------------------------------------
Eval num_timesteps=140000, episode_reward=2.10 +/- 0.83
Episode length: 91.50 +/- 19.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.5        |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.020443816 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.951      |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.01        |
|    loss                 | 0.205       |
|    n_updates            | 340         |
|    policy_gradient_loss | 0.00426     |
|    value_loss           | 0.325       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.85     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 35       |
|    time_elapsed    | 486      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=145000, episode_reward=1.60 +/- 0.49
Episode length: 91.90 +/- 15.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.9        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 145000      |
| train/                  |             |
|    approx_kl            | 0.021335784 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.96       |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.01        |
|    loss                 | 0.137       |
|    n_updates            | 350         |
|    policy_gradient_loss | 0.00382     |
|    value_loss           | 0.3         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 294      |
|    iterations      | 36       |
|    time_elapsed    | 500      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=2.10 +/- 1.04
Episode length: 100.50 +/- 21.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.021911884 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.969      |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.01        |
|    loss                 | 0.139       |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 0.33        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 37       |
|    time_elapsed    | 515      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=155000, episode_reward=1.90 +/- 1.04
Episode length: 99.80 +/- 19.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.8        |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 155000      |
| train/                  |             |
|    approx_kl            | 0.026379257 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.94       |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.01        |
|    loss                 | 0.136       |
|    n_updates            | 370         |
|    policy_gradient_loss | 0.00257     |
|    value_loss           | 0.318       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.24     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 38       |
|    time_elapsed    | 529      |
|    total_timesteps | 155648   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | 3.27        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 39          |
|    time_elapsed         | 542         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.021545723 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.916      |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.01        |
|    loss                 | 0.188       |
|    n_updates            | 380         |
|    policy_gradient_loss | 0.00459     |
|    value_loss           | 0.353       |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=2.00 +/- 0.77
Episode length: 102.80 +/- 13.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.025346624 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.896      |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.01        |
|    loss                 | 0.221       |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.00681     |
|    value_loss           | 0.323       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.24     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 40       |
|    time_elapsed    | 556      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=165000, episode_reward=1.40 +/- 0.49
Episode length: 78.00 +/- 15.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78          |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 165000      |
| train/                  |             |
|    approx_kl            | 0.028736295 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.89       |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.01        |
|    loss                 | 0.171       |
|    n_updates            | 400         |
|    policy_gradient_loss | 0.00605     |
|    value_loss           | 0.306       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 41       |
|    time_elapsed    | 570      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=170000, episode_reward=1.80 +/- 0.87
Episode length: 88.80 +/- 17.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.8        |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.031543687 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.889      |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.01        |
|    loss                 | 0.156       |
|    n_updates            | 410         |
|    policy_gradient_loss | 0.00444     |
|    value_loss           | 0.324       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 42       |
|    time_elapsed    | 585      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=175000, episode_reward=1.80 +/- 0.98
Episode length: 97.60 +/- 17.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.6        |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.031275354 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.869      |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.01        |
|    loss                 | 0.132       |
|    n_updates            | 420         |
|    policy_gradient_loss | 0.00395     |
|    value_loss           | 0.302       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 43       |
|    time_elapsed    | 599      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=180000, episode_reward=1.90 +/- 0.94
Episode length: 92.60 +/- 23.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.6        |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.023090404 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.829      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0968      |
|    n_updates            | 430         |
|    policy_gradient_loss | 0.00445     |
|    value_loss           | 0.331       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 44       |
|    time_elapsed    | 613      |
|    total_timesteps | 180224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 114        |
|    ep_rew_mean          | 3.35       |
| time/                   |            |
|    fps                  | 293        |
|    iterations           | 45         |
|    time_elapsed         | 627        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.02666502 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.841     |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.01       |
|    loss                 | 0.222      |
|    n_updates            | 440        |
|    policy_gradient_loss | 0.0108     |
|    value_loss           | 0.298      |
----------------------------------------
Eval num_timesteps=185000, episode_reward=1.50 +/- 1.43
Episode length: 86.20 +/- 26.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.2        |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 185000      |
| train/                  |             |
|    approx_kl            | 0.029875228 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.859      |
|    explained_variance   | 0.644       |
|    learning_rate        | 0.01        |
|    loss                 | 0.19        |
|    n_updates            | 450         |
|    policy_gradient_loss | 0.00579     |
|    value_loss           | 0.32        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.42     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 46       |
|    time_elapsed    | 640      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=190000, episode_reward=1.90 +/- 0.70
Episode length: 97.40 +/- 13.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.4        |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 0.038903452 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.87       |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.01        |
|    loss                 | 0.24        |
|    n_updates            | 460         |
|    policy_gradient_loss | 0.0089      |
|    value_loss           | 0.308       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.29     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 47       |
|    time_elapsed    | 655      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=195000, episode_reward=1.90 +/- 0.70
Episode length: 96.40 +/- 22.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.4        |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.029287826 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.858      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.01        |
|    loss                 | 0.171       |
|    n_updates            | 470         |
|    policy_gradient_loss | 0.00197     |
|    value_loss           | 0.286       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.4      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 48       |
|    time_elapsed    | 669      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=1.40 +/- 0.49
Episode length: 89.60 +/- 14.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.6        |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.031907458 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.834      |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.01        |
|    loss                 | 0.172       |
|    n_updates            | 480         |
|    policy_gradient_loss | 0.00848     |
|    value_loss           | 0.322       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.33     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 49       |
|    time_elapsed    | 684      |
|    total_timesteps | 200704   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 115        |
|    ep_rew_mean          | 3.36       |
| time/                   |            |
|    fps                  | 293        |
|    iterations           | 50         |
|    time_elapsed         | 697        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.03677468 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.816     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.01       |
|    loss                 | 0.0998     |
|    n_updates            | 490        |
|    policy_gradient_loss | 0.0042     |
|    value_loss           | 0.319      |
----------------------------------------
Eval num_timesteps=205000, episode_reward=1.30 +/- 0.90
Episode length: 90.10 +/- 17.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.1        |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.045084547 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.812      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.01        |
|    loss                 | 0.196       |
|    n_updates            | 500         |
|    policy_gradient_loss | 0.00406     |
|    value_loss           | 0.314       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.26     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 51       |
|    time_elapsed    | 711      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=210000, episode_reward=1.80 +/- 0.60
Episode length: 97.70 +/- 16.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.7       |
|    mean_reward          | 1.8        |
| time/                   |            |
|    total_timesteps      | 210000     |
| train/                  |            |
|    approx_kl            | 0.03594459 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.01       |
|    loss                 | 0.176      |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.00143   |
|    value_loss           | 0.312      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 52       |
|    time_elapsed    | 725      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=215000, episode_reward=1.50 +/- 1.12
Episode length: 89.70 +/- 21.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.7        |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 215000      |
| train/                  |             |
|    approx_kl            | 0.037871264 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.789      |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.01        |
|    loss                 | 0.164       |
|    n_updates            | 520         |
|    policy_gradient_loss | 0.00529     |
|    value_loss           | 0.319       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 53       |
|    time_elapsed    | 739      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=220000, episode_reward=1.60 +/- 0.92
Episode length: 93.20 +/- 16.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 93.2       |
|    mean_reward          | 1.6        |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.03875822 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.791     |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.01       |
|    loss                 | 0.2        |
|    n_updates            | 530        |
|    policy_gradient_loss | 0.00388    |
|    value_loss           | 0.325      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.27     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 54       |
|    time_elapsed    | 754      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=225000, episode_reward=0.80 +/- 1.08
Episode length: 79.20 +/- 20.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 79.2       |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 225000     |
| train/                  |            |
|    approx_kl            | 0.02357847 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.826     |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.01       |
|    loss                 | 0.201      |
|    n_updates            | 540        |
|    policy_gradient_loss | 0.00671    |
|    value_loss           | 0.351      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.34     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 55       |
|    time_elapsed    | 768      |
|    total_timesteps | 225280   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 111        |
|    ep_rew_mean          | 2.89       |
| time/                   |            |
|    fps                  | 293        |
|    iterations           | 56         |
|    time_elapsed         | 780        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.03854303 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.01       |
|    loss                 | 0.148      |
|    n_updates            | 550        |
|    policy_gradient_loss | 0.00461    |
|    value_loss           | 0.324      |
----------------------------------------
Eval num_timesteps=230000, episode_reward=0.60 +/- 0.66
Episode length: 79.60 +/- 15.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.6        |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.033344347 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.85       |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.01        |
|    loss                 | 0.113       |
|    n_updates            | 560         |
|    policy_gradient_loss | 0.0032      |
|    value_loss           | 0.303       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 57       |
|    time_elapsed    | 794      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=235000, episode_reward=1.10 +/- 0.83
Episode length: 83.90 +/- 15.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 83.9      |
|    mean_reward          | 1.1       |
| time/                   |           |
|    total_timesteps      | 235000    |
| train/                  |           |
|    approx_kl            | 0.0378052 |
|    clip_fraction        | 0.207     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.862    |
|    explained_variance   | 0.646     |
|    learning_rate        | 0.01      |
|    loss                 | 0.209     |
|    n_updates            | 570       |
|    policy_gradient_loss | 0.0037    |
|    value_loss           | 0.328     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.92     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 58       |
|    time_elapsed    | 808      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=240000, episode_reward=1.00 +/- 1.00
Episode length: 82.10 +/- 17.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 82.1       |
|    mean_reward          | 1          |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.03455245 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.883     |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.01       |
|    loss                 | 0.127      |
|    n_updates            | 580        |
|    policy_gradient_loss | 0.00401    |
|    value_loss           | 0.315      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.2      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 59       |
|    time_elapsed    | 822      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=245000, episode_reward=0.20 +/- 0.40
Episode length: 67.80 +/- 10.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.8      |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 245000    |
| train/                  |           |
|    approx_kl            | 0.0428957 |
|    clip_fraction        | 0.242     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.913    |
|    explained_variance   | 0.591     |
|    learning_rate        | 0.01      |
|    loss                 | 0.111     |
|    n_updates            | 590       |
|    policy_gradient_loss | 0.00917   |
|    value_loss           | 0.295     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 60       |
|    time_elapsed    | 836      |
|    total_timesteps | 245760   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 2.97        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 61          |
|    time_elapsed         | 849         |
|    total_timesteps      | 249856      |
| train/                  |             |
|    approx_kl            | 0.028885385 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.9        |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.01        |
|    loss                 | 0.183       |
|    n_updates            | 600         |
|    policy_gradient_loss | 0.00645     |
|    value_loss           | 0.34        |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=0.80 +/- 0.75
Episode length: 82.40 +/- 14.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.4        |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.039796464 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.976      |
|    explained_variance   | 0.549       |
|    learning_rate        | 0.01        |
|    loss                 | 0.143       |
|    n_updates            | 610         |
|    policy_gradient_loss | 0.00543     |
|    value_loss           | 0.313       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 62       |
|    time_elapsed    | 863      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=255000, episode_reward=0.50 +/- 0.67
Episode length: 80.40 +/- 19.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.4        |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 255000      |
| train/                  |             |
|    approx_kl            | 0.036170043 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.544       |
|    learning_rate        | 0.01        |
|    loss                 | 0.128       |
|    n_updates            | 620         |
|    policy_gradient_loss | 0.00855     |
|    value_loss           | 0.313       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 63       |
|    time_elapsed    | 876      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=260000, episode_reward=0.80 +/- 0.75
Episode length: 76.00 +/- 12.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76          |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 260000      |
| train/                  |             |
|    approx_kl            | 0.031319153 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.56        |
|    learning_rate        | 0.01        |
|    loss                 | 0.0925      |
|    n_updates            | 630         |
|    policy_gradient_loss | 0.00447     |
|    value_loss           | 0.276       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.81     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 64       |
|    time_elapsed    | 891      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=265000, episode_reward=0.90 +/- 0.83
Episode length: 78.10 +/- 13.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.1        |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 265000      |
| train/                  |             |
|    approx_kl            | 0.045009777 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.01        |
|    loss                 | 0.204       |
|    n_updates            | 640         |
|    policy_gradient_loss | 0.00519     |
|    value_loss           | 0.261       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 65       |
|    time_elapsed    | 905      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=270000, episode_reward=1.10 +/- 0.70
Episode length: 81.70 +/- 11.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 81.7       |
|    mean_reward          | 1.1        |
| time/                   |            |
|    total_timesteps      | 270000     |
| train/                  |            |
|    approx_kl            | 0.03135374 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.994     |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.01       |
|    loss                 | 0.178      |
|    n_updates            | 650        |
|    policy_gradient_loss | 0.00461    |
|    value_loss           | 0.319      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 66       |
|    time_elapsed    | 918      |
|    total_timesteps | 270336   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 109        |
|    ep_rew_mean          | 3.14       |
| time/                   |            |
|    fps                  | 294        |
|    iterations           | 67         |
|    time_elapsed         | 931        |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.03502827 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.979     |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.01       |
|    loss                 | 0.156      |
|    n_updates            | 660        |
|    policy_gradient_loss | 0.00715    |
|    value_loss           | 0.293      |
----------------------------------------
Eval num_timesteps=275000, episode_reward=1.20 +/- 0.75
Episode length: 91.20 +/- 18.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.2        |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.034498587 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.01        |
|    loss                 | 0.163       |
|    n_updates            | 670         |
|    policy_gradient_loss | 0.0015      |
|    value_loss           | 0.321       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 68       |
|    time_elapsed    | 945      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=280000, episode_reward=1.10 +/- 1.04
Episode length: 85.30 +/- 21.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85.3        |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.025965683 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.993      |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.01        |
|    loss                 | 0.209       |
|    n_updates            | 680         |
|    policy_gradient_loss | 0.0043      |
|    value_loss           | 0.339       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 69       |
|    time_elapsed    | 959      |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=285000, episode_reward=1.30 +/- 1.00
Episode length: 86.80 +/- 22.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 86.8       |
|    mean_reward          | 1.3        |
| time/                   |            |
|    total_timesteps      | 285000     |
| train/                  |            |
|    approx_kl            | 0.03406399 |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.572      |
|    learning_rate        | 0.01       |
|    loss                 | 0.1        |
|    n_updates            | 690        |
|    policy_gradient_loss | 0.00582    |
|    value_loss           | 0.308      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 70       |
|    time_elapsed    | 974      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=290000, episode_reward=1.00 +/- 0.63
Episode length: 80.30 +/- 19.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.3        |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.027129877 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.976      |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.01        |
|    loss                 | 0.198       |
|    n_updates            | 700         |
|    policy_gradient_loss | 0.0065      |
|    value_loss           | 0.335       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 71       |
|    time_elapsed    | 988      |
|    total_timesteps | 290816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 3.08        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 72          |
|    time_elapsed         | 1000        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.021884786 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.959      |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.01        |
|    loss                 | 0.165       |
|    n_updates            | 710         |
|    policy_gradient_loss | 0.00507     |
|    value_loss           | 0.334       |
-----------------------------------------
Eval num_timesteps=295000, episode_reward=1.20 +/- 0.98
Episode length: 83.80 +/- 15.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.8        |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.025785927 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.913      |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.01        |
|    loss                 | 0.244       |
|    n_updates            | 720         |
|    policy_gradient_loss | 0.00696     |
|    value_loss           | 0.324       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 73       |
|    time_elapsed    | 1015     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=300000, episode_reward=0.70 +/- 0.64
Episode length: 77.40 +/- 11.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.4        |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.035339545 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.895      |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.01        |
|    loss                 | 0.15        |
|    n_updates            | 730         |
|    policy_gradient_loss | 0.00751     |
|    value_loss           | 0.305       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 294      |
|    iterations      | 74       |
|    time_elapsed    | 1029     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=305000, episode_reward=0.60 +/- 0.66
Episode length: 72.70 +/- 13.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.7        |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 305000      |
| train/                  |             |
|    approx_kl            | 0.038614355 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.929      |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.01        |
|    loss                 | 0.193       |
|    n_updates            | 740         |
|    policy_gradient_loss | 0.00446     |
|    value_loss           | 0.345       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 3.05     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 75       |
|    time_elapsed    | 1044     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=310000, episode_reward=0.40 +/- 0.66
Episode length: 73.40 +/- 16.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.4        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.018212972 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.939      |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0866      |
|    n_updates            | 750         |
|    policy_gradient_loss | 0.00352     |
|    value_loss           | 0.284       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 76       |
|    time_elapsed    | 1057     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=315000, episode_reward=1.40 +/- 0.92
Episode length: 86.60 +/- 19.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 86.6       |
|    mean_reward          | 1.4        |
| time/                   |            |
|    total_timesteps      | 315000     |
| train/                  |            |
|    approx_kl            | 0.04816513 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.942     |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.01       |
|    loss                 | 0.193      |
|    n_updates            | 760        |
|    policy_gradient_loss | 0.0427     |
|    value_loss           | 0.287      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 77       |
|    time_elapsed    | 1072     |
|    total_timesteps | 315392   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 2.82        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 78          |
|    time_elapsed         | 1084        |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.027029607 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.918      |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.01        |
|    loss                 | 0.183       |
|    n_updates            | 770         |
|    policy_gradient_loss | 0.00408     |
|    value_loss           | 0.299       |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=0.80 +/- 0.98
Episode length: 78.70 +/- 16.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 78.7       |
|    mean_reward          | 0.8        |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.02378198 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.869     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.01       |
|    loss                 | 0.158      |
|    n_updates            | 780        |
|    policy_gradient_loss | 9.06e-05   |
|    value_loss           | 0.293      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 79       |
|    time_elapsed    | 1099     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=325000, episode_reward=0.40 +/- 0.66
Episode length: 80.40 +/- 16.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.4        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.030108694 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.859      |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.01        |
|    loss                 | 0.135       |
|    n_updates            | 790         |
|    policy_gradient_loss | 0.00315     |
|    value_loss           | 0.262       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 80       |
|    time_elapsed    | 1113     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=330000, episode_reward=0.40 +/- 0.49
Episode length: 76.00 +/- 17.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76          |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.037084877 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.89       |
|    explained_variance   | 0.614       |
|    learning_rate        | 0.01        |
|    loss                 | 0.201       |
|    n_updates            | 800         |
|    policy_gradient_loss | 0.00354     |
|    value_loss           | 0.288       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 81       |
|    time_elapsed    | 1127     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=335000, episode_reward=0.90 +/- 0.70
Episode length: 78.20 +/- 16.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 78.2       |
|    mean_reward          | 0.9        |
| time/                   |            |
|    total_timesteps      | 335000     |
| train/                  |            |
|    approx_kl            | 0.03340499 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.908     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.01       |
|    loss                 | 0.15       |
|    n_updates            | 810        |
|    policy_gradient_loss | 0.000446   |
|    value_loss           | 0.282      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.28     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 82       |
|    time_elapsed    | 1141     |
|    total_timesteps | 335872   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | 2.26        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 83          |
|    time_elapsed         | 1154        |
|    total_timesteps      | 339968      |
| train/                  |             |
|    approx_kl            | 0.042721543 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0951      |
|    n_updates            | 820         |
|    policy_gradient_loss | 0.0154      |
|    value_loss           | 0.256       |
-----------------------------------------
Eval num_timesteps=340000, episode_reward=1.00 +/- 0.63
Episode length: 85.90 +/- 13.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85.9        |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.020203209 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.846      |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.01        |
|    loss                 | 0.118       |
|    n_updates            | 830         |
|    policy_gradient_loss | 0.000147    |
|    value_loss           | 0.253       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 84       |
|    time_elapsed    | 1168     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=345000, episode_reward=1.00 +/- 0.63
Episode length: 88.00 +/- 17.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88          |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 345000      |
| train/                  |             |
|    approx_kl            | 0.026880704 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.844      |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.01        |
|    loss                 | 0.15        |
|    n_updates            | 840         |
|    policy_gradient_loss | 0.00288     |
|    value_loss           | 0.256       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 85       |
|    time_elapsed    | 1182     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=350000, episode_reward=1.10 +/- 0.54
Episode length: 79.60 +/- 13.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.6        |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.027693398 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.841      |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.01        |
|    loss                 | 0.101       |
|    n_updates            | 850         |
|    policy_gradient_loss | 0.00525     |
|    value_loss           | 0.281       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 86       |
|    time_elapsed    | 1196     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=355000, episode_reward=1.00 +/- 0.63
Episode length: 88.70 +/- 13.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 88.7       |
|    mean_reward          | 1          |
| time/                   |            |
|    total_timesteps      | 355000     |
| train/                  |            |
|    approx_kl            | 0.08367584 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.845     |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.01       |
|    loss                 | 0.199      |
|    n_updates            | 860        |
|    policy_gradient_loss | 0.0033     |
|    value_loss           | 0.292      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.7      |
| time/              |          |
|    fps             | 294      |
|    iterations      | 87       |
|    time_elapsed    | 1210     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=360000, episode_reward=1.50 +/- 1.28
Episode length: 83.30 +/- 19.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 83.3       |
|    mean_reward          | 1.5        |
| time/                   |            |
|    total_timesteps      | 360000     |
| train/                  |            |
|    approx_kl            | 0.03223003 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.807     |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.01       |
|    loss                 | 0.155      |
|    n_updates            | 870        |
|    policy_gradient_loss | 0.00334    |
|    value_loss           | 0.282      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 88       |
|    time_elapsed    | 1225     |
|    total_timesteps | 360448   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 2.59        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 89          |
|    time_elapsed         | 1237        |
|    total_timesteps      | 364544      |
| train/                  |             |
|    approx_kl            | 0.030950189 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.816      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.01        |
|    loss                 | 0.147       |
|    n_updates            | 880         |
|    policy_gradient_loss | 0.00108     |
|    value_loss           | 0.282       |
-----------------------------------------
Eval num_timesteps=365000, episode_reward=1.00 +/- 0.89
Episode length: 90.10 +/- 14.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.1        |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.028529715 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.813      |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.01        |
|    loss                 | 0.148       |
|    n_updates            | 890         |
|    policy_gradient_loss | 0.00799     |
|    value_loss           | 0.292       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 90       |
|    time_elapsed    | 1252     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=370000, episode_reward=1.10 +/- 1.04
Episode length: 79.00 +/- 15.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79          |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.031026056 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0865      |
|    n_updates            | 900         |
|    policy_gradient_loss | 0.00325     |
|    value_loss           | 0.316       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 91       |
|    time_elapsed    | 1266     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=375000, episode_reward=1.60 +/- 0.49
Episode length: 94.60 +/- 18.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 94.6       |
|    mean_reward          | 1.6        |
| time/                   |            |
|    total_timesteps      | 375000     |
| train/                  |            |
|    approx_kl            | 0.03520784 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.765     |
|    explained_variance   | 0.641      |
|    learning_rate        | 0.01       |
|    loss                 | 0.179      |
|    n_updates            | 910        |
|    policy_gradient_loss | 0.00243    |
|    value_loss           | 0.291      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.35     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 92       |
|    time_elapsed    | 1280     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=380000, episode_reward=1.30 +/- 0.90
Episode length: 88.90 +/- 11.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.9        |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 380000      |
| train/                  |             |
|    approx_kl            | 0.034484506 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.789      |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.01        |
|    loss                 | 0.15        |
|    n_updates            | 920         |
|    policy_gradient_loss | 0.00742     |
|    value_loss           | 0.316       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.18     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 93       |
|    time_elapsed    | 1294     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=385000, episode_reward=1.10 +/- 0.83
Episode length: 81.90 +/- 14.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.9        |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 385000      |
| train/                  |             |
|    approx_kl            | 0.050593976 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.779      |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.01        |
|    loss                 | 0.155       |
|    n_updates            | 930         |
|    policy_gradient_loss | 0.0024      |
|    value_loss           | 0.313       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 94       |
|    time_elapsed    | 1308     |
|    total_timesteps | 385024   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 111         |
|    ep_rew_mean          | 3.03        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 95          |
|    time_elapsed         | 1322        |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.027956616 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.769      |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.01        |
|    loss                 | 0.106       |
|    n_updates            | 940         |
|    policy_gradient_loss | 0.00558     |
|    value_loss           | 0.284       |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=1.10 +/- 0.70
Episode length: 89.60 +/- 17.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.6        |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.028499164 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.01        |
|    loss                 | 0.14        |
|    n_updates            | 950         |
|    policy_gradient_loss | 0.00185     |
|    value_loss           | 0.331       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 96       |
|    time_elapsed    | 1336     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=395000, episode_reward=1.00 +/- 0.45
Episode length: 86.70 +/- 10.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.7        |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 395000      |
| train/                  |             |
|    approx_kl            | 0.034775805 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.812      |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.01        |
|    loss                 | 0.15        |
|    n_updates            | 960         |
|    policy_gradient_loss | 0.00256     |
|    value_loss           | 0.327       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.06     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 97       |
|    time_elapsed    | 1351     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=400000, episode_reward=1.00 +/- 1.18
Episode length: 86.30 +/- 18.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.3        |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.034305766 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.786      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.01        |
|    loss                 | 0.146       |
|    n_updates            | 970         |
|    policy_gradient_loss | 0.0078      |
|    value_loss           | 0.305       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 98       |
|    time_elapsed    | 1365     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=405000, episode_reward=1.00 +/- 0.89
Episode length: 78.10 +/- 13.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.1        |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 405000      |
| train/                  |             |
|    approx_kl            | 0.021571465 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0931      |
|    n_updates            | 980         |
|    policy_gradient_loss | 0.00196     |
|    value_loss           | 0.269       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 99       |
|    time_elapsed    | 1379     |
|    total_timesteps | 405504   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 2.73        |
| time/                   |             |
|    fps                  | 294         |
|    iterations           | 100         |
|    time_elapsed         | 1392        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.059635755 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.755      |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.01        |
|    loss                 | 0.154       |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.000163   |
|    value_loss           | 0.307       |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=1.70 +/- 0.78
Episode length: 92.80 +/- 18.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.8        |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.057741653 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.01        |
|    loss                 | 0.166       |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.0096     |
|    value_loss           | 0.299       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.63     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 101      |
|    time_elapsed    | 1406     |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=415000, episode_reward=1.10 +/- 0.70
Episode length: 86.60 +/- 19.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.6        |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 415000      |
| train/                  |             |
|    approx_kl            | 0.020620072 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.01        |
|    loss                 | 0.134       |
|    n_updates            | 1010        |
|    policy_gradient_loss | 0.00292     |
|    value_loss           | 0.324       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    fps             | 294      |
|    iterations      | 102      |
|    time_elapsed    | 1420     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=420000, episode_reward=0.80 +/- 0.87
Episode length: 79.40 +/- 20.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.4        |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.024718756 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.799      |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.01        |
|    loss                 | 0.137       |
|    n_updates            | 1020        |
|    policy_gradient_loss | 0.00339     |
|    value_loss           | 0.256       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 103      |
|    time_elapsed    | 1435     |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=425000, episode_reward=0.90 +/- 0.70
Episode length: 91.10 +/- 15.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.1        |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.024050964 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.01        |
|    loss                 | 0.148       |
|    n_updates            | 1030        |
|    policy_gradient_loss | 0.00221     |
|    value_loss           | 0.302       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 104      |
|    time_elapsed    | 1449     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=430000, episode_reward=2.00 +/- 0.77
Episode length: 99.10 +/- 18.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.1        |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.020666093 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.01        |
|    loss                 | 0.143       |
|    n_updates            | 1040        |
|    policy_gradient_loss | 0.00335     |
|    value_loss           | 0.307       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 105      |
|    time_elapsed    | 1464     |
|    total_timesteps | 430080   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 111         |
|    ep_rew_mean          | 2.94        |
| time/                   |             |
|    fps                  | 293         |
|    iterations           | 106         |
|    time_elapsed         | 1477        |
|    total_timesteps      | 434176      |
| train/                  |             |
|    approx_kl            | 0.021342847 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.777      |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.01        |
|    loss                 | 0.15        |
|    n_updates            | 1050        |
|    policy_gradient_loss | 0.00167     |
|    value_loss           | 0.314       |
-----------------------------------------
Eval num_timesteps=435000, episode_reward=1.50 +/- 0.81
Episode length: 92.60 +/- 19.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 92.6       |
|    mean_reward          | 1.5        |
| time/                   |            |
|    total_timesteps      | 435000     |
| train/                  |            |
|    approx_kl            | 0.02400488 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.807     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.01       |
|    loss                 | 0.147      |
|    n_updates            | 1060       |
|    policy_gradient_loss | 0.00243    |
|    value_loss           | 0.264      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.87     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 107      |
|    time_elapsed    | 1492     |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=440000, episode_reward=1.60 +/- 0.80
Episode length: 100.10 +/- 17.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.026885511 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.846      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.01        |
|    loss                 | 0.168       |
|    n_updates            | 1070        |
|    policy_gradient_loss | 0.0039      |
|    value_loss           | 0.32        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 108      |
|    time_elapsed    | 1506     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=445000, episode_reward=1.30 +/- 0.78
Episode length: 88.70 +/- 16.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.7        |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 445000      |
| train/                  |             |
|    approx_kl            | 0.023643162 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.846      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.01        |
|    loss                 | 0.192       |
|    n_updates            | 1080        |
|    policy_gradient_loss | 0.00255     |
|    value_loss           | 0.333       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 293      |
|    iterations      | 109      |
|    time_elapsed    | 1520     |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=450000, episode_reward=2.20 +/- 0.98
Episode length: 95.70 +/- 18.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.7        |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.034302622 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.823      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.01        |
|    loss                 | 0.106       |
|    n_updates            | 1090        |
|    policy_gradient_loss | 0.00297     |
|    value_loss           | 0.302       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.83     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 110      |
|    time_elapsed    | 1535     |
|    total_timesteps | 450560   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 2.72        |
| time/                   |             |
|    fps                  | 293         |
|    iterations           | 111         |
|    time_elapsed         | 1547        |
|    total_timesteps      | 454656      |
| train/                  |             |
|    approx_kl            | 0.033862267 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.01        |
|    loss                 | 0.16        |
|    n_updates            | 1100        |
|    policy_gradient_loss | 0.00693     |
|    value_loss           | 0.332       |
-----------------------------------------
Eval num_timesteps=455000, episode_reward=1.50 +/- 1.02
Episode length: 92.00 +/- 18.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92          |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 455000      |
| train/                  |             |
|    approx_kl            | 0.031488847 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.812      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.01        |
|    loss                 | 0.144       |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.000526   |
|    value_loss           | 0.292       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 112      |
|    time_elapsed    | 1562     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=460000, episode_reward=1.90 +/- 0.70
Episode length: 94.90 +/- 14.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.9        |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.024727084 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.844      |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.01        |
|    loss                 | 0.14        |
|    n_updates            | 1120        |
|    policy_gradient_loss | 0.00673     |
|    value_loss           | 0.292       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 113      |
|    time_elapsed    | 1576     |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=465000, episode_reward=1.30 +/- 0.64
Episode length: 95.40 +/- 14.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 95.4       |
|    mean_reward          | 1.3        |
| time/                   |            |
|    total_timesteps      | 465000     |
| train/                  |            |
|    approx_kl            | 0.07195963 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.787     |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.01       |
|    loss                 | 0.093      |
|    n_updates            | 1130       |
|    policy_gradient_loss | 0.000331   |
|    value_loss           | 0.342      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.34     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 114      |
|    time_elapsed    | 1591     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=470000, episode_reward=1.30 +/- 1.10
Episode length: 86.70 +/- 27.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.7        |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.046028495 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.828      |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.01        |
|    loss                 | 0.168       |
|    n_updates            | 1140        |
|    policy_gradient_loss | 0.00741     |
|    value_loss           | 0.299       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 3.08     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 115      |
|    time_elapsed    | 1605     |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=475000, episode_reward=1.60 +/- 1.02
Episode length: 93.40 +/- 19.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 93.4       |
|    mean_reward          | 1.6        |
| time/                   |            |
|    total_timesteps      | 475000     |
| train/                  |            |
|    approx_kl            | 0.02218426 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.805     |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.01       |
|    loss                 | 0.153      |
|    n_updates            | 1150       |
|    policy_gradient_loss | 0.0039     |
|    value_loss           | 0.349      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 116      |
|    time_elapsed    | 1619     |
|    total_timesteps | 475136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 3.05        |
| time/                   |             |
|    fps                  | 293         |
|    iterations           | 117         |
|    time_elapsed         | 1632        |
|    total_timesteps      | 479232      |
| train/                  |             |
|    approx_kl            | 0.027470056 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.01        |
|    loss                 | 0.207       |
|    n_updates            | 1160        |
|    policy_gradient_loss | 0.000132    |
|    value_loss           | 0.343       |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=1.20 +/- 0.75
Episode length: 78.70 +/- 17.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.7        |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.023250744 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0802      |
|    n_updates            | 1170        |
|    policy_gradient_loss | 0.00718     |
|    value_loss           | 0.287       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 293      |
|    iterations      | 118      |
|    time_elapsed    | 1646     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=485000, episode_reward=1.60 +/- 0.80
Episode length: 96.70 +/- 21.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.7        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 485000      |
| train/                  |             |
|    approx_kl            | 0.030895205 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.785      |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.01        |
|    loss                 | 0.125       |
|    n_updates            | 1180        |
|    policy_gradient_loss | 0.0112      |
|    value_loss           | 0.285       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.11     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 119      |
|    time_elapsed    | 1660     |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=490000, episode_reward=1.30 +/- 0.90
Episode length: 90.70 +/- 18.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.7        |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.032682016 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.781      |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.01        |
|    loss                 | 0.229       |
|    n_updates            | 1190        |
|    policy_gradient_loss | 0.00334     |
|    value_loss           | 0.342       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.24     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 120      |
|    time_elapsed    | 1675     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=495000, episode_reward=1.30 +/- 0.64
Episode length: 89.10 +/- 21.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.1        |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 495000      |
| train/                  |             |
|    approx_kl            | 0.030042753 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.808      |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.01        |
|    loss                 | 0.132       |
|    n_updates            | 1200        |
|    policy_gradient_loss | 0.00402     |
|    value_loss           | 0.374       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 3.45     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 121      |
|    time_elapsed    | 1689     |
|    total_timesteps | 495616   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | 3.47        |
| time/                   |             |
|    fps                  | 293         |
|    iterations           | 122         |
|    time_elapsed         | 1702        |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.024271764 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.01        |
|    loss                 | 0.191       |
|    n_updates            | 1210        |
|    policy_gradient_loss | 0.00538     |
|    value_loss           | 0.326       |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=2.60 +/- 0.92
Episode length: 111.80 +/- 14.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 2.6         |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.050033797 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.01        |
|    loss                 | 0.272       |
|    n_updates            | 1220        |
|    policy_gradient_loss | 0.00763     |
|    value_loss           | 0.329       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.26     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 123      |
|    time_elapsed    | 1717     |
|    total_timesteps | 503808   |
---------------------------------
Eval num_timesteps=505000, episode_reward=1.60 +/- 1.11
Episode length: 93.60 +/- 19.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.6        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 505000      |
| train/                  |             |
|    approx_kl            | 0.032508377 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.732      |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.01        |
|    loss                 | 0.137       |
|    n_updates            | 1230        |
|    policy_gradient_loss | 0.00568     |
|    value_loss           | 0.334       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.33     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 124      |
|    time_elapsed    | 1731     |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=510000, episode_reward=1.40 +/- 0.92
Episode length: 89.10 +/- 23.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.1        |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.027523197 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.01        |
|    loss                 | 0.159       |
|    n_updates            | 1240        |
|    policy_gradient_loss | 0.0058      |
|    value_loss           | 0.379       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.34     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 125      |
|    time_elapsed    | 1746     |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=515000, episode_reward=1.70 +/- 0.64
Episode length: 92.60 +/- 20.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.6        |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 515000      |
| train/                  |             |
|    approx_kl            | 0.028658405 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.741      |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.01        |
|    loss                 | 0.173       |
|    n_updates            | 1250        |
|    policy_gradient_loss | 0.00209     |
|    value_loss           | 0.328       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.51     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 126      |
|    time_elapsed    | 1760     |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=520000, episode_reward=1.80 +/- 1.17
Episode length: 96.80 +/- 22.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.8        |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 520000      |
| train/                  |             |
|    approx_kl            | 0.039597422 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.01        |
|    loss                 | 0.178       |
|    n_updates            | 1260        |
|    policy_gradient_loss | 0.00506     |
|    value_loss           | 0.296       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.3      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 127      |
|    time_elapsed    | 1774     |
|    total_timesteps | 520192   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 116       |
|    ep_rew_mean          | 3.43      |
| time/                   |           |
|    fps                  | 293       |
|    iterations           | 128       |
|    time_elapsed         | 1787      |
|    total_timesteps      | 524288    |
| train/                  |           |
|    approx_kl            | 0.0535392 |
|    clip_fraction        | 0.273     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.742    |
|    explained_variance   | 0.678     |
|    learning_rate        | 0.01      |
|    loss                 | 0.186     |
|    n_updates            | 1270      |
|    policy_gradient_loss | 0.00842   |
|    value_loss           | 0.326     |
---------------------------------------
Eval num_timesteps=525000, episode_reward=1.70 +/- 1.19
Episode length: 90.80 +/- 25.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.8        |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 525000      |
| train/                  |             |
|    approx_kl            | 0.036452815 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.801      |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.01        |
|    loss                 | 0.0976      |
|    n_updates            | 1280        |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 0.3         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.3      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 129      |
|    time_elapsed    | 1802     |
|    total_timesteps | 528384   |
---------------------------------
Eval num_timesteps=530000, episode_reward=2.00 +/- 1.00
Episode length: 97.30 +/- 16.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.3        |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 530000      |
| train/                  |             |
|    approx_kl            | 0.029177269 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.01        |
|    loss                 | 0.14        |
|    n_updates            | 1290        |
|    policy_gradient_loss | 0.00895     |
|    value_loss           | 0.318       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 130      |
|    time_elapsed    | 1816     |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=535000, episode_reward=1.40 +/- 1.02
Episode length: 86.60 +/- 21.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.6        |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 535000      |
| train/                  |             |
|    approx_kl            | 0.032153614 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.01        |
|    loss                 | 0.181       |
|    n_updates            | 1300        |
|    policy_gradient_loss | 0.00738     |
|    value_loss           | 0.318       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 131      |
|    time_elapsed    | 1830     |
|    total_timesteps | 536576   |
---------------------------------
Eval num_timesteps=540000, episode_reward=1.40 +/- 0.80
Episode length: 85.30 +/- 17.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 85.3       |
|    mean_reward          | 1.4        |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.03465125 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.01       |
|    loss                 | 0.126      |
|    n_updates            | 1310       |
|    policy_gradient_loss | 0.00366    |
|    value_loss           | 0.263      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.12     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 132      |
|    time_elapsed    | 1844     |
|    total_timesteps | 540672   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | 3.18        |
| time/                   |             |
|    fps                  | 293         |
|    iterations           | 133         |
|    time_elapsed         | 1857        |
|    total_timesteps      | 544768      |
| train/                  |             |
|    approx_kl            | 0.025821786 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.01        |
|    loss                 | 0.142       |
|    n_updates            | 1320        |
|    policy_gradient_loss | 0.00469     |
|    value_loss           | 0.325       |
-----------------------------------------
Eval num_timesteps=545000, episode_reward=0.90 +/- 0.70
Episode length: 77.30 +/- 16.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.3        |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 545000      |
| train/                  |             |
|    approx_kl            | 0.043778304 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.01        |
|    loss                 | 0.223       |
|    n_updates            | 1330        |
|    policy_gradient_loss | 0.00347     |
|    value_loss           | 0.347       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.37     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 134      |
|    time_elapsed    | 1871     |
|    total_timesteps | 548864   |
---------------------------------
Eval num_timesteps=550000, episode_reward=1.30 +/- 0.90
Episode length: 93.40 +/- 21.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.4        |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.031069478 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.786      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.01        |
|    loss                 | 0.107       |
|    n_updates            | 1340        |
|    policy_gradient_loss | 0.00962     |
|    value_loss           | 0.302       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.26     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 135      |
|    time_elapsed    | 1885     |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=555000, episode_reward=1.40 +/- 1.02
Episode length: 88.70 +/- 21.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.7        |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 555000      |
| train/                  |             |
|    approx_kl            | 0.030067395 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.808      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.01        |
|    loss                 | 0.119       |
|    n_updates            | 1350        |
|    policy_gradient_loss | 0.011       |
|    value_loss           | 0.284       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.25     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 136      |
|    time_elapsed    | 1899     |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=560000, episode_reward=1.60 +/- 1.02
Episode length: 95.60 +/- 20.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 95.6      |
|    mean_reward          | 1.6       |
| time/                   |           |
|    total_timesteps      | 560000    |
| train/                  |           |
|    approx_kl            | 0.0376663 |
|    clip_fraction        | 0.257     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.784    |
|    explained_variance   | 0.656     |
|    learning_rate        | 0.01      |
|    loss                 | 0.138     |
|    n_updates            | 1360      |
|    policy_gradient_loss | 0.00743   |
|    value_loss           | 0.335     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.1      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 137      |
|    time_elapsed    | 1914     |
|    total_timesteps | 561152   |
---------------------------------
Eval num_timesteps=565000, episode_reward=1.50 +/- 0.81
Episode length: 85.50 +/- 18.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85.5        |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 565000      |
| train/                  |             |
|    approx_kl            | 0.033766977 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.01        |
|    loss                 | 0.159       |
|    n_updates            | 1370        |
|    policy_gradient_loss | 0.00552     |
|    value_loss           | 0.344       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 138      |
|    time_elapsed    | 1928     |
|    total_timesteps | 565248   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 2.94        |
| time/                   |             |
|    fps                  | 293         |
|    iterations           | 139         |
|    time_elapsed         | 1941        |
|    total_timesteps      | 569344      |
| train/                  |             |
|    approx_kl            | 0.033093352 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.838      |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.01        |
|    loss                 | 0.137       |
|    n_updates            | 1380        |
|    policy_gradient_loss | 0.00378     |
|    value_loss           | 0.232       |
-----------------------------------------
Eval num_timesteps=570000, episode_reward=0.80 +/- 0.75
Episode length: 78.20 +/- 24.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.2        |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 570000      |
| train/                  |             |
|    approx_kl            | 0.025108323 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.01        |
|    loss                 | 0.109       |
|    n_updates            | 1390        |
|    policy_gradient_loss | 0.00422     |
|    value_loss           | 0.292       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 140      |
|    time_elapsed    | 1955     |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=575000, episode_reward=1.20 +/- 0.60
Episode length: 85.70 +/- 18.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 85.7       |
|    mean_reward          | 1.2        |
| time/                   |            |
|    total_timesteps      | 575000     |
| train/                  |            |
|    approx_kl            | 0.03360328 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.01       |
|    loss                 | 0.13       |
|    n_updates            | 1400       |
|    policy_gradient_loss | 0.00933    |
|    value_loss           | 0.305      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 141      |
|    time_elapsed    | 1969     |
|    total_timesteps | 577536   |
---------------------------------
Eval num_timesteps=580000, episode_reward=1.30 +/- 0.78
Episode length: 89.80 +/- 21.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 89.8      |
|    mean_reward          | 1.3       |
| time/                   |           |
|    total_timesteps      | 580000    |
| train/                  |           |
|    approx_kl            | 0.0283231 |
|    clip_fraction        | 0.224     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.747    |
|    explained_variance   | 0.662     |
|    learning_rate        | 0.01      |
|    loss                 | 0.189     |
|    n_updates            | 1410      |
|    policy_gradient_loss | 0.00603   |
|    value_loss           | 0.358     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.43     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 142      |
|    time_elapsed    | 1984     |
|    total_timesteps | 581632   |
---------------------------------
Eval num_timesteps=585000, episode_reward=2.20 +/- 0.75
Episode length: 101.70 +/- 13.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 585000      |
| train/                  |             |
|    approx_kl            | 0.020252405 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.01        |
|    loss                 | 0.118       |
|    n_updates            | 1420        |
|    policy_gradient_loss | 0.00586     |
|    value_loss           | 0.364       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.43     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 143      |
|    time_elapsed    | 1998     |
|    total_timesteps | 585728   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 3.67        |
| time/                   |             |
|    fps                  | 293         |
|    iterations           | 144         |
|    time_elapsed         | 2011        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.026236443 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.01        |
|    loss                 | 0.248       |
|    n_updates            | 1430        |
|    policy_gradient_loss | 0.00702     |
|    value_loss           | 0.366       |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=1.30 +/- 0.78
Episode length: 82.70 +/- 15.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.7        |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.021756548 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.757      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.01        |
|    loss                 | 0.171       |
|    n_updates            | 1440        |
|    policy_gradient_loss | 0.00403     |
|    value_loss           | 0.348       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.59     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 145      |
|    time_elapsed    | 2025     |
|    total_timesteps | 593920   |
---------------------------------
Eval num_timesteps=595000, episode_reward=0.80 +/- 0.60
Episode length: 81.70 +/- 16.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.7        |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 595000      |
| train/                  |             |
|    approx_kl            | 0.023646226 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.782      |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.01        |
|    loss                 | 0.125       |
|    n_updates            | 1450        |
|    policy_gradient_loss | 0.00416     |
|    value_loss           | 0.332       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 3.68     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 146      |
|    time_elapsed    | 2039     |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=600000, episode_reward=1.30 +/- 0.78
Episode length: 94.40 +/- 20.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 94.4       |
|    mean_reward          | 1.3        |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.02670047 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.01       |
|    loss                 | 0.155      |
|    n_updates            | 1460       |
|    policy_gradient_loss | 0.00584    |
|    value_loss           | 0.331      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 3.69     |
| time/              |          |
|    fps             | 293      |
|    iterations      | 147      |
|    time_elapsed    | 2053     |
|    total_timesteps | 602112   |
---------------------------------
