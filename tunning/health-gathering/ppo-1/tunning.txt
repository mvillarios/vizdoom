[I 2024-07-20 18:09:27,096] A new study created in memory with name: no-name-8c9ffb6d-0968-4bfa-bed9-99d2b962a3de
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=5000, episode_reward=290.40 +/- 19.20
Episode length: 97.60 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 1003     |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=300.00 +/- 29.50
Episode length: 100.00 +/- 7.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 300          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0042450046 |
|    clip_fraction        | 0.0507       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | 0.00431      |
|    learning_rate        | 3.77e-05     |
|    loss                 | 99.3         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 305          |
------------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=312.80 +/- 33.41
Episode length: 103.20 +/- 8.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 899      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.008620843 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.175       |
|    learning_rate        | 3.77e-05    |
|    loss                 | 139         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000573   |
|    value_loss           | 332         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 378      |
| time/              |          |
|    fps             | 918      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=354.40 +/- 90.28
Episode length: 113.60 +/- 22.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 114        |
|    mean_reward          | 354        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.00416184 |
|    clip_fraction        | 0.053      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.219      |
|    learning_rate        | 3.77e-05   |
|    loss                 | 222        |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.000456   |
|    value_loss           | 412        |
----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=312.80 +/- 43.99
Episode length: 103.20 +/- 11.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 909      |
|    iterations      | 4        |
|    time_elapsed    | 36       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=287.20 +/- 9.60
Episode length: 96.80 +/- 2.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.8       |
|    mean_reward          | 287        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.00940115 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.465      |
|    learning_rate        | 3.77e-05   |
|    loss                 | 268        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00281   |
|    value_loss           | 373        |
----------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 907      |
|    iterations      | 5        |
|    time_elapsed    | 45       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0055045257 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | 0.459        |
|    learning_rate        | 3.77e-05     |
|    loss                 | 249          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.000281    |
|    value_loss           | 433          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 392      |
| time/              |          |
|    fps             | 916      |
|    iterations      | 6        |
|    time_elapsed    | 53       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010965256 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.478       |
|    learning_rate        | 3.77e-05    |
|    loss                 | 107         |
|    n_updates            | 60          |
|    policy_gradient_loss | 9.01e-05    |
|    value_loss           | 467         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 914      |
|    iterations      | 7        |
|    time_elapsed    | 62       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.007316103 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.521       |
|    learning_rate        | 3.77e-05    |
|    loss                 | 284         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00423    |
|    value_loss           | 447         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 428      |
| time/              |          |
|    fps             | 912      |
|    iterations      | 8        |
|    time_elapsed    | 71       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:10:42,053] Trial 0 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 3.765960758262901e-05, 'gamma': 0.9707922325832945, 'gae_lambda': 0.8391719779365188}. Best is trial 0 with value: 284.0.
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=5000, episode_reward=1173.60 +/- 781.01
Episode length: 308.40 +/- 183.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 308      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 979      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=364.00 +/- 61.14
Episode length: 116.00 +/- 15.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 364         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011300176 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.00173    |
|    learning_rate        | 0.000121    |
|    loss                 | 113         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.000316    |
|    value_loss           | 283         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=444.00 +/- 207.88
Episode length: 136.00 +/- 51.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 136      |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 868      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010447415 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.000121    |
|    loss                 | 265         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000314   |
|    value_loss           | 343         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 864      |
|    iterations      | 3        |
|    time_elapsed    | 28       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010716416 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.000121    |
|    loss                 | 560         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.000123    |
|    value_loss           | 402         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 424      |
| time/              |          |
|    fps             | 847      |
|    iterations      | 4        |
|    time_elapsed    | 38       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.4        |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012894433 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.000121    |
|    loss                 | 242         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 418         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=287.20 +/- 9.60
Episode length: 96.80 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 444      |
| time/              |          |
|    fps             | 838      |
|    iterations      | 5        |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=303.20 +/- 38.40
Episode length: 100.80 +/- 9.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 303         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.021632062 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.395       |
|    learning_rate        | 0.000121    |
|    loss                 | 54.3        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00056    |
|    value_loss           | 461         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 839      |
|    iterations      | 6        |
|    time_elapsed    | 58       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.014084349 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.000121    |
|    loss                 | 77.7        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00334    |
|    value_loss           | 470         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 383      |
| time/              |          |
|    fps             | 833      |
|    iterations      | 7        |
|    time_elapsed    | 68       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.017591037 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.263       |
|    learning_rate        | 0.000121    |
|    loss                 | 119         |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.00136     |
|    value_loss           | 652         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 830      |
|    iterations      | 8        |
|    time_elapsed    | 78       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:12:04,308] Trial 1 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.00012144120662744953, 'gamma': 0.9755920090224826, 'gae_lambda': 0.8428470704829933}. Best is trial 0 with value: 284.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=418.40 +/- 135.61
Episode length: 129.60 +/- 33.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 1131     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014761915 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.00227    |
|    learning_rate        | 0.000272    |
|    loss                 | 96          |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0017      |
|    value_loss           | 451         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 833      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.0185683 |
|    clip_fraction        | 0.303     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.35     |
|    explained_variance   | 0.177     |
|    learning_rate        | 0.000272  |
|    loss                 | 116       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.00245   |
|    value_loss           | 506       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 784      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.023338947 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.000272    |
|    loss                 | 130         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.00499     |
|    value_loss           | 510         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 750      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.022357913 |
|    clip_fraction        | 0.337       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.000272    |
|    loss                 | 134         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00267     |
|    value_loss           | 425         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 730      |
|    iterations      | 5        |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.034421194 |
|    clip_fraction        | 0.368       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.351       |
|    learning_rate        | 0.000272    |
|    loss                 | 382         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00534     |
|    value_loss           | 494         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 415      |
| time/              |          |
|    fps             | 724      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.03026283 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 0.44       |
|    learning_rate        | 0.000272   |
|    loss                 | 451        |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.00106    |
|    value_loss           | 476        |
----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 393      |
| time/              |          |
|    fps             | 715      |
|    iterations      | 7        |
|    time_elapsed    | 80       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=319.20 +/- 48.42
Episode length: 104.80 +/- 12.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 105         |
|    mean_reward          | 319         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.031112235 |
|    clip_fraction        | 0.398       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.000272    |
|    loss                 | 145         |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.00666     |
|    value_loss           | 490         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=300.00 +/- 25.80
Episode length: 100.00 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 409      |
| time/              |          |
|    fps             | 707      |
|    iterations      | 8        |
|    time_elapsed    | 92       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:13:42,581] Trial 2 finished with value: 396.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.00027178944684330957, 'gamma': 0.9672232013190494, 'gae_lambda': 0.9217095506446301}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 1273     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.006444578 |
|    clip_fraction        | 0.0492      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.000623    |
|    learning_rate        | 0.000119    |
|    loss                 | 102         |
|    n_updates            | 10          |
|    policy_gradient_loss | -4.67e-05   |
|    value_loss           | 247         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 1057     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=895.20 +/- 513.89
Episode length: 248.80 +/- 128.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 895          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0066296244 |
|    clip_fraction        | 0.0959       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | 0.11         |
|    learning_rate        | 0.000119     |
|    loss                 | 122          |
|    n_updates            | 20           |
|    policy_gradient_loss | 5.51e-05     |
|    value_loss           | 265          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 926      |
|    iterations      | 3        |
|    time_elapsed    | 13       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=287.20 +/- 9.60
Episode length: 96.80 +/- 2.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.8        |
|    mean_reward          | 287         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.005068233 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.000119    |
|    loss                 | 216         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00368    |
|    value_loss           | 275         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 918      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=287.20 +/- 9.60
Episode length: 96.80 +/- 2.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.8        |
|    mean_reward          | 287         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.009273458 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.000119    |
|    loss                 | 89.3        |
|    n_updates            | 40          |
|    policy_gradient_loss | 4.85e-05    |
|    value_loss           | 286         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 404      |
| time/              |          |
|    fps             | 913      |
|    iterations      | 5        |
|    time_elapsed    | 22       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 125         |
|    ep_rew_mean          | 399         |
| time/                   |             |
|    fps                  | 933         |
|    iterations           | 6           |
|    time_elapsed         | 26          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014517146 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.000119    |
|    loss                 | 239         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0047     |
|    value_loss           | 333         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.4        |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009707722 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.243       |
|    learning_rate        | 0.000119    |
|    loss                 | 171         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.000715   |
|    value_loss           | 331         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 386      |
| time/              |          |
|    fps             | 927      |
|    iterations      | 7        |
|    time_elapsed    | 30       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.007764304 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000119    |
|    loss                 | 276         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 272         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 924      |
|    iterations      | 8        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.018093027 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.000119    |
|    loss                 | 249         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.000977   |
|    value_loss           | 337         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 921      |
|    iterations      | 9        |
|    time_elapsed    | 40       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.00872001 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.375      |
|    learning_rate        | 0.000119   |
|    loss                 | 197        |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.000441  |
|    value_loss           | 284        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 917      |
|    iterations      | 10       |
|    time_elapsed    | 44       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=805.60 +/- 335.94
Episode length: 226.40 +/- 83.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | 806          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0071483664 |
|    clip_fraction        | 0.159        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.000119     |
|    loss                 | 90.9         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.000702    |
|    value_loss           | 293          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 900      |
|    iterations      | 11       |
|    time_elapsed    | 50       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 128          |
|    ep_rew_mean          | 414          |
| time/                   |              |
|    fps                  | 910          |
|    iterations           | 12           |
|    time_elapsed         | 54           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0061631612 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.321        |
|    learning_rate        | 0.000119     |
|    loss                 | 184          |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 278          |
------------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009250109 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.000119    |
|    loss                 | 183         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00198    |
|    value_loss           | 325         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 420      |
| time/              |          |
|    fps             | 908      |
|    iterations      | 13       |
|    time_elapsed    | 58       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.015495498 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.000119    |
|    loss                 | 201         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00603    |
|    value_loss           | 281         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 416      |
| time/              |          |
|    fps             | 907      |
|    iterations      | 14       |
|    time_elapsed    | 63       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.008285483 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.251       |
|    learning_rate        | 0.000119    |
|    loss                 | 107         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 330         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 404      |
| time/              |          |
|    fps             | 906      |
|    iterations      | 15       |
|    time_elapsed    | 67       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:14:51,923] Trial 3 finished with value: 284.0 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.00011888721250843753, 'gamma': 0.9296397357505212, 'gae_lambda': 0.835033763870552}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 1256     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 0.1147138 |
|    clip_fraction        | 0.758     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.05     |
|    explained_variance   | 0.000852  |
|    learning_rate        | 0.00286   |
|    loss                 | 227       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.136     |
|    value_loss           | 807       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 383      |
| time/              |          |
|    fps             | 1042     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 2.4132302 |
|    clip_fraction        | 0.892     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.511    |
|    explained_variance   | 0.0579    |
|    learning_rate        | 0.00286   |
|    loss                 | 409       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.295     |
|    value_loss           | 1.39e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 986      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 10.029813 |
|    clip_fraction        | 0.965     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.145    |
|    explained_variance   | 0.0177    |
|    learning_rate        | 0.00286   |
|    loss                 | 656       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.333     |
|    value_loss           | 1.9e+03   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 300      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.6019174 |
|    clip_fraction        | 0.0712    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0623   |
|    explained_variance   | 0.135     |
|    learning_rate        | 0.00286   |
|    loss                 | 1.4e+03   |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0158   |
|    value_loss           | 2.61e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 946      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 96         |
|    ep_rew_mean          | 284        |
| time/                   |            |
|    fps                  | 959        |
|    iterations           | 6          |
|    time_elapsed         | 25         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.97914094 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0739    |
|    explained_variance   | 0.276      |
|    learning_rate        | 0.00286    |
|    loss                 | 865        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0069    |
|    value_loss           | 3.04e+03   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 25000    |
| train/                  |          |
|    approx_kl            | 2.2217   |
|    clip_fraction        | 0.0706   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0122  |
|    explained_variance   | 0.203    |
|    learning_rate        | 0.00286  |
|    loss                 | 1.09e+03 |
|    n_updates            | 60       |
|    policy_gradient_loss | 0.04     |
|    value_loss           | 2.51e+03 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 949      |
|    iterations      | 7        |
|    time_elapsed    | 30       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 30000         |
| train/                  |               |
|    approx_kl            | 1.9252984e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.71e-05     |
|    explained_variance   | 0.513         |
|    learning_rate        | 0.00286       |
|    loss                 | 943           |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.000417     |
|    value_loss           | 2.44e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 942      |
|    iterations      | 8        |
|    time_elapsed    | 34       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0029521182 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000309    |
|    explained_variance   | 0.649        |
|    learning_rate        | 0.00286      |
|    loss                 | 1.38e+03     |
|    n_updates            | 80           |
|    policy_gradient_loss | 0.000603     |
|    value_loss           | 3.2e+03      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 937      |
|    iterations      | 9        |
|    time_elapsed    | 39       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.002412238 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00131    |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.00286     |
|    loss                 | 1.49e+03    |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.000367    |
|    value_loss           | 2.31e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 932      |
|    iterations      | 10       |
|    time_elapsed    | 43       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.002859146 |
|    clip_fraction        | 0.00264     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00344    |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.00286     |
|    loss                 | 1.37e+03    |
|    n_updates            | 100         |
|    policy_gradient_loss | -6.61e-05   |
|    value_loss           | 2.88e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 929      |
|    iterations      | 11       |
|    time_elapsed    | 48       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 937       |
|    iterations           | 12        |
|    time_elapsed         | 52        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.4839035 |
|    clip_fraction        | 0.0548    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0226   |
|    explained_variance   | 0.544     |
|    learning_rate        | 0.00286   |
|    loss                 | 848       |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.011    |
|    value_loss           | 2.46e+03  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.82075363 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0853    |
|    explained_variance   | 0.386      |
|    learning_rate        | 0.00286    |
|    loss                 | 882        |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0148    |
|    value_loss           | 2.06e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 933      |
|    iterations      | 13       |
|    time_elapsed    | 57       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.75378686 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0735    |
|    explained_variance   | 0.371      |
|    learning_rate        | 0.00286    |
|    loss                 | 606        |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 2.03e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 930      |
|    iterations      | 14       |
|    time_elapsed    | 61       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.59214807 |
|    clip_fraction        | 0.0797     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0229    |
|    explained_variance   | 0.371      |
|    learning_rate        | 0.00286    |
|    loss                 | 889        |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.0124     |
|    value_loss           | 2.24e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 927      |
|    iterations      | 15       |
|    time_elapsed    | 66       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:15:59,731] Trial 4 finished with value: 284.0 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.002863087984739037, 'gamma': 0.9972541003884989, 'gae_lambda': 0.9285093225775145}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 372      |
| time/              |          |
|    fps             | 1273     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=309.60 +/- 31.35
Episode length: 102.40 +/- 7.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010359235 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.00536    |
|    learning_rate        | 9.62e-05    |
|    loss                 | 167         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 308         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 1055     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009803417 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.237       |
|    learning_rate        | 9.62e-05    |
|    loss                 | 99          |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000426   |
|    value_loss           | 312         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 384      |
| time/              |          |
|    fps             | 998      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.008803997 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.32        |
|    learning_rate        | 9.62e-05    |
|    loss                 | 192         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.000284   |
|    value_loss           | 395         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 384      |
| time/              |          |
|    fps             | 973      |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010937016 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.445       |
|    learning_rate        | 9.62e-05    |
|    loss                 | 98.8        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00234    |
|    value_loss           | 454         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 389      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 124         |
|    ep_rew_mean          | 395         |
| time/                   |             |
|    fps                  | 971         |
|    iterations           | 6           |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009353107 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.566       |
|    learning_rate        | 9.62e-05    |
|    loss                 | 220         |
|    n_updates            | 50          |
|    policy_gradient_loss | 7e-05       |
|    value_loss           | 448         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=296.80 +/- 29.33
Episode length: 99.20 +/- 7.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.2        |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015281895 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.495       |
|    learning_rate        | 9.62e-05    |
|    loss                 | 209         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.000655   |
|    value_loss           | 551         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 420      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 7        |
|    time_elapsed    | 29       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=623.20 +/- 331.69
Episode length: 180.80 +/- 82.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 181         |
|    mean_reward          | 623         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.016820624 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.6         |
|    learning_rate        | 9.62e-05    |
|    loss                 | 357         |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.000556    |
|    value_loss           | 548         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 441      |
| time/              |          |
|    fps             | 937      |
|    iterations      | 8        |
|    time_elapsed    | 34       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012373916 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.626       |
|    learning_rate        | 9.62e-05    |
|    loss                 | 298         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 593         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 443      |
| time/              |          |
|    fps             | 933      |
|    iterations      | 9        |
|    time_elapsed    | 39       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.01446236 |
|    clip_fraction        | 0.227      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.373      |
|    learning_rate        | 9.62e-05   |
|    loss                 | 246        |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0011     |
|    value_loss           | 904        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 438      |
| time/              |          |
|    fps             | 929      |
|    iterations      | 10       |
|    time_elapsed    | 44       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.021435818 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.385       |
|    learning_rate        | 9.62e-05    |
|    loss                 | 295         |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.00569     |
|    value_loss           | 961         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 926      |
|    iterations      | 11       |
|    time_elapsed    | 48       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 130         |
|    ep_rew_mean          | 420         |
| time/                   |             |
|    fps                  | 934         |
|    iterations           | 12          |
|    time_elapsed         | 52          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.033440165 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.394       |
|    learning_rate        | 9.62e-05    |
|    loss                 | 563         |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00622     |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=290.40 +/- 19.20
Episode length: 97.60 +/- 4.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.6        |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.013137633 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.347       |
|    learning_rate        | 9.62e-05    |
|    loss                 | 771         |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.00254     |
|    value_loss           | 1.15e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 423      |
| time/              |          |
|    fps             | 931      |
|    iterations      | 13       |
|    time_elapsed    | 57       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.016437398 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.404       |
|    learning_rate        | 9.62e-05    |
|    loss                 | 411         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.000593   |
|    value_loss           | 1.06e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 445      |
| time/              |          |
|    fps             | 929      |
|    iterations      | 14       |
|    time_elapsed    | 61       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.024038868 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.465       |
|    learning_rate        | 9.62e-05    |
|    loss                 | 213         |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.00454     |
|    value_loss           | 1.02e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 448      |
| time/              |          |
|    fps             | 927      |
|    iterations      | 15       |
|    time_elapsed    | 66       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:17:07,570] Trial 5 finished with value: 284.0 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 9.621892409121046e-05, 'gamma': 0.9940625740712554, 'gae_lambda': 0.8158986722714408}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 391      |
| time/              |          |
|    fps             | 1159     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0128455125 |
|    clip_fraction        | 0.408        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.00198      |
|    learning_rate        | 0.000851     |
|    loss                 | 77.6         |
|    n_updates            | 10           |
|    policy_gradient_loss | 0.0151       |
|    value_loss           | 427          |
------------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 993      |
|    iterations      | 2        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011013471 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.12        |
|    learning_rate        | 0.000851    |
|    loss                 | 155         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00718     |
|    value_loss           | 498         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 980      |
|    iterations      | 3        |
|    time_elapsed    | 25       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.013591332 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.193       |
|    learning_rate        | 0.000851    |
|    loss                 | 79.9        |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.00549     |
|    value_loss           | 395         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 4        |
|    time_elapsed    | 34       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.02183348 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000851   |
|    loss                 | 54.3       |
|    n_updates            | 40         |
|    policy_gradient_loss | 0.00796    |
|    value_loss           | 350        |
----------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 945      |
|    iterations      | 5        |
|    time_elapsed    | 43       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.029635478 |
|    clip_fraction        | 0.38        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.000851    |
|    loss                 | 58.8        |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00352     |
|    value_loss           | 330         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 436      |
| time/              |          |
|    fps             | 949      |
|    iterations      | 6        |
|    time_elapsed    | 51       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.032558694 |
|    clip_fraction        | 0.382       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.000851    |
|    loss                 | 109         |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00987     |
|    value_loss           | 348         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 941      |
|    iterations      | 7        |
|    time_elapsed    | 60       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0218442 |
|    clip_fraction        | 0.343     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.24     |
|    explained_variance   | 0.0539    |
|    learning_rate        | 0.000851  |
|    loss                 | 53.4      |
|    n_updates            | 70        |
|    policy_gradient_loss | 0.00417   |
|    value_loss           | 483       |
---------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 935      |
|    iterations      | 8        |
|    time_elapsed    | 70       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:18:19,848] Trial 6 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 0.0008507421888397443, 'gamma': 0.9496469688701371, 'gae_lambda': 0.9276949843643121}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 372      |
| time/              |          |
|    fps             | 1275     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.013372456 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.000987    |
|    learning_rate        | 0.000195    |
|    loss                 | 202         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 305         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 1055     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012196187 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.105       |
|    learning_rate        | 0.000195    |
|    loss                 | 223         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000311   |
|    value_loss           | 369         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 998      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0070952447 |
|    clip_fraction        | 0.214        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.243        |
|    learning_rate        | 0.000195     |
|    loss                 | 157          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00226     |
|    value_loss           | 348          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 371      |
| time/              |          |
|    fps             | 970      |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012457975 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.000195    |
|    loss                 | 165         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.000441    |
|    value_loss           | 338         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 954      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 351         |
| time/                   |             |
|    fps                  | 966         |
|    iterations           | 6           |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.015264539 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.195       |
|    learning_rate        | 0.000195    |
|    loss                 | 151         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.000472   |
|    value_loss           | 386         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009652918 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.0929      |
|    learning_rate        | 0.000195    |
|    loss                 | 213         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 400         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 360      |
| time/              |          |
|    fps             | 955      |
|    iterations      | 7        |
|    time_elapsed    | 30       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.013283327 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.000195    |
|    loss                 | 182         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 313         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 384      |
| time/              |          |
|    fps             | 948      |
|    iterations      | 8        |
|    time_elapsed    | 34       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.011007852 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.000195    |
|    loss                 | 144         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 288         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 943      |
|    iterations      | 9        |
|    time_elapsed    | 39       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.010848968 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.000195    |
|    loss                 | 158         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00219    |
|    value_loss           | 308         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 938      |
|    iterations      | 10       |
|    time_elapsed    | 43       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.013246902 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.000195    |
|    loss                 | 175         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.002      |
|    value_loss           | 328         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 934      |
|    iterations      | 11       |
|    time_elapsed    | 48       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 117        |
|    ep_rew_mean          | 367        |
| time/                   |            |
|    fps                  | 942        |
|    iterations           | 12         |
|    time_elapsed         | 52         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.02074482 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.175      |
|    learning_rate        | 0.000195   |
|    loss                 | 167        |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00409   |
|    value_loss           | 392        |
----------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.014774429 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.198       |
|    learning_rate        | 0.000195    |
|    loss                 | 106         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 395         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 348      |
| time/              |          |
|    fps             | 938      |
|    iterations      | 13       |
|    time_elapsed    | 56       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.010910255 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.204       |
|    learning_rate        | 0.000195    |
|    loss                 | 278         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00532    |
|    value_loss           | 400         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 339      |
| time/              |          |
|    fps             | 935      |
|    iterations      | 14       |
|    time_elapsed    | 61       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.020517407 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.972      |
|    explained_variance   | 0.14        |
|    learning_rate        | 0.000195    |
|    loss                 | 153         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00259    |
|    value_loss           | 453         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 321      |
| time/              |          |
|    fps             | 932      |
|    iterations      | 15       |
|    time_elapsed    | 65       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:19:27,280] Trial 7 finished with value: 284.0 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.00019524907282518099, 'gamma': 0.9231102430819347, 'gae_lambda': 0.891548291883455}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 408      |
| time/              |          |
|    fps             | 1276     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=319.20 +/- 39.06
Episode length: 104.80 +/- 9.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 105         |
|    mean_reward          | 319         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009787949 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.00143    |
|    learning_rate        | 0.000202    |
|    loss                 | 177         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.000304   |
|    value_loss           | 355         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 1053     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014376173 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.000202    |
|    loss                 | 110         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00127    |
|    value_loss           | 391         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 371      |
| time/              |          |
|    fps             | 997      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=367.20 +/- 83.69
Episode length: 116.80 +/- 20.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 117          |
|    mean_reward          | 367          |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0070363972 |
|    clip_fraction        | 0.159        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | 0.296        |
|    learning_rate        | 0.000202     |
|    loss                 | 256          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.000146    |
|    value_loss           | 387          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 964      |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012682605 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000202    |
|    loss                 | 175         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.000388   |
|    value_loss           | 443         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 397      |
| time/              |          |
|    fps             | 952      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 135         |
|    ep_rew_mean          | 441         |
| time/                   |             |
|    fps                  | 966         |
|    iterations           | 6           |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.008791676 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.187       |
|    learning_rate        | 0.000202    |
|    loss                 | 151         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00205     |
|    value_loss           | 430         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=1392.80 +/- 533.73
Episode length: 368.20 +/- 127.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 368         |
|    mean_reward          | 1.39e+03    |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.014876366 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.000202    |
|    loss                 | 145         |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00142     |
|    value_loss           | 440         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 491      |
| time/              |          |
|    fps             | 904      |
|    iterations      | 7        |
|    time_elapsed    | 31       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=1264.00 +/- 634.28
Episode length: 333.50 +/- 148.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 334         |
|    mean_reward          | 1.26e+03    |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.010828183 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.189       |
|    learning_rate        | 0.000202    |
|    loss                 | 143         |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.000429    |
|    value_loss           | 387         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 565      |
| time/              |          |
|    fps             | 867      |
|    iterations      | 8        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=1108.80 +/- 626.91
Episode length: 299.70 +/- 152.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 300        |
|    mean_reward          | 1.11e+03   |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.01937525 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.0756     |
|    learning_rate        | 0.000202   |
|    loss                 | 128        |
|    n_updates            | 80         |
|    policy_gradient_loss | 0.00655    |
|    value_loss           | 287        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 619      |
| time/              |          |
|    fps             | 844      |
|    iterations      | 9        |
|    time_elapsed    | 43       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.027995678 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.0486      |
|    learning_rate        | 0.000202    |
|    loss                 | 170         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00305     |
|    value_loss           | 317         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 653      |
| time/              |          |
|    fps             | 849      |
|    iterations      | 10       |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.010627402 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | -0.00751    |
|    learning_rate        | 0.000202    |
|    loss                 | 361         |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.00902     |
|    value_loss           | 535         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 654      |
| time/              |          |
|    fps             | 853      |
|    iterations      | 11       |
|    time_elapsed    | 52       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 181         |
|    ep_rew_mean          | 627         |
| time/                   |             |
|    fps                  | 866         |
|    iterations           | 12          |
|    time_elapsed         | 56          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.009550841 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.0148      |
|    learning_rate        | 0.000202    |
|    loss                 | 250         |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00236     |
|    value_loss           | 476         |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0118878065 |
|    clip_fraction        | 0.171        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.0307       |
|    learning_rate        | 0.000202     |
|    loss                 | 321          |
|    n_updates            | 120          |
|    policy_gradient_loss | 0.00157      |
|    value_loss           | 522          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 493      |
| time/              |          |
|    fps             | 869      |
|    iterations      | 13       |
|    time_elapsed    | 61       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.010855382 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.0571      |
|    learning_rate        | 0.000202    |
|    loss                 | 452         |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.00313     |
|    value_loss           | 640         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 448      |
| time/              |          |
|    fps             | 871      |
|    iterations      | 14       |
|    time_elapsed    | 65       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.00891187 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.0448     |
|    learning_rate        | 0.000202   |
|    loss                 | 342        |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.00369    |
|    value_loss           | 633        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 873      |
|    iterations      | 15       |
|    time_elapsed    | 70       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:20:39,179] Trial 8 finished with value: 284.0 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.00020248084698070218, 'gamma': 0.9466677150835181, 'gae_lambda': 0.8895957147175887}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 1268     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 117        |
|    ep_rew_mean          | 367        |
| time/                   |            |
|    fps                  | 1144       |
|    iterations           | 2          |
|    time_elapsed         | 3          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.00580089 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0.00067    |
|    learning_rate        | 0.000261   |
|    loss                 | 89.5       |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00235   |
|    value_loss           | 237        |
----------------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.00920401 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.156      |
|    learning_rate        | 0.000261   |
|    loss                 | 118        |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.000509   |
|    value_loss           | 271        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 996      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 121         |
|    ep_rew_mean          | 383         |
| time/                   |             |
|    fps                  | 1009        |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.009877594 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.223       |
|    learning_rate        | 0.000261    |
|    loss                 | 150         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00127    |
|    value_loss           | 278         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=472.80 +/- 149.03
Episode length: 143.20 +/- 37.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 143        |
|    mean_reward          | 473        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01032907 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.385      |
|    learning_rate        | 0.000261   |
|    loss                 | 219        |
|    n_updates            | 40         |
|    policy_gradient_loss | -3.93e-05  |
|    value_loss           | 284        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 391      |
| time/              |          |
|    fps             | 931      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 127         |
|    ep_rew_mean          | 410         |
| time/                   |             |
|    fps                  | 948         |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.027046423 |
|    clip_fraction        | 0.376       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.000261    |
|    loss                 | 103         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00209     |
|    value_loss           | 314         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 133         |
|    ep_rew_mean          | 434         |
| time/                   |             |
|    fps                  | 962         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.015355175 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.155       |
|    learning_rate        | 0.000261    |
|    loss                 | 115         |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00658     |
|    value_loss           | 310         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=546.40 +/- 339.20
Episode length: 161.60 +/- 84.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 162        |
|    mean_reward          | 546        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.03295694 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.023      |
|    learning_rate        | 0.000261   |
|    loss                 | 185        |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.0147     |
|    value_loss           | 255        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 462      |
| time/              |          |
|    fps             | 914      |
|    iterations      | 8        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | 517         |
| time/                   |             |
|    fps                  | 926         |
|    iterations           | 9           |
|    time_elapsed         | 19          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.013030758 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.064       |
|    learning_rate        | 0.000261    |
|    loss                 | 110         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00283     |
|    value_loss           | 347         |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=1336.80 +/- 652.84
Episode length: 349.20 +/- 151.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 349        |
|    mean_reward          | 1.34e+03   |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.36136812 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.0378     |
|    learning_rate        | 0.000261   |
|    loss                 | 9.39       |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0305     |
|    value_loss           | 151        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 534      |
| time/              |          |
|    fps             | 847      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 168         |
|    ep_rew_mean          | 574         |
| time/                   |             |
|    fps                  | 861         |
|    iterations           | 11          |
|    time_elapsed         | 26          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.019962236 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.0682      |
|    learning_rate        | 0.000261    |
|    loss                 | 162         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00397    |
|    value_loss           | 338         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 179         |
|    ep_rew_mean          | 622         |
| time/                   |             |
|    fps                  | 873         |
|    iterations           | 12          |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.019560758 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.213       |
|    learning_rate        | 0.000261    |
|    loss                 | 138         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 239         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.13415822 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.094      |
|    learning_rate        | 0.000261   |
|    loss                 | 58.5       |
|    n_updates            | 120        |
|    policy_gradient_loss | 0.0202     |
|    value_loss           | 159        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 646      |
| time/              |          |
|    fps             | 865      |
|    iterations      | 13       |
|    time_elapsed    | 30       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | 661         |
| time/                   |             |
|    fps                  | 875         |
|    iterations           | 14          |
|    time_elapsed         | 32          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.022936925 |
|    clip_fraction        | 0.423       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.0299      |
|    learning_rate        | 0.000261    |
|    loss                 | 346         |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.0129      |
|    value_loss           | 349         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=296.80 +/- 29.33
Episode length: 99.20 +/- 7.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.2        |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.010983503 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.0522      |
|    learning_rate        | 0.000261    |
|    loss                 | 130         |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.00453     |
|    value_loss           | 351         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 672      |
| time/              |          |
|    fps             | 869      |
|    iterations      | 15       |
|    time_elapsed    | 35       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | 632         |
| time/                   |             |
|    fps                  | 878         |
|    iterations           | 16          |
|    time_elapsed         | 37          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.024497584 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.119       |
|    learning_rate        | 0.000261    |
|    loss                 | 235         |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.0134      |
|    value_loss           | 358         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 169         |
|    ep_rew_mean          | 580         |
| time/                   |             |
|    fps                  | 886         |
|    iterations           | 17          |
|    time_elapsed         | 39          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.011363119 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.000261    |
|    loss                 | 72.7        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.000576   |
|    value_loss           | 308         |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=287.20 +/- 9.60
Episode length: 96.80 +/- 2.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.8        |
|    mean_reward          | 287         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.008116938 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.0986      |
|    learning_rate        | 0.000261    |
|    loss                 | 171         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00271    |
|    value_loss           | 450         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 521      |
| time/              |          |
|    fps             | 880      |
|    iterations      | 18       |
|    time_elapsed    | 41       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 133         |
|    ep_rew_mean          | 430         |
| time/                   |             |
|    fps                  | 887         |
|    iterations           | 19          |
|    time_elapsed         | 43          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.008405906 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.000261    |
|    loss                 | 132         |
|    n_updates            | 180         |
|    policy_gradient_loss | 0.000487    |
|    value_loss           | 360         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.016097832 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.22        |
|    learning_rate        | 0.000261    |
|    loss                 | 119         |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 446         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 882      |
|    iterations      | 20       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 117          |
|    ep_rew_mean          | 369          |
| time/                   |              |
|    fps                  | 888          |
|    iterations           | 21           |
|    time_elapsed         | 48           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0145736085 |
|    clip_fraction        | 0.222        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.454        |
|    learning_rate        | 0.000261     |
|    loss                 | 184          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00377     |
|    value_loss           | 337          |
------------------------------------------
Eval num_timesteps=45000, episode_reward=290.40 +/- 19.20
Episode length: 97.60 +/- 4.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.6       |
|    mean_reward          | 290        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.01549953 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.543      |
|    learning_rate        | 0.000261   |
|    loss                 | 112        |
|    n_updates            | 210        |
|    policy_gradient_loss | 0.000877   |
|    value_loss           | 294        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 883      |
|    iterations      | 22       |
|    time_elapsed    | 50       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 115       |
|    ep_rew_mean          | 361       |
| time/                   |           |
|    fps                  | 889       |
|    iterations           | 23        |
|    time_elapsed         | 52        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.0222119 |
|    clip_fraction        | 0.25      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.17     |
|    explained_variance   | 0.293     |
|    learning_rate        | 0.000261  |
|    loss                 | 73        |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0051   |
|    value_loss           | 386       |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 354         |
| time/                   |             |
|    fps                  | 894         |
|    iterations           | 24          |
|    time_elapsed         | 54          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.015551647 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.44        |
|    learning_rate        | 0.000261    |
|    loss                 | 167         |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00277    |
|    value_loss           | 302         |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.022875573 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.000261    |
|    loss                 | 148         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00409    |
|    value_loss           | 328         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 889      |
|    iterations      | 25       |
|    time_elapsed    | 57       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 109         |
|    ep_rew_mean          | 337         |
| time/                   |             |
|    fps                  | 894         |
|    iterations           | 26          |
|    time_elapsed         | 59          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.021965086 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.911      |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.000261    |
|    loss                 | 231         |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00504    |
|    value_loss           | 437         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.024026904 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.839      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.000261    |
|    loss                 | 137         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00886    |
|    value_loss           | 318         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 331      |
| time/              |          |
|    fps             | 890      |
|    iterations      | 27       |
|    time_elapsed    | 62       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 324        |
| time/                   |            |
|    fps                  | 894        |
|    iterations           | 28         |
|    time_elapsed         | 64         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.03462287 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.426      |
|    learning_rate        | 0.000261   |
|    loss                 | 132        |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.00776   |
|    value_loss           | 385        |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 320         |
| time/                   |             |
|    fps                  | 899         |
|    iterations           | 29          |
|    time_elapsed         | 66          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.014043871 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.742      |
|    explained_variance   | 0.411       |
|    learning_rate        | 0.000261    |
|    loss                 | 125         |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00879    |
|    value_loss           | 407         |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.02017723 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.451     |
|    explained_variance   | 0.206      |
|    learning_rate        | 0.000261   |
|    loss                 | 296        |
|    n_updates            | 290        |
|    policy_gradient_loss | 0.000925   |
|    value_loss           | 466        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 895      |
|    iterations      | 30       |
|    time_elapsed    | 68       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:21:49,053] Trial 9 finished with value: 284.0 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0002606012935228702, 'gamma': 0.9543579466173555, 'gae_lambda': 0.8051996115293723}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 1270     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 374          |
| time/                   |              |
|    fps                  | 954          |
|    iterations           | 2            |
|    time_elapsed         | 4            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0058850427 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.000222    |
|    learning_rate        | 1.06e-05     |
|    loss                 | 474          |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 1.09e+03     |
------------------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0027678066 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.133       |
|    learning_rate        | 1.06e-05     |
|    loss                 | 68.1         |
|    n_updates            | 20           |
|    policy_gradient_loss | 0.000156     |
|    value_loss           | 707          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 377      |
| time/              |          |
|    fps             | 704      |
|    iterations      | 3        |
|    time_elapsed    | 8        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 373         |
| time/                   |             |
|    fps                  | 712         |
|    iterations           | 4           |
|    time_elapsed         | 11          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.006885305 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.0811      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 359         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.000852   |
|    value_loss           | 580         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009161478 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.0936      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 437         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.000285   |
|    value_loss           | 644         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 377      |
| time/              |          |
|    fps             | 688      |
|    iterations      | 5        |
|    time_elapsed    | 14       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 124         |
|    ep_rew_mean          | 395         |
| time/                   |             |
|    fps                  | 695         |
|    iterations           | 6           |
|    time_elapsed         | 17          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012227644 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.0528      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 76.7        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 606         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 125         |
|    ep_rew_mean          | 399         |
| time/                   |             |
|    fps                  | 705         |
|    iterations           | 7           |
|    time_elapsed         | 20          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.005249098 |
|    clip_fraction        | 0.0188      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.0436      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 198         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.000713   |
|    value_loss           | 514         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012459107 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.0174      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 196         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.000488   |
|    value_loss           | 662         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 399      |
| time/              |          |
|    fps             | 690      |
|    iterations      | 8        |
|    time_elapsed    | 23       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | 401          |
| time/                   |              |
|    fps                  | 695          |
|    iterations           | 9            |
|    time_elapsed         | 26           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0067272903 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.0625       |
|    learning_rate        | 1.06e-05     |
|    loss                 | 325          |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00216     |
|    value_loss           | 585          |
------------------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.008583324 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.0897      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 294         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00457    |
|    value_loss           | 601         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 685      |
|    iterations      | 10       |
|    time_elapsed    | 29       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | 412         |
| time/                   |             |
|    fps                  | 689         |
|    iterations           | 11          |
|    time_elapsed         | 32          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.012085663 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.15        |
|    learning_rate        | 1.06e-05    |
|    loss                 | 180         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 535         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 124          |
|    ep_rew_mean          | 396          |
| time/                   |              |
|    fps                  | 693          |
|    iterations           | 12           |
|    time_elapsed         | 35           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0055595804 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.0808       |
|    learning_rate        | 1.06e-05     |
|    loss                 | 527          |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.000132    |
|    value_loss           | 550          |
------------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.00725098 |
|    clip_fraction        | 0.0253     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.121      |
|    learning_rate        | 1.06e-05   |
|    loss                 | 249        |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0018    |
|    value_loss           | 594        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 685      |
|    iterations      | 13       |
|    time_elapsed    | 38       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 120        |
|    ep_rew_mean          | 380        |
| time/                   |            |
|    fps                  | 691        |
|    iterations           | 14         |
|    time_elapsed         | 41         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.01118464 |
|    clip_fraction        | 0.0529     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.182      |
|    learning_rate        | 1.06e-05   |
|    loss                 | 380        |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.00201   |
|    value_loss           | 589        |
----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.008842337 |
|    clip_fraction        | 0.0408      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.157       |
|    learning_rate        | 1.06e-05    |
|    loss                 | 411         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00245    |
|    value_loss           | 599         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 378      |
| time/              |          |
|    fps             | 684      |
|    iterations      | 15       |
|    time_elapsed    | 44       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 121         |
|    ep_rew_mean          | 383         |
| time/                   |             |
|    fps                  | 687         |
|    iterations           | 16          |
|    time_elapsed         | 47          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.006459875 |
|    clip_fraction        | 0.0209      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.25        |
|    learning_rate        | 1.06e-05    |
|    loss                 | 261         |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.000237   |
|    value_loss           | 501         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 117        |
|    ep_rew_mean          | 367        |
| time/                   |            |
|    fps                  | 690        |
|    iterations           | 17         |
|    time_elapsed         | 50         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.00309729 |
|    clip_fraction        | 0.00767    |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.196      |
|    learning_rate        | 1.06e-05   |
|    loss                 | 96.2       |
|    n_updates            | 160        |
|    policy_gradient_loss | -5.86e-05  |
|    value_loss           | 519        |
----------------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0046781832 |
|    clip_fraction        | 0.0415       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.197        |
|    learning_rate        | 1.06e-05     |
|    loss                 | 136          |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00145     |
|    value_loss           | 598          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 357      |
| time/              |          |
|    fps             | 685      |
|    iterations      | 18       |
|    time_elapsed    | 53       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 377          |
| time/                   |              |
|    fps                  | 688          |
|    iterations           | 19           |
|    time_elapsed         | 56           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0069476087 |
|    clip_fraction        | 0.0435       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.21         |
|    learning_rate        | 1.06e-05     |
|    loss                 | 389          |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00247     |
|    value_loss           | 558          |
------------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0066940095 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.129        |
|    learning_rate        | 1.06e-05     |
|    loss                 | 449          |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 548          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 685      |
|    iterations      | 20       |
|    time_elapsed    | 59       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 375          |
| time/                   |              |
|    fps                  | 688          |
|    iterations           | 21           |
|    time_elapsed         | 62           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0073973597 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.246        |
|    learning_rate        | 1.06e-05     |
|    loss                 | 101          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 502          |
------------------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0063387887 |
|    clip_fraction        | 0.043        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.226        |
|    learning_rate        | 1.06e-05     |
|    loss                 | 314          |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00169     |
|    value_loss           | 554          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 684      |
|    iterations      | 22       |
|    time_elapsed    | 65       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | 361         |
| time/                   |             |
|    fps                  | 687         |
|    iterations           | 23          |
|    time_elapsed         | 68          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.005206081 |
|    clip_fraction        | 0.0687      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.312       |
|    learning_rate        | 1.06e-05    |
|    loss                 | 92.4        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 511         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | 366         |
| time/                   |             |
|    fps                  | 690         |
|    iterations           | 24          |
|    time_elapsed         | 71          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.012316547 |
|    clip_fraction        | 0.0452      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.221       |
|    learning_rate        | 1.06e-05    |
|    loss                 | 311         |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 549         |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.011092873 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.163       |
|    learning_rate        | 1.06e-05    |
|    loss                 | 144         |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00714    |
|    value_loss           | 555         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 687      |
|    iterations      | 25       |
|    time_elapsed    | 74       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 115          |
|    ep_rew_mean          | 359          |
| time/                   |              |
|    fps                  | 689          |
|    iterations           | 26           |
|    time_elapsed         | 77           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0065007117 |
|    clip_fraction        | 0.0389       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.196        |
|    learning_rate        | 1.06e-05     |
|    loss                 | 238          |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 573          |
------------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.004238588 |
|    clip_fraction        | 0.0474      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.193       |
|    learning_rate        | 1.06e-05    |
|    loss                 | 643         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.000734   |
|    value_loss           | 542         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 374      |
| time/              |          |
|    fps             | 685      |
|    iterations      | 27       |
|    time_elapsed    | 80       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 121          |
|    ep_rew_mean          | 384          |
| time/                   |              |
|    fps                  | 686          |
|    iterations           | 28           |
|    time_elapsed         | 83           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0078537855 |
|    clip_fraction        | 0.044        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.222        |
|    learning_rate        | 1.06e-05     |
|    loss                 | 74.4         |
|    n_updates            | 270          |
|    policy_gradient_loss | 0.000228     |
|    value_loss           | 505          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 121          |
|    ep_rew_mean          | 386          |
| time/                   |              |
|    fps                  | 688          |
|    iterations           | 29           |
|    time_elapsed         | 86           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0065574264 |
|    clip_fraction        | 0.0684       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.32        |
|    explained_variance   | 0.187        |
|    learning_rate        | 1.06e-05     |
|    loss                 | 464          |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 522          |
------------------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.009429239 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.195       |
|    learning_rate        | 1.06e-05    |
|    loss                 | 222         |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 568         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 685      |
|    iterations      | 30       |
|    time_elapsed    | 89       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:23:20,773] Trial 10 finished with value: 284.0 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 1.0616955724893998e-05, 'gamma': 0.9057761199796952, 'gae_lambda': 0.9896839358319636}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=306.40 +/- 57.33
Episode length: 101.60 +/- 14.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 1102     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0051474613 |
|    clip_fraction        | 0.0492       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | 0.00139      |
|    learning_rate        | 2.16e-05     |
|    loss                 | 403          |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0013      |
|    value_loss           | 354          |
------------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 826      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=868.00 +/- 795.22
Episode length: 237.00 +/- 191.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 237          |
|    mean_reward          | 868          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0065540797 |
|    clip_fraction        | 0.06         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | 0.159        |
|    learning_rate        | 2.16e-05     |
|    loss                 | 350          |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000695    |
|    value_loss           | 391          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 757      |
|    iterations      | 3        |
|    time_elapsed    | 32       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=517.60 +/- 181.61
Episode length: 154.40 +/- 45.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 154        |
|    mean_reward          | 518        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01067448 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.341      |
|    learning_rate        | 2.16e-05   |
|    loss                 | 574        |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.00105   |
|    value_loss           | 412        |
----------------------------------------
Eval num_timesteps=30000, episode_reward=351.20 +/- 80.26
Episode length: 112.80 +/- 20.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 351      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 724      |
|    iterations      | 4        |
|    time_elapsed    | 45       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=661.60 +/- 307.87
Episode length: 190.40 +/- 76.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 190        |
|    mean_reward          | 662        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.00709984 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.405      |
|    learning_rate        | 2.16e-05   |
|    loss                 | 600        |
|    n_updates            | 40         |
|    policy_gradient_loss | 5.83e-05   |
|    value_loss           | 479        |
----------------------------------------
Eval num_timesteps=40000, episode_reward=437.60 +/- 127.84
Episode length: 134.40 +/- 31.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 392      |
| time/              |          |
|    fps             | 702      |
|    iterations      | 5        |
|    time_elapsed    | 58       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=312.80 +/- 33.41
Episode length: 103.20 +/- 8.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 313         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.011647251 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.508       |
|    learning_rate        | 2.16e-05    |
|    loss                 | 450         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00197    |
|    value_loss           | 459         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 414      |
| time/              |          |
|    fps             | 702      |
|    iterations      | 6        |
|    time_elapsed    | 69       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009149367 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.392       |
|    learning_rate        | 2.16e-05    |
|    loss                 | 473         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 568         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 453      |
| time/              |          |
|    fps             | 696      |
|    iterations      | 7        |
|    time_elapsed    | 82       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.4        |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.011136652 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.247       |
|    learning_rate        | 2.16e-05    |
|    loss                 | 48.4        |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.000192    |
|    value_loss           | 630         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 465      |
| time/              |          |
|    fps             | 694      |
|    iterations      | 8        |
|    time_elapsed    | 94       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:25:00,518] Trial 11 finished with value: 287.2 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 2.1567272685811586e-05, 'gamma': 0.9713821735053121, 'gae_lambda': 0.8611179400545338}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=306.40 +/- 35.20
Episode length: 101.60 +/- 8.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 399      |
| time/              |          |
|    fps             | 1150     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=300.00 +/- 48.00
Episode length: 100.00 +/- 12.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.034452092 |
|    clip_fraction        | 0.488       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.000707    |
|    learning_rate        | 0.00104     |
|    loss                 | 297         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0334      |
|    value_loss           | 522         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=322.40 +/- 55.05
Episode length: 105.60 +/- 13.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 422      |
| time/              |          |
|    fps             | 850      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=354.40 +/- 66.82
Episode length: 113.60 +/- 16.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 114        |
|    mean_reward          | 354        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.37379587 |
|    clip_fraction        | 0.662      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.209      |
|    learning_rate        | 0.00104    |
|    loss                 | 105        |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.083      |
|    value_loss           | 548        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 790      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=357.60 +/- 53.64
Episode length: 114.40 +/- 13.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 114       |
|    mean_reward          | 358       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 5.0372257 |
|    clip_fraction        | 0.872     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.667    |
|    explained_variance   | 0.226     |
|    learning_rate        | 0.00104   |
|    loss                 | 57.9      |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.179     |
|    value_loss           | 625       |
---------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=376.80 +/- 87.58
Episode length: 119.20 +/- 21.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 556      |
| time/              |          |
|    fps             | 754      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 35000    |
| train/                  |          |
|    approx_kl            | 23.25257 |
|    clip_fraction        | 0.986    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0225  |
|    explained_variance   | -0.485   |
|    learning_rate        | 0.00104  |
|    loss                 | 115      |
|    n_updates            | 40       |
|    policy_gradient_loss | 0.298    |
|    value_loss           | 454      |
--------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 737      |
|    iterations      | 5        |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.03e-09 |
|    explained_variance   | -0.00012  |
|    learning_rate        | 0.00104   |
|    loss                 | 2.13      |
|    n_updates            | 50        |
|    policy_gradient_loss | -7.89e-10 |
|    value_loss           | 259       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 731      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.65e-12 |
|    explained_variance   | 0.87      |
|    learning_rate        | 0.00104   |
|    loss                 | 2.73      |
|    n_updates            | 60        |
|    policy_gradient_loss | -1.95e-10 |
|    value_loss           | 11.4      |
---------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 720      |
|    iterations      | 7        |
|    time_elapsed    | 79       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.02e-11 |
|    explained_variance   | 0.967     |
|    learning_rate        | 0.00104   |
|    loss                 | 0.346     |
|    n_updates            | 70        |
|    policy_gradient_loss | -1.27e-11 |
|    value_loss           | 4.4       |
---------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 713      |
|    iterations      | 8        |
|    time_elapsed    | 91       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:26:37,869] Trial 12 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0010436758573865996, 'gamma': 0.9724502757429541, 'gae_lambda': 0.9275599074112051}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 366      |
| time/              |          |
|    fps             | 1152     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010594424 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.00199     |
|    learning_rate        | 1.55e-05    |
|    loss                 | 277         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 453         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 384      |
| time/              |          |
|    fps             | 842      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.00740707 |
|    clip_fraction        | 0.0899     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.204      |
|    learning_rate        | 1.55e-05   |
|    loss                 | 160        |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00243   |
|    value_loss           | 468        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 795      |
|    iterations      | 3        |
|    time_elapsed    | 30       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008236026 |
|    clip_fraction        | 0.0705      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.424       |
|    learning_rate        | 1.55e-05    |
|    loss                 | 65.5        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.000229   |
|    value_loss           | 503         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 759      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=287.20 +/- 9.60
Episode length: 96.80 +/- 2.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.8         |
|    mean_reward          | 287          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0067767412 |
|    clip_fraction        | 0.0917       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.475        |
|    learning_rate        | 1.55e-05     |
|    loss                 | 55.8         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.000557    |
|    value_loss           | 576          |
------------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 383      |
| time/              |          |
|    fps             | 739      |
|    iterations      | 5        |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0100278435 |
|    clip_fraction        | 0.167        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.586        |
|    learning_rate        | 1.55e-05     |
|    loss                 | 1.05e+03     |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00405     |
|    value_loss           | 614          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 733      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010058958 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.608       |
|    learning_rate        | 1.55e-05    |
|    loss                 | 448         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00363    |
|    value_loss           | 667         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 415      |
| time/              |          |
|    fps             | 727      |
|    iterations      | 7        |
|    time_elapsed    | 78       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.010377914 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.449       |
|    learning_rate        | 1.55e-05    |
|    loss                 | 1.41e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00025    |
|    value_loss           | 856         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 393      |
| time/              |          |
|    fps             | 720      |
|    iterations      | 8        |
|    time_elapsed    | 90       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:28:14,297] Trial 13 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 1.5520496453088817e-05, 'gamma': 0.9814600664245867, 'gae_lambda': 0.8781163152949343}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 386      |
| time/              |          |
|    fps             | 1152     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=306.40 +/- 32.16
Episode length: 101.60 +/- 8.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 306       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 111.89057 |
|    clip_fraction        | 0.999     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00512  |
|    explained_variance   | 0.00255   |
|    learning_rate        | 0.00905   |
|    loss                 | 134       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.426     |
|    value_loss           | 705       |
---------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=335.20 +/- 29.33
Episode length: 108.80 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 335      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 842      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=290.40 +/- 12.80
Episode length: 97.60 +/- 3.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 97.6      |
|    mean_reward          | 290       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.14e-24 |
|    explained_variance   | 0.0986    |
|    learning_rate        | 0.00905   |
|    loss                 | 197       |
|    n_updates            | 20        |
|    policy_gradient_loss | -2.85e-10 |
|    value_loss           | 1.13e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 789      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=328.80 +/- 40.98
Episode length: 107.20 +/- 10.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 107       |
|    mean_reward          | 329       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.76e-25 |
|    explained_variance   | 0.125     |
|    learning_rate        | 0.00905   |
|    loss                 | 556       |
|    n_updates            | 30        |
|    policy_gradient_loss | 5.82e-12  |
|    value_loss           | 1.31e+03  |
---------------------------------------
Eval num_timesteps=30000, episode_reward=303.20 +/- 29.33
Episode length: 100.80 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 303      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 311      |
| time/              |          |
|    fps             | 758      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=332.00 +/- 35.78
Episode length: 108.00 +/- 8.94
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 108      |
|    mean_reward          | 332      |
| time/                   |          |
|    total_timesteps      | 35000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.8e-15 |
|    explained_variance   | 0.124    |
|    learning_rate        | 0.00905  |
|    loss                 | 1.14e+03 |
|    n_updates            | 40       |
|    policy_gradient_loss | 6.94e-10 |
|    value_loss           | 1.35e+03 |
--------------------------------------
Eval num_timesteps=40000, episode_reward=316.00 +/- 40.48
Episode length: 104.00 +/- 10.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 739      |
|    iterations      | 5        |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=306.40 +/- 24.99
Episode length: 101.60 +/- 6.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 306       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.55e-23 |
|    explained_variance   | 0.129     |
|    learning_rate        | 0.00905   |
|    loss                 | 258       |
|    n_updates            | 50        |
|    policy_gradient_loss | 1.31e-10  |
|    value_loss           | 1.4e+03   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 732      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=306.40 +/- 28.80
Episode length: 101.60 +/- 7.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 306       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.08e-23 |
|    explained_variance   | 0.131     |
|    learning_rate        | 0.00905   |
|    loss                 | 667       |
|    n_updates            | 60        |
|    policy_gradient_loss | -2.42e-10 |
|    value_loss           | 1.41e+03  |
---------------------------------------
Eval num_timesteps=55000, episode_reward=303.20 +/- 25.60
Episode length: 100.80 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 303      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 722      |
|    iterations      | 7        |
|    time_elapsed    | 79       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=316.00 +/- 42.93
Episode length: 104.00 +/- 10.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 104       |
|    mean_reward          | 316       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.7e-20  |
|    explained_variance   | 0.126     |
|    learning_rate        | 0.00905   |
|    loss                 | 994       |
|    n_updates            | 70        |
|    policy_gradient_loss | -2.47e-10 |
|    value_loss           | 1.48e+03  |
---------------------------------------
Eval num_timesteps=65000, episode_reward=306.40 +/- 24.99
Episode length: 101.60 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 714      |
|    iterations      | 8        |
|    time_elapsed    | 91       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:29:51,530] Trial 14 finished with value: 319.2 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.009046138387246959, 'gamma': 0.9623676784761174, 'gae_lambda': 0.9604380244238984}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=578.40 +/- 402.44
Episode length: 169.60 +/- 100.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | 578      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 386      |
| time/              |          |
|    fps             | 1082     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 10000    |
| train/                  |          |
|    approx_kl            | 87.05147 |
|    clip_fraction        | 0.998    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0136  |
|    explained_variance   | 0.00151  |
|    learning_rate        | 0.00457  |
|    loss                 | 341      |
|    n_updates            | 10       |
|    policy_gradient_loss | 0.459    |
|    value_loss           | 811      |
--------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 306      |
| time/              |          |
|    fps             | 822      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 3.8409185 |
|    clip_fraction        | 0.0847    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0132   |
|    explained_variance   | -0.00167  |
|    learning_rate        | 0.00457   |
|    loss                 | 355       |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.0185   |
|    value_loss           | 1.2e+03   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 776      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 6.9374323 |
|    clip_fraction        | 0.123     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0172   |
|    explained_variance   | 0.0048    |
|    learning_rate        | 0.00457   |
|    loss                 | 188       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.00246   |
|    value_loss           | 944       |
---------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 744      |
|    iterations      | 4        |
|    time_elapsed    | 44       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 35000    |
| train/                  |          |
|    approx_kl            | 6.402449 |
|    clip_fraction        | 0.167    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0172  |
|    explained_variance   | 0.0438   |
|    learning_rate        | 0.00457  |
|    loss                 | 154      |
|    n_updates            | 40       |
|    policy_gradient_loss | -0.0173  |
|    value_loss           | 873      |
--------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 726      |
|    iterations      | 5        |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 7.3766403 |
|    clip_fraction        | 0.162     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0163   |
|    explained_variance   | 0.0733    |
|    learning_rate        | 0.00457   |
|    loss                 | 129       |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0183   |
|    value_loss           | 882       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 721      |
|    iterations      | 6        |
|    time_elapsed    | 68       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 8.103461 |
|    clip_fraction        | 0.202    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0287  |
|    explained_variance   | 0.018    |
|    learning_rate        | 0.00457  |
|    loss                 | 436      |
|    n_updates            | 60       |
|    policy_gradient_loss | 0.0101   |
|    value_loss           | 764      |
--------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 712      |
|    iterations      | 7        |
|    time_elapsed    | 80       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 60000    |
| train/                  |          |
|    approx_kl            | 7.610166 |
|    clip_fraction        | 0.212    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0338  |
|    explained_variance   | 0.0066   |
|    learning_rate        | 0.00457  |
|    loss                 | 118      |
|    n_updates            | 70       |
|    policy_gradient_loss | -0.0143  |
|    value_loss           | 736      |
--------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.7     |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 705      |
|    iterations      | 8        |
|    time_elapsed    | 92       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:31:29,843] Trial 15 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.004570408783512008, 'gamma': 0.9598876067733865, 'gae_lambda': 0.9726526767259649}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 1268     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | 376         |
| time/                   |             |
|    fps                  | 1065        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012701234 |
|    clip_fraction        | 0.403       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.000514    |
|    learning_rate        | 0.000772    |
|    loss                 | 211         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0211      |
|    value_loss           | 517         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.061197042 |
|    clip_fraction        | 0.435       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.145       |
|    learning_rate        | 0.000772    |
|    loss                 | 153         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.0162      |
|    value_loss           | 500         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 915      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | 364         |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.031065896 |
|    clip_fraction        | 0.405       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | -0.00779    |
|    learning_rate        | 0.000772    |
|    loss                 | 138         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.0164      |
|    value_loss           | 458         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.049809143 |
|    clip_fraction        | 0.413       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | -0.0638     |
|    learning_rate        | 0.000772    |
|    loss                 | 81.9        |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.0205      |
|    value_loss           | 361         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 865      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 112        |
|    ep_rew_mean          | 349        |
| time/                   |            |
|    fps                  | 873        |
|    iterations           | 6          |
|    time_elapsed         | 14         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.02137797 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.145      |
|    learning_rate        | 0.000772   |
|    loss                 | 57         |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.00292    |
|    value_loss           | 364        |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 109        |
|    ep_rew_mean          | 336        |
| time/                   |            |
|    fps                  | 877        |
|    iterations           | 7          |
|    time_elapsed         | 16         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.06259909 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.0837     |
|    learning_rate        | 0.000772   |
|    loss                 | 20.9       |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.0112     |
|    value_loss           | 387        |
----------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.04085068 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | -0.0197    |
|    learning_rate        | 0.000772   |
|    loss                 | 64.1       |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.00601    |
|    value_loss           | 514        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 852      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 320         |
| time/                   |             |
|    fps                  | 858         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.054786265 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | -0.387      |
|    learning_rate        | 0.000772    |
|    loss                 | 67.7        |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.0108      |
|    value_loss           | 497         |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.054349687 |
|    clip_fraction        | 0.336       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -0.178      |
|    learning_rate        | 0.000772    |
|    loss                 | 26.6        |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00997     |
|    value_loss           | 508         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 308      |
| time/              |          |
|    fps             | 843      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 104        |
|    ep_rew_mean          | 316        |
| time/                   |            |
|    fps                  | 849        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.07098331 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | -0.0118    |
|    learning_rate        | 0.000772   |
|    loss                 | 156        |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.00506   |
|    value_loss           | 493        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 107       |
|    ep_rew_mean          | 329       |
| time/                   |           |
|    fps                  | 856       |
|    iterations           | 12        |
|    time_elapsed         | 28        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.3368764 |
|    clip_fraction        | 0.515     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.15     |
|    explained_variance   | -0.179    |
|    learning_rate        | 0.000772  |
|    loss                 | 142       |
|    n_updates            | 110       |
|    policy_gradient_loss | 0.0783    |
|    value_loss           | 319       |
---------------------------------------
Eval num_timesteps=25000, episode_reward=444.00 +/- 77.07
Episode length: 136.00 +/- 19.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 136        |
|    mean_reward          | 444        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.18726276 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.998     |
|    explained_variance   | -0.184     |
|    learning_rate        | 0.000772   |
|    loss                 | 171        |
|    n_updates            | 120        |
|    policy_gradient_loss | 0.078      |
|    value_loss           | 393        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 836      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 113        |
|    ep_rew_mean          | 351        |
| time/                   |            |
|    fps                  | 842        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.06750956 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.859     |
|    explained_variance   | -0.104     |
|    learning_rate        | 0.000772   |
|    loss                 | 277        |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.0221     |
|    value_loss           | 440        |
----------------------------------------
Eval num_timesteps=30000, episode_reward=309.60 +/- 31.35
Episode length: 102.40 +/- 7.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 102        |
|    mean_reward          | 310        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.10439494 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.641     |
|    explained_variance   | 0.0424     |
|    learning_rate        | 0.000772   |
|    loss                 | 94.4       |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.0102     |
|    value_loss           | 548        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 832      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | 370         |
| time/                   |             |
|    fps                  | 838         |
|    iterations           | 16          |
|    time_elapsed         | 39          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.058914922 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.29       |
|    explained_variance   | 0.0469      |
|    learning_rate        | 0.000772    |
|    loss                 | 134         |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.00324     |
|    value_loss           | 552         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 114        |
|    ep_rew_mean          | 355        |
| time/                   |            |
|    fps                  | 842        |
|    iterations           | 17         |
|    time_elapsed         | 41         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.21226388 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | -0.00178   |
|    learning_rate        | 0.000772   |
|    loss                 | 367        |
|    n_updates            | 160        |
|    policy_gradient_loss | 0.0114     |
|    value_loss           | 558        |
----------------------------------------
Eval num_timesteps=35000, episode_reward=309.60 +/- 23.95
Episode length: 102.40 +/- 5.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.020501029 |
|    clip_fraction        | 0.0201      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0574     |
|    explained_variance   | -0.107      |
|    learning_rate        | 0.000772    |
|    loss                 | 373         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0028     |
|    value_loss           | 730         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 834      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 334         |
| time/                   |             |
|    fps                  | 839         |
|    iterations           | 19          |
|    time_elapsed         | 46          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.042855486 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.0518      |
|    learning_rate        | 0.000772    |
|    loss                 | 328         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00531    |
|    value_loss           | 742         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=335.20 +/- 43.41
Episode length: 108.80 +/- 10.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 109        |
|    mean_reward          | 335        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.10972054 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | -0.0863    |
|    learning_rate        | 0.000772   |
|    loss                 | 113        |
|    n_updates            | 190        |
|    policy_gradient_loss | 0.0132     |
|    value_loss           | 460        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 335      |
| time/              |          |
|    fps             | 831      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 326        |
| time/                   |            |
|    fps                  | 835        |
|    iterations           | 21         |
|    time_elapsed         | 51         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.12177649 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.394     |
|    explained_variance   | -0.188     |
|    learning_rate        | 0.000772   |
|    loss                 | 142        |
|    n_updates            | 200        |
|    policy_gradient_loss | 0.0423     |
|    value_loss           | 538        |
----------------------------------------
Eval num_timesteps=45000, episode_reward=332.00 +/- 38.53
Episode length: 108.00 +/- 9.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 332         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.036159813 |
|    clip_fraction        | 0.0944      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.28       |
|    explained_variance   | -0.076      |
|    learning_rate        | 0.000772    |
|    loss                 | 171         |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00941    |
|    value_loss           | 506         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 828      |
|    iterations      | 22       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 106       |
|    ep_rew_mean          | 323       |
| time/                   |           |
|    fps                  | 832       |
|    iterations           | 23        |
|    time_elapsed         | 56        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.1129737 |
|    clip_fraction        | 0.0956    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.113    |
|    explained_variance   | -0.106    |
|    learning_rate        | 0.000772  |
|    loss                 | 300       |
|    n_updates            | 220       |
|    policy_gradient_loss | 0.015     |
|    value_loss           | 583       |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 321         |
| time/                   |             |
|    fps                  | 836         |
|    iterations           | 24          |
|    time_elapsed         | 58          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.028285235 |
|    clip_fraction        | 0.0238      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0639     |
|    explained_variance   | 0.0669      |
|    learning_rate        | 0.000772    |
|    loss                 | 464         |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 769         |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=319.20 +/- 30.19
Episode length: 104.80 +/- 7.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 105           |
|    mean_reward          | 319           |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 0.00014362339 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00896      |
|    explained_variance   | 0.0805        |
|    learning_rate        | 0.000772      |
|    loss                 | 612           |
|    n_updates            | 240           |
|    policy_gradient_loss | -6.39e-05     |
|    value_loss           | 922           |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 830      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 319         |
| time/                   |             |
|    fps                  | 834         |
|    iterations           | 26          |
|    time_elapsed         | 63          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 6.22331e-05 |
|    clip_fraction        | 0.000391    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0109     |
|    explained_variance   | 0.0914      |
|    learning_rate        | 0.000772    |
|    loss                 | 539         |
|    n_updates            | 250         |
|    policy_gradient_loss | -3.44e-05   |
|    value_loss           | 866         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=306.40 +/- 28.80
Episode length: 101.60 +/- 7.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | 306          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0096618645 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0288      |
|    explained_variance   | 0.086        |
|    learning_rate        | 0.000772     |
|    loss                 | 369          |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000625    |
|    value_loss           | 794          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 829      |
|    iterations      | 27       |
|    time_elapsed    | 66       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | 318         |
| time/                   |             |
|    fps                  | 832         |
|    iterations           | 28          |
|    time_elapsed         | 68          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.023655012 |
|    clip_fraction        | 0.0128      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0118     |
|    explained_variance   | 0.0732      |
|    learning_rate        | 0.000772    |
|    loss                 | 217         |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.000939   |
|    value_loss           | 794         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 105          |
|    ep_rew_mean          | 318          |
| time/                   |              |
|    fps                  | 835          |
|    iterations           | 29           |
|    time_elapsed         | 71           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0024241465 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0013      |
|    explained_variance   | 0.0933       |
|    learning_rate        | 0.000772     |
|    loss                 | 541          |
|    n_updates            | 280          |
|    policy_gradient_loss | 0.000184     |
|    value_loss           | 868          |
------------------------------------------
Eval num_timesteps=60000, episode_reward=319.20 +/- 39.06
Episode length: 104.80 +/- 9.77
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 105            |
|    mean_reward          | 319            |
| time/                   |                |
|    total_timesteps      | 60000          |
| train/                  |                |
|    approx_kl            | -4.8603397e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00012       |
|    explained_variance   | 0.0854         |
|    learning_rate        | 0.000772       |
|    loss                 | 814            |
|    n_updates            | 290            |
|    policy_gradient_loss | 8.12e-09       |
|    value_loss           | 890            |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 830      |
|    iterations      | 30       |
|    time_elapsed    | 73       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:32:45,274] Trial 16 finished with value: 309.6 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0007719946433613516, 'gamma': 0.9399621810580755, 'gae_lambda': 0.9534655129297931}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 372      |
| time/              |          |
|    fps             | 1158     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 166.74261 |
|    clip_fraction        | 0.999     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00819  |
|    explained_variance   | 0.00161   |
|    learning_rate        | 0.00675   |
|    loss                 | 394       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.41      |
|    value_loss           | 629       |
---------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.6     |
|    ep_rew_mean     | 298      |
| time/              |          |
|    fps             | 844      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 16.391304 |
|    clip_fraction        | 0.148     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00763  |
|    explained_variance   | -0.0176   |
|    learning_rate        | 0.00675   |
|    loss                 | 0.832     |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.0496   |
|    value_loss           | 109       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 789      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 25000    |
| train/                  |          |
|    approx_kl            | 9.612363 |
|    clip_fraction        | 0.111    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00129 |
|    explained_variance   | 0.475    |
|    learning_rate        | 0.00675  |
|    loss                 | 220      |
|    n_updates            | 30       |
|    policy_gradient_loss | 0.167    |
|    value_loss           | 324      |
--------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 752      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 16.846159 |
|    clip_fraction        | 0.18      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0095   |
|    explained_variance   | 0.717     |
|    learning_rate        | 0.00675   |
|    loss                 | 144       |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0443   |
|    value_loss           | 280       |
---------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 732      |
|    iterations      | 5        |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 6.0708766 |
|    clip_fraction        | 0.169     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00267  |
|    explained_variance   | 0.403     |
|    learning_rate        | 0.00675   |
|    loss                 | 413       |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.00367  |
|    value_loss           | 727       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 726      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 4.305403  |
|    clip_fraction        | 0.177     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000111 |
|    explained_variance   | 0.441     |
|    learning_rate        | 0.00675   |
|    loss                 | 506       |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0435   |
|    value_loss           | 690       |
---------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 720      |
|    iterations      | 7        |
|    time_elapsed    | 79       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 7.937327  |
|    clip_fraction        | 0.176     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000185 |
|    explained_variance   | 0.508     |
|    learning_rate        | 0.00675   |
|    loss                 | 567       |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.039    |
|    value_loss           | 674       |
---------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 712      |
|    iterations      | 8        |
|    time_elapsed    | 92       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:34:22,647] Trial 17 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.006752057834937187, 'gamma': 0.960611440135773, 'gae_lambda': 0.9518380269852202}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=367.20 +/- 128.16
Episode length: 116.80 +/- 32.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 367      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 368      |
| time/              |          |
|    fps             | 1128     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=367.20 +/- 90.74
Episode length: 116.80 +/- 22.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 117       |
|    mean_reward          | 367       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.8307226 |
|    clip_fraction        | 0.795     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.97     |
|    explained_variance   | -0.00364  |
|    learning_rate        | 0.002     |
|    loss                 | 303       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.141     |
|    value_loss           | 489       |
---------------------------------------
Eval num_timesteps=15000, episode_reward=325.60 +/- 35.20
Episode length: 106.40 +/- 8.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 326      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 824      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=312.80 +/- 39.06
Episode length: 103.20 +/- 9.77
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 103      |
|    mean_reward          | 313      |
| time/                   |          |
|    total_timesteps      | 20000    |
| train/                  |          |
|    approx_kl            | 29.13614 |
|    clip_fraction        | 0.993    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0332  |
|    explained_variance   | -0.0984  |
|    learning_rate        | 0.002    |
|    loss                 | 127      |
|    n_updates            | 20       |
|    policy_gradient_loss | 0.297    |
|    value_loss           | 625      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 346      |
| time/              |          |
|    fps             | 778      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=341.60 +/- 27.90
Episode length: 110.40 +/- 6.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 110       |
|    mean_reward          | 342       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.21e-18 |
|    explained_variance   | 0.0747    |
|    learning_rate        | 0.002     |
|    loss                 | 1.08e+03  |
|    n_updates            | 30        |
|    policy_gradient_loss | 2.92e-10  |
|    value_loss           | 1.27e+03  |
---------------------------------------
Eval num_timesteps=30000, episode_reward=306.40 +/- 32.16
Episode length: 101.60 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 747      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=309.60 +/- 31.35
Episode length: 102.40 +/- 7.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 310       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.05e-16 |
|    explained_variance   | 0.156     |
|    learning_rate        | 0.002     |
|    loss                 | 1.18e+03  |
|    n_updates            | 40        |
|    policy_gradient_loss | -7.66e-10 |
|    value_loss           | 1.36e+03  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=319.20 +/- 36.35
Episode length: 104.80 +/- 9.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 733      |
|    iterations      | 5        |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=303.20 +/- 32.63
Episode length: 100.80 +/- 8.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 101       |
|    mean_reward          | 303       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.29e-17 |
|    explained_variance   | 0.18      |
|    learning_rate        | 0.002     |
|    loss                 | 1.32e+03  |
|    n_updates            | 50        |
|    policy_gradient_loss | -2.7e-10  |
|    value_loss           | 1.47e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 729      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=316.00 +/- 35.05
Episode length: 104.00 +/- 8.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 104       |
|    mean_reward          | 316       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.61e-17 |
|    explained_variance   | 0.17      |
|    learning_rate        | 0.002     |
|    loss                 | 535       |
|    n_updates            | 60        |
|    policy_gradient_loss | 9.09e-13  |
|    value_loss           | 1.58e+03  |
---------------------------------------
Eval num_timesteps=55000, episode_reward=309.60 +/- 31.35
Episode length: 102.40 +/- 7.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 311      |
| time/              |          |
|    fps             | 720      |
|    iterations      | 7        |
|    time_elapsed    | 79       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=322.40 +/- 37.32
Episode length: 105.60 +/- 9.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 106       |
|    mean_reward          | 322       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.59e-16 |
|    explained_variance   | 0.201     |
|    learning_rate        | 0.002     |
|    loss                 | 892       |
|    n_updates            | 70        |
|    policy_gradient_loss | -2.25e-10 |
|    value_loss           | 1.66e+03  |
---------------------------------------
Eval num_timesteps=65000, episode_reward=296.80 +/- 15.68
Episode length: 99.20 +/- 3.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | 297      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 307      |
| time/              |          |
|    fps             | 713      |
|    iterations      | 8        |
|    time_elapsed    | 91       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:35:59,886] Trial 18 finished with value: 328.8 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.001998551397052994, 'gamma': 0.9864246509182578, 'gae_lambda': 0.9143210928100163}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=309.60 +/- 34.47
Episode length: 102.40 +/- 8.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 393      |
| time/              |          |
|    fps             | 1146     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.5045178 |
|    clip_fraction        | 0.873     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.683    |
|    explained_variance   | 0.00261   |
|    learning_rate        | 0.00198   |
|    loss                 | 230       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.219     |
|    value_loss           | 535       |
---------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 381      |
| time/              |          |
|    fps             | 844      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 6.5165496 |
|    clip_fraction        | 0.955     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.232    |
|    explained_variance   | 0.152     |
|    learning_rate        | 0.00198   |
|    loss                 | 103       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.22      |
|    value_loss           | 533       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.1     |
|    ep_rew_mean     | 296      |
| time/              |          |
|    fps             | 788      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.12619883 |
|    clip_fraction        | 0.0059     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00569   |
|    explained_variance   | 0.026      |
|    learning_rate        | 0.00198    |
|    loss                 | 882        |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.00256    |
|    value_loss           | 1.2e+03    |
----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 753      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 1.2633469 |
|    clip_fraction        | 0.0814    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0162   |
|    explained_variance   | 0.262     |
|    learning_rate        | 0.00198   |
|    loss                 | 283       |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0206   |
|    value_loss           | 1.19e+03  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 733      |
|    iterations      | 5        |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.96218026 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0583    |
|    explained_variance   | 0.177      |
|    learning_rate        | 0.00198    |
|    loss                 | 620        |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.0657     |
|    value_loss           | 1.27e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 727      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 0.730441 |
|    clip_fraction        | 0.0514   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0195  |
|    explained_variance   | 0.32     |
|    learning_rate        | 0.00198  |
|    loss                 | 775      |
|    n_updates            | 60       |
|    policy_gradient_loss | -0.00868 |
|    value_loss           | 1.18e+03 |
--------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 716      |
|    iterations      | 7        |
|    time_elapsed    | 80       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 1.5647339 |
|    clip_fraction        | 0.0582    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0147   |
|    explained_variance   | 0.298     |
|    learning_rate        | 0.00198   |
|    loss                 | 1.27e+03  |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.00451  |
|    value_loss           | 1.15e+03  |
---------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 708      |
|    iterations      | 8        |
|    time_elapsed    | 92       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:37:37,951] Trial 19 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0019828567881329346, 'gamma': 0.9861505352792516, 'gae_lambda': 0.9141728916077131}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 1262     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 126        |
|    ep_rew_mean          | 405        |
| time/                   |            |
|    fps                  | 1078       |
|    iterations           | 2          |
|    time_elapsed         | 3          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.22748934 |
|    clip_fraction        | 0.656      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | -0.00282   |
|    learning_rate        | 0.00164    |
|    loss                 | 169        |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.0499     |
|    value_loss           | 343        |
----------------------------------------
Eval num_timesteps=5000, episode_reward=312.80 +/- 39.06
Episode length: 103.20 +/- 9.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 103       |
|    mean_reward          | 313       |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 0.4843437 |
|    clip_fraction        | 0.666     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.899    |
|    explained_variance   | 0.179     |
|    learning_rate        | 0.00164   |
|    loss                 | 112       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.0879    |
|    value_loss           | 322       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 411      |
| time/              |          |
|    fps             | 921      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 121       |
|    ep_rew_mean          | 384       |
| time/                   |           |
|    fps                  | 919       |
|    iterations           | 4         |
|    time_elapsed         | 8         |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 1.3239275 |
|    clip_fraction        | 0.669     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.625    |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.00164   |
|    loss                 | 106       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.0849    |
|    value_loss           | 468       |
---------------------------------------
Eval num_timesteps=10000, episode_reward=309.60 +/- 39.97
Episode length: 102.40 +/- 9.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 102        |
|    mean_reward          | 310        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 11.8140955 |
|    clip_fraction        | 0.873      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.0916     |
|    learning_rate        | 0.00164    |
|    loss                 | 368        |
|    n_updates            | 40         |
|    policy_gradient_loss | 0.21       |
|    value_loss           | 814        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 869      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 114       |
|    ep_rew_mean          | 356       |
| time/                   |           |
|    fps                  | 877       |
|    iterations           | 6         |
|    time_elapsed         | 13        |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.33e-09 |
|    explained_variance   | 0.0814    |
|    learning_rate        | 0.00164   |
|    loss                 | 689       |
|    n_updates            | 50        |
|    policy_gradient_loss | 3.43e-10  |
|    value_loss           | 947       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 111       |
|    ep_rew_mean          | 344       |
| time/                   |           |
|    fps                  | 884       |
|    iterations           | 7         |
|    time_elapsed         | 16        |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.07e-11 |
|    explained_variance   | 0.123     |
|    learning_rate        | 0.00164   |
|    loss                 | 245       |
|    n_updates            | 60        |
|    policy_gradient_loss | 4.5e-10   |
|    value_loss           | 1.07e+03  |
---------------------------------------
Eval num_timesteps=15000, episode_reward=325.60 +/- 55.52
Episode length: 106.40 +/- 13.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 106       |
|    mean_reward          | 326       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.13e-11 |
|    explained_variance   | 0.164     |
|    learning_rate        | 0.00164   |
|    loss                 | 241       |
|    n_updates            | 70        |
|    policy_gradient_loss | 7.86e-10  |
|    value_loss           | 1.13e+03  |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 857      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 106       |
|    ep_rew_mean          | 323       |
| time/                   |           |
|    fps                  | 864       |
|    iterations           | 9         |
|    time_elapsed         | 21        |
|    total_timesteps      | 18432     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.74e-12 |
|    explained_variance   | 0.139     |
|    learning_rate        | 0.00164   |
|    loss                 | 345       |
|    n_updates            | 80        |
|    policy_gradient_loss | -8.85e-10 |
|    value_loss           | 1.19e+03  |
---------------------------------------
Eval num_timesteps=20000, episode_reward=325.60 +/- 38.00
Episode length: 106.40 +/- 9.50
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 106      |
|    mean_reward          | 326      |
| time/                   |          |
|    total_timesteps      | 20000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.2e-13 |
|    explained_variance   | 0.157    |
|    learning_rate        | 0.00164  |
|    loss                 | 377      |
|    n_updates            | 90       |
|    policy_gradient_loss | 3.33e-10 |
|    value_loss           | 1.27e+03 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 323      |
| time/              |          |
|    fps             | 846      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 105       |
|    ep_rew_mean          | 321       |
| time/                   |           |
|    fps                  | 854       |
|    iterations           | 11        |
|    time_elapsed         | 26        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.39e-12 |
|    explained_variance   | 0.182     |
|    learning_rate        | 0.00164   |
|    loss                 | 893       |
|    n_updates            | 100       |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 1.34e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 104       |
|    ep_rew_mean          | 316       |
| time/                   |           |
|    fps                  | 860       |
|    iterations           | 12        |
|    time_elapsed         | 28        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.46e-12 |
|    explained_variance   | 0.17      |
|    learning_rate        | 0.00164   |
|    loss                 | 715       |
|    n_updates            | 110       |
|    policy_gradient_loss | -3.75e-10 |
|    value_loss           | 1.34e+03  |
---------------------------------------
Eval num_timesteps=25000, episode_reward=296.80 +/- 21.23
Episode length: 99.20 +/- 5.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 99.2      |
|    mean_reward          | 297       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5e-12    |
|    explained_variance   | 0.164     |
|    learning_rate        | 0.00164   |
|    loss                 | 281       |
|    n_updates            | 120       |
|    policy_gradient_loss | -8.73e-11 |
|    value_loss           | 1.28e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 847      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 103      |
|    ep_rew_mean          | 313      |
| time/                   |          |
|    fps                  | 852      |
|    iterations           | 14       |
|    time_elapsed         | 33       |
|    total_timesteps      | 28672    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -7.8e-13 |
|    explained_variance   | 0.159    |
|    learning_rate        | 0.00164  |
|    loss                 | 324      |
|    n_updates            | 130      |
|    policy_gradient_loss | 9.92e-10 |
|    value_loss           | 1.33e+03 |
--------------------------------------
Eval num_timesteps=30000, episode_reward=328.80 +/- 32.63
Episode length: 107.20 +/- 8.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 107       |
|    mean_reward          | 329       |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.66e-13 |
|    explained_variance   | 0.138     |
|    learning_rate        | 0.00164   |
|    loss                 | 1.1e+03   |
|    n_updates            | 140       |
|    policy_gradient_loss | 5.97e-10  |
|    value_loss           | 1.42e+03  |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 841      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 105       |
|    ep_rew_mean          | 320       |
| time/                   |           |
|    fps                  | 846       |
|    iterations           | 16        |
|    time_elapsed         | 38        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.79e-13 |
|    explained_variance   | 0.159     |
|    learning_rate        | 0.00164   |
|    loss                 | 771       |
|    n_updates            | 150       |
|    policy_gradient_loss | -3.22e-10 |
|    value_loss           | 1.34e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 105       |
|    ep_rew_mean          | 321       |
| time/                   |           |
|    fps                  | 846       |
|    iterations           | 17        |
|    time_elapsed         | 41        |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.97e-12 |
|    explained_variance   | 0.168     |
|    learning_rate        | 0.00164   |
|    loss                 | 555       |
|    n_updates            | 160       |
|    policy_gradient_loss | -6.23e-10 |
|    value_loss           | 1.35e+03  |
---------------------------------------
Eval num_timesteps=35000, episode_reward=309.60 +/- 37.32
Episode length: 102.40 +/- 9.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 310       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.57e-13 |
|    explained_variance   | 0.159     |
|    learning_rate        | 0.00164   |
|    loss                 | 1.33e+03  |
|    n_updates            | 170       |
|    policy_gradient_loss | -1.11e-09 |
|    value_loss           | 1.52e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 838      |
|    iterations      | 18       |
|    time_elapsed    | 43       |
|    total_timesteps | 36864    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 104       |
|    ep_rew_mean          | 316       |
| time/                   |           |
|    fps                  | 843       |
|    iterations           | 19        |
|    time_elapsed         | 46        |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.05e-12 |
|    explained_variance   | 0.198     |
|    learning_rate        | 0.00164   |
|    loss                 | 311       |
|    n_updates            | 180       |
|    policy_gradient_loss | 1.02e-09  |
|    value_loss           | 1.55e+03  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=303.20 +/- 29.33
Episode length: 100.80 +/- 7.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 101       |
|    mean_reward          | 303       |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.39e-13 |
|    explained_variance   | 0.194     |
|    learning_rate        | 0.00164   |
|    loss                 | 430       |
|    n_updates            | 190       |
|    policy_gradient_loss | 2.44e-10  |
|    value_loss           | 1.53e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 309      |
| time/              |          |
|    fps             | 836      |
|    iterations      | 20       |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 103       |
|    ep_rew_mean          | 311       |
| time/                   |           |
|    fps                  | 839       |
|    iterations           | 21        |
|    time_elapsed         | 51        |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.45e-13 |
|    explained_variance   | 0.195     |
|    learning_rate        | 0.00164   |
|    loss                 | 180       |
|    n_updates            | 200       |
|    policy_gradient_loss | 6.69e-10  |
|    value_loss           | 1.55e+03  |
---------------------------------------
Eval num_timesteps=45000, episode_reward=306.40 +/- 24.99
Episode length: 101.60 +/- 6.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 306       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.16e-13 |
|    explained_variance   | 0.178     |
|    learning_rate        | 0.00164   |
|    loss                 | 573       |
|    n_updates            | 210       |
|    policy_gradient_loss | -6.64e-10 |
|    value_loss           | 1.6e+03   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 833      |
|    iterations      | 22       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 104       |
|    ep_rew_mean          | 317       |
| time/                   |           |
|    fps                  | 837       |
|    iterations           | 23        |
|    time_elapsed         | 56        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.06e-13 |
|    explained_variance   | 0.161     |
|    learning_rate        | 0.00164   |
|    loss                 | 648       |
|    n_updates            | 220       |
|    policy_gradient_loss | 5.82e-10  |
|    value_loss           | 1.67e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 104       |
|    ep_rew_mean          | 316       |
| time/                   |           |
|    fps                  | 840       |
|    iterations           | 24        |
|    time_elapsed         | 58        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.72e-14 |
|    explained_variance   | 0.147     |
|    learning_rate        | 0.00164   |
|    loss                 | 871       |
|    n_updates            | 230       |
|    policy_gradient_loss | -4.07e-11 |
|    value_loss           | 1.82e+03  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=300.00 +/- 21.47
Episode length: 100.00 +/- 5.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 100       |
|    mean_reward          | 300       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.89e-13 |
|    explained_variance   | 0.184     |
|    learning_rate        | 0.00164   |
|    loss                 | 974       |
|    n_updates            | 240       |
|    policy_gradient_loss | -6e-10    |
|    value_loss           | 1.72e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 834      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 105      |
|    ep_rew_mean          | 321      |
| time/                   |          |
|    fps                  | 838      |
|    iterations           | 26       |
|    time_elapsed         | 63       |
|    total_timesteps      | 53248    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -5.2e-13 |
|    explained_variance   | 0.163    |
|    learning_rate        | 0.00164  |
|    loss                 | 1.31e+03 |
|    n_updates            | 250      |
|    policy_gradient_loss | 6.37e-10 |
|    value_loss           | 1.59e+03 |
--------------------------------------
Eval num_timesteps=55000, episode_reward=316.00 +/- 37.86
Episode length: 104.00 +/- 9.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 104       |
|    mean_reward          | 316       |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.36e-12 |
|    explained_variance   | 0.176     |
|    learning_rate        | 0.00164   |
|    loss                 | 1.22e+03  |
|    n_updates            | 260       |
|    policy_gradient_loss | 7.13e-10  |
|    value_loss           | 1.67e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 833      |
|    iterations      | 27       |
|    time_elapsed    | 66       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 104       |
|    ep_rew_mean          | 315       |
| time/                   |           |
|    fps                  | 836       |
|    iterations           | 28        |
|    time_elapsed         | 68        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.85e-12 |
|    explained_variance   | 0.176     |
|    learning_rate        | 0.00164   |
|    loss                 | 513       |
|    n_updates            | 270       |
|    policy_gradient_loss | 9.84e-10  |
|    value_loss           | 1.73e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 104       |
|    ep_rew_mean          | 317       |
| time/                   |           |
|    fps                  | 839       |
|    iterations           | 29        |
|    time_elapsed         | 70        |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.51e-12 |
|    explained_variance   | 0.175     |
|    learning_rate        | 0.00164   |
|    loss                 | 360       |
|    n_updates            | 280       |
|    policy_gradient_loss | 9.92e-10  |
|    value_loss           | 1.62e+03  |
---------------------------------------
Eval num_timesteps=60000, episode_reward=309.60 +/- 34.47
Episode length: 102.40 +/- 8.62
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 102      |
|    mean_reward          | 310      |
| time/                   |          |
|    total_timesteps      | 60000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.8e-12 |
|    explained_variance   | 0.202    |
|    learning_rate        | 0.00164  |
|    loss                 | 270      |
|    n_updates            | 290      |
|    policy_gradient_loss | 1.3e-09  |
|    value_loss           | 1.6e+03  |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 834      |
|    iterations      | 30       |
|    time_elapsed    | 73       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:38:53,355] Trial 20 finished with value: 325.6 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0016402624754519911, 'gamma': 0.9886061952911697, 'gae_lambda': 0.8675690459047773}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 1269     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | 387         |
| time/                   |             |
|    fps                  | 1066        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.035998072 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.00117     |
|    learning_rate        | 0.000477    |
|    loss                 | 311         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.00631     |
|    value_loss           | 511         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.02550456 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.000477   |
|    loss                 | 102        |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.00967    |
|    value_loss           | 590        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 914      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 126         |
|    ep_rew_mean          | 406         |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.053694587 |
|    clip_fraction        | 0.46        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.000477    |
|    loss                 | 264         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.0121      |
|    value_loss           | 753         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.029166052 |
|    clip_fraction        | 0.42        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000477    |
|    loss                 | 186         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.0149      |
|    value_loss           | 666         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 874      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 124        |
|    ep_rew_mean          | 396        |
| time/                   |            |
|    fps                  | 885        |
|    iterations           | 6          |
|    time_elapsed         | 13         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.02990627 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | 0.545      |
|    learning_rate        | 0.000477   |
|    loss                 | 397        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.000319  |
|    value_loss           | 713        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 121       |
|    ep_rew_mean          | 385       |
| time/                   |           |
|    fps                  | 889       |
|    iterations           | 7         |
|    time_elapsed         | 16        |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 0.0876804 |
|    clip_fraction        | 0.344     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.14     |
|    explained_variance   | 0.511     |
|    learning_rate        | 0.000477  |
|    loss                 | 213       |
|    n_updates            | 60        |
|    policy_gradient_loss | 0.00425   |
|    value_loss           | 693       |
---------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 1.2288319 |
|    clip_fraction        | 0.667     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.431    |
|    explained_variance   | 0.207     |
|    learning_rate        | 0.000477  |
|    loss                 | 210       |
|    n_updates            | 70        |
|    policy_gradient_loss | 0.0444    |
|    value_loss           | 978       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 864      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 112        |
|    ep_rew_mean          | 347        |
| time/                   |            |
|    fps                  | 871        |
|    iterations           | 9          |
|    time_elapsed         | 21         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.03971963 |
|    clip_fraction        | 0.0318     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.125     |
|    explained_variance   | 0.0954     |
|    learning_rate        | 0.000477   |
|    loss                 | 939        |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.00978   |
|    value_loss           | 1.99e+03   |
----------------------------------------
Eval num_timesteps=20000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.4       |
|    mean_reward          | 294        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.45226538 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.177     |
|    explained_variance   | 0.0467     |
|    learning_rate        | 0.000477   |
|    loss                 | 246        |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.00163    |
|    value_loss           | 1.34e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 338      |
| time/              |          |
|    fps             | 854      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 107        |
|    ep_rew_mean          | 329        |
| time/                   |            |
|    fps                  | 860        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.13366751 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.277     |
|    explained_variance   | 0.291      |
|    learning_rate        | 0.000477   |
|    loss                 | 221        |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.0753     |
|    value_loss           | 1.43e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | 315         |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 12          |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.034290493 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.524       |
|    learning_rate        | 0.000477    |
|    loss                 | 478         |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00883     |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.017505236 |
|    clip_fraction        | 0.0134      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0586     |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.000477    |
|    loss                 | 892         |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.000244    |
|    value_loss           | 2.37e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 311      |
| time/              |          |
|    fps             | 852      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | 312         |
| time/                   |             |
|    fps                  | 856         |
|    iterations           | 14          |
|    time_elapsed         | 33          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.049036477 |
|    clip_fraction        | 0.0825      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.000477    |
|    loss                 | 928         |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.0154      |
|    value_loss           | 1.89e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.011928607 |
|    clip_fraction        | 0.0296      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0825     |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.000477    |
|    loss                 | 1.63e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00626    |
|    value_loss           | 1.73e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 306      |
| time/              |          |
|    fps             | 845      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 98.6       |
|    ep_rew_mean          | 295        |
| time/                   |            |
|    fps                  | 850        |
|    iterations           | 16         |
|    time_elapsed         | 38         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.13749436 |
|    clip_fraction        | 0.0546     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.276      |
|    learning_rate        | 0.000477   |
|    loss                 | 1.1e+03    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 1.42e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 98.6        |
|    ep_rew_mean          | 295         |
| time/                   |             |
|    fps                  | 854         |
|    iterations           | 17          |
|    time_elapsed         | 40          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.093152605 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.244       |
|    learning_rate        | 0.000477    |
|    loss                 | 526         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00932    |
|    value_loss           | 2.07e+03    |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.17178373 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.182     |
|    explained_variance   | 0.311      |
|    learning_rate        | 0.000477   |
|    loss                 | 353        |
|    n_updates            | 170        |
|    policy_gradient_loss | 0.00412    |
|    value_loss           | 1.76e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.1     |
|    ep_rew_mean     | 288      |
| time/              |          |
|    fps             | 845      |
|    iterations      | 18       |
|    time_elapsed    | 43       |
|    total_timesteps | 36864    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96.2      |
|    ep_rew_mean          | 285       |
| time/                   |           |
|    fps                  | 848       |
|    iterations           | 19        |
|    time_elapsed         | 45        |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.5905746 |
|    clip_fraction        | 0.145     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0603   |
|    explained_variance   | 0.214     |
|    learning_rate        | 0.000477  |
|    loss                 | 344       |
|    n_updates            | 180       |
|    policy_gradient_loss | 0.0711    |
|    value_loss           | 1.54e+03  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.06883537 |
|    clip_fraction        | 0.0411     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0483    |
|    explained_variance   | 0.41       |
|    learning_rate        | 0.000477   |
|    loss                 | 761        |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.00922   |
|    value_loss           | 2.05e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 840      |
|    iterations      | 20       |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 96         |
|    ep_rew_mean          | 284        |
| time/                   |            |
|    fps                  | 843        |
|    iterations           | 21         |
|    time_elapsed         | 50         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.05923023 |
|    clip_fraction        | 0.0457     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0469    |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.000477   |
|    loss                 | 435        |
|    n_updates            | 200        |
|    policy_gradient_loss | 0.0116     |
|    value_loss           | 1.46e+03   |
----------------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.07383977 |
|    clip_fraction        | 0.0607     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0708    |
|    explained_variance   | 0.449      |
|    learning_rate        | 0.000477   |
|    loss                 | 529        |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0079    |
|    value_loss           | 1.27e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 837      |
|    iterations      | 22       |
|    time_elapsed    | 53       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 96          |
|    ep_rew_mean          | 284         |
| time/                   |             |
|    fps                  | 839         |
|    iterations           | 23          |
|    time_elapsed         | 56          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.087109745 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00278    |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.000477    |
|    loss                 | 963         |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.00855     |
|    value_loss           | 1.61e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96           |
|    ep_rew_mean          | 284          |
| time/                   |              |
|    fps                  | 842          |
|    iterations           | 24           |
|    time_elapsed         | 58           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0035049529 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00888     |
|    explained_variance   | 0.445        |
|    learning_rate        | 0.000477     |
|    loss                 | 276          |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 1.45e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0033305364 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00397     |
|    explained_variance   | 0.408        |
|    learning_rate        | 0.000477     |
|    loss                 | 787          |
|    n_updates            | 240          |
|    policy_gradient_loss | 0.000755     |
|    value_loss           | 1.47e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 837      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96           |
|    ep_rew_mean          | 284          |
| time/                   |              |
|    fps                  | 840          |
|    iterations           | 26           |
|    time_elapsed         | 63           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0018082121 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00133     |
|    explained_variance   | 0.506        |
|    learning_rate        | 0.000477     |
|    loss                 | 217          |
|    n_updates            | 250          |
|    policy_gradient_loss | -5.98e-05    |
|    value_loss           | 1.45e+03     |
------------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 2.681103e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000865    |
|    explained_variance   | 0.545        |
|    learning_rate        | 0.000477     |
|    loss                 | 406          |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000167    |
|    value_loss           | 1.28e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 834      |
|    iterations      | 27       |
|    time_elapsed    | 66       |
|    total_timesteps | 55296    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 96            |
|    ep_rew_mean          | 284           |
| time/                   |               |
|    fps                  | 837           |
|    iterations           | 28            |
|    time_elapsed         | 68            |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 9.5739146e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000993     |
|    explained_variance   | 0.565         |
|    learning_rate        | 0.000477      |
|    loss                 | 392           |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.000996     |
|    value_loss           | 1.16e+03      |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 96          |
|    ep_rew_mean          | 284         |
| time/                   |             |
|    fps                  | 839         |
|    iterations           | 29          |
|    time_elapsed         | 70          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.008013979 |
|    clip_fraction        | 0.00273     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00341    |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.000477    |
|    loss                 | 500         |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.00343     |
|    value_loss           | 1.36e+03    |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 0.00022812522 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00363      |
|    explained_variance   | 0.607         |
|    learning_rate        | 0.000477      |
|    loss                 | 509           |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.000728     |
|    value_loss           | 1.17e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 834      |
|    iterations      | 30       |
|    time_elapsed    | 73       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:40:08,476] Trial 21 finished with value: 284.0 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0004774771670800777, 'gamma': 0.9850967508385117, 'gae_lambda': 0.9061699839076136}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 1256     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 338         |
| time/                   |             |
|    fps                  | 1055        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.120353445 |
|    clip_fraction        | 0.632       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | -0.00378    |
|    learning_rate        | 0.00182     |
|    loss                 | 300         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0534      |
|    value_loss           | 343         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 98.4      |
|    mean_reward          | 294       |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 4.8217416 |
|    clip_fraction        | 0.943     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.414    |
|    explained_variance   | 0.0461    |
|    learning_rate        | 0.00182   |
|    loss                 | 173       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.253     |
|    value_loss           | 528       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 907      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 121       |
|    ep_rew_mean          | 387       |
| time/                   |           |
|    fps                  | 902       |
|    iterations           | 4         |
|    time_elapsed         | 9         |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 1.9517087 |
|    clip_fraction        | 0.828     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.116    |
|    explained_variance   | -0.573    |
|    learning_rate        | 0.00182   |
|    loss                 | 163       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.21      |
|    value_loss           | 468       |
---------------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 2.3156786 |
|    clip_fraction        | 0.473     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0871   |
|    explained_variance   | -0.0952   |
|    learning_rate        | 0.00182   |
|    loss                 | 130       |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.034     |
|    value_loss           | 750       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 858      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 111       |
|    ep_rew_mean          | 347       |
| time/                   |           |
|    fps                  | 866       |
|    iterations           | 6         |
|    time_elapsed         | 14        |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 0.7656267 |
|    clip_fraction        | 0.15      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0918   |
|    explained_variance   | -0.083    |
|    learning_rate        | 0.00182   |
|    loss                 | 279       |
|    n_updates            | 50        |
|    policy_gradient_loss | 0.00745   |
|    value_loss           | 656       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 108       |
|    ep_rew_mean          | 335       |
| time/                   |           |
|    fps                  | 872       |
|    iterations           | 7         |
|    time_elapsed         | 16        |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 2.8162017 |
|    clip_fraction        | 0.228     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0264   |
|    explained_variance   | 0.212     |
|    learning_rate        | 0.00182   |
|    loss                 | 242       |
|    n_updates            | 60        |
|    policy_gradient_loss | 0.125     |
|    value_loss           | 665       |
---------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 15000    |
| train/                  |          |
|    approx_kl            | 0.712109 |
|    clip_fraction        | 0.0855   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0308  |
|    explained_variance   | 0.518    |
|    learning_rate        | 0.00182  |
|    loss                 | 16.3     |
|    n_updates            | 70       |
|    policy_gradient_loss | -0.0368  |
|    value_loss           | 200      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.8     |
|    ep_rew_mean     | 299      |
| time/              |          |
|    fps             | 849      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 856       |
|    iterations           | 9         |
|    time_elapsed         | 21        |
|    total_timesteps      | 18432     |
| train/                  |           |
|    approx_kl            | 1.1464119 |
|    clip_fraction        | 0.0894    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00886  |
|    explained_variance   | 0.765     |
|    learning_rate        | 0.00182   |
|    loss                 | 134       |
|    n_updates            | 80        |
|    policy_gradient_loss | 0.081     |
|    value_loss           | 242       |
---------------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 5.529728e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.68e-06    |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.00182      |
|    loss                 | 7.25         |
|    n_updates            | 90           |
|    policy_gradient_loss | -1.69e-06    |
|    value_loss           | 49.4         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 839      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 96            |
|    ep_rew_mean          | 284           |
| time/                   |               |
|    fps                  | 846           |
|    iterations           | 11            |
|    time_elapsed         | 26            |
|    total_timesteps      | 22528         |
| train/                  |               |
|    approx_kl            | -2.910383e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.55e-05     |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.00182       |
|    loss                 | 8.58          |
|    n_updates            | 100           |
|    policy_gradient_loss | -5.37e-07     |
|    value_loss           | 26.4          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 96             |
|    ep_rew_mean          | 284            |
| time/                   |                |
|    fps                  | 851            |
|    iterations           | 12             |
|    time_elapsed         | 28             |
|    total_timesteps      | 24576          |
| train/                  |                |
|    approx_kl            | -2.3283064e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.95e-06      |
|    explained_variance   | 0.986          |
|    learning_rate        | 0.00182        |
|    loss                 | 5.67           |
|    n_updates            | 110            |
|    policy_gradient_loss | -2.94e-08      |
|    value_loss           | 15.1           |
--------------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 25000         |
| train/                  |               |
|    approx_kl            | 1.1641532e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.82e-06     |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.00182       |
|    loss                 | 6.84          |
|    n_updates            | 120           |
|    policy_gradient_loss | 2.83e-08      |
|    value_loss           | 18.8          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 839      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96           |
|    ep_rew_mean          | 284          |
| time/                   |              |
|    fps                  | 843          |
|    iterations           | 14           |
|    time_elapsed         | 33           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 1.193257e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.2e-05     |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.00182      |
|    loss                 | 32.9         |
|    n_updates            | 130          |
|    policy_gradient_loss | 2.01e-07     |
|    value_loss           | 21.3         |
------------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 4.947651e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.48e-05    |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.00182      |
|    loss                 | 2.99         |
|    n_updates            | 140          |
|    policy_gradient_loss | 5.02e-07     |
|    value_loss           | 15.9         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 833      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 96            |
|    ep_rew_mean          | 284           |
| time/                   |               |
|    fps                  | 838           |
|    iterations           | 16            |
|    time_elapsed         | 39            |
|    total_timesteps      | 32768         |
| train/                  |               |
|    approx_kl            | 4.3655746e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.87e-05     |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.00182       |
|    loss                 | 2.19          |
|    n_updates            | 150           |
|    policy_gradient_loss | 2.5e-06       |
|    value_loss           | 21.2          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 96             |
|    ep_rew_mean          | 284            |
| time/                   |                |
|    fps                  | 842            |
|    iterations           | 17             |
|    time_elapsed         | 41             |
|    total_timesteps      | 34816          |
| train/                  |                |
|    approx_kl            | -3.4924597e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.21e-05      |
|    explained_variance   | 0.999          |
|    learning_rate        | 0.00182        |
|    loss                 | 3.3            |
|    n_updates            | 160            |
|    policy_gradient_loss | 4.51e-07       |
|    value_loss           | 10.6           |
--------------------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 35000         |
| train/                  |               |
|    approx_kl            | -4.656613e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.03e-05     |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.00182       |
|    loss                 | 1.9           |
|    n_updates            | 170           |
|    policy_gradient_loss | 1.38e-06      |
|    value_loss           | 8.38          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 833      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 96            |
|    ep_rew_mean          | 284           |
| time/                   |               |
|    fps                  | 837           |
|    iterations           | 19            |
|    time_elapsed         | 46            |
|    total_timesteps      | 38912         |
| train/                  |               |
|    approx_kl            | 1.1641532e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.45e-05     |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.00182       |
|    loss                 | 2.65          |
|    n_updates            | 180           |
|    policy_gradient_loss | -1.66e-07     |
|    value_loss           | 9.14          |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 40000         |
| train/                  |               |
|    approx_kl            | 1.7462298e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.82e-05     |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.00182       |
|    loss                 | 2.2           |
|    n_updates            | 190           |
|    policy_gradient_loss | 2.74e-07      |
|    value_loss           | 6.47          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 830      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96           |
|    ep_rew_mean          | 284          |
| time/                   |              |
|    fps                  | 833          |
|    iterations           | 21           |
|    time_elapsed         | 51           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 5.820766e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79e-05    |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.00182      |
|    loss                 | 0.973        |
|    n_updates            | 200          |
|    policy_gradient_loss | 1.38e-06     |
|    value_loss           | 7.49         |
------------------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 45000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.4e-05 |
|    explained_variance   | 0.999    |
|    learning_rate        | 0.00182  |
|    loss                 | 2.13     |
|    n_updates            | 210      |
|    policy_gradient_loss | 1.31e-07 |
|    value_loss           | 4.71     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 827      |
|    iterations      | 22       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 96            |
|    ep_rew_mean          | 284           |
| time/                   |               |
|    fps                  | 830           |
|    iterations           | 23            |
|    time_elapsed         | 56            |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 5.2386895e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.35e-05     |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.00182       |
|    loss                 | 2.08          |
|    n_updates            | 220           |
|    policy_gradient_loss | 3.08e-07      |
|    value_loss           | 3.98          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 96            |
|    ep_rew_mean          | 284           |
| time/                   |               |
|    fps                  | 834           |
|    iterations           | 24            |
|    time_elapsed         | 58            |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | -8.731149e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.17e-05     |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.00182       |
|    loss                 | 0.855         |
|    n_updates            | 230           |
|    policy_gradient_loss | -4.14e-08     |
|    value_loss           | 3.6           |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 96             |
|    mean_reward          | 284            |
| time/                   |                |
|    total_timesteps      | 50000          |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.56e-05      |
|    explained_variance   | 1              |
|    learning_rate        | 0.00182        |
|    loss                 | 1.28           |
|    n_updates            | 240            |
|    policy_gradient_loss | 4.27e-08       |
|    value_loss           | 3.79           |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 828      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 96             |
|    ep_rew_mean          | 284            |
| time/                   |                |
|    fps                  | 831            |
|    iterations           | 26             |
|    time_elapsed         | 64             |
|    total_timesteps      | 53248          |
| train/                  |                |
|    approx_kl            | -4.0745363e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.39e-05      |
|    explained_variance   | 1              |
|    learning_rate        | 0.00182        |
|    loss                 | 2.69           |
|    n_updates            | 250            |
|    policy_gradient_loss | 4.45e-08       |
|    value_loss           | 3.67           |
--------------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.9e-06  |
|    explained_variance   | 1         |
|    learning_rate        | 0.00182   |
|    loss                 | 1.44      |
|    n_updates            | 260       |
|    policy_gradient_loss | -9.12e-08 |
|    value_loss           | 2.84      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 826      |
|    iterations      | 27       |
|    time_elapsed    | 66       |
|    total_timesteps | 55296    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 96            |
|    ep_rew_mean          | 284           |
| time/                   |               |
|    fps                  | 828           |
|    iterations           | 28            |
|    time_elapsed         | 69            |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 1.4551915e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.27e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 0.00182       |
|    loss                 | 0.318         |
|    n_updates            | 270           |
|    policy_gradient_loss | 3.79e-08      |
|    value_loss           | 5.53          |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 96            |
|    ep_rew_mean          | 284           |
| time/                   |               |
|    fps                  | 831           |
|    iterations           | 29            |
|    time_elapsed         | 71            |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | -2.910383e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.33e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 0.00182       |
|    loss                 | 0.645         |
|    n_updates            | 280           |
|    policy_gradient_loss | -7.8e-08      |
|    value_loss           | 2.83          |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.56e-06 |
|    explained_variance   | 1         |
|    learning_rate        | 0.00182   |
|    loss                 | 1.32      |
|    n_updates            | 290       |
|    policy_gradient_loss | -1.28e-07 |
|    value_loss           | 2.59      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 826      |
|    iterations      | 30       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:41:24,305] Trial 22 finished with value: 284.0 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0018173339916398527, 'gamma': 0.9914116789682367, 'gae_lambda': 0.8763964742435248}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 1183     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 347         |
| time/                   |             |
|    fps                  | 1029        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.039949786 |
|    clip_fraction        | 0.421       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | -0.000812   |
|    learning_rate        | 0.000409    |
|    loss                 | 83.1        |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0149      |
|    value_loss           | 532         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.017103756 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.0303      |
|    learning_rate        | 0.000409    |
|    loss                 | 506         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 734         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 340      |
| time/              |          |
|    fps             | 895      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 342         |
| time/                   |             |
|    fps                  | 901         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.025182491 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.199       |
|    learning_rate        | 0.000409    |
|    loss                 | 364         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.000863    |
|    value_loss           | 784         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.013458885 |
|    clip_fraction        | 0.364       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.000409    |
|    loss                 | 491         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00961     |
|    value_loss           | 981         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 338      |
| time/              |          |
|    fps             | 856      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 334         |
| time/                   |             |
|    fps                  | 866         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.021009209 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.13        |
|    learning_rate        | 0.000409    |
|    loss                 | 203         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00379     |
|    value_loss           | 1.1e+03     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 107         |
|    ep_rew_mean          | 328         |
| time/                   |             |
|    fps                  | 872         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.026979908 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.114       |
|    learning_rate        | 0.000409    |
|    loss                 | 446         |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00572     |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.017889079 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.000409    |
|    loss                 | 531         |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.00364     |
|    value_loss           | 1.54e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 326      |
| time/              |          |
|    fps             | 849      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | 317         |
| time/                   |             |
|    fps                  | 856         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.020691881 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.164       |
|    learning_rate        | 0.000409    |
|    loss                 | 917         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00693     |
|    value_loss           | 1.41e+03    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.016384559 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.903      |
|    explained_variance   | 0.174       |
|    learning_rate        | 0.000409    |
|    loss                 | 308         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00415     |
|    value_loss           | 1.49e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 840      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 104        |
|    ep_rew_mean          | 318        |
| time/                   |            |
|    fps                  | 846        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.04737182 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.439      |
|    learning_rate        | 0.000409   |
|    loss                 | 679        |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.00228    |
|    value_loss           | 1.24e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 323         |
| time/                   |             |
|    fps                  | 851         |
|    iterations           | 12          |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.036731593 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.961      |
|    explained_variance   | 0.434       |
|    learning_rate        | 0.000409    |
|    loss                 | 249         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00153    |
|    value_loss           | 964         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0555142 |
|    clip_fraction        | 0.298     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.813    |
|    explained_variance   | 0.326     |
|    learning_rate        | 0.000409  |
|    loss                 | 732       |
|    n_updates            | 120       |
|    policy_gradient_loss | 0.00905   |
|    value_loss           | 1.36e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 323      |
| time/              |          |
|    fps             | 838      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 319         |
| time/                   |             |
|    fps                  | 842         |
|    iterations           | 14          |
|    time_elapsed         | 34          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.012397548 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.101       |
|    learning_rate        | 0.000409    |
|    loss                 | 518         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 1.44e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.052531816 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.86       |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.000409    |
|    loss                 | 283         |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.0199      |
|    value_loss           | 1.1e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 832      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | 314         |
| time/                   |             |
|    fps                  | 837         |
|    iterations           | 16          |
|    time_elapsed         | 39          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.031774666 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.829      |
|    explained_variance   | 0.251       |
|    learning_rate        | 0.000409    |
|    loss                 | 168         |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.00131     |
|    value_loss           | 1.11e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 307         |
| time/                   |             |
|    fps                  | 841         |
|    iterations           | 17          |
|    time_elapsed         | 41          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.015374808 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.000409    |
|    loss                 | 185         |
|    n_updates            | 160         |
|    policy_gradient_loss | 0.00195     |
|    value_loss           | 1.13e+03    |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.030346531 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.774      |
|    explained_variance   | 0.165       |
|    learning_rate        | 0.000409    |
|    loss                 | 645         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00535    |
|    value_loss           | 1.4e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 833      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 103        |
|    ep_rew_mean          | 312        |
| time/                   |            |
|    fps                  | 837        |
|    iterations           | 19         |
|    time_elapsed         | 46         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.19813038 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.317      |
|    learning_rate        | 0.000409   |
|    loss                 | 175        |
|    n_updates            | 180        |
|    policy_gradient_loss | 0.0518     |
|    value_loss           | 1.07e+03   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.024566324 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.0823      |
|    learning_rate        | 0.000409    |
|    loss                 | 415         |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00454    |
|    value_loss           | 1.37e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 830      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 105        |
|    ep_rew_mean          | 320        |
| time/                   |            |
|    fps                  | 834        |
|    iterations           | 21         |
|    time_elapsed         | 51         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.11300269 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.732     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.000409   |
|    loss                 | 236        |
|    n_updates            | 200        |
|    policy_gradient_loss | 0.0446     |
|    value_loss           | 786        |
----------------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.023963787 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.712      |
|    explained_variance   | 0.213       |
|    learning_rate        | 0.000409    |
|    loss                 | 436         |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00237    |
|    value_loss           | 1.16e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 827      |
|    iterations      | 22       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | 318         |
| time/                   |             |
|    fps                  | 831         |
|    iterations           | 23          |
|    time_elapsed         | 56          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.064186096 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.673      |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000409    |
|    loss                 | 156         |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.000564    |
|    value_loss           | 931         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 331         |
| time/                   |             |
|    fps                  | 834         |
|    iterations           | 24          |
|    time_elapsed         | 58          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.035086818 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.000409    |
|    loss                 | 341         |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.0216      |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=303.20 +/- 57.60
Episode length: 100.80 +/- 14.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 101        |
|    mean_reward          | 303        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.02334234 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.538     |
|    explained_variance   | 0.371      |
|    learning_rate        | 0.000409   |
|    loss                 | 336        |
|    n_updates            | 240        |
|    policy_gradient_loss | 0.0078     |
|    value_loss           | 829        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 344      |
| time/              |          |
|    fps             | 828      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 352         |
| time/                   |             |
|    fps                  | 832         |
|    iterations           | 26          |
|    time_elapsed         | 63          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.026211236 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.473      |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.000409    |
|    loss                 | 212         |
|    n_updates            | 250         |
|    policy_gradient_loss | 0.00297     |
|    value_loss           | 670         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=332.00 +/- 98.37
Episode length: 108.00 +/- 24.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 332         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.013895528 |
|    clip_fraction        | 0.0785      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.311      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000409    |
|    loss                 | 317         |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.00976     |
|    value_loss           | 911         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 368      |
| time/              |          |
|    fps             | 826      |
|    iterations      | 27       |
|    time_elapsed    | 66       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | 369         |
| time/                   |             |
|    fps                  | 830         |
|    iterations           | 28          |
|    time_elapsed         | 69          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.035474375 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000409    |
|    loss                 | 507         |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.00341     |
|    value_loss           | 992         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 121        |
|    ep_rew_mean          | 383        |
| time/                   |            |
|    fps                  | 833        |
|    iterations           | 29         |
|    time_elapsed         | 71         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.07376537 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.393     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000409   |
|    loss                 | 442        |
|    n_updates            | 280        |
|    policy_gradient_loss | 0.00895    |
|    value_loss           | 1.09e+03   |
----------------------------------------
Eval num_timesteps=60000, episode_reward=332.00 +/- 77.40
Episode length: 108.00 +/- 19.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 108        |
|    mean_reward          | 332        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.04576975 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.398     |
|    explained_variance   | 0.384      |
|    learning_rate        | 0.000409   |
|    loss                 | 382        |
|    n_updates            | 290        |
|    policy_gradient_loss | 0.00745    |
|    value_loss           | 979        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 828      |
|    iterations      | 30       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:42:40,011] Trial 23 finished with value: 284.0 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.00040927249691193203, 'gamma': 0.9823760596001072, 'gae_lambda': 0.9096834617528937}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 1271     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 124         |
|    ep_rew_mean          | 397         |
| time/                   |             |
|    fps                  | 1065        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.058815148 |
|    clip_fraction        | 0.593       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | -0.0046     |
|    learning_rate        | 0.00135     |
|    loss                 | 52.5        |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0422      |
|    value_loss           | 375         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.031195983 |
|    clip_fraction        | 0.665       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.16        |
|    learning_rate        | 0.00135     |
|    loss                 | 40.8        |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.0656      |
|    value_loss           | 404         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 916      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 123        |
|    ep_rew_mean          | 390        |
| time/                   |            |
|    fps                  | 915        |
|    iterations           | 4          |
|    time_elapsed         | 8          |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.13199659 |
|    clip_fraction        | 0.667      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.215      |
|    learning_rate        | 0.00135    |
|    loss                 | 132        |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0617     |
|    value_loss           | 500        |
----------------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.7834475 |
|    clip_fraction        | 0.701     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.679    |
|    explained_variance   | -0.0336   |
|    learning_rate        | 0.00135   |
|    loss                 | 121       |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.0813    |
|    value_loss           | 838       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 866      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 110       |
|    ep_rew_mean          | 340       |
| time/                   |           |
|    fps                  | 873       |
|    iterations           | 6         |
|    time_elapsed         | 14        |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 4.7212553 |
|    clip_fraction        | 0.943     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0568   |
|    explained_variance   | -0.0452   |
|    learning_rate        | 0.00135   |
|    loss                 | 197       |
|    n_updates            | 50        |
|    policy_gradient_loss | 0.1       |
|    value_loss           | 1.25e+03  |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 105           |
|    ep_rew_mean          | 320           |
| time/                   |               |
|    fps                  | 877           |
|    iterations           | 7             |
|    time_elapsed         | 16            |
|    total_timesteps      | 14336         |
| train/                  |               |
|    approx_kl            | -9.604264e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.25e-05     |
|    explained_variance   | -0.168        |
|    learning_rate        | 0.00135       |
|    loss                 | 50.5          |
|    n_updates            | 60            |
|    policy_gradient_loss | -2.57e-06     |
|    value_loss           | 785           |
-------------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.16e-07 |
|    explained_variance   | 0.747     |
|    learning_rate        | 0.00135   |
|    loss                 | 54.8      |
|    n_updates            | 70        |
|    policy_gradient_loss | 3.67e-07  |
|    value_loss           | 150       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99       |
|    ep_rew_mean     | 296      |
| time/              |          |
|    fps             | 853      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 96             |
|    ep_rew_mean          | 284            |
| time/                   |                |
|    fps                  | 859            |
|    iterations           | 9              |
|    time_elapsed         | 21             |
|    total_timesteps      | 18432          |
| train/                  |                |
|    approx_kl            | -2.0372681e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.39e-07      |
|    explained_variance   | 0.896          |
|    learning_rate        | 0.00135        |
|    loss                 | 42.2           |
|    n_updates            | 80             |
|    policy_gradient_loss | 3.37e-08       |
|    value_loss           | 94.2           |
--------------------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.26e-07 |
|    explained_variance   | 0.942     |
|    learning_rate        | 0.00135   |
|    loss                 | 3.93      |
|    n_updates            | 90        |
|    policy_gradient_loss | 2.31e-08  |
|    value_loss           | 51.4      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 841      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 847       |
|    iterations           | 11        |
|    time_elapsed         | 26        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.12e-07 |
|    explained_variance   | 0.964     |
|    learning_rate        | 0.00135   |
|    loss                 | 3.18      |
|    n_updates            | 100       |
|    policy_gradient_loss | -2.18e-10 |
|    value_loss           | 38.3      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 854       |
|    iterations           | 12        |
|    time_elapsed         | 28        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.88e-08 |
|    explained_variance   | 0.978     |
|    learning_rate        | 0.00135   |
|    loss                 | 6.63      |
|    n_updates            | 110       |
|    policy_gradient_loss | 8.24e-10  |
|    value_loss           | 31.1      |
---------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.19e-09 |
|    explained_variance   | 0.986     |
|    learning_rate        | 0.00135   |
|    loss                 | 4.43      |
|    n_updates            | 120       |
|    policy_gradient_loss | -1.38e-09 |
|    value_loss           | 29.8      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 841      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 846       |
|    iterations           | 14        |
|    time_elapsed         | 33        |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.4e-09  |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.00135   |
|    loss                 | 6.42      |
|    n_updates            | 130       |
|    policy_gradient_loss | -6.17e-10 |
|    value_loss           | 27        |
---------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.25e-09 |
|    explained_variance   | 0.994     |
|    learning_rate        | 0.00135   |
|    loss                 | 4.42      |
|    n_updates            | 140       |
|    policy_gradient_loss | -1.7e-09  |
|    value_loss           | 23.5      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 836      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 840       |
|    iterations           | 16        |
|    time_elapsed         | 38        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.13e-09 |
|    explained_variance   | 0.997     |
|    learning_rate        | 0.00135   |
|    loss                 | 11.1      |
|    n_updates            | 150       |
|    policy_gradient_loss | 7.23e-10  |
|    value_loss           | 17.2      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 844       |
|    iterations           | 17        |
|    time_elapsed         | 41        |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.23e-09 |
|    explained_variance   | 0.997     |
|    learning_rate        | 0.00135   |
|    loss                 | 4.17      |
|    n_updates            | 160       |
|    policy_gradient_loss | 5.44e-09  |
|    value_loss           | 15.4      |
---------------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.55e-09 |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.00135   |
|    loss                 | 6.59      |
|    n_updates            | 170       |
|    policy_gradient_loss | -6.66e-10 |
|    value_loss           | 13.3      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 836      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 839       |
|    iterations           | 19        |
|    time_elapsed         | 46        |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.94e-09 |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.00135   |
|    loss                 | 4.55      |
|    n_updates            | 180       |
|    policy_gradient_loss | 4.67e-09  |
|    value_loss           | 12.6      |
---------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.52e-08 |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.00135   |
|    loss                 | 4.24      |
|    n_updates            | 190       |
|    policy_gradient_loss | -4.62e-09 |
|    value_loss           | 9.25      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 832      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 836       |
|    iterations           | 21        |
|    time_elapsed         | 51        |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.54e-08 |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.00135   |
|    loss                 | 5.52      |
|    n_updates            | 200       |
|    policy_gradient_loss | 1.46e-09  |
|    value_loss           | 10.3      |
---------------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.03e-07 |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.00135   |
|    loss                 | 2.53      |
|    n_updates            | 210       |
|    policy_gradient_loss | 1.48e-09  |
|    value_loss           | 9.26      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 829      |
|    iterations      | 22       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 833       |
|    iterations           | 23        |
|    time_elapsed         | 56        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.05e-07 |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.00135   |
|    loss                 | 6.5       |
|    n_updates            | 220       |
|    policy_gradient_loss | 1.58e-09  |
|    value_loss           | 7.7       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 836       |
|    iterations           | 24        |
|    time_elapsed         | 58        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.47e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 0.00135   |
|    loss                 | 5.08      |
|    n_updates            | 230       |
|    policy_gradient_loss | -8.24e-10 |
|    value_loss           | 6.66      |
---------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 96             |
|    mean_reward          | 284            |
| time/                   |                |
|    total_timesteps      | 50000          |
| train/                  |                |
|    approx_kl            | -2.6193447e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.43e-07      |
|    explained_variance   | 1              |
|    learning_rate        | 0.00135        |
|    loss                 | 2.61           |
|    n_updates            | 240            |
|    policy_gradient_loss | 1.71e-09       |
|    value_loss           | 6.24           |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 830      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 96             |
|    ep_rew_mean          | 284            |
| time/                   |                |
|    fps                  | 833            |
|    iterations           | 26             |
|    time_elapsed         | 63             |
|    total_timesteps      | 53248          |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.23e-07      |
|    explained_variance   | 1              |
|    learning_rate        | 0.00135        |
|    loss                 | 1.8            |
|    n_updates            | 250            |
|    policy_gradient_loss | 6.87e-09       |
|    value_loss           | 5.75           |
--------------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 55000         |
| train/                  |               |
|    approx_kl            | -8.731149e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.03e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 0.00135       |
|    loss                 | 3.87          |
|    n_updates            | 260           |
|    policy_gradient_loss | 3.44e-09      |
|    value_loss           | 5.99          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 828      |
|    iterations      | 27       |
|    time_elapsed    | 66       |
|    total_timesteps | 55296    |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 96             |
|    ep_rew_mean          | 284            |
| time/                   |                |
|    fps                  | 829            |
|    iterations           | 28             |
|    time_elapsed         | 69             |
|    total_timesteps      | 57344          |
| train/                  |                |
|    approx_kl            | -2.6193447e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.48e-06      |
|    explained_variance   | 1              |
|    learning_rate        | 0.00135        |
|    loss                 | 6.32           |
|    n_updates            | 270            |
|    policy_gradient_loss | 6.46e-09       |
|    value_loss           | 5.12           |
--------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 96             |
|    ep_rew_mean          | 284            |
| time/                   |                |
|    fps                  | 832            |
|    iterations           | 29             |
|    time_elapsed         | 71             |
|    total_timesteps      | 59392          |
| train/                  |                |
|    approx_kl            | -1.4551915e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.5e-06       |
|    explained_variance   | 1              |
|    learning_rate        | 0.00135        |
|    loss                 | 3.1            |
|    n_updates            | 280            |
|    policy_gradient_loss | 2.56e-08       |
|    value_loss           | 6.72           |
--------------------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | -5.529728e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.71e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 0.00135       |
|    loss                 | 2.52          |
|    n_updates            | 290           |
|    policy_gradient_loss | 8.34e-10      |
|    value_loss           | 5.42          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 827      |
|    iterations      | 30       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:43:55,765] Trial 24 finished with value: 284.0 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0013475792223369204, 'gamma': 0.998672576675792, 'gae_lambda': 0.8704960792641177}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 1157     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=316.00 +/- 35.05
Episode length: 104.00 +/- 8.76
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 104      |
|    mean_reward          | 316      |
| time/                   |          |
|    total_timesteps      | 10000    |
| train/                  |          |
|    approx_kl            | 30.75483 |
|    clip_fraction        | 0.997    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0274  |
|    explained_variance   | -0.00156 |
|    learning_rate        | 0.00327  |
|    loss                 | 95.4     |
|    n_updates            | 10       |
|    policy_gradient_loss | 0.413    |
|    value_loss           | 524      |
--------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=325.60 +/- 43.05
Episode length: 106.40 +/- 10.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 326      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 331      |
| time/              |          |
|    fps             | 844      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=296.80 +/- 25.60
Episode length: 99.20 +/- 6.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 99.2      |
|    mean_reward          | 297       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.48e-09 |
|    explained_variance   | 0.0406    |
|    learning_rate        | 0.00327   |
|    loss                 | 764       |
|    n_updates            | 20        |
|    policy_gradient_loss | 2.91e-12  |
|    value_loss           | 1.01e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 788      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=322.40 +/- 39.97
Episode length: 105.60 +/- 9.99
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 106      |
|    mean_reward          | 322      |
| time/                   |          |
|    total_timesteps      | 25000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.7e-08 |
|    explained_variance   | 0.119    |
|    learning_rate        | 0.00327  |
|    loss                 | 791      |
|    n_updates            | 30       |
|    policy_gradient_loss | 3.01e-10 |
|    value_loss           | 1.09e+03 |
--------------------------------------
Eval num_timesteps=30000, episode_reward=300.00 +/- 25.80
Episode length: 100.00 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 752      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=312.80 +/- 39.06
Episode length: 103.20 +/- 9.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 103       |
|    mean_reward          | 313       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.32e-08 |
|    explained_variance   | 0.128     |
|    learning_rate        | 0.00327   |
|    loss                 | 583       |
|    n_updates            | 40        |
|    policy_gradient_loss | -4.18e-11 |
|    value_loss           | 1.19e+03  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=325.60 +/- 35.20
Episode length: 106.40 +/- 8.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 326      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 732      |
|    iterations      | 5        |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=316.00 +/- 24.79
Episode length: 104.00 +/- 6.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 104       |
|    mean_reward          | 316       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.11e-07 |
|    explained_variance   | 0.128     |
|    learning_rate        | 0.00327   |
|    loss                 | 791       |
|    n_updates            | 50        |
|    policy_gradient_loss | -8.84e-11 |
|    value_loss           | 1.14e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 727      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=306.40 +/- 38.00
Episode length: 101.60 +/- 9.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 306       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.15e-07 |
|    explained_variance   | 0.124     |
|    learning_rate        | 0.00327   |
|    loss                 | 684       |
|    n_updates            | 60        |
|    policy_gradient_loss | -1.48e-10 |
|    value_loss           | 1.22e+03  |
---------------------------------------
Eval num_timesteps=55000, episode_reward=319.20 +/- 33.41
Episode length: 104.80 +/- 8.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 312      |
| time/              |          |
|    fps             | 717      |
|    iterations      | 7        |
|    time_elapsed    | 79       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=316.00 +/- 35.05
Episode length: 104.00 +/- 8.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 104           |
|    mean_reward          | 316           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | -8.600182e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.05e-06     |
|    explained_variance   | 0.112         |
|    learning_rate        | 0.00327       |
|    loss                 | 910           |
|    n_updates            | 70            |
|    policy_gradient_loss | -1.2e-09      |
|    value_loss           | 1.19e+03      |
-------------------------------------------
Eval num_timesteps=65000, episode_reward=309.60 +/- 34.47
Episode length: 102.40 +/- 8.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 305      |
| time/              |          |
|    fps             | 711      |
|    iterations      | 8        |
|    time_elapsed    | 92       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:45:33,467] Trial 25 finished with value: 300.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0032653839938608016, 'gamma': 0.9663496450336406, 'gae_lambda': 0.9385387565617519}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 1272     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 123        |
|    ep_rew_mean          | 393        |
| time/                   |            |
|    fps                  | 961        |
|    iterations           | 2          |
|    time_elapsed         | 4          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.05869419 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.0015     |
|    learning_rate        | 0.000609   |
|    loss                 | 171        |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.00837    |
|    value_loss           | 280        |
----------------------------------------
Eval num_timesteps=5000, episode_reward=319.20 +/- 41.60
Episode length: 104.80 +/- 10.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 105        |
|    mean_reward          | 319        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.03601797 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.132      |
|    learning_rate        | 0.000609   |
|    loss                 | 21.8       |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.0189     |
|    value_loss           | 309        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 811      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 125        |
|    ep_rew_mean          | 399        |
| time/                   |            |
|    fps                  | 798        |
|    iterations           | 4          |
|    time_elapsed         | 10         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.09387156 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 0.151      |
|    learning_rate        | 0.000609   |
|    loss                 | 112        |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0199     |
|    value_loss           | 400        |
----------------------------------------
Eval num_timesteps=10000, episode_reward=348.00 +/- 32.00
Episode length: 112.00 +/- 8.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 112        |
|    mean_reward          | 348        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.04264451 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.194      |
|    learning_rate        | 0.000609   |
|    loss                 | 122        |
|    n_updates            | 40         |
|    policy_gradient_loss | 0.0302     |
|    value_loss           | 480        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 407      |
| time/              |          |
|    fps             | 746      |
|    iterations      | 5        |
|    time_elapsed    | 13       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 125        |
|    ep_rew_mean          | 401        |
| time/                   |            |
|    fps                  | 746        |
|    iterations           | 6          |
|    time_elapsed         | 16         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.16752774 |
|    clip_fraction        | 0.57       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.968     |
|    explained_variance   | 0.117      |
|    learning_rate        | 0.000609   |
|    loss                 | 40.2       |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.054      |
|    value_loss           | 396        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 123       |
|    ep_rew_mean          | 391       |
| time/                   |           |
|    fps                  | 751       |
|    iterations           | 7         |
|    time_elapsed         | 19        |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 1.2519042 |
|    clip_fraction        | 0.684     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.646    |
|    explained_variance   | 0.161     |
|    learning_rate        | 0.000609  |
|    loss                 | 128       |
|    n_updates            | 60        |
|    policy_gradient_loss | 0.0899    |
|    value_loss           | 669       |
---------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 5.3282995 |
|    clip_fraction        | 0.988     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0457   |
|    explained_variance   | 0.0407    |
|    learning_rate        | 0.000609  |
|    loss                 | 79.8      |
|    n_updates            | 70        |
|    policy_gradient_loss | 0.231     |
|    value_loss           | 1.08e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 729      |
|    iterations      | 8        |
|    time_elapsed    | 22       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 110        |
|    ep_rew_mean          | 341        |
| time/                   |            |
|    fps                  | 731        |
|    iterations           | 9          |
|    time_elapsed         | 25         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 5.6027e-05 |
|    clip_fraction        | 0.000146   |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00151   |
|    explained_variance   | 0.00452    |
|    learning_rate        | 0.000609   |
|    loss                 | 146        |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.000592  |
|    value_loss           | 1.41e+03   |
----------------------------------------
Eval num_timesteps=20000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.4        |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.022111151 |
|    clip_fraction        | 0.0308      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.000609    |
|    loss                 | 150         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 1.14e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 716      |
|    iterations      | 10       |
|    time_elapsed    | 28       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | 304       |
| time/                   |           |
|    fps                  | 720       |
|    iterations           | 11        |
|    time_elapsed         | 31        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.8683616 |
|    clip_fraction        | 0.325     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.34     |
|    explained_variance   | 0.485     |
|    learning_rate        | 0.000609  |
|    loss                 | 64.1      |
|    n_updates            | 100       |
|    policy_gradient_loss | 0.0741    |
|    value_loss           | 742       |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 99.5       |
|    ep_rew_mean          | 298        |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 12         |
|    time_elapsed         | 34         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.06542429 |
|    clip_fraction        | 0.0271     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0383    |
|    explained_variance   | 0.202      |
|    learning_rate        | 0.000609   |
|    loss                 | 740        |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00335   |
|    value_loss           | 1.09e+03   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.053738296 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0731     |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000609    |
|    loss                 | 150         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 1.02e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.5     |
|    ep_rew_mean     | 298      |
| time/              |          |
|    fps             | 712      |
|    iterations      | 13       |
|    time_elapsed    | 37       |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 99.5      |
|    ep_rew_mean          | 298       |
| time/                   |           |
|    fps                  | 714       |
|    iterations           | 14        |
|    time_elapsed         | 40        |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.2134375 |
|    clip_fraction        | 0.0505    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0366   |
|    explained_variance   | 0.436     |
|    learning_rate        | 0.000609  |
|    loss                 | 277       |
|    n_updates            | 130       |
|    policy_gradient_loss | 0.00882   |
|    value_loss           | 1.03e+03  |
---------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0006570056 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00323     |
|    explained_variance   | 0.391        |
|    learning_rate        | 0.000609     |
|    loss                 | 675          |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000876    |
|    value_loss           | 1.1e+03      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 705      |
|    iterations      | 15       |
|    time_elapsed    | 43       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 96.5       |
|    ep_rew_mean          | 286        |
| time/                   |            |
|    fps                  | 708        |
|    iterations           | 16         |
|    time_elapsed         | 46         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.03584036 |
|    clip_fraction        | 0.0144     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0235    |
|    explained_variance   | 0.562      |
|    learning_rate        | 0.000609   |
|    loss                 | 115        |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.00589   |
|    value_loss           | 889        |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 96.5        |
|    ep_rew_mean          | 286         |
| time/                   |             |
|    fps                  | 710         |
|    iterations           | 17          |
|    time_elapsed         | 49          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.082041405 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.401       |
|    learning_rate        | 0.000609    |
|    loss                 | 180         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00836    |
|    value_loss           | 874         |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.1354245 |
|    clip_fraction        | 0.0985    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.112    |
|    explained_variance   | 0.351     |
|    learning_rate        | 0.000609  |
|    loss                 | 107       |
|    n_updates            | 170       |
|    policy_gradient_loss | 0.0241    |
|    value_loss           | 1.01e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 702      |
|    iterations      | 18       |
|    time_elapsed    | 52       |
|    total_timesteps | 36864    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96.5      |
|    ep_rew_mean          | 286       |
| time/                   |           |
|    fps                  | 704       |
|    iterations           | 19        |
|    time_elapsed         | 55        |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.3395623 |
|    clip_fraction        | 0.0563    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0169   |
|    explained_variance   | 0.325     |
|    learning_rate        | 0.000609  |
|    loss                 | 956       |
|    n_updates            | 180       |
|    policy_gradient_loss | 0.0168    |
|    value_loss           | 1.07e+03  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.005809573 |
|    clip_fraction        | 0.00308     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00348    |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000609    |
|    loss                 | 145         |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 1.02e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97       |
|    ep_rew_mean     | 288      |
| time/              |          |
|    fps             | 699      |
|    iterations      | 20       |
|    time_elapsed    | 58       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 98.1       |
|    ep_rew_mean          | 292        |
| time/                   |            |
|    fps                  | 701        |
|    iterations           | 21         |
|    time_elapsed         | 61         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.20068726 |
|    clip_fraction        | 0.0623     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0437    |
|    explained_variance   | 0.431      |
|    learning_rate        | 0.000609   |
|    loss                 | 453        |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.00938   |
|    value_loss           | 865        |
----------------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 1.2005873 |
|    clip_fraction        | 0.221     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.164    |
|    explained_variance   | 0.588     |
|    learning_rate        | 0.000609  |
|    loss                 | 89        |
|    n_updates            | 210       |
|    policy_gradient_loss | 0.0601    |
|    value_loss           | 836       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.8     |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 695      |
|    iterations      | 22       |
|    time_elapsed    | 64       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 98.8        |
|    ep_rew_mean          | 295         |
| time/                   |             |
|    fps                  | 697         |
|    iterations           | 23          |
|    time_elapsed         | 67          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.011575097 |
|    clip_fraction        | 0.00806     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00818    |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.000609    |
|    loss                 | 330         |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 925         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 98.8       |
|    ep_rew_mean          | 295        |
| time/                   |            |
|    fps                  | 698        |
|    iterations           | 24         |
|    time_elapsed         | 70         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.09503205 |
|    clip_fraction        | 0.0565     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0548    |
|    explained_variance   | 0.553      |
|    learning_rate        | 0.000609   |
|    loss                 | 164        |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0039    |
|    value_loss           | 767        |
----------------------------------------
Eval num_timesteps=50000, episode_reward=287.20 +/- 9.60
Episode length: 96.80 +/- 2.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96.8      |
|    mean_reward          | 287       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.1118374 |
|    clip_fraction        | 0.0295    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0191   |
|    explained_variance   | 0.457     |
|    learning_rate        | 0.000609  |
|    loss                 | 183       |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.00544  |
|    value_loss           | 753       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.7     |
|    ep_rew_mean     | 299      |
| time/              |          |
|    fps             | 693      |
|    iterations      | 25       |
|    time_elapsed    | 73       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 100        |
|    ep_rew_mean          | 300        |
| time/                   |            |
|    fps                  | 695        |
|    iterations           | 26         |
|    time_elapsed         | 76         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.15489078 |
|    clip_fraction        | 0.0955     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0805    |
|    explained_variance   | 0.475      |
|    learning_rate        | 0.000609   |
|    loss                 | 158        |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.00526   |
|    value_loss           | 732        |
----------------------------------------
Eval num_timesteps=55000, episode_reward=322.40 +/- 63.68
Episode length: 105.60 +/- 15.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 106        |
|    mean_reward          | 322        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.09788574 |
|    clip_fraction        | 0.0725     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.000609   |
|    loss                 | 363        |
|    n_updates            | 260        |
|    policy_gradient_loss | 0.00669    |
|    value_loss           | 658        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 304      |
| time/              |          |
|    fps             | 690      |
|    iterations      | 27       |
|    time_elapsed    | 80       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 100       |
|    ep_rew_mean          | 301       |
| time/                   |           |
|    fps                  | 691       |
|    iterations           | 28        |
|    time_elapsed         | 82        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 2.1659617 |
|    clip_fraction        | 0.198     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0814   |
|    explained_variance   | 0.45      |
|    learning_rate        | 0.000609  |
|    loss                 | 669       |
|    n_updates            | 270       |
|    policy_gradient_loss | 0.0678    |
|    value_loss           | 642       |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | 302        |
| time/                   |            |
|    fps                  | 692        |
|    iterations           | 29         |
|    time_elapsed         | 85         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.16465749 |
|    clip_fraction        | 0.0292     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0108    |
|    explained_variance   | 0.363      |
|    learning_rate        | 0.000609   |
|    loss                 | 288        |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.00914   |
|    value_loss           | 667        |
----------------------------------------
Eval num_timesteps=60000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.4       |
|    mean_reward          | 294        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.15074608 |
|    clip_fraction        | 0.0557     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0497    |
|    explained_variance   | 0.312      |
|    learning_rate        | 0.000609   |
|    loss                 | 365        |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.00257   |
|    value_loss           | 645        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.9     |
|    ep_rew_mean     | 296      |
| time/              |          |
|    fps             | 689      |
|    iterations      | 30       |
|    time_elapsed    | 89       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:47:04,760] Trial 26 finished with value: 303.2 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 0.0006087334751766564, 'gamma': 0.9802197335194854, 'gae_lambda': 0.8539750368123844}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 1143     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0074261543 |
|    clip_fraction        | 0.0773       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | 0.000184     |
|    learning_rate        | 6.01e-05     |
|    loss                 | 251          |
|    n_updates            | 10           |
|    policy_gradient_loss | 0.000114     |
|    value_loss           | 546          |
------------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 937      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010521978 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.256       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 429         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000608   |
|    value_loss           | 638         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 383      |
| time/              |          |
|    fps             | 905      |
|    iterations      | 3        |
|    time_elapsed    | 27       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010422058 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.295       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 281         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.000504    |
|    value_loss           | 889         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 374      |
| time/              |          |
|    fps             | 874      |
|    iterations      | 4        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.00917117 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.293      |
|    learning_rate        | 6.01e-05   |
|    loss                 | 461        |
|    n_updates            | 40         |
|    policy_gradient_loss | 0.00184    |
|    value_loss           | 1.17e+03   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 847      |
|    iterations      | 5        |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0091217775 |
|    clip_fraction        | 0.18         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.343        |
|    learning_rate        | 6.01e-05     |
|    loss                 | 513          |
|    n_updates            | 50           |
|    policy_gradient_loss | 0.00253      |
|    value_loss           | 1.24e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 848      |
|    iterations      | 6        |
|    time_elapsed    | 57       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015090534 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.553       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 757         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 841      |
|    iterations      | 7        |
|    time_elapsed    | 68       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.020241108 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.698       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 484         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.000439   |
|    value_loss           | 894         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 386      |
| time/              |          |
|    fps             | 835      |
|    iterations      | 8        |
|    time_elapsed    | 78       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:48:26,564] Trial 27 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 6.0089676115256575e-05, 'gamma': 0.9892919150654218, 'gae_lambda': 0.9021249113056882}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=309.60 +/- 37.32
Episode length: 102.40 +/- 9.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 1148     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 24.075548 |
|    clip_fraction        | 0.993     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0723   |
|    explained_variance   | 0.00139   |
|    learning_rate        | 0.00412   |
|    loss                 | 56.5      |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.357     |
|    value_loss           | 404       |
---------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 839      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 1.2846067 |
|    clip_fraction        | 0.0624    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0495   |
|    explained_variance   | -0.00237  |
|    learning_rate        | 0.00412   |
|    loss                 | 215       |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.0181   |
|    value_loss           | 645       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 786      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=322.40 +/- 47.03
Episode length: 105.60 +/- 11.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 106       |
|    mean_reward          | 322       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 2.0347958 |
|    clip_fraction        | 0.0519    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0129   |
|    explained_variance   | 0.129     |
|    learning_rate        | 0.00412   |
|    loss                 | 234       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.00718   |
|    value_loss           | 673       |
---------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=316.00 +/- 59.01
Episode length: 104.00 +/- 14.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.8     |
|    ep_rew_mean     | 299      |
| time/              |          |
|    fps             | 748      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=300.00 +/- 48.00
Episode length: 100.00 +/- 12.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 100       |
|    mean_reward          | 300       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 1.8507385 |
|    clip_fraction        | 0.13      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0437   |
|    explained_variance   | 0.24      |
|    learning_rate        | 0.00412   |
|    loss                 | 332       |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.0149    |
|    value_loss           | 663       |
---------------------------------------
Eval num_timesteps=40000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 728      |
|    iterations      | 5        |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 4.2124715 |
|    clip_fraction        | 0.144     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0247   |
|    explained_variance   | 0.217     |
|    learning_rate        | 0.00412   |
|    loss                 | 198       |
|    n_updates            | 50        |
|    policy_gradient_loss | 0.0298    |
|    value_loss           | 661       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 724      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 2.042173 |
|    clip_fraction        | 0.0827   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0191  |
|    explained_variance   | 0.162    |
|    learning_rate        | 0.00412  |
|    loss                 | 239      |
|    n_updates            | 60       |
|    policy_gradient_loss | -0.00864 |
|    value_loss           | 737      |
--------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | 285      |
| time/              |          |
|    fps             | 714      |
|    iterations      | 7        |
|    time_elapsed    | 80       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 2.0423436 |
|    clip_fraction        | 0.0922    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0213   |
|    explained_variance   | 0.252     |
|    learning_rate        | 0.00412   |
|    loss                 | 271       |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.0113   |
|    value_loss           | 756       |
---------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 709      |
|    iterations      | 8        |
|    time_elapsed    | 92       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:50:04,571] Trial 28 finished with value: 303.2 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.004116156443749312, 'gamma': 0.9786230312763502, 'gae_lambda': 0.8926819226834837}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 1264     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 112        |
|    ep_rew_mean          | 350        |
| time/                   |            |
|    fps                  | 1063       |
|    iterations           | 2          |
|    time_elapsed         | 3          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.29858646 |
|    clip_fraction        | 0.712      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | -0.00166   |
|    learning_rate        | 0.00173    |
|    loss                 | 159        |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.0923     |
|    value_loss           | 455        |
----------------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.07661351 |
|    clip_fraction        | 0.672      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.995     |
|    explained_variance   | 0.071      |
|    learning_rate        | 0.00173    |
|    loss                 | 50         |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.0778     |
|    value_loss           | 367        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 337      |
| time/              |          |
|    fps             | 913      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 324        |
| time/                   |            |
|    fps                  | 913        |
|    iterations           | 4          |
|    time_elapsed         | 8          |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.19596231 |
|    clip_fraction        | 0.556      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.662     |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.00173    |
|    loss                 | 52.5       |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0405     |
|    value_loss           | 326        |
----------------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.55429554 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.38      |
|    explained_variance   | 0.232      |
|    learning_rate        | 0.00173    |
|    loss                 | 60         |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 312        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 319      |
| time/              |          |
|    fps             | 865      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 99.8       |
|    ep_rew_mean          | 299        |
| time/                   |            |
|    fps                  | 873        |
|    iterations           | 6          |
|    time_elapsed         | 14         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.31996477 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.238     |
|    explained_variance   | 0.165      |
|    learning_rate        | 0.00173    |
|    loss                 | 47.3       |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.025      |
|    value_loss           | 536        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 102       |
|    ep_rew_mean          | 307       |
| time/                   |           |
|    fps                  | 879       |
|    iterations           | 7         |
|    time_elapsed         | 16        |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 1.2469214 |
|    clip_fraction        | 0.388     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.228    |
|    explained_variance   | 0.0253    |
|    learning_rate        | 0.00173   |
|    loss                 | 257       |
|    n_updates            | 60        |
|    policy_gradient_loss | 0.074     |
|    value_loss           | 690       |
---------------------------------------
Eval num_timesteps=15000, episode_reward=296.80 +/- 38.40
Episode length: 99.20 +/- 9.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 99.2       |
|    mean_reward          | 297        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.23775159 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.238     |
|    explained_variance   | 0.393      |
|    learning_rate        | 0.00173    |
|    loss                 | 302        |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.0291     |
|    value_loss           | 628        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 306      |
| time/              |          |
|    fps             | 855      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 102        |
|    ep_rew_mean          | 310        |
| time/                   |            |
|    fps                  | 862        |
|    iterations           | 9          |
|    time_elapsed         | 21         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.10496816 |
|    clip_fraction        | 0.0649     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0978    |
|    explained_variance   | 0.177      |
|    learning_rate        | 0.00173    |
|    loss                 | 532        |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 911        |
----------------------------------------
Eval num_timesteps=20000, episode_reward=364.00 +/- 86.16
Episode length: 116.00 +/- 21.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 116        |
|    mean_reward          | 364        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.19185036 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.132     |
|    explained_variance   | 0.0628     |
|    learning_rate        | 0.00173    |
|    loss                 | 146        |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.00832   |
|    value_loss           | 821        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 841      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 107        |
|    ep_rew_mean          | 327        |
| time/                   |            |
|    fps                  | 847        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.06533638 |
|    clip_fraction        | 0.059      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0606    |
|    explained_variance   | 0.000181   |
|    learning_rate        | 0.00173    |
|    loss                 | 451        |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.000898   |
|    value_loss           | 1.07e+03   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 324        |
| time/                   |            |
|    fps                  | 853        |
|    iterations           | 12         |
|    time_elapsed         | 28         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.27337882 |
|    clip_fraction        | 0.0632     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0374    |
|    explained_variance   | 0.0942     |
|    learning_rate        | 0.00173    |
|    loss                 | 118        |
|    n_updates            | 110        |
|    policy_gradient_loss | 0.00621    |
|    value_loss           | 764        |
----------------------------------------
Eval num_timesteps=25000, episode_reward=309.60 +/- 53.16
Episode length: 102.40 +/- 13.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.113456935 |
|    clip_fraction        | 0.0485      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0455     |
|    explained_variance   | 0.107       |
|    learning_rate        | 0.00173     |
|    loss                 | 535         |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.0019      |
|    value_loss           | 921         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 329      |
| time/              |          |
|    fps             | 840      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 324        |
| time/                   |            |
|    fps                  | 844        |
|    iterations           | 14         |
|    time_elapsed         | 33         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.08696867 |
|    clip_fraction        | 0.0681     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0461    |
|    explained_variance   | 0.136      |
|    learning_rate        | 0.00173    |
|    loss                 | 271        |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.0262     |
|    value_loss           | 768        |
----------------------------------------
Eval num_timesteps=30000, episode_reward=325.60 +/- 67.20
Episode length: 106.40 +/- 16.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 326         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.033362195 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0158     |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.00173     |
|    loss                 | 329         |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.000801    |
|    value_loss           | 1.05e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 833      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 104        |
|    ep_rew_mean          | 318        |
| time/                   |            |
|    fps                  | 838        |
|    iterations           | 16         |
|    time_elapsed         | 39         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.05795636 |
|    clip_fraction        | 0.0115     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0134    |
|    explained_variance   | 0.148      |
|    learning_rate        | 0.00173    |
|    loss                 | 379        |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.00244   |
|    value_loss           | 857        |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 103        |
|    ep_rew_mean          | 313        |
| time/                   |            |
|    fps                  | 842        |
|    iterations           | 17         |
|    time_elapsed         | 41         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.05699912 |
|    clip_fraction        | 0.0319     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0216    |
|    explained_variance   | 0.185      |
|    learning_rate        | 0.00173    |
|    loss                 | 728        |
|    n_updates            | 160        |
|    policy_gradient_loss | 0.00521    |
|    value_loss           | 930        |
----------------------------------------
Eval num_timesteps=35000, episode_reward=290.40 +/- 19.20
Episode length: 97.60 +/- 4.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.6       |
|    mean_reward          | 290        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.17816404 |
|    clip_fraction        | 0.0514     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0249    |
|    explained_variance   | 0.263      |
|    learning_rate        | 0.00173    |
|    loss                 | 601        |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.00489   |
|    value_loss           | 880        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 308      |
| time/              |          |
|    fps             | 834      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 103        |
|    ep_rew_mean          | 311        |
| time/                   |            |
|    fps                  | 837        |
|    iterations           | 19         |
|    time_elapsed         | 46         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.13914825 |
|    clip_fraction        | 0.0232     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0159    |
|    explained_variance   | 0.163      |
|    learning_rate        | 0.00173    |
|    loss                 | 253        |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.000817  |
|    value_loss           | 900        |
----------------------------------------
Eval num_timesteps=40000, episode_reward=335.20 +/- 87.28
Episode length: 108.80 +/- 21.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 109        |
|    mean_reward          | 335        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.30959785 |
|    clip_fraction        | 0.09       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0769    |
|    explained_variance   | 0.123      |
|    learning_rate        | 0.00173    |
|    loss                 | 217        |
|    n_updates            | 190        |
|    policy_gradient_loss | 0.00145    |
|    value_loss           | 803        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 829      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 103       |
|    ep_rew_mean          | 313       |
| time/                   |           |
|    fps                  | 833       |
|    iterations           | 21        |
|    time_elapsed         | 51        |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.5843506 |
|    clip_fraction        | 0.0575    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0217   |
|    explained_variance   | 0.124     |
|    learning_rate        | 0.00173   |
|    loss                 | 199       |
|    n_updates            | 200       |
|    policy_gradient_loss | 0.0222    |
|    value_loss           | 704       |
---------------------------------------
Eval num_timesteps=45000, episode_reward=306.40 +/- 35.20
Episode length: 101.60 +/- 8.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 102        |
|    mean_reward          | 306        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.18727428 |
|    clip_fraction        | 0.0336     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0122    |
|    explained_variance   | 0.111      |
|    learning_rate        | 0.00173    |
|    loss                 | 743        |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.00385   |
|    value_loss           | 920        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 826      |
|    iterations      | 22       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 105       |
|    ep_rew_mean          | 319       |
| time/                   |           |
|    fps                  | 829       |
|    iterations           | 23        |
|    time_elapsed         | 56        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.1940737 |
|    clip_fraction        | 0.0553    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0286   |
|    explained_variance   | 0.145     |
|    learning_rate        | 0.00173   |
|    loss                 | 500       |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.00258  |
|    value_loss           | 988       |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 105        |
|    ep_rew_mean          | 320        |
| time/                   |            |
|    fps                  | 833        |
|    iterations           | 24         |
|    time_elapsed         | 59         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.06390563 |
|    clip_fraction        | 0.0186     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0116    |
|    explained_variance   | 0.179      |
|    learning_rate        | 0.00173    |
|    loss                 | 534        |
|    n_updates            | 230        |
|    policy_gradient_loss | 0.000898   |
|    value_loss           | 1.02e+03   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=322.40 +/- 47.03
Episode length: 105.60 +/- 11.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 322         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.116027415 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0229     |
|    explained_variance   | 0.237       |
|    learning_rate        | 0.00173     |
|    loss                 | 649         |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.00418     |
|    value_loss           | 921         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 321      |
| time/              |          |
|    fps             | 827      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 325        |
| time/                   |            |
|    fps                  | 830        |
|    iterations           | 26         |
|    time_elapsed         | 64         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.10855632 |
|    clip_fraction        | 0.0293     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0116    |
|    explained_variance   | 0.207      |
|    learning_rate        | 0.00173    |
|    loss                 | 239        |
|    n_updates            | 250        |
|    policy_gradient_loss | 0.00609    |
|    value_loss           | 993        |
----------------------------------------
Eval num_timesteps=55000, episode_reward=322.40 +/- 87.99
Episode length: 105.60 +/- 22.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 322         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.068191364 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0235     |
|    explained_variance   | 0.221       |
|    learning_rate        | 0.00173     |
|    loss                 | 485         |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.00244     |
|    value_loss           | 963         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 322      |
| time/              |          |
|    fps             | 824      |
|    iterations      | 27       |
|    time_elapsed    | 67       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 325        |
| time/                   |            |
|    fps                  | 827        |
|    iterations           | 28         |
|    time_elapsed         | 69         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.59993196 |
|    clip_fraction        | 0.068      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0314    |
|    explained_variance   | 0.18       |
|    learning_rate        | 0.00173    |
|    loss                 | 423        |
|    n_updates            | 270        |
|    policy_gradient_loss | 0.000195   |
|    value_loss           | 975        |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 106       |
|    ep_rew_mean          | 322       |
| time/                   |           |
|    fps                  | 830       |
|    iterations           | 29        |
|    time_elapsed         | 71        |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 1.2110121 |
|    clip_fraction        | 0.232     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.083    |
|    explained_variance   | 0.0839    |
|    learning_rate        | 0.00173   |
|    loss                 | 443       |
|    n_updates            | 280       |
|    policy_gradient_loss | 0.0405    |
|    value_loss           | 835       |
---------------------------------------
Eval num_timesteps=60000, episode_reward=303.20 +/- 38.40
Episode length: 100.80 +/- 9.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 101        |
|    mean_reward          | 303        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.47189635 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0364    |
|    explained_variance   | 0.184      |
|    learning_rate        | 0.00173    |
|    loss                 | 251        |
|    n_updates            | 290        |
|    policy_gradient_loss | 0.0339     |
|    value_loss           | 912        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 825      |
|    iterations      | 30       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 18:51:20,509] Trial 29 finished with value: 312.8 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0017337402291560456, 'gamma': 0.9681775412988141, 'gae_lambda': 0.9168766578919973}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=520.80 +/- 257.28
Episode length: 155.20 +/- 64.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 521      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 1081     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=408.80 +/- 84.00
Episode length: 127.20 +/- 21.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 127        |
|    mean_reward          | 409        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01970254 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | -0.00131   |
|    learning_rate        | 0.000376   |
|    loss                 | 545        |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.0135     |
|    value_loss           | 848        |
----------------------------------------
Eval num_timesteps=15000, episode_reward=453.60 +/- 108.09
Episode length: 138.40 +/- 27.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 404      |
| time/              |          |
|    fps             | 802      |
|    iterations      | 2        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=312.80 +/- 61.47
Episode length: 103.20 +/- 15.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 313         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.027181534 |
|    clip_fraction        | 0.404       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.386       |
|    learning_rate        | 0.000376    |
|    loss                 | 445         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.0149      |
|    value_loss           | 948         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 384      |
| time/              |          |
|    fps             | 769      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.028984087 |
|    clip_fraction        | 0.42        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.000376    |
|    loss                 | 255         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.0156      |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 420      |
| time/              |          |
|    fps             | 739      |
|    iterations      | 4        |
|    time_elapsed    | 44       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.047354043 |
|    clip_fraction        | 0.472       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.000376    |
|    loss                 | 521         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.0205      |
|    value_loss           | 1.88e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 501      |
| time/              |          |
|    fps             | 721      |
|    iterations      | 5        |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.27789766 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.146      |
|    learning_rate        | 0.000376   |
|    loss                 | 387        |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.0403     |
|    value_loss           | 3.64e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 543      |
| time/              |          |
|    fps             | 715      |
|    iterations      | 6        |
|    time_elapsed    | 68       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.058534466 |
|    clip_fraction        | 0.457       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.145       |
|    learning_rate        | 0.000376    |
|    loss                 | 1.75e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.0222      |
|    value_loss           | 4.37e+03    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 449      |
| time/              |          |
|    fps             | 706      |
|    iterations      | 7        |
|    time_elapsed    | 81       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.048649825 |
|    clip_fraction        | 0.414       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.984      |
|    explained_variance   | 0.0748      |
|    learning_rate        | 0.000376    |
|    loss                 | 1.41e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.0302      |
|    value_loss           | 6.64e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 435      |
| time/              |          |
|    fps             | 700      |
|    iterations      | 8        |
|    time_elapsed    | 93       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:52:59,575] Trial 30 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.00037625728802211496, 'gamma': 0.9994836850151331, 'gae_lambda': 0.9398388114173457}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 1154     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=890.40 +/- 665.58
Episode length: 242.60 +/- 157.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 243       |
|    mean_reward          | 890       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 4.8514605 |
|    clip_fraction        | 0.975     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.241    |
|    explained_variance   | 0.000697  |
|    learning_rate        | 0.00274   |
|    loss                 | 469       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.363     |
|    value_loss           | 656       |
---------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=600.00 +/- 514.73
Episode length: 172.50 +/- 121.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 172      |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 556      |
| time/              |          |
|    fps             | 782      |
|    iterations      | 2        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 17.451263 |
|    clip_fraction        | 0.978     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00305  |
|    explained_variance   | -0.261    |
|    learning_rate        | 0.00274   |
|    loss                 | 198       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.321     |
|    value_loss           | 504       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 751      |
|    iterations      | 3        |
|    time_elapsed    | 32       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 6.2028785 |
|    clip_fraction        | 0.202     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0286   |
|    explained_variance   | -0.0145   |
|    learning_rate        | 0.00274   |
|    loss                 | 13.7      |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.0486   |
|    value_loss           | 228       |
---------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 726      |
|    iterations      | 4        |
|    time_elapsed    | 45       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 35000    |
| train/                  |          |
|    approx_kl            | 3.758185 |
|    clip_fraction        | 0.236    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00577 |
|    explained_variance   | 0.382    |
|    learning_rate        | 0.00274  |
|    loss                 | 224      |
|    n_updates            | 40       |
|    policy_gradient_loss | 0.0878   |
|    value_loss           | 510      |
--------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 710      |
|    iterations      | 5        |
|    time_elapsed    | 57       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 45000    |
| train/                  |          |
|    approx_kl            | 3.710753 |
|    clip_fraction        | 0.147    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00231 |
|    explained_variance   | 0.525    |
|    learning_rate        | 0.00274  |
|    loss                 | 234      |
|    n_updates            | 50       |
|    policy_gradient_loss | -0.0397  |
|    value_loss           | 536      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 707      |
|    iterations      | 6        |
|    time_elapsed    | 69       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 1.8089569 |
|    clip_fraction        | 0.146     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000261 |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.00274   |
|    loss                 | 221       |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0318   |
|    value_loss           | 520       |
---------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 700      |
|    iterations      | 7        |
|    time_elapsed    | 81       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 1.7203975 |
|    clip_fraction        | 0.156     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00209  |
|    explained_variance   | 0.618     |
|    learning_rate        | 0.00274   |
|    loss                 | 346       |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.0389   |
|    value_loss           | 528       |
---------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 696      |
|    iterations      | 8        |
|    time_elapsed    | 94       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:54:39,082] Trial 31 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0027439730489543235, 'gamma': 0.9622387428045991, 'gae_lambda': 0.9569208603995959}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 1160     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 244.86702 |
|    clip_fraction        | 0.998     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0178   |
|    explained_variance   | 0.00219   |
|    learning_rate        | 0.00975   |
|    loss                 | 248       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.463     |
|    value_loss           | 1.02e+03  |
---------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 300      |
| time/              |          |
|    fps             | 842      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 10.270809 |
|    clip_fraction        | 0.0814    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00694  |
|    explained_variance   | 0.00309   |
|    learning_rate        | 0.00975   |
|    loss                 | 1.22e+03  |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.0181   |
|    value_loss           | 1.46e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 793      |
|    iterations      | 3        |
|    time_elapsed    | 30       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 11.059114 |
|    clip_fraction        | 0.139     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0115   |
|    explained_variance   | 0.0651    |
|    learning_rate        | 0.00975   |
|    loss                 | 273       |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.0194   |
|    value_loss           | 1.09e+03  |
---------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 755      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 35000    |
| train/                  |          |
|    approx_kl            | 8.459731 |
|    clip_fraction        | 0.162    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0135  |
|    explained_variance   | 0.09     |
|    learning_rate        | 0.00975  |
|    loss                 | 392      |
|    n_updates            | 40       |
|    policy_gradient_loss | -0.0161  |
|    value_loss           | 1.15e+03 |
--------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | 285      |
| time/              |          |
|    fps             | 735      |
|    iterations      | 5        |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 45000    |
| train/                  |          |
|    approx_kl            | 9.216656 |
|    clip_fraction        | 0.17     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0202  |
|    explained_variance   | 0.0985   |
|    learning_rate        | 0.00975  |
|    loss                 | 728      |
|    n_updates            | 50       |
|    policy_gradient_loss | -0.0115  |
|    value_loss           | 1.18e+03 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 727      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 6.5992885 |
|    clip_fraction        | 0.168     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0218   |
|    explained_variance   | 0.0429    |
|    learning_rate        | 0.00975   |
|    loss                 | 567       |
|    n_updates            | 60        |
|    policy_gradient_loss | 0.00155   |
|    value_loss           | 1.26e+03  |
---------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 716      |
|    iterations      | 7        |
|    time_elapsed    | 80       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 11.171333 |
|    clip_fraction        | 0.179     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0152   |
|    explained_variance   | 0.0784    |
|    learning_rate        | 0.00975   |
|    loss                 | 255       |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.00482  |
|    value_loss           | 1e+03     |
---------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 709      |
|    iterations      | 8        |
|    time_elapsed    | 92       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:56:16,897] Trial 32 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.009745243684410168, 'gamma': 0.9731150819381011, 'gae_lambda': 0.9671159820683667}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 1158     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 46.526737 |
|    clip_fraction        | 0.999     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00778  |
|    explained_variance   | 0.00144   |
|    learning_rate        | 0.00979   |
|    loss                 | 244       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.322     |
|    value_loss           | 236       |
---------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.7     |
|    ep_rew_mean     | 299      |
| time/              |          |
|    fps             | 838      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 20000    |
| train/                  |          |
|    approx_kl            | 9.541687 |
|    clip_fraction        | 0.0665   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00487 |
|    explained_variance   | 0.00345  |
|    learning_rate        | 0.00979  |
|    loss                 | 1.05     |
|    n_updates            | 20       |
|    policy_gradient_loss | -0.032   |
|    value_loss           | 117      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 784      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.33241045 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00155   |
|    explained_variance   | 0.264      |
|    learning_rate        | 0.00979    |
|    loss                 | 26.5       |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0172     |
|    value_loss           | 172        |
----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 750      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.049310163 |
|    clip_fraction        | 0.00792     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00139    |
|    explained_variance   | 0.38        |
|    learning_rate        | 0.00979     |
|    loss                 | 4.73        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.000229   |
|    value_loss           | 187         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 729      |
|    iterations      | 5        |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 45000    |
| train/                  |          |
|    approx_kl            | 4.263449 |
|    clip_fraction        | 0.0824   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0069  |
|    explained_variance   | 0.784    |
|    learning_rate        | 0.00979  |
|    loss                 | 20.6     |
|    n_updates            | 50       |
|    policy_gradient_loss | -0.0183  |
|    value_loss           | 123      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 724      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.22521484 |
|    clip_fraction        | 0.00876    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00152   |
|    explained_variance   | 0.293      |
|    learning_rate        | 0.00979    |
|    loss                 | 59.7       |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.00443    |
|    value_loss           | 300        |
----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 715      |
|    iterations      | 7        |
|    time_elapsed    | 80       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 1.3932666e-05 |
|    clip_fraction        | 0.024         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00426      |
|    explained_variance   | 0.622         |
|    learning_rate        | 0.00979       |
|    loss                 | 211           |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.00346      |
|    value_loss           | 249           |
-------------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 708      |
|    iterations      | 8        |
|    time_elapsed    | 92       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:57:54,961] Trial 33 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.009785337437218277, 'gamma': 0.9392537313520761, 'gae_lambda': 0.8355714764069662}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 1158     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 10000    |
| train/                  |          |
|    approx_kl            | 95.76292 |
|    clip_fraction        | 0.995    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.033   |
|    explained_variance   | -0.00126 |
|    learning_rate        | 0.00626  |
|    loss                 | 332      |
|    n_updates            | 10       |
|    policy_gradient_loss | 0.456    |
|    value_loss           | 876      |
--------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.1     |
|    ep_rew_mean     | 296      |
| time/              |          |
|    fps             | 840      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 15.722077 |
|    clip_fraction        | 0.214     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00925  |
|    explained_variance   | -0.0143   |
|    learning_rate        | 0.00626   |
|    loss                 | 2.59      |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.0593   |
|    value_loss           | 276       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 785      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=341.60 +/- 76.80
Episode length: 110.40 +/- 19.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 110       |
|    mean_reward          | 342       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 1.8251126 |
|    clip_fraction        | 0.161     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00633  |
|    explained_variance   | 0.44      |
|    learning_rate        | 0.00626   |
|    loss                 | 425       |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.0359   |
|    value_loss           | 498       |
---------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=322.40 +/- 63.68
Episode length: 105.60 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 319      |
| time/              |          |
|    fps             | 747      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=360.80 +/- 94.06
Episode length: 115.20 +/- 23.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 115       |
|    mean_reward          | 361       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 3.8856559 |
|    clip_fraction        | 0.179     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0277   |
|    explained_variance   | 0.247     |
|    learning_rate        | 0.00626   |
|    loss                 | 199       |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.00158  |
|    value_loss           | 636       |
---------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=341.60 +/- 47.03
Episode length: 110.40 +/- 11.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 366      |
| time/              |          |
|    fps             | 725      |
|    iterations      | 5        |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=408.80 +/- 121.81
Episode length: 127.20 +/- 30.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 127       |
|    mean_reward          | 409       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 1.5209243 |
|    clip_fraction        | 0.18      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0778   |
|    explained_variance   | 0.398     |
|    learning_rate        | 0.00626   |
|    loss                 | 853       |
|    n_updates            | 50        |
|    policy_gradient_loss | 0.0671    |
|    value_loss           | 861       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 373      |
| time/              |          |
|    fps             | 718      |
|    iterations      | 6        |
|    time_elapsed    | 68       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=421.60 +/- 119.78
Episode length: 130.40 +/- 29.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 130       |
|    mean_reward          | 422       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 3.8374794 |
|    clip_fraction        | 0.224     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0405   |
|    explained_variance   | 0.299     |
|    learning_rate        | 0.00626   |
|    loss                 | 514       |
|    n_updates            | 60        |
|    policy_gradient_loss | 0.104     |
|    value_loss           | 950       |
---------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=392.80 +/- 122.44
Episode length: 123.20 +/- 30.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 393      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 397      |
| time/              |          |
|    fps             | 707      |
|    iterations      | 7        |
|    time_elapsed    | 81       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=319.20 +/- 33.41
Episode length: 104.80 +/- 8.35
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 105      |
|    mean_reward          | 319      |
| time/                   |          |
|    total_timesteps      | 60000    |
| train/                  |          |
|    approx_kl            | 33.29646 |
|    clip_fraction        | 0.45     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0019  |
|    explained_variance   | 0.297    |
|    learning_rate        | 0.00626  |
|    loss                 | 342      |
|    n_updates            | 70       |
|    policy_gradient_loss | 0.0247   |
|    value_loss           | 899      |
--------------------------------------
Eval num_timesteps=65000, episode_reward=303.20 +/- 25.60
Episode length: 100.80 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 303      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 704      |
|    iterations      | 8        |
|    time_elapsed    | 92       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 18:59:33,369] Trial 34 finished with value: 287.2 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.006264159891092369, 'gamma': 0.955929747979788, 'gae_lambda': 0.9830437421437355}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=312.80 +/- 43.99
Episode length: 103.20 +/- 11.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 1151     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012084691 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.000986    |
|    learning_rate        | 7.25e-05    |
|    loss                 | 265         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 682         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 951      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=316.00 +/- 37.86
Episode length: 104.00 +/- 9.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | 316          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0075110435 |
|    clip_fraction        | 0.142        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | 0.356        |
|    learning_rate        | 7.25e-05     |
|    loss                 | 323          |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000533    |
|    value_loss           | 680          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 378      |
| time/              |          |
|    fps             | 915      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011894926 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.462       |
|    learning_rate        | 7.25e-05    |
|    loss                 | 533         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.000846    |
|    value_loss           | 765         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 878      |
|    iterations      | 4        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.010073015 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.441       |
|    learning_rate        | 7.25e-05    |
|    loss                 | 396         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.000345   |
|    value_loss           | 859         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 860      |
|    iterations      | 5        |
|    time_elapsed    | 47       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.011135528 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.303       |
|    learning_rate        | 7.25e-05    |
|    loss                 | 314         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00159     |
|    value_loss           | 1.07e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 415      |
| time/              |          |
|    fps             | 858      |
|    iterations      | 6        |
|    time_elapsed    | 57       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.012267897 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.468       |
|    learning_rate        | 7.25e-05    |
|    loss                 | 447         |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00134     |
|    value_loss           | 861         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=290.40 +/- 12.80
Episode length: 97.60 +/- 3.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 414      |
| time/              |          |
|    fps             | 850      |
|    iterations      | 7        |
|    time_elapsed    | 67       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=290.40 +/- 19.20
Episode length: 97.60 +/- 4.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.6        |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.014798491 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.516       |
|    learning_rate        | 7.25e-05    |
|    loss                 | 424         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.000576   |
|    value_loss           | 766         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=287.20 +/- 9.60
Episode length: 96.80 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 843      |
|    iterations      | 8        |
|    time_elapsed    | 77       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 19:00:54,421] Trial 35 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 7.250748247367667e-05, 'gamma': 0.9665205503685744, 'gae_lambda': 0.9410898183760862}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 386      |
| time/              |          |
|    fps             | 1252     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01342997 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.000587   |
|    learning_rate        | 0.000149   |
|    loss                 | 75.1       |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.0033     |
|    value_loss           | 545        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 864      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.017257731 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.208       |
|    learning_rate        | 0.000149    |
|    loss                 | 450         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00226     |
|    value_loss           | 650         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 785      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.015620818 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.241       |
|    learning_rate        | 0.000149    |
|    loss                 | 808         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.0054      |
|    value_loss           | 870         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 749      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.022662897 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.000149    |
|    loss                 | 955         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00311     |
|    value_loss           | 930         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 734      |
|    iterations      | 5        |
|    time_elapsed    | 27       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 121         |
|    ep_rew_mean          | 384         |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 6           |
|    time_elapsed         | 33          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.018964384 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.000149    |
|    loss                 | 842         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00345     |
|    value_loss           | 1.19e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01637943 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 0.289      |
|    learning_rate        | 0.000149   |
|    loss                 | 496        |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.00121    |
|    value_loss           | 1.28e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 381      |
| time/              |          |
|    fps             | 721      |
|    iterations      | 7        |
|    time_elapsed    | 39       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.020450402 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.224       |
|    learning_rate        | 0.000149    |
|    loss                 | 557         |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.00424     |
|    value_loss           | 1.2e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 399      |
| time/              |          |
|    fps             | 714      |
|    iterations      | 8        |
|    time_elapsed    | 45       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.021785596 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.223       |
|    learning_rate        | 0.000149    |
|    loss                 | 1.42e+03    |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00379     |
|    value_loss           | 1.12e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 707      |
|    iterations      | 9        |
|    time_elapsed    | 52       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.015818547 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.212       |
|    learning_rate        | 0.000149    |
|    loss                 | 850         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00314     |
|    value_loss           | 1.09e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 365      |
| time/              |          |
|    fps             | 703      |
|    iterations      | 10       |
|    time_elapsed    | 58       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.014491573 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.233       |
|    learning_rate        | 0.000149    |
|    loss                 | 539         |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.00174     |
|    value_loss           | 1.46e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 333      |
| time/              |          |
|    fps             | 699      |
|    iterations      | 11       |
|    time_elapsed    | 64       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | 312         |
| time/                   |             |
|    fps                  | 701         |
|    iterations           | 12          |
|    time_elapsed         | 70          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.014648784 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.161       |
|    learning_rate        | 0.000149    |
|    loss                 | 744         |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00157     |
|    value_loss           | 1.61e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010956733 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.118       |
|    learning_rate        | 0.000149    |
|    loss                 | 655         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.000523   |
|    value_loss           | 1.5e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 312      |
| time/              |          |
|    fps             | 700      |
|    iterations      | 13       |
|    time_elapsed    | 76       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.015253403 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.186       |
|    learning_rate        | 0.000149    |
|    loss                 | 244         |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.000925    |
|    value_loss           | 1.29e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 308      |
| time/              |          |
|    fps             | 697      |
|    iterations      | 14       |
|    time_elapsed    | 82       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.014931552 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.221       |
|    learning_rate        | 0.000149    |
|    loss                 | 747         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00099    |
|    value_loss           | 1.31e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.5     |
|    ep_rew_mean     | 298      |
| time/              |          |
|    fps             | 694      |
|    iterations      | 15       |
|    time_elapsed    | 88       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:02:26,097] Trial 36 finished with value: 284.0 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.00014889948058048475, 'gamma': 0.9761653323506594, 'gae_lambda': 0.9202776230313973}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 368      |
| time/              |          |
|    fps             | 1149     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.4        |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.055177063 |
|    clip_fraction        | 0.505       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.00173     |
|    learning_rate        | 0.00103     |
|    loss                 | 263         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0254      |
|    value_loss           | 259         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 843      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.083955124 |
|    clip_fraction        | 0.597       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.157       |
|    learning_rate        | 0.00103     |
|    loss                 | 81.3        |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.0368      |
|    value_loss           | 248         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 411      |
| time/              |          |
|    fps             | 787      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=303.20 +/- 25.60
Episode length: 100.80 +/- 6.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 101       |
|    mean_reward          | 303       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.6758257 |
|    clip_fraction        | 0.73      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.06     |
|    explained_variance   | 0.231     |
|    learning_rate        | 0.00103   |
|    loss                 | 12.5      |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.0919    |
|    value_loss           | 252       |
---------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=328.80 +/- 38.40
Episode length: 107.20 +/- 9.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 377      |
| time/              |          |
|    fps             | 752      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=322.40 +/- 39.97
Episode length: 105.60 +/- 9.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 106        |
|    mean_reward          | 322        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.11206661 |
|    clip_fraction        | 0.598      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.226      |
|    learning_rate        | 0.00103    |
|    loss                 | 61.7       |
|    n_updates            | 40         |
|    policy_gradient_loss | 0.0441     |
|    value_loss           | 367        |
----------------------------------------
Eval num_timesteps=40000, episode_reward=332.00 +/- 50.09
Episode length: 108.00 +/- 12.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 731      |
|    iterations      | 5        |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=769.60 +/- 571.79
Episode length: 214.90 +/- 137.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 770        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.15718856 |
|    clip_fraction        | 0.634      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.963     |
|    explained_variance   | 0.166      |
|    learning_rate        | 0.00103    |
|    loss                 | 26.5       |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.0619     |
|    value_loss           | 388        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 502      |
| time/              |          |
|    fps             | 717      |
|    iterations      | 6        |
|    time_elapsed    | 68       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 6.543389 |
|    clip_fraction        | 0.912    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.253   |
|    explained_variance   | -0.00528 |
|    learning_rate        | 0.00103  |
|    loss                 | 26.7     |
|    n_updates            | 60       |
|    policy_gradient_loss | 0.247    |
|    value_loss           | 312      |
--------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 711      |
|    iterations      | 7        |
|    time_elapsed    | 80       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 96             |
|    mean_reward          | 284            |
| time/                   |                |
|    total_timesteps      | 60000          |
| train/                  |                |
|    approx_kl            | -2.1100277e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -8.42e-05      |
|    explained_variance   | -0.0871        |
|    learning_rate        | 0.00103        |
|    loss                 | 6.77           |
|    n_updates            | 70             |
|    policy_gradient_loss | -8.85e-06      |
|    value_loss           | 187            |
--------------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 704      |
|    iterations      | 8        |
|    time_elapsed    | 93       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 19:04:04,568] Trial 37 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.001030431154886584, 'gamma': 0.9912825047419781, 'gae_lambda': 0.8233635776749471}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 1078     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=332.00 +/- 50.09
Episode length: 108.00 +/- 12.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 108       |
|    mean_reward          | 332       |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 14.141773 |
|    clip_fraction        | 0.973     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.234    |
|    explained_variance   | 0.00282   |
|    learning_rate        | 0.00575   |
|    loss                 | 185       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.264     |
|    value_loss           | 220       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 915      |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=303.20 +/- 32.63
Episode length: 100.80 +/- 8.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 101       |
|    mean_reward          | 303       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.3407578 |
|    clip_fraction        | 0.033     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0106   |
|    explained_variance   | -0.112    |
|    learning_rate        | 0.00575   |
|    loss                 | 78.5      |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.0267    |
|    value_loss           | 334       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 336      |
| time/              |          |
|    fps             | 838      |
|    iterations      | 3        |
|    time_elapsed    | 14       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=306.40 +/- 28.80
Episode length: 101.60 +/- 7.20
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 102            |
|    mean_reward          | 306            |
| time/                   |                |
|    total_timesteps      | 15000          |
| train/                  |                |
|    approx_kl            | -4.8894435e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.92e-05      |
|    explained_variance   | 0.0368         |
|    learning_rate        | 0.00575        |
|    loss                 | 306            |
|    n_updates            | 30             |
|    policy_gradient_loss | -6.23e-08      |
|    value_loss           | 418            |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 829      |
|    iterations      | 4        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=309.60 +/- 34.47
Episode length: 102.40 +/- 8.62
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 102      |
|    mean_reward          | 310      |
| time/                   |          |
|    total_timesteps      | 20000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0.00125  |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00302 |
|    explained_variance   | 0.051    |
|    learning_rate        | 0.00575  |
|    loss                 | 305      |
|    n_updates            | 40       |
|    policy_gradient_loss | 0.0001   |
|    value_loss           | 441      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 822      |
|    iterations      | 5        |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 103       |
|    ep_rew_mean          | 314       |
| time/                   |           |
|    fps                  | 836       |
|    iterations           | 6         |
|    time_elapsed         | 29        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.37e-14 |
|    explained_variance   | 0.0693    |
|    learning_rate        | 0.00575   |
|    loss                 | 268       |
|    n_updates            | 50        |
|    policy_gradient_loss | 3.97e-10  |
|    value_loss           | 462       |
---------------------------------------
Eval num_timesteps=25000, episode_reward=338.40 +/- 57.33
Episode length: 109.60 +/- 14.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 110       |
|    mean_reward          | 338       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.02e-14 |
|    explained_variance   | 0.0853    |
|    learning_rate        | 0.00575   |
|    loss                 | 163       |
|    n_updates            | 60        |
|    policy_gradient_loss | 6.48e-10  |
|    value_loss           | 461       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 832      |
|    iterations      | 7        |
|    time_elapsed    | 34       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=300.00 +/- 29.50
Episode length: 100.00 +/- 7.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 100       |
|    mean_reward          | 300       |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.77e-13 |
|    explained_variance   | 0.0665    |
|    learning_rate        | 0.00575   |
|    loss                 | 125       |
|    n_updates            | 70        |
|    policy_gradient_loss | -1.53e-11 |
|    value_loss           | 466       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 828      |
|    iterations      | 8        |
|    time_elapsed    | 39       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=338.40 +/- 38.00
Episode length: 109.60 +/- 9.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 110       |
|    mean_reward          | 338       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.91e-12 |
|    explained_variance   | 0.0686    |
|    learning_rate        | 0.00575   |
|    loss                 | 117       |
|    n_updates            | 80        |
|    policy_gradient_loss | -8.75e-10 |
|    value_loss           | 493       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 311      |
| time/              |          |
|    fps             | 825      |
|    iterations      | 9        |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=325.60 +/- 38.00
Episode length: 106.40 +/- 9.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 106       |
|    mean_reward          | 326       |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.55e-11 |
|    explained_variance   | 0.0728    |
|    learning_rate        | 0.00575   |
|    loss                 | 274       |
|    n_updates            | 90        |
|    policy_gradient_loss | -1.08e-10 |
|    value_loss           | 504       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 307      |
| time/              |          |
|    fps             | 821      |
|    iterations      | 10       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=322.40 +/- 39.97
Episode length: 105.60 +/- 9.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 106       |
|    mean_reward          | 322       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.02e-10 |
|    explained_variance   | 0.0856    |
|    learning_rate        | 0.00575   |
|    loss                 | 214       |
|    n_updates            | 100       |
|    policy_gradient_loss | -1.12e-09 |
|    value_loss           | 493       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 307      |
| time/              |          |
|    fps             | 819      |
|    iterations      | 11       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 103       |
|    ep_rew_mean          | 312       |
| time/                   |           |
|    fps                  | 827       |
|    iterations           | 12        |
|    time_elapsed         | 59        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.44e-10 |
|    explained_variance   | 0.0965    |
|    learning_rate        | 0.00575   |
|    loss                 | 365       |
|    n_updates            | 110       |
|    policy_gradient_loss | 9.85e-10  |
|    value_loss           | 506       |
---------------------------------------
Eval num_timesteps=50000, episode_reward=332.00 +/- 32.79
Episode length: 108.00 +/- 8.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 108       |
|    mean_reward          | 332       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.82e-09 |
|    explained_variance   | 0.103     |
|    learning_rate        | 0.00575   |
|    loss                 | 260       |
|    n_updates            | 120       |
|    policy_gradient_loss | 9.02e-11  |
|    value_loss           | 492       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 825      |
|    iterations      | 13       |
|    time_elapsed    | 64       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=332.00 +/- 50.09
Episode length: 108.00 +/- 12.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 108       |
|    mean_reward          | 332       |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.94e-08 |
|    explained_variance   | 0.105     |
|    learning_rate        | 0.00575   |
|    loss                 | 372       |
|    n_updates            | 130       |
|    policy_gradient_loss | -1.17e-09 |
|    value_loss           | 504       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 824      |
|    iterations      | 14       |
|    time_elapsed    | 69       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=300.00 +/- 32.79
Episode length: 100.00 +/- 8.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 100       |
|    mean_reward          | 300       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1e-08    |
|    explained_variance   | 0.0777    |
|    learning_rate        | 0.00575   |
|    loss                 | 83.4      |
|    n_updates            | 140       |
|    policy_gradient_loss | -4.09e-10 |
|    value_loss           | 501       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 822      |
|    iterations      | 15       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:05:21,414] Trial 38 finished with value: 306.4 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.005751059135056499, 'gamma': 0.9423834622685101, 'gae_lambda': 0.8456109780110339}. Best is trial 2 with value: 396.0.
Using cuda device
Eval num_timesteps=5000, episode_reward=351.20 +/- 96.48
Episode length: 112.80 +/- 24.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 351      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 1138     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012363864 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.000601    |
|    learning_rate        | 0.000286    |
|    loss                 | 224         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.00145     |
|    value_loss           | 522         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 409      |
| time/              |          |
|    fps             | 1003     |
|    iterations      | 2        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.009351522 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.11        |
|    learning_rate        | 0.000286    |
|    loss                 | 209         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00335     |
|    value_loss           | 536         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 988      |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010384025 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.12        |
|    learning_rate        | 0.000286    |
|    loss                 | 357         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 578         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 963      |
|    iterations      | 4        |
|    time_elapsed    | 33       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.00982473 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.153      |
|    learning_rate        | 0.000286   |
|    loss                 | 310        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00162   |
|    value_loss           | 525        |
----------------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 404      |
| time/              |          |
|    fps             | 949      |
|    iterations      | 5        |
|    time_elapsed    | 43       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.008897707 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.0509      |
|    learning_rate        | 0.000286    |
|    loss                 | 207         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.002      |
|    value_loss           | 591         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 389      |
| time/              |          |
|    fps             | 951      |
|    iterations      | 6        |
|    time_elapsed    | 51       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009377489 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.134       |
|    learning_rate        | 0.000286    |
|    loss                 | 144         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.000766   |
|    value_loss           | 513         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 942      |
|    iterations      | 7        |
|    time_elapsed    | 60       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.013281055 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.0999      |
|    learning_rate        | 0.000286    |
|    loss                 | 275         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 496         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 935      |
|    iterations      | 8        |
|    time_elapsed    | 70       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 19:06:33,777] Trial 39 finished with value: 284.0 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 0.0002863560494875341, 'gamma': 0.9252185037700033, 'gae_lambda': 0.9669356059795692}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 372      |
| time/              |          |
|    fps             | 1256     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 7.9177976 |
|    clip_fraction        | 0.852     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.902    |
|    explained_variance   | 0.00147   |
|    learning_rate        | 0.00249   |
|    loss                 | 20        |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.162     |
|    value_loss           | 372       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 863      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 1.5634232 |
|    clip_fraction        | 0.827     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.163    |
|    explained_variance   | -0.000192 |
|    learning_rate        | 0.00249   |
|    loss                 | 122       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.0556    |
|    value_loss           | 725       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 301      |
| time/              |          |
|    fps             | 781      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 96       |
|    mean_reward          | 284      |
| time/                   |          |
|    total_timesteps      | 15000    |
| train/                  |          |
|    approx_kl            | 8.579338 |
|    clip_fraction        | 0.346    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0397  |
|    explained_variance   | 0.109    |
|    learning_rate        | 0.00249  |
|    loss                 | 66.2     |
|    n_updates            | 30       |
|    policy_gradient_loss | 0.147    |
|    value_loss           | 597      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 747      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=312.80 +/- 61.47
Episode length: 103.20 +/- 15.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 103       |
|    mean_reward          | 313       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 1.8250866 |
|    clip_fraction        | 0.0374    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00304  |
|    explained_variance   | 0.017     |
|    learning_rate        | 0.00249   |
|    loss                 | 9.05      |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0206   |
|    value_loss           | 193       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.9     |
|    ep_rew_mean     | 296      |
| time/              |          |
|    fps             | 724      |
|    iterations      | 5        |
|    time_elapsed    | 28       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 111       |
|    ep_rew_mean          | 342       |
| time/                   |           |
|    fps                  | 727       |
|    iterations           | 6         |
|    time_elapsed         | 33        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 2.9242887 |
|    clip_fraction        | 0.115     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0261   |
|    explained_variance   | 0.37      |
|    learning_rate        | 0.00249   |
|    loss                 | 16.9      |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0234   |
|    value_loss           | 292       |
---------------------------------------
Eval num_timesteps=25000, episode_reward=476.00 +/- 121.43
Episode length: 144.00 +/- 30.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 144       |
|    mean_reward          | 476       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 1.8555323 |
|    clip_fraction        | 0.348     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0443   |
|    explained_variance   | 0.64      |
|    learning_rate        | 0.00249   |
|    loss                 | 261       |
|    n_updates            | 60        |
|    policy_gradient_loss | 0.204     |
|    value_loss           | 616       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 392      |
| time/              |          |
|    fps             | 711      |
|    iterations      | 7        |
|    time_elapsed    | 40       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=447.20 +/- 121.81
Episode length: 136.80 +/- 30.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 137       |
|    mean_reward          | 447       |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 1.1259967 |
|    clip_fraction        | 0.103     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0229   |
|    explained_variance   | 0.777     |
|    learning_rate        | 0.00249   |
|    loss                 | 166       |
|    n_updates            | 70        |
|    policy_gradient_loss | 0.0189    |
|    value_loss           | 387       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 702      |
|    iterations      | 8        |
|    time_elapsed    | 46       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=306.40 +/- 40.60
Episode length: 101.60 +/- 10.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 306       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 45.738968 |
|    clip_fraction        | 0.291     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0105   |
|    explained_variance   | 0.771     |
|    learning_rate        | 0.00249   |
|    loss                 | 311       |
|    n_updates            | 80        |
|    policy_gradient_loss | 0.0443    |
|    value_loss           | 567       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 392      |
| time/              |          |
|    fps             | 698      |
|    iterations      | 9        |
|    time_elapsed    | 52       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=309.60 +/- 39.97
Episode length: 102.40 +/- 9.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 310       |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.35e-12 |
|    explained_variance   | -0.106    |
|    learning_rate        | 0.00249   |
|    loss                 | 645       |
|    n_updates            | 90        |
|    policy_gradient_loss | -1.45e-07 |
|    value_loss           | 863       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 337      |
| time/              |          |
|    fps             | 695      |
|    iterations      | 10       |
|    time_elapsed    | 58       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=306.40 +/- 35.20
Episode length: 101.60 +/- 8.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 306       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.14e-21 |
|    explained_variance   | 0.158     |
|    learning_rate        | 0.00249   |
|    loss                 | 960       |
|    n_updates            | 100       |
|    policy_gradient_loss | -6.28e-10 |
|    value_loss           | 1.07e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 692      |
|    iterations      | 11       |
|    time_elapsed    | 65       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 103       |
|    ep_rew_mean          | 313       |
| time/                   |           |
|    fps                  | 697       |
|    iterations           | 12        |
|    time_elapsed         | 70        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.34e-21 |
|    explained_variance   | 0.156     |
|    learning_rate        | 0.00249   |
|    loss                 | 1.43e+03  |
|    n_updates            | 110       |
|    policy_gradient_loss | 9.98e-10  |
|    value_loss           | 1.16e+03  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=303.20 +/- 25.60
Episode length: 100.80 +/- 6.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 101       |
|    mean_reward          | 303       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.31e-23 |
|    explained_variance   | 0.134     |
|    learning_rate        | 0.00249   |
|    loss                 | 919       |
|    n_updates            | 120       |
|    policy_gradient_loss | -1.26e-09 |
|    value_loss           | 1.4e+03   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 695      |
|    iterations      | 13       |
|    time_elapsed    | 76       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=325.60 +/- 40.60
Episode length: 106.40 +/- 10.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 106       |
|    mean_reward          | 326       |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.83e-24 |
|    explained_variance   | 0.135     |
|    learning_rate        | 0.00249   |
|    loss                 | 1.76e+03  |
|    n_updates            | 130       |
|    policy_gradient_loss | -9.09e-11 |
|    value_loss           | 1.51e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 693      |
|    iterations      | 14       |
|    time_elapsed    | 82       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=309.60 +/- 34.47
Episode length: 102.40 +/- 8.62
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 102      |
|    mean_reward          | 310      |
| time/                   |          |
|    total_timesteps      | 60000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -2.8e-24 |
|    explained_variance   | 0.141    |
|    learning_rate        | 0.00249  |
|    loss                 | 323      |
|    n_updates            | 140      |
|    policy_gradient_loss | 8.4e-10  |
|    value_loss           | 1.64e+03 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 691      |
|    iterations      | 15       |
|    time_elapsed    | 88       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:08:05,927] Trial 40 finished with value: 332.0 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.002486822597740292, 'gamma': 0.9944700464089696, 'gae_lambda': 0.8807945717433429}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 1273     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 15.368452 |
|    clip_fraction        | 0.978     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.236    |
|    explained_variance   | 0.00338   |
|    learning_rate        | 0.00289   |
|    loss                 | 98.1      |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.327     |
|    value_loss           | 363       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 329      |
| time/              |          |
|    fps             | 885      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.19056903 |
|    clip_fraction        | 0.0526     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.000731   |
|    learning_rate        | 0.00289    |
|    loss                 | 690        |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 729        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 797      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=351.20 +/- 172.00
Episode length: 112.80 +/- 43.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 113       |
|    mean_reward          | 351       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 1.4553679 |
|    clip_fraction        | 0.25      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.173    |
|    explained_variance   | 0.349     |
|    learning_rate        | 0.00289   |
|    loss                 | 531       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.0374    |
|    value_loss           | 557       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 755      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=332.00 +/- 35.78
Episode length: 108.00 +/- 8.94
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 108      |
|    mean_reward          | 332      |
| time/                   |          |
|    total_timesteps      | 20000    |
| train/                  |          |
|    approx_kl            | 23.53769 |
|    clip_fraction        | 0.615    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0576  |
|    explained_variance   | -0.011   |
|    learning_rate        | 0.00289  |
|    loss                 | 41.1     |
|    n_updates            | 40       |
|    policy_gradient_loss | 0.158    |
|    value_loss           | 266      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 733      |
|    iterations      | 5        |
|    time_elapsed    | 27       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | 394         |
| time/                   |             |
|    fps                  | 735         |
|    iterations           | 6           |
|    time_elapsed         | 33          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.019256841 |
|    clip_fraction        | 0.000415    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000183   |
|    explained_variance   | 0.0132      |
|    learning_rate        | 0.00289     |
|    loss                 | 240         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.000289   |
|    value_loss           | 943         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=322.40 +/- 39.97
Episode length: 105.60 +/- 9.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 106       |
|    mean_reward          | 322       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.56e-18 |
|    explained_variance   | 0.0659    |
|    learning_rate        | 0.00289   |
|    loss                 | 351       |
|    n_updates            | 60        |
|    policy_gradient_loss | 1.03e-09  |
|    value_loss           | 1.02e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 312      |
| time/              |          |
|    fps             | 725      |
|    iterations      | 7        |
|    time_elapsed    | 39       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=290.40 +/- 19.20
Episode length: 97.60 +/- 4.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 97.6      |
|    mean_reward          | 290       |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.29e-20 |
|    explained_variance   | 0.112     |
|    learning_rate        | 0.00289   |
|    loss                 | 465       |
|    n_updates            | 70        |
|    policy_gradient_loss | 1.36e-10  |
|    value_loss           | 1.14e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 718      |
|    iterations      | 8        |
|    time_elapsed    | 45       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=309.60 +/- 34.47
Episode length: 102.40 +/- 8.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | 310       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.38e-21 |
|    explained_variance   | 0.125     |
|    learning_rate        | 0.00289   |
|    loss                 | 491       |
|    n_updates            | 80        |
|    policy_gradient_loss | 6.78e-10  |
|    value_loss           | 1.29e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 712      |
|    iterations      | 9        |
|    time_elapsed    | 51       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=303.20 +/- 29.33
Episode length: 100.80 +/- 7.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 101       |
|    mean_reward          | 303       |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.12e-22 |
|    explained_variance   | 0.16      |
|    learning_rate        | 0.00289   |
|    loss                 | 653       |
|    n_updates            | 90        |
|    policy_gradient_loss | -1.56e-10 |
|    value_loss           | 1.33e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 707      |
|    iterations      | 10       |
|    time_elapsed    | 57       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=303.20 +/- 32.63
Episode length: 100.80 +/- 8.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 101       |
|    mean_reward          | 303       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.06e-23 |
|    explained_variance   | 0.167     |
|    learning_rate        | 0.00289   |
|    loss                 | 1.08e+03  |
|    n_updates            | 100       |
|    policy_gradient_loss | 9.05e-10  |
|    value_loss           | 1.41e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 704      |
|    iterations      | 11       |
|    time_elapsed    | 63       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 104       |
|    ep_rew_mean          | 316       |
| time/                   |           |
|    fps                  | 709       |
|    iterations           | 12        |
|    time_elapsed         | 69        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.08e-24 |
|    explained_variance   | 0.144     |
|    learning_rate        | 0.00289   |
|    loss                 | 1.59e+03  |
|    n_updates            | 110       |
|    policy_gradient_loss | 2.8e-10   |
|    value_loss           | 1.49e+03  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=312.80 +/- 39.06
Episode length: 103.20 +/- 9.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 103       |
|    mean_reward          | 313       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.49e-23 |
|    explained_variance   | 0.171     |
|    learning_rate        | 0.00289   |
|    loss                 | 530       |
|    n_updates            | 120       |
|    policy_gradient_loss | -4.78e-10 |
|    value_loss           | 1.4e+03   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 312      |
| time/              |          |
|    fps             | 706      |
|    iterations      | 13       |
|    time_elapsed    | 75       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=328.80 +/- 35.63
Episode length: 107.20 +/- 8.91
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 107      |
|    mean_reward          | 329      |
| time/                   |          |
|    total_timesteps      | 55000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.3e-24 |
|    explained_variance   | 0.176    |
|    learning_rate        | 0.00289  |
|    loss                 | 694      |
|    n_updates            | 130      |
|    policy_gradient_loss | 9.24e-11 |
|    value_loss           | 1.56e+03 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 702      |
|    iterations      | 14       |
|    time_elapsed    | 81       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=319.20 +/- 33.41
Episode length: 104.80 +/- 8.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 105       |
|    mean_reward          | 319       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.11e-24 |
|    explained_variance   | 0.154     |
|    learning_rate        | 0.00289   |
|    loss                 | 327       |
|    n_updates            | 140       |
|    policy_gradient_loss | 6.25e-10  |
|    value_loss           | 1.51e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 700      |
|    iterations      | 15       |
|    time_elapsed    | 87       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:09:36,745] Trial 41 finished with value: 303.2 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.002893274945659454, 'gamma': 0.9870538880239327, 'gae_lambda': 0.8852263939214553}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 378      |
| time/              |          |
|    fps             | 1259     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 6.9065037 |
|    clip_fraction        | 0.898     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.684    |
|    explained_variance   | 0.000836  |
|    learning_rate        | 0.00232   |
|    loss                 | 46.7      |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.204     |
|    value_loss           | 328       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 326      |
| time/              |          |
|    fps             | 863      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.2118495 |
|    clip_fraction        | 0.0746    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.142    |
|    explained_variance   | -0.00195  |
|    learning_rate        | 0.00232   |
|    loss                 | 411       |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.0203   |
|    value_loss           | 613       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 787      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 1.8334169 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.127    |
|    explained_variance   | 0.17      |
|    learning_rate        | 0.00232   |
|    loss                 | 118       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.0138    |
|    value_loss           | 462       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 751      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 1.4086053 |
|    clip_fraction        | 0.0674    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0132   |
|    explained_variance   | 0.176     |
|    learning_rate        | 0.00232   |
|    loss                 | 292       |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.0149    |
|    value_loss           | 818       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | 285      |
| time/              |          |
|    fps             | 730      |
|    iterations      | 5        |
|    time_elapsed    | 28       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 96         |
|    ep_rew_mean          | 284        |
| time/                   |            |
|    fps                  | 730        |
|    iterations           | 6          |
|    time_elapsed         | 33         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.61628574 |
|    clip_fraction        | 0.0285     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00729   |
|    explained_variance   | 0.44       |
|    learning_rate        | 0.00232    |
|    loss                 | 737        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.00446   |
|    value_loss           | 897        |
----------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.7199523 |
|    clip_fraction        | 0.0455    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0146   |
|    explained_variance   | 0.389     |
|    learning_rate        | 0.00232   |
|    loss                 | 301       |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.00633  |
|    value_loss           | 865       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 719      |
|    iterations      | 7        |
|    time_elapsed    | 39       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.73000175 |
|    clip_fraction        | 0.0419     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0159    |
|    explained_variance   | 0.415      |
|    learning_rate        | 0.00232    |
|    loss                 | 911        |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.000408   |
|    value_loss           | 1.01e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 714      |
|    iterations      | 8        |
|    time_elapsed    | 45       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.79093087 |
|    clip_fraction        | 0.0295     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0098    |
|    explained_variance   | 0.401      |
|    learning_rate        | 0.00232    |
|    loss                 | 408        |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.00922   |
|    value_loss           | 1.07e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 708      |
|    iterations      | 9        |
|    time_elapsed    | 52       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 1.0686274 |
|    clip_fraction        | 0.0799    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0271   |
|    explained_variance   | 0.447     |
|    learning_rate        | 0.00232   |
|    loss                 | 326       |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.0154   |
|    value_loss           | 1.13e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 702      |
|    iterations      | 10       |
|    time_elapsed    | 58       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.97025394 |
|    clip_fraction        | 0.0572     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0187    |
|    explained_variance   | 0.412      |
|    learning_rate        | 0.00232    |
|    loss                 | 194        |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 972        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 698      |
|    iterations      | 11       |
|    time_elapsed    | 64       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 700       |
|    iterations           | 12        |
|    time_elapsed         | 70        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 1.2150741 |
|    clip_fraction        | 0.0429    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00851  |
|    explained_variance   | 0.413     |
|    learning_rate        | 0.00232   |
|    loss                 | 221       |
|    n_updates            | 110       |
|    policy_gradient_loss | 0.0225    |
|    value_loss           | 949       |
---------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.52345645 |
|    clip_fraction        | 0.0186     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00715   |
|    explained_variance   | 0.439      |
|    learning_rate        | 0.00232    |
|    loss                 | 257        |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.00699   |
|    value_loss           | 1.16e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 696      |
|    iterations      | 13       |
|    time_elapsed    | 76       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 1.4430596 |
|    clip_fraction        | 0.132     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.069    |
|    explained_variance   | 0.459     |
|    learning_rate        | 0.00232   |
|    loss                 | 262       |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.00417  |
|    value_loss           | 799       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 693      |
|    iterations      | 14       |
|    time_elapsed    | 82       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 3.7499285 |
|    clip_fraction        | 0.224     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0623   |
|    explained_variance   | 0.37      |
|    learning_rate        | 0.00232   |
|    loss                 | 795       |
|    n_updates            | 140       |
|    policy_gradient_loss | 0.0102    |
|    value_loss           | 851       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 691      |
|    iterations      | 15       |
|    time_elapsed    | 88       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:11:09,010] Trial 42 finished with value: 344.8 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.002323872821653335, 'gamma': 0.994782842416329, 'gae_lambda': 0.8678670642920288}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 1274     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 0.6931101 |
|    clip_fraction        | 0.794     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.01     |
|    explained_variance   | -0.000362 |
|    learning_rate        | 0.00228   |
|    loss                 | 88        |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.119     |
|    value_loss           | 342       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 463      |
| time/              |          |
|    fps             | 870      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 6.4089127 |
|    clip_fraction        | 0.972     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.169    |
|    explained_variance   | 0.00166   |
|    learning_rate        | 0.00228   |
|    loss                 | 44.5      |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.233     |
|    value_loss           | 355       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 787      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.61e-06 |
|    explained_variance   | -0.0797   |
|    learning_rate        | 0.00228   |
|    loss                 | 11.8      |
|    n_updates            | 30        |
|    policy_gradient_loss | -8.47e-07 |
|    value_loss           | 212       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 751      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.88e-09 |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.00228   |
|    loss                 | 0.803     |
|    n_updates            | 40        |
|    policy_gradient_loss | -3.17e-10 |
|    value_loss           | 12.2      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 731      |
|    iterations      | 5        |
|    time_elapsed    | 28       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 96        |
|    ep_rew_mean          | 284       |
| time/                   |           |
|    fps                  | 730       |
|    iterations           | 6         |
|    time_elapsed         | 33        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.22e-10 |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.00228   |
|    loss                 | 1.28      |
|    n_updates            | 50        |
|    policy_gradient_loss | 7.33e-10  |
|    value_loss           | 10.1      |
---------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.05e-11 |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.00228   |
|    loss                 | 1.08      |
|    n_updates            | 60        |
|    policy_gradient_loss | -5.09e-11 |
|    value_loss           | 9.18      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 723      |
|    iterations      | 7        |
|    time_elapsed    | 39       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.02e-12 |
|    explained_variance   | 0.971     |
|    learning_rate        | 0.00228   |
|    loss                 | 3.29      |
|    n_updates            | 70        |
|    policy_gradient_loss | 1.07e-09  |
|    value_loss           | 9.25      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 714      |
|    iterations      | 8        |
|    time_elapsed    | 45       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.81e-13 |
|    explained_variance   | 0.982     |
|    learning_rate        | 0.00228   |
|    loss                 | 10.4      |
|    n_updates            | 80        |
|    policy_gradient_loss | 2.12e-10  |
|    value_loss           | 8.78      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 708      |
|    iterations      | 9        |
|    time_elapsed    | 51       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.21e-14 |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.00228   |
|    loss                 | 1.2       |
|    n_updates            | 90        |
|    policy_gradient_loss | 1.25e-09  |
|    value_loss           | 6.75      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 704      |
|    iterations      | 10       |
|    time_elapsed    | 58       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.74e-14 |
|    explained_variance   | 0.993     |
|    learning_rate        | 0.00228   |
|    loss                 | 0.904     |
|    n_updates            | 100       |
|    policy_gradient_loss | 6.82e-10  |
|    value_loss           | 7.39      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 699      |
|    iterations      | 11       |
|    time_elapsed    | 64       |
|    total_timesteps | 45056    |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 96       |
|    ep_rew_mean          | 284      |
| time/                   |          |
|    fps                  | 702      |
|    iterations           | 12       |
|    time_elapsed         | 69       |
|    total_timesteps      | 49152    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -3.7e-15 |
|    explained_variance   | 0.996    |
|    learning_rate        | 0.00228  |
|    loss                 | 2.82     |
|    n_updates            | 110      |
|    policy_gradient_loss | -2.7e-10 |
|    value_loss           | 6.7      |
--------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.16e-15 |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.00228   |
|    loss                 | 7.54      |
|    n_updates            | 120       |
|    policy_gradient_loss | -5.29e-10 |
|    value_loss           | 5.59      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 698      |
|    iterations      | 13       |
|    time_elapsed    | 76       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.77e-16 |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.00228   |
|    loss                 | 1.15      |
|    n_updates            | 130       |
|    policy_gradient_loss | -2.05e-09 |
|    value_loss           | 5.19      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 695      |
|    iterations      | 14       |
|    time_elapsed    | 82       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -8.1e-16  |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.00228   |
|    loss                 | 2.76      |
|    n_updates            | 140       |
|    policy_gradient_loss | -6.25e-10 |
|    value_loss           | 4.92      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 692      |
|    iterations      | 15       |
|    time_elapsed    | 88       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:12:40,920] Trial 43 finished with value: 284.0 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.0022786438456016064, 'gamma': 0.9951190639058161, 'gae_lambda': 0.8638277845968361}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 1258     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=1537.60 +/- 631.95
Episode length: 396.90 +/- 146.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 397         |
|    mean_reward          | 1.54e+03    |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.045690525 |
|    clip_fraction        | 0.42        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.000505    |
|    learning_rate        | 0.00145     |
|    loss                 | 123         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0124      |
|    value_loss           | 335         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 446      |
| time/              |          |
|    fps             | 842      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=1123.20 +/- 664.81
Episode length: 298.30 +/- 155.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 1.12e+03  |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 1.6745253 |
|    clip_fraction        | 0.895     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.684    |
|    explained_variance   | 0.0207    |
|    learning_rate        | 0.00145   |
|    loss                 | 196       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.203     |
|    value_loss           | 385       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 488      |
| time/              |          |
|    fps             | 784      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=335.20 +/- 61.05
Episode length: 108.80 +/- 15.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 109       |
|    mean_reward          | 335       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.1182867 |
|    clip_fraction        | 0.589     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.903    |
|    explained_variance   | 0.0434    |
|    learning_rate        | 0.00145   |
|    loss                 | 86.3      |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.0478    |
|    value_loss           | 377       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 539      |
| time/              |          |
|    fps             | 807      |
|    iterations      | 4        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=293.60 +/- 20.49
Episode length: 98.40 +/- 5.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 98.4      |
|    mean_reward          | 294       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 2.9733734 |
|    clip_fraction        | 0.867     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.452    |
|    explained_variance   | 0.108     |
|    learning_rate        | 0.00145   |
|    loss                 | 46.7      |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.204     |
|    value_loss           | 276       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 561      |
| time/              |          |
|    fps             | 823      |
|    iterations      | 5        |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 145        |
|    ep_rew_mean          | 481        |
| time/                   |            |
|    fps                  | 854        |
|    iterations           | 6          |
|    time_elapsed         | 28         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.10401076 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.412     |
|    explained_variance   | 0.0741     |
|    learning_rate        | 0.00145    |
|    loss                 | 316        |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.0066     |
|    value_loss           | 589        |
----------------------------------------
Eval num_timesteps=25000, episode_reward=316.00 +/- 35.05
Episode length: 104.00 +/- 8.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 104        |
|    mean_reward          | 316        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.03480664 |
|    clip_fraction        | 0.0144     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0201    |
|    explained_variance   | 0.0571     |
|    learning_rate        | 0.00145    |
|    loss                 | 215        |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.00146    |
|    value_loss           | 957        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 859      |
|    iterations      | 7        |
|    time_elapsed    | 33       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=319.20 +/- 33.41
Episode length: 104.80 +/- 8.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 105          |
|    mean_reward          | 319          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0027844328 |
|    clip_fraction        | 0.000806     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00414     |
|    explained_variance   | 0.0989       |
|    learning_rate        | 0.00145      |
|    loss                 | 252          |
|    n_updates            | 70           |
|    policy_gradient_loss | -3.77e-05    |
|    value_loss           | 1.31e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 864      |
|    iterations      | 8        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=319.20 +/- 39.06
Episode length: 104.80 +/- 9.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 105           |
|    mean_reward          | 319           |
| time/                   |               |
|    total_timesteps      | 35000         |
| train/                  |               |
|    approx_kl            | -1.717126e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000126     |
|    explained_variance   | 0.125         |
|    learning_rate        | 0.00145       |
|    loss                 | 633           |
|    n_updates            | 80            |
|    policy_gradient_loss | 1.07e-07      |
|    value_loss           | 1.44e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 868      |
|    iterations      | 9        |
|    time_elapsed    | 42       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=312.80 +/- 30.19
Episode length: 103.20 +/- 7.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 103           |
|    mean_reward          | 313           |
| time/                   |               |
|    total_timesteps      | 40000         |
| train/                  |               |
|    approx_kl            | -1.193257e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.68e-05     |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.00145       |
|    loss                 | 833           |
|    n_updates            | 90            |
|    policy_gradient_loss | 6.17e-08      |
|    value_loss           | 1.52e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 871      |
|    iterations      | 10       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=312.80 +/- 36.35
Episode length: 103.20 +/- 9.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 103           |
|    mean_reward          | 313           |
| time/                   |               |
|    total_timesteps      | 45000         |
| train/                  |               |
|    approx_kl            | -2.910383e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.66e-05     |
|    explained_variance   | 0.149         |
|    learning_rate        | 0.00145       |
|    loss                 | 1.05e+03      |
|    n_updates            | 100           |
|    policy_gradient_loss | 3.18e-08      |
|    value_loss           | 1.55e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 874      |
|    iterations      | 11       |
|    time_elapsed    | 51       |
|    total_timesteps | 45056    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 104           |
|    ep_rew_mean          | 316           |
| time/                   |               |
|    fps                  | 886           |
|    iterations           | 12            |
|    time_elapsed         | 55            |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | -4.671165e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.85e-05     |
|    explained_variance   | 0.125         |
|    learning_rate        | 0.00145       |
|    loss                 | 396           |
|    n_updates            | 110           |
|    policy_gradient_loss | -1.21e-08     |
|    value_loss           | 1.72e+03      |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=300.00 +/- 25.80
Episode length: 100.00 +/- 6.45
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 100      |
|    mean_reward          | 300      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.9e-05 |
|    explained_variance   | 0.161    |
|    learning_rate        | 0.00145  |
|    loss                 | 1.04e+03 |
|    n_updates            | 120      |
|    policy_gradient_loss | 2.33e-09 |
|    value_loss           | 1.73e+03 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 887      |
|    iterations      | 13       |
|    time_elapsed    | 59       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=322.40 +/- 39.97
Episode length: 105.60 +/- 9.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 106       |
|    mean_reward          | 322       |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.76e-05 |
|    explained_variance   | 0.18      |
|    learning_rate        | 0.00145   |
|    loss                 | 1.22e+03  |
|    n_updates            | 130       |
|    policy_gradient_loss | 2.76e-09  |
|    value_loss           | 1.79e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 888      |
|    iterations      | 14       |
|    time_elapsed    | 64       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=303.20 +/- 47.89
Episode length: 100.80 +/- 11.97
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 101            |
|    mean_reward          | 303            |
| time/                   |                |
|    total_timesteps      | 60000          |
| train/                  |                |
|    approx_kl            | -1.8175342e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.12e-05      |
|    explained_variance   | 0.181          |
|    learning_rate        | 0.00145        |
|    loss                 | 351            |
|    n_updates            | 140            |
|    policy_gradient_loss | -3.06e-09      |
|    value_loss           | 1.87e+03       |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 308      |
| time/              |          |
|    fps             | 889      |
|    iterations      | 15       |
|    time_elapsed    | 69       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:13:51,639] Trial 44 finished with value: 312.8 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.0014544630330648913, 'gamma': 0.9946030642286271, 'gae_lambda': 0.8556448373137883}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 1265     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=316.00 +/- 49.57
Episode length: 104.00 +/- 12.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 104        |
|    mean_reward          | 316        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.02259471 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.000331   |
|    learning_rate        | 0.000666   |
|    loss                 | 83.5       |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.0173     |
|    value_loss           | 455        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 415      |
| time/              |          |
|    fps             | 864      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.046097394 |
|    clip_fraction        | 0.518       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.000666    |
|    loss                 | 55.8        |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.0277      |
|    value_loss           | 402         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 399      |
| time/              |          |
|    fps             | 784      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.12848344 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.388      |
|    learning_rate        | 0.000666   |
|    loss                 | 262        |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0214     |
|    value_loss           | 477        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 386      |
| time/              |          |
|    fps             | 751      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.22300974 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.947     |
|    explained_variance   | 0.217      |
|    learning_rate        | 0.000666   |
|    loss                 | 210        |
|    n_updates            | 40         |
|    policy_gradient_loss | 0.0452     |
|    value_loss           | 774        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 714      |
|    iterations      | 5        |
|    time_elapsed    | 28       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 121        |
|    ep_rew_mean          | 385        |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 6          |
|    time_elapsed         | 34         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.21903118 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.992     |
|    explained_variance   | 0.372      |
|    learning_rate        | 0.000666   |
|    loss                 | 146        |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.0263     |
|    value_loss           | 663        |
----------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.06308136 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.936     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.000666   |
|    loss                 | 78.4       |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.0231     |
|    value_loss           | 710        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 711      |
|    iterations      | 7        |
|    time_elapsed    | 40       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.20667727 |
|    clip_fraction        | 0.392      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.505     |
|    explained_variance   | 0.0679     |
|    learning_rate        | 0.000666   |
|    loss                 | 308        |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.0293     |
|    value_loss           | 1.76e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 705      |
|    iterations      | 8        |
|    time_elapsed    | 46       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.5614867 |
|    clip_fraction        | 0.117     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.128    |
|    explained_variance   | -0.0538   |
|    learning_rate        | 0.000666  |
|    loss                 | 201       |
|    n_updates            | 80        |
|    policy_gradient_loss | 0.0185    |
|    value_loss           | 1.83e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.8     |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 703      |
|    iterations      | 9        |
|    time_elapsed    | 52       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.10676952 |
|    clip_fraction        | 0.0718     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | -0.0281    |
|    learning_rate        | 0.000666   |
|    loss                 | 35.5       |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0358    |
|    value_loss           | 520        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.3     |
|    ep_rew_mean     | 285      |
| time/              |          |
|    fps             | 699      |
|    iterations      | 10       |
|    time_elapsed    | 58       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.14198092 |
|    clip_fraction        | 0.0798     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0301    |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.000666   |
|    loss                 | 88.5       |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.0413     |
|    value_loss           | 267        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 695      |
|    iterations      | 11       |
|    time_elapsed    | 64       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 96         |
|    ep_rew_mean          | 284        |
| time/                   |            |
|    fps                  | 698        |
|    iterations           | 12         |
|    time_elapsed         | 70         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.07171039 |
|    clip_fraction        | 0.0267     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0034    |
|    explained_variance   | 0.933      |
|    learning_rate        | 0.000666   |
|    loss                 | 9.66       |
|    n_updates            | 110        |
|    policy_gradient_loss | 0.00148    |
|    value_loss           | 52.9       |
----------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 3.0559022e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0002       |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.000666      |
|    loss                 | 3.09          |
|    n_updates            | 120           |
|    policy_gradient_loss | -1.3e-06      |
|    value_loss           | 14.8          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 693      |
|    iterations      | 13       |
|    time_elapsed    | 76       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 7.858034e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000126    |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.000666     |
|    loss                 | 8.64         |
|    n_updates            | 130          |
|    policy_gradient_loss | -4.69e-06    |
|    value_loss           | 11.5         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 689      |
|    iterations      | 14       |
|    time_elapsed    | 83       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 1.0186341e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.87e-05     |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.000666      |
|    loss                 | 3.96          |
|    n_updates            | 140           |
|    policy_gradient_loss | -1.6e-06      |
|    value_loss           | 9.53          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 688      |
|    iterations      | 15       |
|    time_elapsed    | 89       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:15:24,218] Trial 45 finished with value: 284.0 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.0006657532239162156, 'gamma': 0.9927604672786807, 'gae_lambda': 0.9018123374750072}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 1270     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.08248259 |
|    clip_fraction        | 0.629      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.000685   |
|    learning_rate        | 0.00134    |
|    loss                 | 27         |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.052      |
|    value_loss           | 355        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 871      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 2.3162525 |
|    clip_fraction        | 0.784     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.933    |
|    explained_variance   | 0.151     |
|    learning_rate        | 0.00134   |
|    loss                 | 126       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.11      |
|    value_loss           | 385       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 790      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 1.9719685 |
|    clip_fraction        | 0.0618    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0311   |
|    explained_variance   | -0.00364  |
|    learning_rate        | 0.00134   |
|    loss                 | 423       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.00743   |
|    value_loss           | 677       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.5     |
|    ep_rew_mean     | 298      |
| time/              |          |
|    fps             | 753      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.4        |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.082422234 |
|    clip_fraction        | 0.00728     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00235    |
|    explained_variance   | 0.0814      |
|    learning_rate        | 0.00134     |
|    loss                 | 714         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 998         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.2     |
|    ep_rew_mean     | 293      |
| time/              |          |
|    fps             | 731      |
|    iterations      | 5        |
|    time_elapsed    | 27       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 99.5       |
|    ep_rew_mean          | 298        |
| time/                   |            |
|    fps                  | 731        |
|    iterations           | 6          |
|    time_elapsed         | 33         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.48294622 |
|    clip_fraction        | 0.0691     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0377    |
|    explained_variance   | 0.424      |
|    learning_rate        | 0.00134    |
|    loss                 | 450        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.000378  |
|    value_loss           | 811        |
----------------------------------------
Eval num_timesteps=25000, episode_reward=296.80 +/- 29.33
Episode length: 99.20 +/- 7.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 99.2       |
|    mean_reward          | 297        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.56304175 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.051     |
|    explained_variance   | 0.424      |
|    learning_rate        | 0.00134    |
|    loss                 | 460        |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.0526     |
|    value_loss           | 875        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 303      |
| time/              |          |
|    fps             | 718      |
|    iterations      | 7        |
|    time_elapsed    | 39       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.09448686 |
|    clip_fraction        | 0.0694     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0427    |
|    explained_variance   | 0.376      |
|    learning_rate        | 0.00134    |
|    loss                 | 497        |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.00839    |
|    value_loss           | 944        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.3     |
|    ep_rew_mean     | 297      |
| time/              |          |
|    fps             | 712      |
|    iterations      | 8        |
|    time_elapsed    | 45       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.4       |
|    mean_reward          | 294        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.39634869 |
|    clip_fraction        | 0.0885     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0535    |
|    explained_variance   | 0.392      |
|    learning_rate        | 0.00134    |
|    loss                 | 654        |
|    n_updates            | 80         |
|    policy_gradient_loss | 0.0116     |
|    value_loss           | 761        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 301      |
| time/              |          |
|    fps             | 706      |
|    iterations      | 9        |
|    time_elapsed    | 52       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.4       |
|    mean_reward          | 294        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.26460493 |
|    clip_fraction        | 0.05       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0273    |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.00134    |
|    loss                 | 724        |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.00548    |
|    value_loss           | 972        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 301      |
| time/              |          |
|    fps             | 700      |
|    iterations      | 10       |
|    time_elapsed    | 58       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=332.00 +/- 77.40
Episode length: 108.00 +/- 19.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 108       |
|    mean_reward          | 332       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.2258311 |
|    clip_fraction        | 0.0263    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0123   |
|    explained_variance   | 0.294     |
|    learning_rate        | 0.00134   |
|    loss                 | 122       |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.00431  |
|    value_loss           | 928       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.8     |
|    ep_rew_mean     | 299      |
| time/              |          |
|    fps             | 695      |
|    iterations      | 11       |
|    time_elapsed    | 64       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 102        |
|    ep_rew_mean          | 310        |
| time/                   |            |
|    fps                  | 697        |
|    iterations           | 12         |
|    time_elapsed         | 70         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.39691818 |
|    clip_fraction        | 0.0672     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0458    |
|    explained_variance   | 0.331      |
|    learning_rate        | 0.00134    |
|    loss                 | 575        |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 874        |
----------------------------------------
Eval num_timesteps=50000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 98.4     |
|    mean_reward          | 294      |
| time/                   |          |
|    total_timesteps      | 50000    |
| train/                  |          |
|    approx_kl            | 0.560946 |
|    clip_fraction        | 0.0424   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0174  |
|    explained_variance   | 0.19     |
|    learning_rate        | 0.00134  |
|    loss                 | 207      |
|    n_updates            | 120      |
|    policy_gradient_loss | 0.0181   |
|    value_loss           | 933      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 694      |
|    iterations      | 13       |
|    time_elapsed    | 76       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=309.60 +/- 39.97
Episode length: 102.40 +/- 9.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 102        |
|    mean_reward          | 310        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.39174694 |
|    clip_fraction        | 0.038      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0207    |
|    explained_variance   | 0.253      |
|    learning_rate        | 0.00134    |
|    loss                 | 323        |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.00548   |
|    value_loss           | 883        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 691      |
|    iterations      | 14       |
|    time_elapsed    | 82       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=293.60 +/- 28.80
Episode length: 98.40 +/- 7.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.4       |
|    mean_reward          | 294        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.38838226 |
|    clip_fraction        | 0.0407     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.019     |
|    explained_variance   | 0.167      |
|    learning_rate        | 0.00134    |
|    loss                 | 76.6       |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.00716    |
|    value_loss           | 858        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 689      |
|    iterations      | 15       |
|    time_elapsed    | 89       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:16:56,686] Trial 46 finished with value: 312.8 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.0013395465939831628, 'gamma': 0.9844452863685811, 'gae_lambda': 0.8842504018161104}. Best is trial 2 with value: 396.0.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 346      |
| time/              |          |
|    fps             | 1257     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011220619 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.00157    |
|    learning_rate        | 0.000204    |
|    loss                 | 199         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.00232     |
|    value_loss           | 372         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 864      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=380.00 +/- 89.37
Episode length: 120.00 +/- 22.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 120         |
|    mean_reward          | 380         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.018624056 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.000204    |
|    loss                 | 506         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00236     |
|    value_loss           | 424         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 774      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=306.40 +/- 35.20
Episode length: 101.60 +/- 8.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.018693097 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.000204    |
|    loss                 | 329         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.00211     |
|    value_loss           | 486         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 740      |
|    iterations      | 4        |
|    time_elapsed    | 22       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=511.20 +/- 231.84
Episode length: 152.80 +/- 57.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 153        |
|    mean_reward          | 511        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.02305596 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.435      |
|    learning_rate        | 0.000204   |
|    loss                 | 259        |
|    n_updates            | 40         |
|    policy_gradient_loss | 0.00341    |
|    value_loss           | 524        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 413      |
| time/              |          |
|    fps             | 712      |
|    iterations      | 5        |
|    time_elapsed    | 28       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 131         |
|    ep_rew_mean          | 426         |
| time/                   |             |
|    fps                  | 715         |
|    iterations           | 6           |
|    time_elapsed         | 34          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.020111725 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.481       |
|    learning_rate        | 0.000204    |
|    loss                 | 713         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00421     |
|    value_loss           | 505         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=501.60 +/- 140.07
Episode length: 150.40 +/- 35.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | 502         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.020759925 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.498       |
|    learning_rate        | 0.000204    |
|    loss                 | 92.9        |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00575     |
|    value_loss           | 749         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 436      |
| time/              |          |
|    fps             | 700      |
|    iterations      | 7        |
|    time_elapsed    | 40       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=364.00 +/- 97.32
Episode length: 116.00 +/- 24.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 116        |
|    mean_reward          | 364        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.02103414 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.431      |
|    learning_rate        | 0.000204   |
|    loss                 | 66.8       |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.00787    |
|    value_loss           | 899        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 430      |
| time/              |          |
|    fps             | 692      |
|    iterations      | 8        |
|    time_elapsed    | 47       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=364.00 +/- 108.28
Episode length: 116.00 +/- 27.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 364         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.034819037 |
|    clip_fraction        | 0.419       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.2         |
|    learning_rate        | 0.000204    |
|    loss                 | 127         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.0147      |
|    value_loss           | 1.06e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 438      |
| time/              |          |
|    fps             | 687      |
|    iterations      | 9        |
|    time_elapsed    | 53       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=287.20 +/- 9.60
Episode length: 96.80 +/- 2.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.8        |
|    mean_reward          | 287         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.041911323 |
|    clip_fraction        | 0.463       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.000204    |
|    loss                 | 301         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.0144      |
|    value_loss           | 1.09e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 440      |
| time/              |          |
|    fps             | 683      |
|    iterations      | 10       |
|    time_elapsed    | 59       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=287.20 +/- 9.60
Episode length: 96.80 +/- 2.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.8        |
|    mean_reward          | 287         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.027800802 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.341       |
|    learning_rate        | 0.000204    |
|    loss                 | 100         |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.0104      |
|    value_loss           | 1.17e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 462      |
| time/              |          |
|    fps             | 680      |
|    iterations      | 11       |
|    time_elapsed    | 66       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 134        |
|    ep_rew_mean          | 438        |
| time/                   |            |
|    fps                  | 684        |
|    iterations           | 12         |
|    time_elapsed         | 71         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.17052239 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.349      |
|    learning_rate        | 0.000204   |
|    loss                 | 1.28e+03   |
|    n_updates            | 110        |
|    policy_gradient_loss | 0.017      |
|    value_loss           | 1.15e+03   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.05071873 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.957     |
|    explained_variance   | 0.317      |
|    learning_rate        | 0.000204   |
|    loss                 | 157        |
|    n_updates            | 120        |
|    policy_gradient_loss | 0.00481    |
|    value_loss           | 1.29e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 411      |
| time/              |          |
|    fps             | 682      |
|    iterations      | 13       |
|    time_elapsed    | 78       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=412.00 +/- 106.13
Episode length: 128.00 +/- 26.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 412         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.036108647 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.938      |
|    explained_variance   | 0.187       |
|    learning_rate        | 0.000204    |
|    loss                 | 1.8e+03     |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.0144      |
|    value_loss           | 1.69e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 414      |
| time/              |          |
|    fps             | 680      |
|    iterations      | 14       |
|    time_elapsed    | 84       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=328.80 +/- 49.99
Episode length: 107.20 +/- 12.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 107         |
|    mean_reward          | 329         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.046424255 |
|    clip_fraction        | 0.38        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.984      |
|    explained_variance   | 0.0516      |
|    learning_rate        | 0.000204    |
|    loss                 | 253         |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.0108      |
|    value_loss           | 1.47e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 443      |
| time/              |          |
|    fps             | 678      |
|    iterations      | 15       |
|    time_elapsed    | 90       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:18:31,051] Trial 47 finished with value: 642.4 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.0002044222964460872, 'gamma': 0.9893820692716155, 'gae_lambda': 0.8687303559921297}. Best is trial 47 with value: 642.4.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 1269     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=357.60 +/- 114.53
Episode length: 114.40 +/- 28.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 358         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.013094066 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.00312     |
|    learning_rate        | 0.000192    |
|    loss                 | 15.7        |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.000541    |
|    value_loss           | 302         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 875      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=338.40 +/- 106.18
Episode length: 109.60 +/- 26.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 338         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011924806 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.174       |
|    learning_rate        | 0.000192    |
|    loss                 | 58.5        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000167   |
|    value_loss           | 360         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 366      |
| time/              |          |
|    fps             | 785      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01288775 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.254      |
|    learning_rate        | 0.000192   |
|    loss                 | 55.9       |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.00177    |
|    value_loss           | 379        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 750      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.016766246 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.000192    |
|    loss                 | 75.9        |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.000943    |
|    value_loss           | 366         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 733      |
|    iterations      | 5        |
|    time_elapsed    | 27       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 357         |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 6           |
|    time_elapsed         | 33          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.023738416 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.000192    |
|    loss                 | 230         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00132     |
|    value_loss           | 503         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.021368809 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.245       |
|    learning_rate        | 0.000192    |
|    loss                 | 66.4        |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00739     |
|    value_loss           | 589         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 722      |
|    iterations      | 7        |
|    time_elapsed    | 39       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.016222985 |
|    clip_fraction        | 0.342       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.184       |
|    learning_rate        | 0.000192    |
|    loss                 | 470         |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.00293     |
|    value_loss           | 727         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 718      |
|    iterations      | 8        |
|    time_elapsed    | 45       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.015961478 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.333       |
|    learning_rate        | 0.000192    |
|    loss                 | 814         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00335     |
|    value_loss           | 592         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 713      |
|    iterations      | 9        |
|    time_elapsed    | 51       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.027331006 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000192    |
|    loss                 | 212         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00502     |
|    value_loss           | 699         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 707      |
|    iterations      | 10       |
|    time_elapsed    | 57       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.020728394 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.000192    |
|    loss                 | 145         |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.00208     |
|    value_loss           | 635         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 300      |
| time/              |          |
|    fps             | 703      |
|    iterations      | 11       |
|    time_elapsed    | 64       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 97.9        |
|    ep_rew_mean          | 292         |
| time/                   |             |
|    fps                  | 706         |
|    iterations           | 12          |
|    time_elapsed         | 69          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.047561105 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.0485      |
|    learning_rate        | 0.000192    |
|    loss                 | 156         |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00572     |
|    value_loss           | 969         |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96        |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0183335 |
|    clip_fraction        | 0.297     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.04     |
|    explained_variance   | 0.0237    |
|    learning_rate        | 0.000192  |
|    loss                 | 309       |
|    n_updates            | 120       |
|    policy_gradient_loss | 0.0016    |
|    value_loss           | 897       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.7     |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 702      |
|    iterations      | 13       |
|    time_elapsed    | 75       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.017462902 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.98       |
|    explained_variance   | 0.0219      |
|    learning_rate        | 0.000192    |
|    loss                 | 119         |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.000723    |
|    value_loss           | 762         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.3     |
|    ep_rew_mean     | 293      |
| time/              |          |
|    fps             | 699      |
|    iterations      | 14       |
|    time_elapsed    | 81       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.030593796 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.962      |
|    explained_variance   | 0.15        |
|    learning_rate        | 0.000192    |
|    loss                 | 61.5        |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.00724     |
|    value_loss           | 563         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 300      |
| time/              |          |
|    fps             | 697      |
|    iterations      | 15       |
|    time_elapsed    | 88       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:20:02,440] Trial 48 finished with value: 284.0 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.00019203378366461644, 'gamma': 0.9757894367967952, 'gae_lambda': 0.8474716918960055}. Best is trial 47 with value: 642.4.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 1265     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012428335 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.00388    |
|    learning_rate        | 0.000107    |
|    loss                 | 51.6        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.000341   |
|    value_loss           | 437         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 870      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009261421 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.171       |
|    learning_rate        | 0.000107    |
|    loss                 | 87.2        |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00179     |
|    value_loss           | 553         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 417      |
| time/              |          |
|    fps             | 790      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=1827.20 +/- 498.26
Episode length: 464.30 +/- 115.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | 1.83e+03    |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.015714638 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.000107    |
|    loss                 | 665         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.00749     |
|    value_loss           | 724         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 462      |
| time/              |          |
|    fps             | 676      |
|    iterations      | 4        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=684.00 +/- 308.68
Episode length: 196.00 +/- 77.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 196        |
|    mean_reward          | 684        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.02189451 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.226      |
|    learning_rate        | 0.000107   |
|    loss                 | 178        |
|    n_updates            | 40         |
|    policy_gradient_loss | 0.0105     |
|    value_loss           | 848        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 501      |
| time/              |          |
|    fps             | 660      |
|    iterations      | 5        |
|    time_elapsed    | 31       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | 544         |
| time/                   |             |
|    fps                  | 672         |
|    iterations           | 6           |
|    time_elapsed         | 36          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.052346922 |
|    clip_fraction        | 0.408       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.000107    |
|    loss                 | 379         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.0108      |
|    value_loss           | 880         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=1004.80 +/- 742.85
Episode length: 268.70 +/- 174.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 269        |
|    mean_reward          | 1e+03      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.03717365 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.36       |
|    learning_rate        | 0.000107   |
|    loss                 | 422        |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.00706    |
|    value_loss           | 1.14e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 614      |
| time/              |          |
|    fps             | 654      |
|    iterations      | 7        |
|    time_elapsed    | 43       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=606.40 +/- 510.35
Episode length: 174.10 +/- 120.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 174        |
|    mean_reward          | 606        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.68110573 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.171      |
|    learning_rate        | 0.000107   |
|    loss                 | 1.02e+03   |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.0459     |
|    value_loss           | 1.04e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 718      |
| time/              |          |
|    fps             | 648      |
|    iterations      | 8        |
|    time_elapsed    | 50       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=341.60 +/- 49.16
Episode length: 110.40 +/- 12.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 110        |
|    mean_reward          | 342        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.13327426 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.645     |
|    explained_variance   | 0.121      |
|    learning_rate        | 0.000107   |
|    loss                 | 135        |
|    n_updates            | 80         |
|    policy_gradient_loss | 0.0403     |
|    value_loss           | 654        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 721      |
| time/              |          |
|    fps             | 652      |
|    iterations      | 9        |
|    time_elapsed    | 56       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=322.40 +/- 34.47
Episode length: 105.60 +/- 8.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 322         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.018854592 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.587      |
|    explained_variance   | -0.00195    |
|    learning_rate        | 0.000107    |
|    loss                 | 1.03e+03    |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.009       |
|    value_loss           | 1.7e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 652      |
|    iterations      | 10       |
|    time_elapsed    | 62       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=290.40 +/- 19.20
Episode length: 97.60 +/- 4.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 97.6      |
|    mean_reward          | 290       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 2.7979765 |
|    clip_fraction        | 0.829     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.193    |
|    explained_variance   | -0.00632  |
|    learning_rate        | 0.000107  |
|    loss                 | 595       |
|    n_updates            | 100       |
|    policy_gradient_loss | 0.133     |
|    value_loss           | 518       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 207      |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 653      |
|    iterations      | 11       |
|    time_elapsed    | 68       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 148          |
|    ep_rew_mean          | 497          |
| time/                   |              |
|    fps                  | 660          |
|    iterations           | 12           |
|    time_elapsed         | 74           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 3.768946e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000244    |
|    explained_variance   | 0.0054       |
|    learning_rate        | 0.000107     |
|    loss                 | 2.15e+03     |
|    n_updates            | 110          |
|    policy_gradient_loss | -1.21e-06    |
|    value_loss           | 3.59e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=328.80 +/- 38.40
Episode length: 107.20 +/- 9.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 107           |
|    mean_reward          | 329           |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 1.0652002e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000536     |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.000107      |
|    loss                 | 2.19e+03      |
|    n_updates            | 120           |
|    policy_gradient_loss | -7.16e-07     |
|    value_loss           | 3.48e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 658      |
|    iterations      | 13       |
|    time_elapsed    | 80       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=306.40 +/- 35.20
Episode length: 101.60 +/- 8.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | 306          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 6.039045e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000923    |
|    explained_variance   | 0.156        |
|    learning_rate        | 0.000107     |
|    loss                 | 1.63e+03     |
|    n_updates            | 130          |
|    policy_gradient_loss | -4.72e-08    |
|    value_loss           | 3.11e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 658      |
|    iterations      | 14       |
|    time_elapsed    | 87       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=316.00 +/- 35.05
Episode length: 104.00 +/- 8.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | 316          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 4.947651e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000815    |
|    explained_variance   | 0.155        |
|    learning_rate        | 0.000107     |
|    loss                 | 1.68e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -2.45e-08    |
|    value_loss           | 3.14e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 312      |
| time/              |          |
|    fps             | 659      |
|    iterations      | 15       |
|    time_elapsed    | 93       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 19:21:38,870] Trial 49 finished with value: 306.4 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.0001074203157004859, 'gamma': 0.9967342687391838, 'gae_lambda': 0.8768569097244487}. Best is trial 47 with value: 642.4.
PPO Best trial: 642.4
PPO Best hyperparameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.0002044222964460872, 'gamma': 0.9893820692716155, 'gae_lambda': 0.8687303559921297}
