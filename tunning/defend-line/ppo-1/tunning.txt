[I 2024-07-16 15:01:03,096] A new study created in memory with name: no-name-6fef34f5-d60f-420d-8ce9-8ab51e4406ca
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=5000, episode_reward=5.20 +/- 2.48
Episode length: 134.30 +/- 43.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 5.2      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 7.91     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 1        |
|    time_elapsed    | 124      |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=14.70 +/- 4.82
Episode length: 165.10 +/- 60.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | 14.7        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.018515242 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0247     |
|    learning_rate        | 0.0032      |
|    loss                 | 0.0386      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.206       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=14.30 +/- 4.58
Episode length: 158.90 +/- 52.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 9.37     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 2        |
|    time_elapsed    | 217      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=15.20 +/- 5.78
Episode length: 172.00 +/- 72.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 172         |
|    mean_reward          | 15.2        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.039059293 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.0032      |
|    loss                 | 0.0187      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0312     |
|    value_loss           | 0.167       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 12.3     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 3        |
|    time_elapsed    | 306      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=16.60 +/- 4.48
Episode length: 172.70 +/- 50.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 173        |
|    mean_reward          | 16.6       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.06110883 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.425      |
|    learning_rate        | 0.0032     |
|    loss                 | 0.0255     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0399    |
|    value_loss           | 0.165      |
----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=19.30 +/- 4.08
Episode length: 192.50 +/- 38.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 19.3     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 4        |
|    time_elapsed    | 401      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=16.70 +/- 5.42
Episode length: 172.80 +/- 53.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 173        |
|    mean_reward          | 16.7       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.08446133 |
|    clip_fraction        | 0.417      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0032     |
|    loss                 | 0.0199     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0415    |
|    value_loss           | 0.161      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=18.30 +/- 5.83
Episode length: 181.50 +/- 57.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | 18.3     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 5        |
|    time_elapsed    | 543      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=17.50 +/- 4.90
Episode length: 180.20 +/- 52.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 180        |
|    mean_reward          | 17.5       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.10759321 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.0032     |
|    loss                 | -0.0177    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0429    |
|    value_loss           | 0.17       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 6        |
|    time_elapsed    | 637      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=17.80 +/- 4.38
Episode length: 176.50 +/- 41.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 176        |
|    mean_reward          | 17.8       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.13171807 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.718      |
|    learning_rate        | 0.0032     |
|    loss                 | 0.0535     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 0.18       |
----------------------------------------
Eval num_timesteps=55000, episode_reward=16.90 +/- 4.18
Episode length: 162.20 +/- 40.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 162      |
|    mean_reward     | 16.9     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 7        |
|    time_elapsed    | 731      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=20.90 +/- 5.11
Episode length: 221.30 +/- 54.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 221        |
|    mean_reward          | 20.9       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.16785303 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.742      |
|    learning_rate        | 0.0032     |
|    loss                 | 0.0242     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0382    |
|    value_loss           | 0.183      |
----------------------------------------
New best mean reward!
Eval num_timesteps=65000, episode_reward=17.80 +/- 5.25
Episode length: 185.00 +/- 57.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 8        |
|    time_elapsed    | 828      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-16 15:15:17,456] Trial 0 finished with value: 18.2 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.0031969608132666025, 'gamma': 0.9883093693420921, 'gae_lambda': 0.8305420427070646}. Best is trial 0 with value: 18.2.
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 6.94     |
| time/              |          |
|    fps             | 135      |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 126        |
|    ep_rew_mean          | 7.09       |
| time/                   |            |
|    fps                  | 94         |
|    iterations           | 2          |
|    time_elapsed         | 43         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.02392102 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -0.0597    |
|    learning_rate        | 0.00338    |
|    loss                 | 0.0247     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0187    |
|    value_loss           | 0.238      |
----------------------------------------
Eval num_timesteps=5000, episode_reward=13.90 +/- 4.11
Episode length: 162.00 +/- 42.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 162       |
|    mean_reward          | 13.9      |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 0.0493204 |
|    clip_fraction        | 0.301     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.33     |
|    explained_variance   | 0.295     |
|    learning_rate        | 0.00338   |
|    loss                 | 0.00164   |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.036    |
|    value_loss           | 0.116     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 8.16     |
| time/              |          |
|    fps             | 61       |
|    iterations      | 3        |
|    time_elapsed    | 99       |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 143        |
|    ep_rew_mean          | 9.2        |
| time/                   |            |
|    fps                  | 56         |
|    iterations           | 4          |
|    time_elapsed         | 146        |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.09321724 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.213      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0456    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0514    |
|    value_loss           | 0.119      |
----------------------------------------
Eval num_timesteps=10000, episode_reward=16.90 +/- 3.27
Episode length: 186.30 +/- 38.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 186        |
|    mean_reward          | 16.9       |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.11219457 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.278      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0848    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.129      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    fps             | 57       |
|    iterations      | 5        |
|    time_elapsed    | 177      |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 150        |
|    ep_rew_mean          | 10.8       |
| time/                   |            |
|    fps                  | 60         |
|    iterations           | 6          |
|    time_elapsed         | 201        |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.15099195 |
|    clip_fraction        | 0.518      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.246      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0272    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0509    |
|    value_loss           | 0.132      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 155        |
|    ep_rew_mean          | 11.4       |
| time/                   |            |
|    fps                  | 63         |
|    iterations           | 7          |
|    time_elapsed         | 225        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.18791324 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.257      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.066     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.124      |
----------------------------------------
Eval num_timesteps=15000, episode_reward=19.90 +/- 3.96
Episode length: 200.00 +/- 40.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 200       |
|    mean_reward          | 19.9      |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.2111027 |
|    clip_fraction        | 0.593     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08     |
|    explained_variance   | 0.282     |
|    learning_rate        | 0.00338   |
|    loss                 | -0.096    |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.054    |
|    value_loss           | 0.125     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 11.9     |
| time/              |          |
|    fps             | 63       |
|    iterations      | 8        |
|    time_elapsed    | 258      |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 163        |
|    ep_rew_mean          | 12.9       |
| time/                   |            |
|    fps                  | 65         |
|    iterations           | 9          |
|    time_elapsed         | 282        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.28372288 |
|    clip_fraction        | 0.618      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.269      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.00247   |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0525    |
|    value_loss           | 0.132      |
----------------------------------------
Eval num_timesteps=20000, episode_reward=15.00 +/- 5.73
Episode length: 177.30 +/- 56.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 177        |
|    mean_reward          | 15         |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.26471925 |
|    clip_fraction        | 0.615      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.044     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0585    |
|    value_loss           | 0.117      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 10       |
|    time_elapsed    | 313      |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 14.2       |
| time/                   |            |
|    fps                  | 66         |
|    iterations           | 11         |
|    time_elapsed         | 337        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.26361668 |
|    clip_fraction        | 0.622      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.37       |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0547    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0521    |
|    value_loss           | 0.13       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 169       |
|    ep_rew_mean          | 14.6      |
| time/                   |           |
|    fps                  | 67        |
|    iterations           | 12        |
|    time_elapsed         | 362       |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.3113318 |
|    clip_fraction        | 0.617     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.914    |
|    explained_variance   | 0.224     |
|    learning_rate        | 0.00338   |
|    loss                 | -0.0746   |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0496   |
|    value_loss           | 0.129     |
---------------------------------------
Eval num_timesteps=25000, episode_reward=17.90 +/- 4.01
Episode length: 185.90 +/- 44.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 186       |
|    mean_reward          | 17.9      |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.3499714 |
|    clip_fraction        | 0.609     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.918    |
|    explained_variance   | 0.313     |
|    learning_rate        | 0.00338   |
|    loss                 | -0.0185   |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.0521   |
|    value_loss           | 0.12      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 13       |
|    time_elapsed    | 394      |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 15.1       |
| time/                   |            |
|    fps                  | 68         |
|    iterations           | 14         |
|    time_elapsed         | 418        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.34119958 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.854     |
|    explained_variance   | 0.445      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.066     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0423    |
|    value_loss           | 0.125      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=19.20 +/- 4.94
Episode length: 188.60 +/- 46.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 189        |
|    mean_reward          | 19.2       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.34820533 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.84      |
|    explained_variance   | 0.376      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0338    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.052     |
|    value_loss           | 0.118      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 15       |
|    time_elapsed    | 451      |
|    total_timesteps | 30720    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 176       |
|    ep_rew_mean          | 15.8      |
| time/                   |           |
|    fps                  | 69        |
|    iterations           | 16        |
|    time_elapsed         | 471       |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 0.4668833 |
|    clip_fraction        | 0.585     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.765    |
|    explained_variance   | 0.377     |
|    learning_rate        | 0.00338   |
|    loss                 | -0.0292   |
|    n_updates            | 150       |
|    policy_gradient_loss | -0.0463   |
|    value_loss           | 0.128     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 16.2       |
| time/                   |            |
|    fps                  | 67         |
|    iterations           | 17         |
|    time_elapsed         | 518        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.37907863 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.786     |
|    explained_variance   | 0.409      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0677    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0493    |
|    value_loss           | 0.116      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=19.00 +/- 5.16
Episode length: 193.00 +/- 47.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 193        |
|    mean_reward          | 19         |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.47452152 |
|    clip_fraction        | 0.608      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.743     |
|    explained_variance   | 0.441      |
|    learning_rate        | 0.00338    |
|    loss                 | 0.00439    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0466    |
|    value_loss           | 0.115      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 63       |
|    iterations      | 18       |
|    time_elapsed    | 577      |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 185        |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 64         |
|    iterations           | 19         |
|    time_elapsed         | 605        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.57413554 |
|    clip_fraction        | 0.602      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.397      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0416    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0498    |
|    value_loss           | 0.118      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=20.70 +/- 6.33
Episode length: 228.60 +/- 74.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 229       |
|    mean_reward          | 20.7      |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.4251418 |
|    clip_fraction        | 0.623     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.772    |
|    explained_variance   | 0.28      |
|    learning_rate        | 0.00338   |
|    loss                 | -0.0424   |
|    n_updates            | 190       |
|    policy_gradient_loss | -0.0424   |
|    value_loss           | 0.118     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 64       |
|    iterations      | 20       |
|    time_elapsed    | 639      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 17.3       |
| time/                   |            |
|    fps                  | 64         |
|    iterations           | 21         |
|    time_elapsed         | 663        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.53121436 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.75      |
|    explained_variance   | 0.407      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0165    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.119      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=16.20 +/- 4.58
Episode length: 169.80 +/- 47.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 170       |
|    mean_reward          | 16.2      |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.5590351 |
|    clip_fraction        | 0.611     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.737    |
|    explained_variance   | 0.44      |
|    learning_rate        | 0.00338   |
|    loss                 | -0.031    |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.0328   |
|    value_loss           | 0.132     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 64       |
|    iterations      | 22       |
|    time_elapsed    | 694      |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 187       |
|    ep_rew_mean          | 17.8      |
| time/                   |           |
|    fps                  | 65        |
|    iterations           | 23        |
|    time_elapsed         | 718       |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.4737448 |
|    clip_fraction        | 0.596     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.712    |
|    explained_variance   | 0.444     |
|    learning_rate        | 0.00338   |
|    loss                 | -0.109    |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0372   |
|    value_loss           | 0.126     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 188        |
|    ep_rew_mean          | 17.8       |
| time/                   |            |
|    fps                  | 66         |
|    iterations           | 24         |
|    time_elapsed         | 742        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.52726936 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.454      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0847    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0325    |
|    value_loss           | 0.129      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=18.70 +/- 3.61
Episode length: 198.00 +/- 40.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 198       |
|    mean_reward          | 18.7      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.5792108 |
|    clip_fraction        | 0.604     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.676    |
|    explained_variance   | 0.476     |
|    learning_rate        | 0.00338   |
|    loss                 | -0.00203  |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0453   |
|    value_loss           | 0.117     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 66       |
|    iterations      | 25       |
|    time_elapsed    | 774      |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 17.3       |
| time/                   |            |
|    fps                  | 66         |
|    iterations           | 26         |
|    time_elapsed         | 798        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.56204844 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.675     |
|    explained_variance   | 0.414      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0592    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0386    |
|    value_loss           | 0.126      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=18.50 +/- 6.45
Episode length: 188.10 +/- 75.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 188        |
|    mean_reward          | 18.5       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.65929496 |
|    clip_fraction        | 0.626      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.729     |
|    explained_variance   | 0.417      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0277    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.125      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 66       |
|    iterations      | 27       |
|    time_elapsed    | 830      |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 181        |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 67         |
|    iterations           | 28         |
|    time_elapsed         | 854        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.52992535 |
|    clip_fraction        | 0.617      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.713     |
|    explained_variance   | 0.455      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.051     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.033     |
|    value_loss           | 0.126      |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 183      |
|    ep_rew_mean          | 17.2     |
| time/                   |          |
|    fps                  | 67       |
|    iterations           | 29       |
|    time_elapsed         | 878      |
|    total_timesteps      | 59392    |
| train/                  |          |
|    approx_kl            | 0.522076 |
|    clip_fraction        | 0.627    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.726   |
|    explained_variance   | 0.373    |
|    learning_rate        | 0.00338  |
|    loss                 | -0.0715  |
|    n_updates            | 280      |
|    policy_gradient_loss | -0.032   |
|    value_loss           | 0.125    |
--------------------------------------
Eval num_timesteps=60000, episode_reward=21.90 +/- 3.91
Episode length: 215.90 +/- 36.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 21.9       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.47688854 |
|    clip_fraction        | 0.598      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.458      |
|    learning_rate        | 0.00338    |
|    loss                 | -0.0316    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0518    |
|    value_loss           | 0.124      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 30       |
|    time_elapsed    | 910      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 15:30:42,386] Trial 1 finished with value: 16.9 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 0.0033818033864194534, 'gamma': 0.9224478428077248, 'gae_lambda': 0.8243700572627521}. Best is trial 0 with value: 18.2.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 1        |
|    time_elapsed    | 62       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=4.80 +/- 2.82
Episode length: 103.80 +/- 45.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 4.8         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012349369 |
|    clip_fraction        | 0.0651      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0855     |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0596      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00972    |
|    value_loss           | 0.186       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 60       |
|    iterations      | 2        |
|    time_elapsed    | 135      |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=9.10 +/- 3.05
Episode length: 123.60 +/- 39.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 124         |
|    mean_reward          | 9.1         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011414678 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.25        |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.074       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.183       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 8.12     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 3        |
|    time_elapsed    | 187      |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=9.50 +/- 2.33
Episode length: 122.60 +/- 35.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 123         |
|    mean_reward          | 9.5         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.010936349 |
|    clip_fraction        | 0.0943      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.389       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0597      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.195       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 9.18     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 4        |
|    time_elapsed    | 241      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=10.10 +/- 2.43
Episode length: 129.30 +/- 27.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 129         |
|    mean_reward          | 10.1        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011240087 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.338       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0882      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.198       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 69       |
|    iterations      | 5        |
|    time_elapsed    | 294      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | 12.6        |
| time/                   |             |
|    fps                  | 71          |
|    iterations           | 6           |
|    time_elapsed         | 342         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.011276279 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.453       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.06        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.198       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=13.60 +/- 3.26
Episode length: 147.10 +/- 34.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 147         |
|    mean_reward          | 13.6        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011215306 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.466       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0406      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.206       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 72       |
|    iterations      | 7        |
|    time_elapsed    | 397      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=15.40 +/- 3.56
Episode length: 168.70 +/- 40.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 15.4        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.010315618 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.539       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0056      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.188       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 70       |
|    iterations      | 8        |
|    time_elapsed    | 466      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=12.70 +/- 2.76
Episode length: 137.60 +/- 35.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 138         |
|    mean_reward          | 12.7        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.010240023 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.56        |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0465      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.184       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 9        |
|    time_elapsed    | 559      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=15.40 +/- 3.44
Episode length: 162.40 +/- 39.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 162         |
|    mean_reward          | 15.4        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.011023246 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.548       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0561      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.19        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 66       |
|    iterations      | 10       |
|    time_elapsed    | 613      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=15.80 +/- 3.60
Episode length: 165.80 +/- 44.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 15.8        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.009155396 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.578       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0722      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.181       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 11       |
|    time_elapsed    | 668      |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 196         |
|    ep_rew_mean          | 18.9        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 12          |
|    time_elapsed         | 716         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.012824338 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.61        |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0742      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.179       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=19.10 +/- 3.30
Episode length: 185.80 +/- 37.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 186         |
|    mean_reward          | 19.1        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.012540438 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.972      |
|    explained_variance   | 0.613       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0791      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.175       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 13       |
|    time_elapsed    | 771      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=19.00 +/- 2.97
Episode length: 195.20 +/- 35.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 195         |
|    mean_reward          | 19          |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.009460656 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.939      |
|    explained_variance   | 0.592       |
|    learning_rate        | 5.81e-05    |
|    loss                 | 0.0386      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.193       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 19.2     |
| time/              |          |
|    fps             | 69       |
|    iterations      | 14       |
|    time_elapsed    | 828      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=17.40 +/- 4.03
Episode length: 175.60 +/- 46.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 176          |
|    mean_reward          | 17.4         |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0108688045 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.899       |
|    explained_variance   | 0.616        |
|    learning_rate        | 5.81e-05     |
|    loss                 | 0.0457       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.02        |
|    value_loss           | 0.186        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 69       |
|    iterations      | 15       |
|    time_elapsed    | 887      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 15:46:10,904] Trial 2 finished with value: 18.6 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 5.8064879762479614e-05, 'gamma': 0.9467843801242147, 'gae_lambda': 0.9046803090419456}. Best is trial 2 with value: 18.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 8.57     |
| time/              |          |
|    fps             | 64       |
|    iterations      | 1        |
|    time_elapsed    | 31       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 135         |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 62          |
|    iterations           | 2           |
|    time_elapsed         | 65          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011369603 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.000637    |
|    learning_rate        | 0.0014      |
|    loss                 | 0.0304      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.252       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=11.90 +/- 3.88
Episode length: 154.10 +/- 40.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 154         |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.028926216 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.0014      |
|    loss                 | 0.00343     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0358     |
|    value_loss           | 0.199       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 8.66     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 3        |
|    time_elapsed    | 90       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 145         |
|    ep_rew_mean          | 9.75        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 4           |
|    time_elapsed         | 108         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.036816653 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0014      |
|    loss                 | -0.00832    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0423     |
|    value_loss           | 0.226       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=15.70 +/- 6.63
Episode length: 160.10 +/- 65.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 160         |
|    mean_reward          | 15.7        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.051117036 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.0014      |
|    loss                 | -0.0262     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0501     |
|    value_loss           | 0.227       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 5        |
|    time_elapsed    | 132      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 152         |
|    ep_rew_mean          | 11.1        |
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 6           |
|    time_elapsed         | 150         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.067719474 |
|    clip_fraction        | 0.43        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.0014      |
|    loss                 | -0.023      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0534     |
|    value_loss           | 0.23        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 157         |
|    ep_rew_mean          | 11.6        |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 7           |
|    time_elapsed         | 168         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.068511605 |
|    clip_fraction        | 0.438       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.0014      |
|    loss                 | -0.0452     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0606     |
|    value_loss           | 0.21        |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=19.50 +/- 5.08
Episode length: 201.70 +/- 60.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 19.5        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.079719916 |
|    clip_fraction        | 0.463       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0014      |
|    loss                 | -0.0338     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0526     |
|    value_loss           | 0.196       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 12.1     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 8        |
|    time_elapsed    | 195      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | 12.9        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 9           |
|    time_elapsed         | 213         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.081415236 |
|    clip_fraction        | 0.472       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0014      |
|    loss                 | -0.0372     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0503     |
|    value_loss           | 0.184       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=16.90 +/- 3.08
Episode length: 165.60 +/- 32.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 166        |
|    mean_reward          | 16.9       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.09752157 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.757      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0284    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.189      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 10       |
|    time_elapsed    | 237      |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | 14.5       |
| time/                   |            |
|    fps                  | 88         |
|    iterations           | 11         |
|    time_elapsed         | 255        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.10409404 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.804      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0368    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0512    |
|    value_loss           | 0.207      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 14.9       |
| time/                   |            |
|    fps                  | 89         |
|    iterations           | 12         |
|    time_elapsed         | 274        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.10629148 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0326    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0546    |
|    value_loss           | 0.214      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=16.90 +/- 5.28
Episode length: 202.00 +/- 57.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 16.9       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.13370378 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.769      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0427    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0639    |
|    value_loss           | 0.184      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 13       |
|    time_elapsed    | 300      |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 15.8       |
| time/                   |            |
|    fps                  | 90         |
|    iterations           | 14         |
|    time_elapsed         | 318        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.15351063 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.963     |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0471    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0592    |
|    value_loss           | 0.179      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=15.60 +/- 5.46
Episode length: 184.30 +/- 52.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 184        |
|    mean_reward          | 15.6       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.16254382 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.936     |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0339    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.064     |
|    value_loss           | 0.208      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 15       |
|    time_elapsed    | 344      |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | 16.2       |
| time/                   |            |
|    fps                  | 90         |
|    iterations           | 16         |
|    time_elapsed         | 362        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.16411418 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.906     |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0014     |
|    loss                 | 5.33e-05   |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0611    |
|    value_loss           | 0.22       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 16.5       |
| time/                   |            |
|    fps                  | 91         |
|    iterations           | 17         |
|    time_elapsed         | 379        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.16425665 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.926     |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0443    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.066     |
|    value_loss           | 0.186      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=13.70 +/- 4.98
Episode length: 174.40 +/- 50.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 174        |
|    mean_reward          | 13.7       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.20779453 |
|    clip_fraction        | 0.564      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.921     |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0628    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0655    |
|    value_loss           | 0.182      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 18       |
|    time_elapsed    | 413      |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 19         |
|    time_elapsed         | 449        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.21488446 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.853     |
|    explained_variance   | 0.733      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0845    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0639    |
|    value_loss           | 0.16       |
----------------------------------------
Eval num_timesteps=40000, episode_reward=15.80 +/- 5.00
Episode length: 166.60 +/- 50.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 167        |
|    mean_reward          | 15.8       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.19306609 |
|    clip_fraction        | 0.542      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.86      |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0572    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0635    |
|    value_loss           | 0.177      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 20       |
|    time_elapsed    | 495      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 198        |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 82         |
|    iterations           | 21         |
|    time_elapsed         | 519        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.21903676 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.864     |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0507    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0631    |
|    value_loss           | 0.172      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=19.00 +/- 5.16
Episode length: 203.90 +/- 52.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 204        |
|    mean_reward          | 19         |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.25120664 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.831     |
|    explained_variance   | 0.757      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0688    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0644    |
|    value_loss           | 0.186      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 17.1     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 22       |
|    time_elapsed    | 545      |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 194        |
|    ep_rew_mean          | 16.9       |
| time/                   |            |
|    fps                  | 83         |
|    iterations           | 23         |
|    time_elapsed         | 563        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.23666947 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.829     |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.077     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0715    |
|    value_loss           | 0.165      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 24         |
|    time_elapsed         | 580        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.25003615 |
|    clip_fraction        | 0.545      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.808     |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0677    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0705    |
|    value_loss           | 0.143      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=17.70 +/- 5.24
Episode length: 180.90 +/- 56.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 181       |
|    mean_reward          | 17.7      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.2645552 |
|    clip_fraction        | 0.538     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.762    |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0014    |
|    loss                 | -0.059    |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0621   |
|    value_loss           | 0.167     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 25       |
|    time_elapsed    | 606      |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | 17.7       |
| time/                   |            |
|    fps                  | 85         |
|    iterations           | 26         |
|    time_elapsed         | 624        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.29362547 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.734      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.074     |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0682    |
|    value_loss           | 0.159      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=17.30 +/- 4.41
Episode length: 168.50 +/- 36.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 168       |
|    mean_reward          | 17.3      |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.2629767 |
|    clip_fraction        | 0.535     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.752    |
|    explained_variance   | 0.706     |
|    learning_rate        | 0.0014    |
|    loss                 | -0.0837   |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.0622   |
|    value_loss           | 0.164     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 85       |
|    iterations      | 27       |
|    time_elapsed    | 650      |
|    total_timesteps | 55296    |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 203      |
|    ep_rew_mean          | 18.6     |
| time/                   |          |
|    fps                  | 85       |
|    iterations           | 28       |
|    time_elapsed         | 667      |
|    total_timesteps      | 57344    |
| train/                  |          |
|    approx_kl            | 0.293419 |
|    clip_fraction        | 0.545    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.689   |
|    explained_variance   | 0.707    |
|    learning_rate        | 0.0014   |
|    loss                 | -0.0532  |
|    n_updates            | 270      |
|    policy_gradient_loss | -0.0577  |
|    value_loss           | 0.176    |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 203        |
|    ep_rew_mean          | 18.9       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 29         |
|    time_elapsed         | 685        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.24770881 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.688     |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0631    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0634    |
|    value_loss           | 0.174      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=19.60 +/- 3.85
Episode length: 189.00 +/- 37.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 189        |
|    mean_reward          | 19.6       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.24117002 |
|    clip_fraction        | 0.508      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.69      |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0014     |
|    loss                 | -0.0581    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.066     |
|    value_loss           | 0.18       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 30       |
|    time_elapsed    | 711      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 15:58:14,287] Trial 3 finished with value: 22.4 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0013950960025191528, 'gamma': 0.9739906898202355, 'gae_lambda': 0.9112039534303118}. Best is trial 3 with value: 22.4.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 7.48     |
| time/              |          |
|    fps             | 131      |
|    iterations      | 1        |
|    time_elapsed    | 31       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=15.60 +/- 4.59
Episode length: 181.00 +/- 44.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 181         |
|    mean_reward          | 15.6        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.015703905 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0191     |
|    learning_rate        | 0.00116     |
|    loss                 | 0.0314      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.132       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 8.61     |
| time/              |          |
|    fps             | 103      |
|    iterations      | 2        |
|    time_elapsed    | 79       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=15.40 +/- 4.27
Episode length: 181.70 +/- 50.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 182         |
|    mean_reward          | 15.4        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.035754763 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.179       |
|    learning_rate        | 0.00116     |
|    loss                 | -0.0386     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0434     |
|    value_loss           | 0.133       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 9.62     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 3        |
|    time_elapsed    | 137      |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=15.90 +/- 5.03
Episode length: 168.80 +/- 46.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 15.9        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.048422202 |
|    clip_fraction        | 0.393       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.00116     |
|    loss                 | -0.0392     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0507     |
|    value_loss           | 0.117       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    fps             | 72       |
|    iterations      | 4        |
|    time_elapsed    | 224      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=15.40 +/- 6.70
Episode length: 158.20 +/- 69.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 158        |
|    mean_reward          | 15.4       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.06546704 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.00116    |
|    loss                 | -0.0285    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0592    |
|    value_loss           | 0.116      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 5        |
|    time_elapsed    | 272      |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | 14.1       |
| time/                   |            |
|    fps                  | 78         |
|    iterations           | 6          |
|    time_elapsed         | 312        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.07877005 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.386      |
|    learning_rate        | 0.00116    |
|    loss                 | -0.0597    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0627    |
|    value_loss           | 0.117      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=20.80 +/- 6.73
Episode length: 202.90 +/- 57.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 203        |
|    mean_reward          | 20.8       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.09951502 |
|    clip_fraction        | 0.502      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.39       |
|    learning_rate        | 0.00116    |
|    loss                 | -0.0798    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0647    |
|    value_loss           | 0.101      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 7        |
|    time_elapsed    | 361      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=21.10 +/- 5.41
Episode length: 211.40 +/- 69.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 211        |
|    mean_reward          | 21.1       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.11557172 |
|    clip_fraction        | 0.523      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.45       |
|    learning_rate        | 0.00116    |
|    loss                 | -0.0742    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0646    |
|    value_loss           | 0.104      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 8        |
|    time_elapsed    | 410      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=19.90 +/- 4.21
Episode length: 182.30 +/- 43.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 182        |
|    mean_reward          | 19.9       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.13789688 |
|    clip_fraction        | 0.549      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.441      |
|    learning_rate        | 0.00116    |
|    loss                 | -0.0792    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0719    |
|    value_loss           | 0.0973     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 80       |
|    iterations      | 9        |
|    time_elapsed    | 458      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=21.00 +/- 4.58
Episode length: 198.60 +/- 42.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 199        |
|    mean_reward          | 21         |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.14864825 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.957     |
|    explained_variance   | 0.482      |
|    learning_rate        | 0.00116    |
|    loss                 | -0.0808    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0705    |
|    value_loss           | 0.0981     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 80       |
|    iterations      | 10       |
|    time_elapsed    | 506      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=20.30 +/- 5.76
Episode length: 186.80 +/- 56.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 187        |
|    mean_reward          | 20.3       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.17567292 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.936     |
|    explained_variance   | 0.485      |
|    learning_rate        | 0.00116    |
|    loss                 | -0.0815    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.065     |
|    value_loss           | 0.093      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 11       |
|    time_elapsed    | 552      |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 19.3       |
| time/                   |            |
|    fps                  | 78         |
|    iterations           | 12         |
|    time_elapsed         | 627        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.17975995 |
|    clip_fraction        | 0.567      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.914     |
|    explained_variance   | 0.494      |
|    learning_rate        | 0.00116    |
|    loss                 | -0.112     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0695    |
|    value_loss           | 0.0929     |
----------------------------------------
Eval num_timesteps=50000, episode_reward=22.00 +/- 4.63
Episode length: 201.30 +/- 50.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 201        |
|    mean_reward          | 22         |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.23002209 |
|    clip_fraction        | 0.555      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.852     |
|    explained_variance   | 0.477      |
|    learning_rate        | 0.00116    |
|    loss                 | -0.0959    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0692    |
|    value_loss           | 0.0879     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 13       |
|    time_elapsed    | 693      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=25.30 +/- 3.58
Episode length: 235.90 +/- 46.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 236        |
|    mean_reward          | 25.3       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.22929779 |
|    clip_fraction        | 0.544      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.824     |
|    explained_variance   | 0.508      |
|    learning_rate        | 0.00116    |
|    loss                 | -0.101     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0712    |
|    value_loss           | 0.0904     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 14       |
|    time_elapsed    | 743      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=23.80 +/- 3.82
Episode length: 228.60 +/- 54.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 229       |
|    mean_reward          | 23.8      |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.2397294 |
|    clip_fraction        | 0.555     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.788    |
|    explained_variance   | 0.497     |
|    learning_rate        | 0.00116   |
|    loss                 | -0.0999   |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0673   |
|    value_loss           | 0.0876    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 15       |
|    time_elapsed    | 793      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 16:11:44,962] Trial 4 finished with value: 21.2 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.0011639075099704743, 'gamma': 0.9130546942830957, 'gae_lambda': 0.864288817856581}. Best is trial 3 with value: 22.4.
Using cuda device
Eval num_timesteps=5000, episode_reward=2.10 +/- 2.17
Episode length: 84.40 +/- 37.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.4     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 8.07     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 1        |
|    time_elapsed    | 63       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=8.90 +/- 1.14
Episode length: 119.30 +/- 20.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 119          |
|    mean_reward          | 8.9          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0073409057 |
|    clip_fraction        | 0.0777       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.00327     |
|    learning_rate        | 1.24e-05     |
|    loss                 | 0.0934       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00733     |
|    value_loss           | 0.202        |
------------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=8.50 +/- 1.86
Episode length: 116.30 +/- 28.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 8.5      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 8.19     |
| time/              |          |
|    fps             | 113      |
|    iterations      | 2        |
|    time_elapsed    | 144      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=10.30 +/- 1.95
Episode length: 141.60 +/- 21.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 142         |
|    mean_reward          | 10.3        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.008620109 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.198       |
|    learning_rate        | 1.24e-05    |
|    loss                 | 0.0885      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00478    |
|    value_loss           | 0.214       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 8.68     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 3        |
|    time_elapsed    | 255      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=10.00 +/- 2.32
Episode length: 130.70 +/- 33.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 10          |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012329864 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.328       |
|    learning_rate        | 1.24e-05    |
|    loss                 | 0.116       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 0.243       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=9.70 +/- 1.35
Episode length: 135.10 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 135      |
|    mean_reward     | 9.7      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 8.85     |
| time/              |          |
|    fps             | 91       |
|    iterations      | 4        |
|    time_elapsed    | 359      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=9.20 +/- 2.60
Episode length: 122.10 +/- 31.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 122         |
|    mean_reward          | 9.2         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.013648584 |
|    clip_fraction        | 0.0458      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.423       |
|    learning_rate        | 1.24e-05    |
|    loss                 | 0.153       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 0.257       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=9.90 +/- 2.21
Episode length: 132.00 +/- 31.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 9.9      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 9.82     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 5        |
|    time_elapsed    | 441      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=9.40 +/- 1.85
Episode length: 124.70 +/- 29.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 9.4         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.011932334 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.508       |
|    learning_rate        | 1.24e-05    |
|    loss                 | 0.137       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00539    |
|    value_loss           | 0.278       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 11       |
| time/              |          |
|    fps             | 94       |
|    iterations      | 6        |
|    time_elapsed    | 517      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=9.20 +/- 1.72
Episode length: 125.10 +/- 25.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 9.2         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.007162781 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.557       |
|    learning_rate        | 1.24e-05    |
|    loss                 | 0.145       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 0.311       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=9.40 +/- 1.56
Episode length: 123.70 +/- 27.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 9.4      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 12.1     |
| time/              |          |
|    fps             | 95       |
|    iterations      | 7        |
|    time_elapsed    | 599      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=8.70 +/- 3.03
Episode length: 121.40 +/- 39.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 121          |
|    mean_reward          | 8.7          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0054348307 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0.603        |
|    learning_rate        | 1.24e-05     |
|    loss                 | 0.144        |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00351     |
|    value_loss           | 0.306        |
------------------------------------------
Eval num_timesteps=65000, episode_reward=9.90 +/- 3.24
Episode length: 131.40 +/- 47.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 9.9      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 8        |
|    time_elapsed    | 725      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-16 16:24:12,242] Trial 5 finished with value: 8.9 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 1.2427725102939599e-05, 'gamma': 0.9928501996293755, 'gae_lambda': 0.844717689874176}. Best is trial 3 with value: 22.4.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 6.93     |
| time/              |          |
|    fps             | 137      |
|    iterations      | 1        |
|    time_elapsed    | 14       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | 8.32        |
| time/                   |             |
|    fps                  | 105         |
|    iterations           | 2           |
|    time_elapsed         | 38          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.019660434 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.123      |
|    learning_rate        | 0.000563    |
|    loss                 | -0.032      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.116       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=17.10 +/- 2.95
Episode length: 177.10 +/- 37.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 177        |
|    mean_reward          | 17.1       |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.04167105 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.118      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0612    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0605    |
|    value_loss           | 0.109      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 9.52     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 3        |
|    time_elapsed    | 70       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 144         |
|    ep_rew_mean          | 9.86        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 4           |
|    time_elapsed         | 94          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.053426072 |
|    clip_fraction        | 0.43        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.000563    |
|    loss                 | -0.0373     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0666     |
|    value_loss           | 0.116       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=19.20 +/- 3.37
Episode length: 194.00 +/- 34.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 194        |
|    mean_reward          | 19.2       |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.07569255 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.044     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0695    |
|    value_loss           | 0.104      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 5        |
|    time_elapsed    | 126      |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 158        |
|    ep_rew_mean          | 11.8       |
| time/                   |            |
|    fps                  | 81         |
|    iterations           | 6          |
|    time_elapsed         | 149        |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.08505687 |
|    clip_fraction        | 0.522      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.077     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0715    |
|    value_loss           | 0.108      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | 12.5       |
| time/                   |            |
|    fps                  | 82         |
|    iterations           | 7          |
|    time_elapsed         | 173        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.09199092 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.402      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.00655   |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0659    |
|    value_loss           | 0.107      |
----------------------------------------
Eval num_timesteps=15000, episode_reward=21.60 +/- 6.86
Episode length: 209.10 +/- 76.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 209        |
|    mean_reward          | 21.6       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.10610552 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.51       |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0903    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0689    |
|    value_loss           | 0.0937     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 8        |
|    time_elapsed    | 205      |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | 14.1       |
| time/                   |            |
|    fps                  | 80         |
|    iterations           | 9          |
|    time_elapsed         | 229        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.11983566 |
|    clip_fraction        | 0.529      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1         |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0725    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0649    |
|    value_loss           | 0.0955     |
----------------------------------------
Eval num_timesteps=20000, episode_reward=19.00 +/- 3.22
Episode length: 184.90 +/- 30.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 185        |
|    mean_reward          | 19         |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.15270449 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.957     |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0758    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0673    |
|    value_loss           | 0.0918     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 10       |
|    time_elapsed    | 261      |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 16.2       |
| time/                   |            |
|    fps                  | 78         |
|    iterations           | 11         |
|    time_elapsed         | 285        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.14431562 |
|    clip_fraction        | 0.527      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.915     |
|    explained_variance   | 0.498      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0613    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0623    |
|    value_loss           | 0.1        |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 79         |
|    iterations           | 12         |
|    time_elapsed         | 309        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.14816132 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.872     |
|    explained_variance   | 0.556      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.111     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0641    |
|    value_loss           | 0.0921     |
----------------------------------------
Eval num_timesteps=25000, episode_reward=20.80 +/- 6.00
Episode length: 187.10 +/- 58.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 187        |
|    mean_reward          | 20.8       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.16482337 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.875     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0866    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0635    |
|    value_loss           | 0.0864     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 75       |
|    iterations      | 13       |
|    time_elapsed    | 352      |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 193        |
|    ep_rew_mean          | 18.6       |
| time/                   |            |
|    fps                  | 72         |
|    iterations           | 14         |
|    time_elapsed         | 397        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.17257136 |
|    clip_fraction        | 0.536      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.817     |
|    explained_variance   | 0.522      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0396    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0638    |
|    value_loss           | 0.09       |
----------------------------------------
Eval num_timesteps=30000, episode_reward=22.80 +/- 5.34
Episode length: 215.70 +/- 59.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 22.8       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.16443658 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.827     |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0773    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0625    |
|    value_loss           | 0.0913     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 68       |
|    iterations      | 15       |
|    time_elapsed    | 450      |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 19.2       |
| time/                   |            |
|    fps                  | 69         |
|    iterations           | 16         |
|    time_elapsed         | 473        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.19725192 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.796     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.000563   |
|    loss                 | -0.125     |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0654    |
|    value_loss           | 0.095      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 195       |
|    ep_rew_mean          | 19.5      |
| time/                   |           |
|    fps                  | 69        |
|    iterations           | 17        |
|    time_elapsed         | 497       |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.2202642 |
|    clip_fraction        | 0.523     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.788    |
|    explained_variance   | 0.571     |
|    learning_rate        | 0.000563  |
|    loss                 | -0.0933   |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0584   |
|    value_loss           | 0.0972    |
---------------------------------------
Eval num_timesteps=35000, episode_reward=22.70 +/- 7.86
Episode length: 210.40 +/- 75.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 22.7       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.21420532 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0664    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0624    |
|    value_loss           | 0.0969     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 19.8     |
| time/              |          |
|    fps             | 69       |
|    iterations      | 18       |
|    time_elapsed    | 530      |
|    total_timesteps | 36864    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | 20.3      |
| time/                   |           |
|    fps                  | 70        |
|    iterations           | 19        |
|    time_elapsed         | 554       |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.2126857 |
|    clip_fraction        | 0.503     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.694    |
|    explained_variance   | 0.572     |
|    learning_rate        | 0.000563  |
|    loss                 | -0.0429   |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.056    |
|    value_loss           | 0.0953    |
---------------------------------------
Eval num_timesteps=40000, episode_reward=25.60 +/- 8.14
Episode length: 239.70 +/- 78.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 240        |
|    mean_reward          | 25.6       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.19414806 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.745     |
|    explained_variance   | 0.594      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0954    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0601    |
|    value_loss           | 0.095      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 69       |
|    iterations      | 20       |
|    time_elapsed    | 587      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | 20.5       |
| time/                   |            |
|    fps                  | 70         |
|    iterations           | 21         |
|    time_elapsed         | 611        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.22488633 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.737     |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.000563   |
|    loss                 | -0.107     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0665    |
|    value_loss           | 0.0881     |
----------------------------------------
Eval num_timesteps=45000, episode_reward=24.30 +/- 3.98
Episode length: 231.40 +/- 40.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 231        |
|    mean_reward          | 24.3       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.23733002 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.69      |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0889    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0579    |
|    value_loss           | 0.0828     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 69       |
|    iterations      | 22       |
|    time_elapsed    | 644      |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 21.2       |
| time/                   |            |
|    fps                  | 70         |
|    iterations           | 23         |
|    time_elapsed         | 668        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.22611126 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.687     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.099     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.06      |
|    value_loss           | 0.0902     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 207        |
|    ep_rew_mean          | 21.6       |
| time/                   |            |
|    fps                  | 71         |
|    iterations           | 24         |
|    time_elapsed         | 692        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.23752965 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.633     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.102     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0604    |
|    value_loss           | 0.0931     |
----------------------------------------
Eval num_timesteps=50000, episode_reward=24.30 +/- 3.66
Episode length: 218.20 +/- 41.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 218        |
|    mean_reward          | 24.3       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.22050802 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0492    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0626    |
|    value_loss           | 0.0842     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 21.5     |
| time/              |          |
|    fps             | 70       |
|    iterations      | 25       |
|    time_elapsed    | 725      |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 210        |
|    ep_rew_mean          | 22         |
| time/                   |            |
|    fps                  | 71         |
|    iterations           | 26         |
|    time_elapsed         | 749        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.22423759 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.66      |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0789    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0595    |
|    value_loss           | 0.0841     |
----------------------------------------
Eval num_timesteps=55000, episode_reward=25.10 +/- 3.33
Episode length: 237.40 +/- 30.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 237        |
|    mean_reward          | 25.1       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.19592896 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0752    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.063     |
|    value_loss           | 0.0905     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 22.4     |
| time/              |          |
|    fps             | 70       |
|    iterations      | 27       |
|    time_elapsed    | 786      |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 218        |
|    ep_rew_mean          | 22.7       |
| time/                   |            |
|    fps                  | 68         |
|    iterations           | 28         |
|    time_elapsed         | 832        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.22069135 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.726     |
|    explained_variance   | 0.716      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.109     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0688    |
|    value_loss           | 0.0895     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 223        |
|    ep_rew_mean          | 23.2       |
| time/                   |            |
|    fps                  | 67         |
|    iterations           | 29         |
|    time_elapsed         | 878        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.21338896 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.666     |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0918    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0603    |
|    value_loss           | 0.0904     |
----------------------------------------
Eval num_timesteps=60000, episode_reward=22.30 +/- 2.53
Episode length: 201.90 +/- 28.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 22.3       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.20791489 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.675     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0498    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0697    |
|    value_loss           | 0.0788     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 30       |
|    time_elapsed    | 913      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 16:39:43,832] Trial 6 finished with value: 22.9 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 0.0005627688468176378, 'gamma': 0.9357867252516964, 'gae_lambda': 0.8184184137127519}. Best is trial 6 with value: 22.9.
Using cuda device
Eval num_timesteps=5000, episode_reward=2.00 +/- 1.48
Episode length: 82.70 +/- 18.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.7     |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 8.39     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 1        |
|    time_elapsed    | 65       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=15.30 +/- 3.32
Episode length: 199.30 +/- 38.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 15.3        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.007245523 |
|    clip_fraction        | 0.0446      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0247     |
|    learning_rate        | 2.85e-05    |
|    loss                 | 0.0936      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00727    |
|    value_loss           | 0.241       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=14.40 +/- 4.18
Episode length: 181.00 +/- 59.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 14.4     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 8.03     |
| time/              |          |
|    fps             | 101      |
|    iterations      | 2        |
|    time_elapsed    | 161      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=9.80 +/- 2.44
Episode length: 132.20 +/- 30.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 132        |
|    mean_reward          | 9.8        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.00888252 |
|    clip_fraction        | 0.057      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.251      |
|    learning_rate        | 2.85e-05   |
|    loss                 | 0.0847     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00867   |
|    value_loss           | 0.229      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 8.93     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 3        |
|    time_elapsed    | 248      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=9.70 +/- 2.97
Episode length: 120.40 +/- 28.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 120         |
|    mean_reward          | 9.7         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.007862102 |
|    clip_fraction        | 0.0488      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.31        |
|    learning_rate        | 2.85e-05    |
|    loss                 | 0.112       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 0.223       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=11.50 +/- 2.50
Episode length: 143.90 +/- 37.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 144      |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 9.86     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 4        |
|    time_elapsed    | 369      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=14.70 +/- 4.98
Episode length: 159.00 +/- 52.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 159         |
|    mean_reward          | 14.7        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.011664573 |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.332       |
|    learning_rate        | 2.85e-05    |
|    loss                 | 0.116       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00898    |
|    value_loss           | 0.22        |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=14.20 +/- 3.79
Episode length: 155.70 +/- 38.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 156      |
|    mean_reward     | 14.2     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 11.6     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 5        |
|    time_elapsed    | 473      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=10.50 +/- 2.54
Episode length: 136.90 +/- 28.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 137         |
|    mean_reward          | 10.5        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.011619106 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.314       |
|    learning_rate        | 2.85e-05    |
|    loss                 | 0.131       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 6        |
|    time_elapsed    | 559      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=9.90 +/- 2.70
Episode length: 123.00 +/- 31.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 123         |
|    mean_reward          | 9.9         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009396229 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.346       |
|    learning_rate        | 2.85e-05    |
|    loss                 | 0.136       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00907    |
|    value_loss           | 0.218       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=9.60 +/- 3.07
Episode length: 122.90 +/- 34.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 9.6      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 7        |
|    time_elapsed    | 649      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=11.20 +/- 5.33
Episode length: 142.70 +/- 66.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 143        |
|    mean_reward          | 11.2       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.00640645 |
|    clip_fraction        | 0.0732     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.334      |
|    learning_rate        | 2.85e-05   |
|    loss                 | 0.0839     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.00846   |
|    value_loss           | 0.218      |
----------------------------------------
Eval num_timesteps=65000, episode_reward=8.80 +/- 2.32
Episode length: 108.20 +/- 29.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 8.8      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 8        |
|    time_elapsed    | 746      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-16 16:52:49,074] Trial 7 finished with value: 9.7 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 2.8455469050400548e-05, 'gamma': 0.9044248662993944, 'gae_lambda': 0.975092919145797}. Best is trial 6 with value: 22.9.
Using cuda device
Eval num_timesteps=5000, episode_reward=2.30 +/- 2.05
Episode length: 80.90 +/- 22.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.9     |
|    mean_reward     | 2.3      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 8.16     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 1        |
|    time_elapsed    | 93       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=8.20 +/- 1.54
Episode length: 112.30 +/- 25.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 112          |
|    mean_reward          | 8.2          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0069367313 |
|    clip_fraction        | 0.0651       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.0718      |
|    learning_rate        | 2.92e-05     |
|    loss                 | 0.0973       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0069      |
|    value_loss           | 0.221        |
------------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=8.10 +/- 2.26
Episode length: 116.10 +/- 33.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 8.1      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 8.11     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 2        |
|    time_elapsed    | 174      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=9.90 +/- 3.05
Episode length: 128.30 +/- 33.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 9.9         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010076376 |
|    clip_fraction        | 0.0572      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.27        |
|    learning_rate        | 2.92e-05    |
|    loss                 | 0.0796      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00722    |
|    value_loss           | 0.21        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 8.9      |
| time/              |          |
|    fps             | 98       |
|    iterations      | 3        |
|    time_elapsed    | 250      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=9.10 +/- 2.39
Episode length: 119.30 +/- 32.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 119         |
|    mean_reward          | 9.1         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012472667 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.38        |
|    learning_rate        | 2.92e-05    |
|    loss                 | 0.102       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00767    |
|    value_loss           | 0.219       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=9.60 +/- 2.94
Episode length: 129.10 +/- 46.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 129      |
|    mean_reward     | 9.6      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 9.82     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 4        |
|    time_elapsed    | 333      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=9.70 +/- 2.61
Episode length: 133.90 +/- 35.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 9.7         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.007316676 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.431       |
|    learning_rate        | 2.92e-05    |
|    loss                 | 0.126       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00769    |
|    value_loss           | 0.234       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=11.20 +/- 2.40
Episode length: 146.90 +/- 30.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 147      |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 5        |
|    time_elapsed    | 440      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=9.20 +/- 1.47
Episode length: 124.00 +/- 21.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 124         |
|    mean_reward          | 9.2         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.009512863 |
|    clip_fraction        | 0.0693      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.456       |
|    learning_rate        | 2.92e-05    |
|    loss                 | 0.0824      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00837    |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 6        |
|    time_elapsed    | 544      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=10.50 +/- 4.01
Episode length: 142.10 +/- 52.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 142          |
|    mean_reward          | 10.5         |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0077351313 |
|    clip_fraction        | 0.0612       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.491        |
|    learning_rate        | 2.92e-05     |
|    loss                 | 0.0972       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00796     |
|    value_loss           | 0.234        |
------------------------------------------
Eval num_timesteps=55000, episode_reward=10.60 +/- 2.91
Episode length: 139.30 +/- 41.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 91       |
|    iterations      | 7        |
|    time_elapsed    | 628      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=10.80 +/- 2.64
Episode length: 143.30 +/- 37.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | 10.8        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.007713514 |
|    clip_fraction        | 0.073       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.473       |
|    learning_rate        | 2.92e-05    |
|    loss                 | 0.105       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00835    |
|    value_loss           | 0.231       |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=10.60 +/- 2.20
Episode length: 130.20 +/- 30.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 10.6     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 8        |
|    time_elapsed    | 711      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-16 17:04:55,856] Trial 8 finished with value: 12.5 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 2.9245339887673042e-05, 'gamma': 0.9458216050293908, 'gae_lambda': 0.9038290730186662}. Best is trial 6 with value: 22.9.
Using cuda device
Eval num_timesteps=5000, episode_reward=12.30 +/- 4.08
Episode length: 172.80 +/- 50.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 7.52     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 1        |
|    time_elapsed    | 70       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=10.30 +/- 4.00
Episode length: 140.40 +/- 37.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 140         |
|    mean_reward          | 10.3        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011814386 |
|    clip_fraction        | 0.0318      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.062      |
|    learning_rate        | 1.56e-05    |
|    loss                 | 0.127       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00546    |
|    value_loss           | 0.292       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=10.30 +/- 3.74
Episode length: 150.60 +/- 44.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 151      |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 6.97     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 2        |
|    time_elapsed    | 200      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=8.30 +/- 1.95
Episode length: 113.90 +/- 23.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 8.3         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010952283 |
|    clip_fraction        | 0.0794      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.233       |
|    learning_rate        | 1.56e-05    |
|    loss                 | 0.124       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00921    |
|    value_loss           | 0.312       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 7.86     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 3        |
|    time_elapsed    | 282      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=9.20 +/- 1.40
Episode length: 122.60 +/- 20.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 123         |
|    mean_reward          | 9.2         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010455974 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.315       |
|    learning_rate        | 1.56e-05    |
|    loss                 | 0.115       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 0.318       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=9.50 +/- 2.73
Episode length: 130.00 +/- 43.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 9.5      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 9.07     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 4        |
|    time_elapsed    | 364      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=9.60 +/- 3.72
Episode length: 129.20 +/- 52.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 129         |
|    mean_reward          | 9.6         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.013255623 |
|    clip_fraction        | 0.0762      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.396       |
|    learning_rate        | 1.56e-05    |
|    loss                 | 0.158       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00816    |
|    value_loss           | 0.316       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=10.10 +/- 2.81
Episode length: 135.00 +/- 48.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 135      |
|    mean_reward     | 10.1     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    fps             | 91       |
|    iterations      | 5        |
|    time_elapsed    | 447      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=7.60 +/- 1.43
Episode length: 103.80 +/- 18.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 7.6         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.008947166 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.45        |
|    learning_rate        | 1.56e-05    |
|    loss                 | 0.147       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00258    |
|    value_loss           | 0.305       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 6        |
|    time_elapsed    | 523      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=8.80 +/- 2.32
Episode length: 120.40 +/- 27.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 120         |
|    mean_reward          | 8.8         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.011414755 |
|    clip_fraction        | 0.0309      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.479       |
|    learning_rate        | 1.56e-05    |
|    loss                 | 0.0809      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 0.325       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=10.30 +/- 2.87
Episode length: 142.50 +/- 43.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 11.7     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 7        |
|    time_elapsed    | 661      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=9.70 +/- 2.90
Episode length: 131.20 +/- 37.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 9.7         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.006493197 |
|    clip_fraction        | 0.0323      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.487       |
|    learning_rate        | 1.56e-05    |
|    loss                 | 0.14        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 0.322       |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=9.30 +/- 3.58
Episode length: 126.80 +/- 48.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 9.3      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 12.7     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 8        |
|    time_elapsed    | 743      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-16 17:17:34,587] Trial 9 finished with value: 10.3 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 1.5565488935070014e-05, 'gamma': 0.9535584463147919, 'gae_lambda': 0.9435386037787161}. Best is trial 6 with value: 22.9.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 7.71     |
| time/              |          |
|    fps             | 133      |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 136         |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 2           |
|    time_elapsed         | 40          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.013752798 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.141      |
|    learning_rate        | 0.000193    |
|    loss                 | 0.0776      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.156       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=7.80 +/- 1.94
Episode length: 142.00 +/- 33.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 142         |
|    mean_reward          | 7.8         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010571202 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.165       |
|    learning_rate        | 0.000193    |
|    loss                 | 0.0577      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.14        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 8.23     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 3        |
|    time_elapsed    | 69       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 143         |
|    ep_rew_mean          | 8.98        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 4           |
|    time_elapsed         | 93          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.018123016 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.203       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.0314     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0392     |
|    value_loss           | 0.135       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=17.30 +/- 6.33
Episode length: 189.70 +/- 77.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 190         |
|    mean_reward          | 17.3        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.023301527 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.0659     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.047      |
|    value_loss           | 0.14        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 9.84     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 5        |
|    time_elapsed    | 125      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | 10.6        |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 6           |
|    time_elapsed         | 149         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.026194962 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.000193    |
|    loss                 | 0.0306      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0497     |
|    value_loss           | 0.135       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 155         |
|    ep_rew_mean          | 11.1        |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 7           |
|    time_elapsed         | 173         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.030396793 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.385       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.0328     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.053      |
|    value_loss           | 0.147       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=20.50 +/- 5.73
Episode length: 211.70 +/- 68.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 212       |
|    mean_reward          | 20.5      |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.0332378 |
|    clip_fraction        | 0.323     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.16     |
|    explained_variance   | 0.465     |
|    learning_rate        | 0.000193  |
|    loss                 | -0.0418   |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.0537   |
|    value_loss           | 0.123     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 8        |
|    time_elapsed    | 205      |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 165        |
|    ep_rew_mean          | 13         |
| time/                   |            |
|    fps                  | 80         |
|    iterations           | 9          |
|    time_elapsed         | 228        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.03813438 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.388      |
|    learning_rate        | 0.000193   |
|    loss                 | -0.0951    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.129      |
----------------------------------------
Eval num_timesteps=20000, episode_reward=20.90 +/- 5.26
Episode length: 201.20 +/- 51.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 201         |
|    mean_reward          | 20.9        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.034076095 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.0425     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0492     |
|    value_loss           | 0.12        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 71       |
|    iterations      | 10       |
|    time_elapsed    | 287      |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 15         |
| time/                   |            |
|    fps                  | 67         |
|    iterations           | 11         |
|    time_elapsed         | 334        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.03557664 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.000193   |
|    loss                 | -0.0421    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0514    |
|    value_loss           | 0.121      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 16.2       |
| time/                   |            |
|    fps                  | 68         |
|    iterations           | 12         |
|    time_elapsed         | 361        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.04397711 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.53       |
|    learning_rate        | 0.000193   |
|    loss                 | -0.0344    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.125      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=23.20 +/- 4.60
Episode length: 214.70 +/- 50.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 215         |
|    mean_reward          | 23.2        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.047708593 |
|    clip_fraction        | 0.359       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.988      |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.065      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0553     |
|    value_loss           | 0.116       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 13       |
|    time_elapsed    | 394      |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 17.7        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 14          |
|    time_elapsed         | 418         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.051927567 |
|    clip_fraction        | 0.379       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.97       |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.0445     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0534     |
|    value_loss           | 0.123       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=23.30 +/- 5.42
Episode length: 211.70 +/- 54.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 23.3       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.05005439 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.917     |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.000193   |
|    loss                 | -0.0419    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0516    |
|    value_loss           | 0.123      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 15       |
|    time_elapsed    | 451      |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 18.8       |
| time/                   |            |
|    fps                  | 68         |
|    iterations           | 16         |
|    time_elapsed         | 475        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.06507251 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.828     |
|    explained_variance   | 0.501      |
|    learning_rate        | 0.000193   |
|    loss                 | -0.042     |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.13       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 196         |
|    ep_rew_mean          | 19.6        |
| time/                   |             |
|    fps                  | 69          |
|    iterations           | 17          |
|    time_elapsed         | 499         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.067453414 |
|    clip_fraction        | 0.377       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.0548     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.05       |
|    value_loss           | 0.125       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=21.90 +/- 7.50
Episode length: 199.30 +/- 70.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 199        |
|    mean_reward          | 21.9       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.07180448 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.775     |
|    explained_variance   | 0.538      |
|    learning_rate        | 0.000193   |
|    loss                 | -0.0974    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.118      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 20       |
| time/              |          |
|    fps             | 69       |
|    iterations      | 18       |
|    time_elapsed    | 531      |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | 20.5        |
| time/                   |             |
|    fps                  | 69          |
|    iterations           | 19          |
|    time_elapsed         | 555         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.080973305 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.0237     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0462     |
|    value_loss           | 0.13        |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=24.80 +/- 8.68
Episode length: 221.40 +/- 85.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 221         |
|    mean_reward          | 24.8        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.066743106 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.0427     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0451     |
|    value_loss           | 0.121       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 21       |
| time/              |          |
|    fps             | 69       |
|    iterations      | 20       |
|    time_elapsed    | 589      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 206        |
|    ep_rew_mean          | 21.5       |
| time/                   |            |
|    fps                  | 70         |
|    iterations           | 21         |
|    time_elapsed         | 613        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.06794566 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.679     |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.000193   |
|    loss                 | -0.0394    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0391    |
|    value_loss           | 0.126      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=25.70 +/- 2.93
Episode length: 234.20 +/- 31.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | 25.7        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.078842185 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.68       |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.0389     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.046      |
|    value_loss           | 0.127       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 69       |
|    iterations      | 22       |
|    time_elapsed    | 646      |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 211        |
|    ep_rew_mean          | 22.3       |
| time/                   |            |
|    fps                  | 70         |
|    iterations           | 23         |
|    time_elapsed         | 670        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.07937018 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.669     |
|    explained_variance   | 0.551      |
|    learning_rate        | 0.000193   |
|    loss                 | -0.0264    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0455    |
|    value_loss           | 0.124      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 213         |
|    ep_rew_mean          | 22.7        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 24          |
|    time_elapsed         | 715         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.089606255 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.00872    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.043      |
|    value_loss           | 0.11        |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=26.00 +/- 5.53
Episode length: 234.00 +/- 59.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 234        |
|    mean_reward          | 26         |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.09517376 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.6       |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.000193   |
|    loss                 | 0.0049     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 0.116      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 23.1     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 25       |
|    time_elapsed    | 777      |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 23.7       |
| time/                   |            |
|    fps                  | 66         |
|    iterations           | 26         |
|    time_elapsed         | 804        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.09104219 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.554     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.000193   |
|    loss                 | -0.0197    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0385    |
|    value_loss           | 0.11       |
----------------------------------------
Eval num_timesteps=55000, episode_reward=25.60 +/- 5.61
Episode length: 218.20 +/- 52.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 218        |
|    mean_reward          | 25.6       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.08293812 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.524     |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.000193   |
|    loss                 | -0.023     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0354    |
|    value_loss           | 0.125      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 66       |
|    iterations      | 27       |
|    time_elapsed    | 837      |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 224         |
|    ep_rew_mean          | 24.2        |
| time/                   |             |
|    fps                  | 66          |
|    iterations           | 28          |
|    time_elapsed         | 861         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.092006505 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.000193    |
|    loss                 | -0.00151    |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0399     |
|    value_loss           | 0.118       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 67         |
|    iterations           | 29         |
|    time_elapsed         | 885        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.07260065 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.49      |
|    explained_variance   | 0.567      |
|    learning_rate        | 0.000193   |
|    loss                 | -0.0255    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0386    |
|    value_loss           | 0.116      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=24.90 +/- 4.13
Episode length: 220.50 +/- 40.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 220         |
|    mean_reward          | 24.9        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.093342185 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.000193    |
|    loss                 | -0.0383     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.042      |
|    value_loss           | 0.117       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 24       |
| time/              |          |
|    fps             | 66       |
|    iterations      | 30       |
|    time_elapsed    | 918      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 17:33:10,206] Trial 10 finished with value: 22.4 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 0.00019338108619582813, 'gamma': 0.931230496774958, 'gae_lambda': 0.8738637216137015}. Best is trial 6 with value: 22.9.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 5.65     |
| time/              |          |
|    fps             | 134      |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 132         |
|    ep_rew_mean          | 7.2         |
| time/                   |             |
|    fps                  | 105         |
|    iterations           | 2           |
|    time_elapsed         | 38          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.016570609 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0228     |
|    learning_rate        | 0.000638    |
|    loss                 | -0.00172    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.104       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=12.50 +/- 2.46
Episode length: 153.60 +/- 28.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 154         |
|    mean_reward          | 12.5        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.037344765 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.184       |
|    learning_rate        | 0.000638    |
|    loss                 | -0.0577     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0483     |
|    value_loss           | 0.0978      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 8.2      |
| time/              |          |
|    fps             | 88       |
|    iterations      | 3        |
|    time_elapsed    | 69       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 146         |
|    ep_rew_mean          | 9.33        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 4           |
|    time_elapsed         | 93          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.056393854 |
|    clip_fraction        | 0.434       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.23        |
|    learning_rate        | 0.000638    |
|    loss                 | -0.0463     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0657     |
|    value_loss           | 0.117       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=15.40 +/- 3.20
Episode length: 158.40 +/- 31.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 15.4        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.075497665 |
|    clip_fraction        | 0.478       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.000638    |
|    loss                 | -0.0937     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0671     |
|    value_loss           | 0.128       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 5        |
|    time_elapsed    | 124      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 152         |
|    ep_rew_mean          | 11.1        |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 6           |
|    time_elapsed         | 148         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.089476906 |
|    clip_fraction        | 0.513       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.000638    |
|    loss                 | -0.0659     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.068      |
|    value_loss           | 0.12        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 152        |
|    ep_rew_mean          | 11.4       |
| time/                   |            |
|    fps                  | 83         |
|    iterations           | 7          |
|    time_elapsed         | 171        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.10065463 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.553      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0627    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0654    |
|    value_loss           | 0.115      |
----------------------------------------
Eval num_timesteps=15000, episode_reward=19.90 +/- 4.91
Episode length: 196.00 +/- 49.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 196        |
|    mean_reward          | 19.9       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.10728407 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.56       |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0976    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0666    |
|    value_loss           | 0.109      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 12.3     |
| time/              |          |
|    fps             | 73       |
|    iterations      | 8        |
|    time_elapsed    | 224      |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | 13.4       |
| time/                   |            |
|    fps                  | 68         |
|    iterations           | 9          |
|    time_elapsed         | 270        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.12780213 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0866    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0702    |
|    value_loss           | 0.104      |
----------------------------------------
Eval num_timesteps=20000, episode_reward=15.30 +/- 5.24
Episode length: 155.00 +/- 57.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 155        |
|    mean_reward          | 15.3       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.12957437 |
|    clip_fraction        | 0.529      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.961     |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.113     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0641    |
|    value_loss           | 0.102      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 10       |
|    time_elapsed    | 310      |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | 15.3       |
| time/                   |            |
|    fps                  | 67         |
|    iterations           | 11         |
|    time_elapsed         | 335        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.14681256 |
|    clip_fraction        | 0.542      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.958     |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.113     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0687    |
|    value_loss           | 0.107      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | 15.9       |
| time/                   |            |
|    fps                  | 68         |
|    iterations           | 12         |
|    time_elapsed         | 359        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.13551712 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.905     |
|    explained_variance   | 0.706      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.102     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0681    |
|    value_loss           | 0.0993     |
----------------------------------------
Eval num_timesteps=25000, episode_reward=22.60 +/- 5.68
Episode length: 217.90 +/- 56.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 218        |
|    mean_reward          | 22.6       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.15245807 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.916     |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.000638   |
|    loss                 | -0.063     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.064     |
|    value_loss           | 0.111      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 13       |
|    time_elapsed    | 392      |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 178        |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 68         |
|    iterations           | 14         |
|    time_elapsed         | 417        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.15502794 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.897     |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0953    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0634    |
|    value_loss           | 0.114      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=16.30 +/- 7.55
Episode length: 160.40 +/- 63.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 160        |
|    mean_reward          | 16.3       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.18179093 |
|    clip_fraction        | 0.529      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.87      |
|    explained_variance   | 0.769      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0719    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0677    |
|    value_loss           | 0.109      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 15       |
|    time_elapsed    | 448      |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 187        |
|    ep_rew_mean          | 18.1       |
| time/                   |            |
|    fps                  | 69         |
|    iterations           | 16         |
|    time_elapsed         | 471        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.17367145 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.878     |
|    explained_variance   | 0.814      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0979    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0742    |
|    value_loss           | 0.0995     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 18.7       |
| time/                   |            |
|    fps                  | 70         |
|    iterations           | 17         |
|    time_elapsed         | 496        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.19746697 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.825     |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.000638   |
|    loss                 | -0.12      |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0669    |
|    value_loss           | 0.0984     |
----------------------------------------
Eval num_timesteps=35000, episode_reward=19.80 +/- 5.53
Episode length: 187.30 +/- 51.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 187        |
|    mean_reward          | 19.8       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.20189993 |
|    clip_fraction        | 0.556      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.816     |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.103     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0673    |
|    value_loss           | 0.106      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 69       |
|    iterations      | 18       |
|    time_elapsed    | 528      |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 198        |
|    ep_rew_mean          | 19.6       |
| time/                   |            |
|    fps                  | 70         |
|    iterations           | 19         |
|    time_elapsed         | 552        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.21600115 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.778     |
|    explained_variance   | 0.797      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0878    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0666    |
|    value_loss           | 0.107      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=21.90 +/- 6.04
Episode length: 206.00 +/- 65.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 206        |
|    mean_reward          | 21.9       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.19356464 |
|    clip_fraction        | 0.523      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.766     |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0692    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0658    |
|    value_loss           | 0.104      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 69       |
|    iterations      | 20       |
|    time_elapsed    | 585      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 202        |
|    ep_rew_mean          | 20.3       |
| time/                   |            |
|    fps                  | 70         |
|    iterations           | 21         |
|    time_elapsed         | 609        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.22881521 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.128     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0599    |
|    value_loss           | 0.116      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=21.40 +/- 7.71
Episode length: 200.20 +/- 67.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 200        |
|    mean_reward          | 21.4       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.21152806 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.675     |
|    explained_variance   | 0.811      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.121     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0653    |
|    value_loss           | 0.106      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 207      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 22       |
|    time_elapsed    | 656      |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 208        |
|    ep_rew_mean          | 20.9       |
| time/                   |            |
|    fps                  | 67         |
|    iterations           | 23         |
|    time_elapsed         | 702        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.22646838 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.715     |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0746    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0615    |
|    value_loss           | 0.106      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 20.7       |
| time/                   |            |
|    fps                  | 66         |
|    iterations           | 24         |
|    time_elapsed         | 742        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.23303741 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.751     |
|    explained_variance   | 0.861      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0744    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.068     |
|    value_loss           | 0.0993     |
----------------------------------------
Eval num_timesteps=50000, episode_reward=22.30 +/- 2.49
Episode length: 210.30 +/- 17.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 22.3       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.27481955 |
|    clip_fraction        | 0.568      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0947    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.075     |
|    value_loss           | 0.107      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 66       |
|    iterations      | 25       |
|    time_elapsed    | 774      |
|    total_timesteps | 51200    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 208       |
|    ep_rew_mean          | 21.1      |
| time/                   |           |
|    fps                  | 66        |
|    iterations           | 26        |
|    time_elapsed         | 798       |
|    total_timesteps      | 53248     |
| train/                  |           |
|    approx_kl            | 0.2610979 |
|    clip_fraction        | 0.534     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.7      |
|    explained_variance   | 0.811     |
|    learning_rate        | 0.000638  |
|    loss                 | -0.0927   |
|    n_updates            | 250       |
|    policy_gradient_loss | -0.066    |
|    value_loss           | 0.102     |
---------------------------------------
Eval num_timesteps=55000, episode_reward=21.90 +/- 4.66
Episode length: 221.10 +/- 43.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 221        |
|    mean_reward          | 21.9       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.25806326 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0851    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0658    |
|    value_loss           | 0.098      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 21       |
| time/              |          |
|    fps             | 66       |
|    iterations      | 27       |
|    time_elapsed    | 832      |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 20.4       |
| time/                   |            |
|    fps                  | 66         |
|    iterations           | 28         |
|    time_elapsed         | 855        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.25024146 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.687     |
|    explained_variance   | 0.847      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.108     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0653    |
|    value_loss           | 0.108      |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 198      |
|    ep_rew_mean          | 20.4     |
| time/                   |          |
|    fps                  | 67       |
|    iterations           | 29       |
|    time_elapsed         | 880      |
|    total_timesteps      | 59392    |
| train/                  |          |
|    approx_kl            | 0.278656 |
|    clip_fraction        | 0.516    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.655   |
|    explained_variance   | 0.843    |
|    learning_rate        | 0.000638 |
|    loss                 | -0.106   |
|    n_updates            | 280      |
|    policy_gradient_loss | -0.0705  |
|    value_loss           | 0.108    |
--------------------------------------
Eval num_timesteps=60000, episode_reward=21.10 +/- 2.70
Episode length: 201.70 +/- 37.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 21.1       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.26126474 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.624     |
|    explained_variance   | 0.845      |
|    learning_rate        | 0.000638   |
|    loss                 | -0.0965    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0638    |
|    value_loss           | 0.106      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 20.2     |
| time/              |          |
|    fps             | 67       |
|    iterations      | 30       |
|    time_elapsed    | 912      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 17:48:39,915] Trial 11 finished with value: 21.5 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 0.0006383255352363083, 'gamma': 0.972786086998928, 'gae_lambda': 0.8009579865988589}. Best is trial 6 with value: 22.9.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 7.79     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 141         |
|    ep_rew_mean          | 8.48        |
| time/                   |             |
|    fps                  | 121         |
|    iterations           | 2           |
|    time_elapsed         | 33          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009413002 |
|    clip_fraction        | 0.0514      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0258     |
|    learning_rate        | 0.000301    |
|    loss                 | 0.058       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00948    |
|    value_loss           | 0.262       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=11.90 +/- 3.30
Episode length: 143.80 +/- 36.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009790844 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0549      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.236       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 9.02     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 3        |
|    time_elapsed    | 57       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 151         |
|    ep_rew_mean          | 10.1        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 4           |
|    time_elapsed         | 75          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.010456919 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.362       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0357      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.271       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=19.70 +/- 3.35
Episode length: 195.20 +/- 39.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 195         |
|    mean_reward          | 19.7        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012067966 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0306      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.272       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 11       |
| time/              |          |
|    fps             | 100      |
|    iterations      | 5        |
|    time_elapsed    | 101      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 162         |
|    ep_rew_mean          | 11.8        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 6           |
|    time_elapsed         | 119         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012287425 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0308      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0299     |
|    value_loss           | 0.244       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 164         |
|    ep_rew_mean          | 12.2        |
| time/                   |             |
|    fps                  | 106         |
|    iterations           | 7           |
|    time_elapsed         | 134         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.012327518 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.544       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0183      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0287     |
|    value_loss           | 0.245       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=19.00 +/- 6.91
Episode length: 189.60 +/- 76.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 190         |
|    mean_reward          | 19          |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.017193075 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.621       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0408      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0383     |
|    value_loss           | 0.249       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 12.7     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 8        |
|    time_elapsed    | 181      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 171         |
|    ep_rew_mean          | 13.7        |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 9           |
|    time_elapsed         | 217         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.017192777 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0528      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.238       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=18.90 +/- 3.48
Episode length: 193.90 +/- 45.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | 18.9        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.018348984 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.000301    |
|    loss                 | 0.00638     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0371     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 10       |
|    time_elapsed    | 258      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | 15.4        |
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 11          |
|    time_elapsed         | 275         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.026549567 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0124      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0367     |
|    value_loss           | 0.26        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | 16.1        |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 12          |
|    time_elapsed         | 294         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.022487635 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0106      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0398     |
|    value_loss           | 0.237       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=20.70 +/- 4.15
Episode length: 191.70 +/- 44.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 192         |
|    mean_reward          | 20.7        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.021443872 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0674      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0376     |
|    value_loss           | 0.303       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 13       |
|    time_elapsed    | 320      |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | 16.9       |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 14         |
|    time_elapsed         | 338        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.02663847 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.000301   |
|    loss                 | 0.0386     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0388    |
|    value_loss           | 0.274      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=19.30 +/- 7.04
Episode length: 187.20 +/- 65.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 187       |
|    mean_reward          | 19.3      |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.0231533 |
|    clip_fraction        | 0.223     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.993    |
|    explained_variance   | 0.723     |
|    learning_rate        | 0.000301  |
|    loss                 | 0.0196    |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0369   |
|    value_loss           | 0.248     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 15       |
|    time_elapsed    | 364      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 17.8        |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 16          |
|    time_elapsed         | 382         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.023509713 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.00469     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0364     |
|    value_loss           | 0.212       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 18.3        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 17          |
|    time_elapsed         | 400         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.024837457 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.992      |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.00381     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0384     |
|    value_loss           | 0.221       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=25.50 +/- 5.52
Episode length: 240.00 +/- 68.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 240         |
|    mean_reward          | 25.5        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.028009601 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.965      |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0165      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.039      |
|    value_loss           | 0.26        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 18       |
|    time_elapsed    | 428      |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 19          |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 19          |
|    time_elapsed         | 447         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.024919469 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.921      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0538      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0393     |
|    value_loss           | 0.253       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=20.00 +/- 5.50
Episode length: 184.80 +/- 47.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 185         |
|    mean_reward          | 20          |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.029993372 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.88       |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0333      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0362     |
|    value_loss           | 0.294       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 19.4     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 20       |
|    time_elapsed    | 472      |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 196         |
|    ep_rew_mean          | 20          |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 21          |
|    time_elapsed         | 491         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.025714178 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.84       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0732      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0365     |
|    value_loss           | 0.292       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=22.00 +/- 3.26
Episode length: 185.60 +/- 37.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 186        |
|    mean_reward          | 22         |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.02776729 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.83      |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.000301   |
|    loss                 | 0.0222     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.277      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 20.2     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 22       |
|    time_elapsed    | 516      |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 196        |
|    ep_rew_mean          | 20.4       |
| time/                   |            |
|    fps                  | 88         |
|    iterations           | 23         |
|    time_elapsed         | 535        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.02733427 |
|    clip_fraction        | 0.235      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.808     |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.000301   |
|    loss                 | 0.0233     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0373    |
|    value_loss           | 0.227      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 197        |
|    ep_rew_mean          | 20.8       |
| time/                   |            |
|    fps                  | 88         |
|    iterations           | 24         |
|    time_elapsed         | 553        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.03362342 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.786     |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.000301   |
|    loss                 | -0.0026    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0434    |
|    value_loss           | 0.22       |
----------------------------------------
Eval num_timesteps=50000, episode_reward=23.80 +/- 2.89
Episode length: 225.20 +/- 28.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 225        |
|    mean_reward          | 23.8       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.03543032 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.796     |
|    explained_variance   | 0.801      |
|    learning_rate        | 0.000301   |
|    loss                 | 0.0138     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.041     |
|    value_loss           | 0.218      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 25       |
|    time_elapsed    | 577      |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | 21.2        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 26          |
|    time_elapsed         | 613         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.034075137 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.769      |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0392      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0379     |
|    value_loss           | 0.268       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=23.90 +/- 3.39
Episode length: 213.10 +/- 38.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 213         |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.048090663 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.00754     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0406     |
|    value_loss           | 0.266       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 21.4     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 27       |
|    time_elapsed    | 661      |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 207        |
|    ep_rew_mean          | 21.8       |
| time/                   |            |
|    fps                  | 82         |
|    iterations           | 28         |
|    time_elapsed         | 694        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.04172554 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.732     |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.000301   |
|    loss                 | 0.00489    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0409    |
|    value_loss           | 0.22       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 207         |
|    ep_rew_mean          | 21.9        |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 29          |
|    time_elapsed         | 711         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.034883887 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.0228      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0425     |
|    value_loss           | 0.209       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=27.80 +/- 6.23
Episode length: 259.00 +/- 60.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 259         |
|    mean_reward          | 27.8        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.037932776 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.000301    |
|    loss                 | -0.00327    |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0359     |
|    value_loss           | 0.246       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 207      |
|    ep_rew_mean     | 22       |
| time/              |          |
|    fps             | 82       |
|    iterations      | 30       |
|    time_elapsed    | 740      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 18:01:13,471] Trial 12 finished with value: 27.6 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00030139223340292865, 'gamma': 0.9684476514714841, 'gae_lambda': 0.9333364953320186}. Best is trial 12 with value: 27.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 7.93     |
| time/              |          |
|    fps             | 131      |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 121         |
|    iterations           | 2           |
|    time_elapsed         | 33          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.007463202 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0552     |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0707      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00729    |
|    value_loss           | 0.305       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=15.20 +/- 4.60
Episode length: 179.40 +/- 60.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 179         |
|    mean_reward          | 15.2        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011002056 |
|    clip_fraction        | 0.0958      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0696      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.298       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 8.2      |
| time/              |          |
|    fps             | 103      |
|    iterations      | 3        |
|    time_elapsed    | 59       |
|    total_timesteps | 6144     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 142          |
|    ep_rew_mean          | 8.77         |
| time/                   |              |
|    fps                  | 105          |
|    iterations           | 4            |
|    time_elapsed         | 77           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0063335467 |
|    clip_fraction        | 0.0755       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.424        |
|    learning_rate        | 0.000227     |
|    loss                 | 0.0601       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.015       |
|    value_loss           | 0.24         |
------------------------------------------
Eval num_timesteps=10000, episode_reward=13.10 +/- 4.48
Episode length: 151.50 +/- 42.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 152         |
|    mean_reward          | 13.1        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009894799 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.059       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 8.82     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 5        |
|    time_elapsed    | 102      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 146         |
|    ep_rew_mean          | 9.61        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 6           |
|    time_elapsed         | 120         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010434845 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0458      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0272     |
|    value_loss           | 0.279       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 152         |
|    ep_rew_mean          | 10.5        |
| time/                   |             |
|    fps                  | 103         |
|    iterations           | 7           |
|    time_elapsed         | 138         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.013547324 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0666      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0253     |
|    value_loss           | 0.311       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=15.60 +/- 4.94
Episode length: 169.00 +/- 57.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 15.6        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012381975 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.034       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.255       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 11.3     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 8        |
|    time_elapsed    | 163      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | 12.3        |
| time/                   |             |
|    fps                  | 101         |
|    iterations           | 9           |
|    time_elapsed         | 181         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.010741711 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.041       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 0.226       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=15.80 +/- 5.02
Episode length: 176.50 +/- 49.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 176         |
|    mean_reward          | 15.8        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012499645 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.519       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.124       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.319       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 10       |
|    time_elapsed    | 207      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 173         |
|    ep_rew_mean          | 14.2        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 11          |
|    time_elapsed         | 225         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.016102921 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0443      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.226       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 179         |
|    ep_rew_mean          | 15.3        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 12          |
|    time_elapsed         | 243         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014981924 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0372      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0299     |
|    value_loss           | 0.249       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=15.20 +/- 5.10
Episode length: 157.50 +/- 46.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 15.2        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.016697027 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.952      |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0219      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.228       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 99       |
|    iterations      | 13       |
|    time_elapsed    | 268      |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 191         |
|    ep_rew_mean          | 17.1        |
| time/                   |             |
|    fps                  | 94          |
|    iterations           | 14          |
|    time_elapsed         | 304         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.016689718 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.939      |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0133      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 0.2         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=15.80 +/- 2.89
Episode length: 158.40 +/- 34.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 158        |
|    mean_reward          | 15.8       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.01569695 |
|    clip_fraction        | 0.179      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.926     |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.000227   |
|    loss                 | 0.028      |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0328    |
|    value_loss           | 0.209      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 15       |
|    time_elapsed    | 349      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 201         |
|    ep_rew_mean          | 18.7        |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 16          |
|    time_elapsed         | 381         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.015904177 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.855      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0344      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0311     |
|    value_loss           | 0.2         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 198         |
|    ep_rew_mean          | 18.6        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 17          |
|    time_elapsed         | 398         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.020069445 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.881      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0206      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0314     |
|    value_loss           | 0.252       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=15.50 +/- 5.64
Episode length: 155.90 +/- 58.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.021215238 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.816      |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0416      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0367     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 87       |
|    iterations      | 18       |
|    time_elapsed    | 423      |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | 19.8        |
| time/                   |             |
|    fps                  | 88          |
|    iterations           | 19          |
|    time_elapsed         | 441         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.019657832 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.771      |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0294      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0302     |
|    value_loss           | 0.22        |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=21.40 +/- 4.48
Episode length: 184.00 +/- 46.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 184         |
|    mean_reward          | 21.4        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.018559994 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0816      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0293     |
|    value_loss           | 0.281       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 20       |
|    time_elapsed    | 467      |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | 21          |
| time/                   |             |
|    fps                  | 88          |
|    iterations           | 21          |
|    time_elapsed         | 485         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.029317634 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.725      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0595      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0367     |
|    value_loss           | 0.268       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=21.50 +/- 3.75
Episode length: 196.00 +/- 36.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 196         |
|    mean_reward          | 21.5        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.018232483 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0414      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.03       |
|    value_loss           | 0.234       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | 21.1     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 22       |
|    time_elapsed    | 511      |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 211         |
|    ep_rew_mean          | 21.1        |
| time/                   |             |
|    fps                  | 89          |
|    iterations           | 23          |
|    time_elapsed         | 528         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.022590369 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.668      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0632      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.242       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 207         |
|    ep_rew_mean          | 21.1        |
| time/                   |             |
|    fps                  | 89          |
|    iterations           | 24          |
|    time_elapsed         | 546         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.022839198 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0417      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0324     |
|    value_loss           | 0.25        |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=24.50 +/- 4.70
Episode length: 234.40 +/- 52.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 234        |
|    mean_reward          | 24.5       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.01815844 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.656     |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.000227   |
|    loss                 | 0.0487     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0278    |
|    value_loss           | 0.229      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 25       |
|    time_elapsed    | 574      |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 203         |
|    ep_rew_mean          | 20.8        |
| time/                   |             |
|    fps                  | 89          |
|    iterations           | 26          |
|    time_elapsed         | 592         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.024987577 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0539      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.229       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=22.40 +/- 5.43
Episode length: 201.60 +/- 51.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 22.4        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.021774754 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.737      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0279      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.225       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 21.2     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 27       |
|    time_elapsed    | 618      |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 21.2       |
| time/                   |            |
|    fps                  | 90         |
|    iterations           | 28         |
|    time_elapsed         | 636        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.03361898 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.626     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.000227   |
|    loss                 | 0.0434     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0311    |
|    value_loss           | 0.242      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | 21.3        |
| time/                   |             |
|    fps                  | 90          |
|    iterations           | 29          |
|    time_elapsed         | 654         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.025191184 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.657      |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0121      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 0.234       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=24.50 +/- 5.94
Episode length: 218.80 +/- 57.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 219         |
|    mean_reward          | 24.5        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.025314936 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.625      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.000227    |
|    loss                 | 0.0408      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.246       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 21.5     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 30       |
|    time_elapsed    | 681      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 18:12:45,981] Trial 13 finished with value: 23.0 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00022734531048390215, 'gamma': 0.9610328685436598, 'gae_lambda': 0.9407891341680574}. Best is trial 12 with value: 27.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 8.92     |
| time/              |          |
|    fps             | 128      |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 141         |
|    ep_rew_mean          | 8.1         |
| time/                   |             |
|    fps                  | 78          |
|    iterations           | 2           |
|    time_elapsed         | 51          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.004007474 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.00789    |
|    learning_rate        | 0.000164    |
|    loss                 | 0.115       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00535    |
|    value_loss           | 0.316       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=10.20 +/- 3.97
Episode length: 132.90 +/- 40.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 133         |
|    mean_reward          | 10.2        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.007653174 |
|    clip_fraction        | 0.0351      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.219       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0958      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00741    |
|    value_loss           | 0.328       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 8.93     |
| time/              |          |
|    fps             | 64       |
|    iterations      | 3        |
|    time_elapsed    | 94       |
|    total_timesteps | 6144     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 146       |
|    ep_rew_mean          | 8.85      |
| time/                   |           |
|    fps                  | 63        |
|    iterations           | 4         |
|    time_elapsed         | 129       |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0093431 |
|    clip_fraction        | 0.0958    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.34     |
|    explained_variance   | 0.326     |
|    learning_rate        | 0.000164  |
|    loss                 | 0.0895    |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.0154   |
|    value_loss           | 0.325     |
---------------------------------------
Eval num_timesteps=10000, episode_reward=12.90 +/- 4.99
Episode length: 156.10 +/- 57.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 12.9        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008017004 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0875      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.297       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 9.55     |
| time/              |          |
|    fps             | 66       |
|    iterations      | 5        |
|    time_elapsed    | 153      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | 10.1        |
| time/                   |             |
|    fps                  | 71          |
|    iterations           | 6           |
|    time_elapsed         | 172         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010997342 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0795      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.303       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | 10.5        |
| time/                   |             |
|    fps                  | 75          |
|    iterations           | 7           |
|    time_elapsed         | 190         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009577372 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.511       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0806      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.324       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=11.60 +/- 2.24
Episode length: 132.80 +/- 25.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 133         |
|    mean_reward          | 11.6        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.008855104 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.538       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.102       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.296       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 11       |
| time/              |          |
|    fps             | 76       |
|    iterations      | 8        |
|    time_elapsed    | 213      |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | 12           |
| time/                   |              |
|    fps                  | 79           |
|    iterations           | 9            |
|    time_elapsed         | 231          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0061881077 |
|    clip_fraction        | 0.0915       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.61         |
|    learning_rate        | 0.000164     |
|    loss                 | 0.0807       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.0165      |
|    value_loss           | 0.305        |
------------------------------------------
Eval num_timesteps=20000, episode_reward=13.00 +/- 5.51
Episode length: 152.10 +/- 57.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 152         |
|    mean_reward          | 13          |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.007402516 |
|    clip_fraction        | 0.0814      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.519       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.123       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.299       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 79       |
|    iterations      | 10       |
|    time_elapsed    | 256      |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | 13.9       |
| time/                   |            |
|    fps                  | 82         |
|    iterations           | 11         |
|    time_elapsed         | 274        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.01441381 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.000164   |
|    loss                 | 0.0669     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0242    |
|    value_loss           | 0.273      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 171         |
|    ep_rew_mean          | 14.7        |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 12          |
|    time_elapsed         | 292         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.008716555 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.978      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0524      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.241       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=11.40 +/- 4.41
Episode length: 140.30 +/- 52.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 140         |
|    mean_reward          | 11.4        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.006915723 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.92       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0729      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.256       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 13       |
|    time_elapsed    | 316      |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 177         |
|    ep_rew_mean          | 16.1        |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 14          |
|    time_elapsed         | 334         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.009660765 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.924      |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.055       |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.255       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=12.80 +/- 3.16
Episode length: 146.90 +/- 33.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 147         |
|    mean_reward          | 12.8        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.010681323 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.893      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0677      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 15       |
|    time_elapsed    | 359      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | 17.1        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 16          |
|    time_elapsed         | 377         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.009761744 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.883      |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.124       |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.3         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 17.7        |
| time/                   |             |
|    fps                  | 88          |
|    iterations           | 17          |
|    time_elapsed         | 395         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.008891601 |
|    clip_fraction        | 0.0991      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.834      |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.109       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.336       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=14.80 +/- 3.74
Episode length: 156.20 +/- 43.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 14.8        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.016043637 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.779      |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0664      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.286       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 18       |
|    time_elapsed    | 420      |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 17.7        |
| time/                   |             |
|    fps                  | 88          |
|    iterations           | 19          |
|    time_elapsed         | 438         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.009800275 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0702      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.278       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=14.10 +/- 4.30
Episode length: 150.60 +/- 47.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 151         |
|    mean_reward          | 14.1        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.010269361 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.824      |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0719      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.242       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 20       |
|    time_elapsed    | 467      |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 17.8        |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 21          |
|    time_elapsed         | 502         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.010317564 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.772      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0369      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.228       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=17.10 +/- 5.09
Episode length: 175.10 +/- 59.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 175          |
|    mean_reward          | 17.1         |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0138806645 |
|    clip_fraction        | 0.129        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.782       |
|    explained_variance   | 0.743        |
|    learning_rate        | 0.000164     |
|    loss                 | 0.0439       |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.0229      |
|    value_loss           | 0.251        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 22       |
|    time_elapsed    | 548      |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 17.9        |
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 23          |
|    time_elapsed         | 578         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.015863882 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.808      |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.055       |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.237       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | 17.9        |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 24          |
|    time_elapsed         | 595         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.011193996 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.853      |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0434      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0258     |
|    value_loss           | 0.25        |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=18.90 +/- 5.28
Episode length: 194.60 +/- 65.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 195         |
|    mean_reward          | 18.9        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.011006538 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.823      |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0629      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0279     |
|    value_loss           | 0.245       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 25       |
|    time_elapsed    | 621      |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 191         |
|    ep_rew_mean          | 18.3        |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 26          |
|    time_elapsed         | 639         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.014983083 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0529      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.269       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=20.10 +/- 3.21
Episode length: 200.20 +/- 33.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 200         |
|    mean_reward          | 20.1        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.015376585 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.787      |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.05        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.253       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 27       |
|    time_elapsed    | 666      |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | 18.1        |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 28          |
|    time_elapsed         | 684         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.013908915 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0762      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.295       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 185       |
|    ep_rew_mean          | 18.1      |
| time/                   |           |
|    fps                  | 84        |
|    iterations           | 29        |
|    time_elapsed         | 703       |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 0.0155686 |
|    clip_fraction        | 0.15      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.742    |
|    explained_variance   | 0.689     |
|    learning_rate        | 0.000164  |
|    loss                 | 0.073     |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0314   |
|    value_loss           | 0.267     |
---------------------------------------
Eval num_timesteps=60000, episode_reward=17.70 +/- 4.12
Episode length: 165.00 +/- 36.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.015398113 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.753      |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.000164    |
|    loss                 | 0.0747      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0294     |
|    value_loss           | 0.26        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 30       |
|    time_elapsed    | 727      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 18:25:02,712] Trial 14 finished with value: 17.2 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00016351113901523777, 'gamma': 0.9655150332422526, 'gae_lambda': 0.9449939490871041}. Best is trial 12 with value: 27.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 8.79     |
| time/              |          |
|    fps             | 131      |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 144         |
|    ep_rew_mean          | 8.61        |
| time/                   |             |
|    fps                  | 122         |
|    iterations           | 2           |
|    time_elapsed         | 33          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.015739463 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | -0.000125   |
|    learning_rate        | 0.00934     |
|    loss                 | 0.188       |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.00292     |
|    value_loss           | 18.7        |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=12.50 +/- 4.36
Episode length: 183.90 +/- 50.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 184         |
|    mean_reward          | 12.5        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.027143784 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.00934     |
|    loss                 | 0.131       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.449       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 9.53     |
| time/              |          |
|    fps             | 103      |
|    iterations      | 3        |
|    time_elapsed    | 59       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 155         |
|    ep_rew_mean          | 10.1        |
| time/                   |             |
|    fps                  | 106         |
|    iterations           | 4           |
|    time_elapsed         | 77          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.031248705 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.436       |
|    learning_rate        | 0.00934     |
|    loss                 | 0.151       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.444       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=9.60 +/- 2.42
Episode length: 124.80 +/- 24.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 125        |
|    mean_reward          | 9.6        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.04217776 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 0.417      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.181      |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0219    |
|    value_loss           | 0.451      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 101      |
|    iterations      | 5        |
|    time_elapsed    | 100      |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 11.8       |
| time/                   |            |
|    fps                  | 103        |
|    iterations           | 6          |
|    time_elapsed         | 119        |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.06079054 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.147      |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0275    |
|    value_loss           | 0.456      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 12.1       |
| time/                   |            |
|    fps                  | 104        |
|    iterations           | 7          |
|    time_elapsed         | 137        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.07332215 |
|    clip_fraction        | 0.394      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.149      |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.423      |
----------------------------------------
Eval num_timesteps=15000, episode_reward=12.00 +/- 3.63
Episode length: 147.50 +/- 51.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 148        |
|    mean_reward          | 12         |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.07522943 |
|    clip_fraction        | 0.408      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.146      |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0192    |
|    value_loss           | 0.456      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 103      |
|    iterations      | 8        |
|    time_elapsed    | 158      |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 13.1       |
| time/                   |            |
|    fps                  | 95         |
|    iterations           | 9          |
|    time_elapsed         | 192        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.09441863 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.572      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.107      |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0227    |
|    value_loss           | 0.363      |
----------------------------------------
Eval num_timesteps=20000, episode_reward=15.50 +/- 3.53
Episode length: 164.60 +/- 37.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 165        |
|    mean_reward          | 15.5       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.08966045 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.367      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.0574     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0206    |
|    value_loss           | 0.399      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 10       |
|    time_elapsed    | 237      |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 181        |
|    ep_rew_mean          | 14.6       |
| time/                   |            |
|    fps                  | 82         |
|    iterations           | 11         |
|    time_elapsed         | 273        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.10637668 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.469      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.0665     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0258    |
|    value_loss           | 0.41       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | 15          |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 12          |
|    time_elapsed         | 292         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.105651736 |
|    clip_fraction        | 0.438       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.00934     |
|    loss                 | 0.178       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.492       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=16.70 +/- 4.41
Episode length: 177.40 +/- 41.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 177        |
|    mean_reward          | 16.7       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.14147168 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.424      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.0547     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.438      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 13       |
|    time_elapsed    | 317      |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 179       |
|    ep_rew_mean          | 15.3      |
| time/                   |           |
|    fps                  | 85        |
|    iterations           | 14        |
|    time_elapsed         | 336       |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.1926681 |
|    clip_fraction        | 0.502     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.03     |
|    explained_variance   | 0.454     |
|    learning_rate        | 0.00934   |
|    loss                 | 0.0706    |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.031    |
|    value_loss           | 0.39      |
---------------------------------------
Eval num_timesteps=30000, episode_reward=19.40 +/- 5.10
Episode length: 205.00 +/- 56.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 205       |
|    mean_reward          | 19.4      |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.1870749 |
|    clip_fraction        | 0.496     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.959    |
|    explained_variance   | 0.436     |
|    learning_rate        | 0.00934   |
|    loss                 | 0.0916    |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0261   |
|    value_loss           | 0.417     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 15       |
|    time_elapsed    | 362      |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 178        |
|    ep_rew_mean          | 15.5       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 16         |
|    time_elapsed         | 380        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.24811284 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.961     |
|    explained_variance   | 0.425      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.087      |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0241    |
|    value_loss           | 0.376      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 15.8       |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 17         |
|    time_elapsed         | 399        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.26064485 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.941     |
|    explained_variance   | 0.469      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.0578     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0238    |
|    value_loss           | 0.318      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=16.30 +/- 3.03
Episode length: 180.60 +/- 33.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 181       |
|    mean_reward          | 16.3      |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.2556607 |
|    clip_fraction        | 0.561     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.901    |
|    explained_variance   | 0.376     |
|    learning_rate        | 0.00934   |
|    loss                 | 0.12      |
|    n_updates            | 170       |
|    policy_gradient_loss | -0.0164   |
|    value_loss           | 0.419     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 18       |
|    time_elapsed    | 424      |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 181        |
|    ep_rew_mean          | 16.2       |
| time/                   |            |
|    fps                  | 88         |
|    iterations           | 19         |
|    time_elapsed         | 441        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.28048602 |
|    clip_fraction        | 0.535      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.874     |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.115      |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0243    |
|    value_loss           | 0.429      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=13.90 +/- 3.18
Episode length: 164.10 +/- 38.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 164        |
|    mean_reward          | 13.9       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.24318074 |
|    clip_fraction        | 0.541      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.882     |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.0478     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0107    |
|    value_loss           | 0.379      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 20       |
|    time_elapsed    | 466      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 187        |
|    ep_rew_mean          | 16.6       |
| time/                   |            |
|    fps                  | 88         |
|    iterations           | 21         |
|    time_elapsed         | 484        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.25932544 |
|    clip_fraction        | 0.539      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.918     |
|    explained_variance   | 0.453      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.109      |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0131    |
|    value_loss           | 0.48       |
----------------------------------------
Eval num_timesteps=45000, episode_reward=14.50 +/- 2.42
Episode length: 160.30 +/- 31.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 160       |
|    mean_reward          | 14.5      |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.2359787 |
|    clip_fraction        | 0.496     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.883    |
|    explained_variance   | 0.523     |
|    learning_rate        | 0.00934   |
|    loss                 | 0.108     |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.022    |
|    value_loss           | 0.394     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 22       |
|    time_elapsed    | 509      |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 89         |
|    iterations           | 23         |
|    time_elapsed         | 527        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.21062627 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.873     |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.0714     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0233    |
|    value_loss           | 0.397      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 191       |
|    ep_rew_mean          | 16.8      |
| time/                   |           |
|    fps                  | 90        |
|    iterations           | 24        |
|    time_elapsed         | 545       |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.2763911 |
|    clip_fraction        | 0.559     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.88     |
|    explained_variance   | 0.489     |
|    learning_rate        | 0.00934   |
|    loss                 | 0.143     |
|    n_updates            | 230       |
|    policy_gradient_loss | 0.0121    |
|    value_loss           | 0.41      |
---------------------------------------
Eval num_timesteps=50000, episode_reward=16.80 +/- 3.97
Episode length: 201.00 +/- 61.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 201        |
|    mean_reward          | 16.8       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.33278775 |
|    clip_fraction        | 0.486      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.791     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.00934    |
|    loss                 | 0.0868     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0327    |
|    value_loss           | 0.359      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 25       |
|    time_elapsed    | 571      |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 16.6       |
| time/                   |            |
|    fps                  | 90         |
|    iterations           | 26         |
|    time_elapsed         | 589        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.31478918 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.858     |
|    explained_variance   | 0.433      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.068      |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0164    |
|    value_loss           | 0.451      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=15.80 +/- 5.27
Episode length: 174.80 +/- 56.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 175        |
|    mean_reward          | 15.8       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.28881428 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.887     |
|    explained_variance   | 0.535      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.0541     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 0.394      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 27       |
|    time_elapsed    | 618      |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 16.2       |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 28         |
|    time_elapsed         | 653        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.32331407 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.846     |
|    explained_variance   | 0.521      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.11       |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0069    |
|    value_loss           | 0.411      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 29         |
|    time_elapsed         | 688        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.34081239 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.861     |
|    explained_variance   | 0.567      |
|    learning_rate        | 0.00934    |
|    loss                 | 0.0717     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 0.314      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=12.40 +/- 3.32
Episode length: 138.50 +/- 29.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 138       |
|    mean_reward          | 12.4      |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.4106558 |
|    clip_fraction        | 0.552     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.886    |
|    explained_variance   | 0.488     |
|    learning_rate        | 0.00934   |
|    loss                 | 0.157     |
|    n_updates            | 290       |
|    policy_gradient_loss | -0.00952  |
|    value_loss           | 0.456     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 84       |
|    iterations      | 30       |
|    time_elapsed    | 725      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 18:37:17,868] Trial 15 finished with value: 15.4 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.009337526125387139, 'gamma': 0.962944538085098, 'gae_lambda': 0.9894131141562781}. Best is trial 12 with value: 27.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 9.15     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 150         |
|    ep_rew_mean          | 9.15        |
| time/                   |             |
|    fps                  | 122         |
|    iterations           | 2           |
|    time_elapsed         | 33          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011207314 |
|    clip_fraction        | 0.0492      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0273     |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.154       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00571    |
|    value_loss           | 0.373       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=10.00 +/- 3.35
Episode length: 138.50 +/- 48.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 138         |
|    mean_reward          | 10          |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.008482045 |
|    clip_fraction        | 0.0159      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.313       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.151       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00436    |
|    value_loss           | 0.402       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 9.29     |
| time/              |          |
|    fps             | 107      |
|    iterations      | 3        |
|    time_elapsed    | 57       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 145         |
|    ep_rew_mean          | 9.2         |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 4           |
|    time_elapsed         | 75          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.007163789 |
|    clip_fraction        | 0.081       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.448       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.221       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00826    |
|    value_loss           | 0.419       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=9.40 +/- 2.69
Episode length: 123.90 +/- 37.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 124         |
|    mean_reward          | 9.4         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009873648 |
|    clip_fraction        | 0.0919      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.547       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.144       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0088     |
|    value_loss           | 0.473       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 9.86     |
| time/              |          |
|    fps             | 103      |
|    iterations      | 5        |
|    time_elapsed    | 98       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 153        |
|    ep_rew_mean          | 9.99       |
| time/                   |            |
|    fps                  | 105        |
|    iterations           | 6          |
|    time_elapsed         | 116        |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.00882944 |
|    clip_fraction        | 0.0356     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.545      |
|    learning_rate        | 9.73e-05   |
|    loss                 | 0.157      |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.00642   |
|    value_loss           | 0.418      |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 155          |
|    ep_rew_mean          | 10.4         |
| time/                   |              |
|    fps                  | 106          |
|    iterations           | 7            |
|    time_elapsed         | 134          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0088289175 |
|    clip_fraction        | 0.0931       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.561        |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.192        |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00908     |
|    value_loss           | 0.462        |
------------------------------------------
Eval num_timesteps=15000, episode_reward=10.10 +/- 2.84
Episode length: 136.60 +/- 40.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 137        |
|    mean_reward          | 10.1       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.00810566 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.668      |
|    learning_rate        | 9.73e-05   |
|    loss                 | 0.147      |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 0.442      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    fps             | 103      |
|    iterations      | 8        |
|    time_elapsed    | 158      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 156         |
|    ep_rew_mean          | 10.8        |
| time/                   |             |
|    fps                  | 104         |
|    iterations           | 9           |
|    time_elapsed         | 176         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.010201598 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.691       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.177       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00955    |
|    value_loss           | 0.442       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=8.30 +/- 2.87
Episode length: 115.30 +/- 39.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 115         |
|    mean_reward          | 8.3         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011568536 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.751       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.154       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.413       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 11.5     |
| time/              |          |
|    fps             | 102      |
|    iterations      | 10       |
|    time_elapsed    | 198      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | 12.1        |
| time/                   |             |
|    fps                  | 103         |
|    iterations           | 11          |
|    time_elapsed         | 216         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.010748526 |
|    clip_fraction        | 0.0958      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.704       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.168       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.481       |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 170          |
|    ep_rew_mean          | 12.6         |
| time/                   |              |
|    fps                  | 104          |
|    iterations           | 12           |
|    time_elapsed         | 234          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0057718907 |
|    clip_fraction        | 0.0462       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1           |
|    explained_variance   | 0.706        |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.2          |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00528     |
|    value_loss           | 0.496        |
------------------------------------------
Eval num_timesteps=25000, episode_reward=8.00 +/- 3.13
Episode length: 114.00 +/- 41.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 114          |
|    mean_reward          | 8            |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0115030315 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1           |
|    explained_variance   | 0.753        |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.152        |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00977     |
|    value_loss           | 0.488        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 103      |
|    iterations      | 13       |
|    time_elapsed    | 257      |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 174          |
|    ep_rew_mean          | 13.6         |
| time/                   |              |
|    fps                  | 103          |
|    iterations           | 14           |
|    time_elapsed         | 276          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0044431947 |
|    clip_fraction        | 0.0415       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.681        |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.188        |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00535     |
|    value_loss           | 0.484        |
------------------------------------------
Eval num_timesteps=30000, episode_reward=7.90 +/- 3.56
Episode length: 108.30 +/- 52.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 7.9         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.008767469 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.967      |
|    explained_variance   | 0.742       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.149       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00839    |
|    value_loss           | 0.5         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 102      |
|    iterations      | 15       |
|    time_elapsed    | 298      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 172         |
|    ep_rew_mean          | 14          |
| time/                   |             |
|    fps                  | 103         |
|    iterations           | 16          |
|    time_elapsed         | 317         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.005314118 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.971      |
|    explained_variance   | 0.762       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.195       |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00875    |
|    value_loss           | 0.456       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 174         |
|    ep_rew_mean          | 14.3        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 17          |
|    time_elapsed         | 353         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.006299526 |
|    clip_fraction        | 0.0667      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.929      |
|    explained_variance   | 0.747       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.185       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00941    |
|    value_loss           | 0.464       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=8.90 +/- 2.62
Episode length: 125.30 +/- 35.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 125          |
|    mean_reward          | 8.9          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0057050604 |
|    clip_fraction        | 0.0717       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.926       |
|    explained_variance   | 0.737        |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.179        |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00736     |
|    value_loss           | 0.429        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 18       |
|    time_elapsed    | 396      |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 176          |
|    ep_rew_mean          | 14.9         |
| time/                   |              |
|    fps                  | 90           |
|    iterations           | 19           |
|    time_elapsed         | 428          |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0044472655 |
|    clip_fraction        | 0.0844       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.997       |
|    explained_variance   | 0.747        |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.128        |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00738     |
|    value_loss           | 0.431        |
------------------------------------------
Eval num_timesteps=40000, episode_reward=10.00 +/- 1.90
Episode length: 139.30 +/- 25.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 139          |
|    mean_reward          | 10           |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0062282584 |
|    clip_fraction        | 0.0613       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.79         |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.212        |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00801     |
|    value_loss           | 0.417        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 20       |
|    time_elapsed    | 452      |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 179          |
|    ep_rew_mean          | 15.5         |
| time/                   |              |
|    fps                  | 91           |
|    iterations           | 21           |
|    time_elapsed         | 470          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0047910972 |
|    clip_fraction        | 0.0621       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.961       |
|    explained_variance   | 0.7          |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.177        |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 0.483        |
------------------------------------------
Eval num_timesteps=45000, episode_reward=12.80 +/- 2.09
Episode length: 149.00 +/- 25.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 149          |
|    mean_reward          | 12.8         |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0034369156 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.962       |
|    explained_variance   | 0.76         |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.16         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00546     |
|    value_loss           | 0.461        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 22       |
|    time_elapsed    | 495      |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 182          |
|    ep_rew_mean          | 16           |
| time/                   |              |
|    fps                  | 91           |
|    iterations           | 23           |
|    time_elapsed         | 513          |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0071007167 |
|    clip_fraction        | 0.0497       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.936       |
|    explained_variance   | 0.723        |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.117        |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00797     |
|    value_loss           | 0.48         |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 187          |
|    ep_rew_mean          | 16.4         |
| time/                   |              |
|    fps                  | 92           |
|    iterations           | 24           |
|    time_elapsed         | 531          |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0044842185 |
|    clip_fraction        | 0.0762       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.889       |
|    explained_variance   | 0.757        |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.25         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.0083      |
|    value_loss           | 0.499        |
------------------------------------------
Eval num_timesteps=50000, episode_reward=15.30 +/- 5.14
Episode length: 188.90 +/- 61.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 189        |
|    mean_reward          | 15.3       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.00651052 |
|    clip_fraction        | 0.0856     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.854     |
|    explained_variance   | 0.785      |
|    learning_rate        | 9.73e-05   |
|    loss                 | 0.18       |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.00832   |
|    value_loss           | 0.451      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 91       |
|    iterations      | 25       |
|    time_elapsed    | 557      |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 188          |
|    ep_rew_mean          | 16.8         |
| time/                   |              |
|    fps                  | 92           |
|    iterations           | 26           |
|    time_elapsed         | 575          |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0068718884 |
|    clip_fraction        | 0.0841       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.826       |
|    explained_variance   | 0.836        |
|    learning_rate        | 9.73e-05     |
|    loss                 | 0.124        |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00904     |
|    value_loss           | 0.382        |
------------------------------------------
Eval num_timesteps=55000, episode_reward=13.10 +/- 2.91
Episode length: 148.60 +/- 31.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 149         |
|    mean_reward          | 13.1        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.004415012 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.91       |
|    explained_variance   | 0.822       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.148       |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00965    |
|    value_loss           | 0.406       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 27       |
|    time_elapsed    | 599      |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 16.9        |
| time/                   |             |
|    fps                  | 92          |
|    iterations           | 28          |
|    time_elapsed         | 617         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.005444576 |
|    clip_fraction        | 0.0577      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.859      |
|    explained_variance   | 0.726       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.22        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 0.536       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 17          |
| time/                   |             |
|    fps                  | 93          |
|    iterations           | 29          |
|    time_elapsed         | 635         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.008675601 |
|    clip_fraction        | 0.0763      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.921      |
|    explained_variance   | 0.806       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.207       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0097     |
|    value_loss           | 0.414       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=11.80 +/- 4.17
Episode length: 139.90 +/- 41.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 140         |
|    mean_reward          | 11.8        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.004495761 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.945      |
|    explained_variance   | 0.822       |
|    learning_rate        | 9.73e-05    |
|    loss                 | 0.154       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00917    |
|    value_loss           | 0.372       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 30       |
|    time_elapsed    | 660      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 18:48:29,320] Trial 16 finished with value: 19.0 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 9.73469367524839e-05, 'gamma': 0.9819462670747148, 'gae_lambda': 0.9387624843922263}. Best is trial 12 with value: 27.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 6.94     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | 6.73        |
| time/                   |             |
|    fps                  | 122         |
|    iterations           | 2           |
|    time_elapsed         | 33          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.007178845 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.025      |
|    learning_rate        | 0.000355    |
|    loss                 | 0.061       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.201       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=10.50 +/- 2.66
Episode length: 158.50 +/- 48.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 10.5        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010596552 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.213       |
|    learning_rate        | 0.000355    |
|    loss                 | 0.0449      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.204       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 105      |
|    iterations      | 3        |
|    time_elapsed    | 58       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 134         |
|    ep_rew_mean          | 8.3         |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 4           |
|    time_elapsed         | 75          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011602633 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.000355    |
|    loss                 | 0.028       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.241       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=17.00 +/- 6.48
Episode length: 188.90 +/- 68.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 189         |
|    mean_reward          | 17          |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.018958023 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000355    |
|    loss                 | 0.0386      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0374     |
|    value_loss           | 0.246       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 9.49     |
| time/              |          |
|    fps             | 90       |
|    iterations      | 5        |
|    time_elapsed    | 113      |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 148        |
|    ep_rew_mean          | 10.1       |
| time/                   |            |
|    fps                  | 82         |
|    iterations           | 6          |
|    time_elapsed         | 148        |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01786432 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.000355   |
|    loss                 | 0.0411     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0355    |
|    value_loss           | 0.239      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 153         |
|    ep_rew_mean          | 10.9        |
| time/                   |             |
|    fps                  | 77          |
|    iterations           | 7           |
|    time_elapsed         | 184         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.020136159 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.416       |
|    learning_rate        | 0.000355    |
|    loss                 | 0.0242      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0435     |
|    value_loss           | 0.283       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=16.80 +/- 4.83
Episode length: 165.60 +/- 45.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 16.8        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.021547377 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.485       |
|    learning_rate        | 0.000355    |
|    loss                 | -0.00466    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0435     |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 11.5     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 8        |
|    time_elapsed    | 214      |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 162         |
|    ep_rew_mean          | 12.6        |
| time/                   |             |
|    fps                  | 79          |
|    iterations           | 9           |
|    time_elapsed         | 233         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.020452395 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.000355    |
|    loss                 | 0.0182      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0412     |
|    value_loss           | 0.25        |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=18.60 +/- 5.14
Episode length: 187.60 +/- 53.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 188        |
|    mean_reward          | 18.6       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.02366666 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.000355   |
|    loss                 | 0.0157     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0391    |
|    value_loss           | 0.231      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 78       |
|    iterations      | 10       |
|    time_elapsed    | 259      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 175         |
|    ep_rew_mean          | 14.8        |
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 11          |
|    time_elapsed         | 277         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.025904555 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.544       |
|    learning_rate        | 0.000355    |
|    loss                 | 0.0159      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0406     |
|    value_loss           | 0.235       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | 15.7        |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 12          |
|    time_elapsed         | 295         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.027379699 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.544       |
|    learning_rate        | 0.000355    |
|    loss                 | 0.0243      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0398     |
|    value_loss           | 0.229       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=19.20 +/- 6.79
Episode length: 198.10 +/- 79.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 198         |
|    mean_reward          | 19.2        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.026383277 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.000355    |
|    loss                 | 0.0203      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0428     |
|    value_loss           | 0.225       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 13       |
|    time_elapsed    | 322      |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | 16.8       |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 14         |
|    time_elapsed         | 340        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.03199265 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.995     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.000355   |
|    loss                 | -0.00633   |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0494    |
|    value_loss           | 0.192      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=19.80 +/- 4.02
Episode length: 182.80 +/- 32.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 183        |
|    mean_reward          | 19.8       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.03171641 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.978     |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.000355   |
|    loss                 | -0.0164    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0474    |
|    value_loss           | 0.198      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 15       |
|    time_elapsed    | 365      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 17.8        |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 16          |
|    time_elapsed         | 383         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.033137828 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.917      |
|    explained_variance   | 0.645       |
|    learning_rate        | 0.000355    |
|    loss                 | -0.00894    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0467     |
|    value_loss           | 0.201       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | 18.2        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 17          |
|    time_elapsed         | 401         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.029858232 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.911      |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.000355    |
|    loss                 | 0.0094      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0443     |
|    value_loss           | 0.205       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=22.20 +/- 5.38
Episode length: 207.60 +/- 58.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 208        |
|    mean_reward          | 22.2       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.03391369 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.875     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.000355   |
|    loss                 | -0.00364   |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0437    |
|    value_loss           | 0.182      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 18       |
|    time_elapsed    | 428      |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 191         |
|    ep_rew_mean          | 18.9        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 19          |
|    time_elapsed         | 446         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.044939414 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.000355    |
|    loss                 | -0.0222     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0473     |
|    value_loss           | 0.226       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=19.80 +/- 4.45
Episode length: 180.70 +/- 40.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 181        |
|    mean_reward          | 19.8       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.03942558 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.827     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.000355   |
|    loss                 | -0.00147   |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0449    |
|    value_loss           | 0.219      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 20       |
|    time_elapsed    | 472      |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 193         |
|    ep_rew_mean          | 19.3        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 21          |
|    time_elapsed         | 490         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.042815864 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.804      |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.000355    |
|    loss                 | 0.00445     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0425     |
|    value_loss           | 0.222       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=23.80 +/- 7.98
Episode length: 219.20 +/- 76.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 219         |
|    mean_reward          | 23.8        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.040318854 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.644       |
|    learning_rate        | 0.000355    |
|    loss                 | -0.00549    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0443     |
|    value_loss           | 0.218       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 19.7     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 22       |
|    time_elapsed    | 517      |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | 20.1       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 23         |
|    time_elapsed         | 545        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.05685791 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.792     |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.000355   |
|    loss                 | -0.0107    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.201      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | 20.4        |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 24          |
|    time_elapsed         | 580         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.050502818 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.000355    |
|    loss                 | -0.0358     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0442     |
|    value_loss           | 0.213       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=25.40 +/- 7.90
Episode length: 241.40 +/- 78.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 25.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.043618113 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.000355    |
|    loss                 | 0.027       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0375     |
|    value_loss           | 0.213       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 20.7     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 25       |
|    time_elapsed    | 630      |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 204         |
|    ep_rew_mean          | 20.7        |
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 26          |
|    time_elapsed         | 652         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.051295273 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.751      |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.000355    |
|    loss                 | -0.00214    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0447     |
|    value_loss           | 0.221       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=22.00 +/- 2.65
Episode length: 197.00 +/- 33.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 197        |
|    mean_reward          | 22         |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.06382271 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.000355   |
|    loss                 | -0.00545   |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.219      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 20.7     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 27       |
|    time_elapsed    | 678      |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 207         |
|    ep_rew_mean          | 21.3        |
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 28          |
|    time_elapsed         | 697         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.053303428 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.673      |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.000355    |
|    loss                 | -0.015      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0404     |
|    value_loss           | 0.204       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 211        |
|    ep_rew_mean          | 21.8       |
| time/                   |            |
|    fps                  | 83         |
|    iterations           | 29         |
|    time_elapsed         | 714        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.05339001 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.612     |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.000355   |
|    loss                 | 0.0128     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0383    |
|    value_loss           | 0.219      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=22.30 +/- 4.45
Episode length: 206.00 +/- 45.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 206        |
|    mean_reward          | 22.3       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.05140123 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.608     |
|    explained_variance   | 0.705      |
|    learning_rate        | 0.000355   |
|    loss                 | 0.00759    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0416    |
|    value_loss           | 0.222      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 22.2     |
| time/              |          |
|    fps             | 82       |
|    iterations      | 30       |
|    time_elapsed    | 741      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:01:04,056] Trial 17 finished with value: 28.1 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0003546094553104662, 'gamma': 0.9601406990014538, 'gae_lambda': 0.9302489069990426}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 7.53     |
| time/              |          |
|    fps             | 131      |
|    iterations      | 1        |
|    time_elapsed    | 31       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=14.30 +/- 5.00
Episode length: 172.90 +/- 58.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 173         |
|    mean_reward          | 14.3        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.006821491 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0142     |
|    learning_rate        | 0.000443    |
|    loss                 | 0.181       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00734    |
|    value_loss           | 0.545       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 8.71     |
| time/              |          |
|    fps             | 111      |
|    iterations      | 2        |
|    time_elapsed    | 73       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=13.40 +/- 2.87
Episode length: 152.10 +/- 28.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 152         |
|    mean_reward          | 13.4        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012381794 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.000443    |
|    loss                 | 0.335       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.801       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 9.12     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 3        |
|    time_elapsed    | 115      |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=13.10 +/- 4.55
Episode length: 156.50 +/- 49.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 13.1        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012552966 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.57        |
|    learning_rate        | 0.000443    |
|    loss                 | 0.219       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.847       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 10       |
| time/              |          |
|    fps             | 103      |
|    iterations      | 4        |
|    time_elapsed    | 158      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=15.00 +/- 4.54
Episode length: 168.60 +/- 52.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 15          |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.014138218 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.000443    |
|    loss                 | 0.213       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 1.04        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 11.4     |
| time/              |          |
|    fps             | 101      |
|    iterations      | 5        |
|    time_elapsed    | 201      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | 12.3        |
| time/                   |             |
|    fps                  | 95          |
|    iterations           | 6           |
|    time_elapsed         | 257         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.013795528 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.000443    |
|    loss                 | 0.338       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 1.05        |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=18.30 +/- 4.27
Episode length: 204.20 +/- 44.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 204         |
|    mean_reward          | 18.3        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.014382165 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.000443    |
|    loss                 | 0.238       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 1.09        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 7        |
|    time_elapsed    | 306      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=20.40 +/- 6.42
Episode length: 225.40 +/- 75.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 20.4        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.014038949 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.000443    |
|    loss                 | 0.31        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 1.19        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 14.1     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 8        |
|    time_elapsed    | 312      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=17.90 +/- 5.13
Episode length: 201.10 +/- 62.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 201         |
|    mean_reward          | 17.9        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.015359028 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.000443    |
|    loss                 | 0.46        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 1.29        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 9        |
|    time_elapsed    | 317      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=17.70 +/- 4.36
Episode length: 196.10 +/- 43.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 196         |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.014292928 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.000443    |
|    loss                 | 0.342       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 1.1         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 126      |
|    iterations      | 10       |
|    time_elapsed    | 323      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=17.90 +/- 5.70
Episode length: 193.20 +/- 67.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 193         |
|    mean_reward          | 17.9        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.012775455 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.000443    |
|    loss                 | 0.413       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 1.27        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 137      |
|    iterations      | 11       |
|    time_elapsed    | 328      |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 197         |
|    ep_rew_mean          | 16.3        |
| time/                   |             |
|    fps                  | 147         |
|    iterations           | 12          |
|    time_elapsed         | 332         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.015128089 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.000443    |
|    loss                 | 0.241       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 1.09        |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=19.00 +/- 4.05
Episode length: 209.00 +/- 47.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 19          |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015130579 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.86        |
|    learning_rate        | 0.000443    |
|    loss                 | 0.411       |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 1.41        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 13       |
|    time_elapsed    | 338      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=14.20 +/- 4.85
Episode length: 157.80 +/- 53.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 14.2        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.014228023 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.000443    |
|    loss                 | 0.483       |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 1.62        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 14       |
|    time_elapsed    | 343      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=14.00 +/- 6.08
Episode length: 161.60 +/- 72.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 162         |
|    mean_reward          | 14          |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.015536915 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.000443    |
|    loss                 | 0.411       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 1.32        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 176      |
|    iterations      | 15       |
|    time_elapsed    | 348      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:06:54,718] Trial 18 finished with value: 17.3 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.0004426449817527962, 'gamma': 0.9969094286685266, 'gae_lambda': 0.9662807110316984}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 5.94     |
| time/              |          |
|    fps             | 1232     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 130         |
|    ep_rew_mean          | 7.26        |
| time/                   |             |
|    fps                  | 1115        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010845572 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0509     |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.141       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00504    |
|    value_loss           | 0.305       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=11.00 +/- 4.24
Episode length: 150.50 +/- 59.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 150        |
|    mean_reward          | 11         |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01246731 |
|    clip_fraction        | 0.0835     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.239      |
|    learning_rate        | 8.5e-05    |
|    loss                 | 0.107      |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 0.347      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 918      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 130         |
|    ep_rew_mean          | 7.5         |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.017626725 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.48        |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.134       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.316       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=9.20 +/- 2.60
Episode length: 125.90 +/- 34.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 9.2         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.007472451 |
|    clip_fraction        | 0.0996      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.477       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.175       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00884    |
|    value_loss           | 0.394       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 885      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 135         |
|    ep_rew_mean          | 8.3         |
| time/                   |             |
|    fps                  | 905         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.009806008 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.501       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.148       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.409       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 140         |
|    ep_rew_mean          | 9.02        |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 7           |
|    time_elapsed         | 15          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.012287863 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.538       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.177       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00891    |
|    value_loss           | 0.431       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=9.50 +/- 2.69
Episode length: 127.30 +/- 40.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 127         |
|    mean_reward          | 9.5         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.002635682 |
|    clip_fraction        | 0.00615     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.642       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.163       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0035     |
|    value_loss           | 0.384       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 9.63     |
| time/              |          |
|    fps             | 887      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 145          |
|    ep_rew_mean          | 10           |
| time/                   |              |
|    fps                  | 899          |
|    iterations           | 9            |
|    time_elapsed         | 20           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0074658543 |
|    clip_fraction        | 0.0508       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.687        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.173        |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00711     |
|    value_loss           | 0.421        |
------------------------------------------
Eval num_timesteps=20000, episode_reward=9.40 +/- 2.11
Episode length: 125.60 +/- 27.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 126          |
|    mean_reward          | 9.4          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0030568426 |
|    clip_fraction        | 0.0583       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.994       |
|    explained_variance   | 0.627        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.188        |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00589     |
|    value_loss           | 0.426        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 877      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 157          |
|    ep_rew_mean          | 11.8         |
| time/                   |              |
|    fps                  | 888          |
|    iterations           | 11           |
|    time_elapsed         | 25           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0038461494 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.928       |
|    explained_variance   | 0.693        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.189        |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00405     |
|    value_loss           | 0.398        |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | 12.5         |
| time/                   |              |
|    fps                  | 897          |
|    iterations           | 12           |
|    time_elapsed         | 27           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0058707446 |
|    clip_fraction        | 0.0934       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.821       |
|    explained_variance   | 0.593        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.225        |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.0106      |
|    value_loss           | 0.519        |
------------------------------------------
Eval num_timesteps=25000, episode_reward=8.90 +/- 2.12
Episode length: 119.90 +/- 28.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 120          |
|    mean_reward          | 8.9          |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0050794324 |
|    clip_fraction        | 0.067        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.785       |
|    explained_variance   | 0.709        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.217        |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00674     |
|    value_loss           | 0.413        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 881      |
|    iterations      | 13       |
|    time_elapsed    | 30       |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 171          |
|    ep_rew_mean          | 13.9         |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 14           |
|    time_elapsed         | 32           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0054618716 |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.777       |
|    explained_variance   | 0.702        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.153        |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00791     |
|    value_loss           | 0.353        |
------------------------------------------
Eval num_timesteps=30000, episode_reward=9.70 +/- 2.53
Episode length: 133.40 +/- 39.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 133          |
|    mean_reward          | 9.7          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0026732069 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.752       |
|    explained_variance   | 0.739        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.162        |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00382     |
|    value_loss           | 0.374        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 874      |
|    iterations      | 15       |
|    time_elapsed    | 35       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 179          |
|    ep_rew_mean          | 15           |
| time/                   |              |
|    fps                  | 882          |
|    iterations           | 16           |
|    time_elapsed         | 37           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0047247624 |
|    clip_fraction        | 0.0886       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.76        |
|    explained_variance   | 0.706        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.132        |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00842     |
|    value_loss           | 0.354        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 183         |
|    ep_rew_mean          | 15.5        |
| time/                   |             |
|    fps                  | 888         |
|    iterations           | 17          |
|    time_elapsed         | 39          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.004041834 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.687      |
|    explained_variance   | 0.734       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.188       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 0.351       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=10.00 +/- 2.86
Episode length: 136.50 +/- 34.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 136          |
|    mean_reward          | 10           |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0048573506 |
|    clip_fraction        | 0.029        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.693       |
|    explained_variance   | 0.708        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.156        |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00432     |
|    value_loss           | 0.404        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 875      |
|    iterations      | 18       |
|    time_elapsed    | 42       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | 15.8         |
| time/                   |              |
|    fps                  | 881          |
|    iterations           | 19           |
|    time_elapsed         | 44           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0035559866 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.713       |
|    explained_variance   | 0.763        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.159        |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00497     |
|    value_loss           | 0.441        |
------------------------------------------
Eval num_timesteps=40000, episode_reward=8.80 +/- 1.72
Episode length: 121.10 +/- 18.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 121          |
|    mean_reward          | 8.8          |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0020752032 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.727       |
|    explained_variance   | 0.754        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.131        |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00535     |
|    value_loss           | 0.312        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 872      |
|    iterations      | 20       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | 15.9        |
| time/                   |             |
|    fps                  | 877         |
|    iterations           | 21          |
|    time_elapsed         | 48          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.004279854 |
|    clip_fraction        | 0.041       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.729      |
|    explained_variance   | 0.795       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.106       |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 0.325       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=11.20 +/- 4.33
Episode length: 138.80 +/- 50.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 139         |
|    mean_reward          | 11.2        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.002978241 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.69       |
|    explained_variance   | 0.755       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.152       |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 0.407       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 867      |
|    iterations      | 22       |
|    time_elapsed    | 51       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 16.2       |
| time/                   |            |
|    fps                  | 873        |
|    iterations           | 23         |
|    time_elapsed         | 53         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.00462348 |
|    clip_fraction        | 0.0515     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.729      |
|    learning_rate        | 8.5e-05    |
|    loss                 | 0.171      |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.00519   |
|    value_loss           | 0.318      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | 15.8        |
| time/                   |             |
|    fps                  | 878         |
|    iterations           | 24          |
|    time_elapsed         | 55          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.002452491 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.701      |
|    explained_variance   | 0.809       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.119       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 0.332       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=10.40 +/- 5.12
Episode length: 130.80 +/- 51.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 10.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.004689013 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.674      |
|    explained_variance   | 0.739       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.164       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00893    |
|    value_loss           | 0.372       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 869      |
|    iterations      | 25       |
|    time_elapsed    | 58       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 181          |
|    ep_rew_mean          | 16.2         |
| time/                   |              |
|    fps                  | 874          |
|    iterations           | 26           |
|    time_elapsed         | 60           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0037950622 |
|    clip_fraction        | 0.0527       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.648       |
|    explained_variance   | 0.749        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.139        |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00825     |
|    value_loss           | 0.392        |
------------------------------------------
Eval num_timesteps=55000, episode_reward=9.80 +/- 1.83
Episode length: 126.90 +/- 23.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 127          |
|    mean_reward          | 9.8          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0036330433 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.658       |
|    explained_variance   | 0.722        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.161        |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.0064      |
|    value_loss           | 0.392        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 867      |
|    iterations      | 27       |
|    time_elapsed    | 63       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 181          |
|    ep_rew_mean          | 16.4         |
| time/                   |              |
|    fps                  | 871          |
|    iterations           | 28           |
|    time_elapsed         | 65           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0037513543 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.684       |
|    explained_variance   | 0.726        |
|    learning_rate        | 8.5e-05      |
|    loss                 | 0.129        |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00699     |
|    value_loss           | 0.389        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | 16.4        |
| time/                   |             |
|    fps                  | 875         |
|    iterations           | 29          |
|    time_elapsed         | 67          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.005606475 |
|    clip_fraction        | 0.052       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.824       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.103       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.289       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=9.40 +/- 3.01
Episode length: 117.60 +/- 34.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 9.4         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.004697651 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.632      |
|    explained_variance   | 0.794       |
|    learning_rate        | 8.5e-05     |
|    loss                 | 0.145       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00692    |
|    value_loss           | 0.344       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 870      |
|    iterations      | 30       |
|    time_elapsed    | 70       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:08:06,961] Trial 19 finished with value: 11.9 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 8.49543715246502e-05, 'gamma': 0.9798987182468459, 'gae_lambda': 0.9228724398088708}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 7.83     |
| time/              |          |
|    fps             | 1230     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=14.70 +/- 5.60
Episode length: 173.70 +/- 61.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 174         |
|    mean_reward          | 14.7        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.019280007 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0781     |
|    learning_rate        | 0.00105     |
|    loss                 | -0.00746    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.161       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 8.59     |
| time/              |          |
|    fps             | 902      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=15.50 +/- 3.20
Episode length: 158.10 +/- 34.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.038029306 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.315       |
|    learning_rate        | 0.00105     |
|    loss                 | -0.0453     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0444     |
|    value_loss           | 0.153       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 9.46     |
| time/              |          |
|    fps             | 835      |
|    iterations      | 3        |
|    time_elapsed    | 14       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=19.50 +/- 3.75
Episode length: 200.60 +/- 41.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 201        |
|    mean_reward          | 19.5       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.06333117 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 0.475      |
|    learning_rate        | 0.00105    |
|    loss                 | -0.0432    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0602    |
|    value_loss           | 0.14       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    fps             | 794      |
|    iterations      | 4        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=18.10 +/- 2.84
Episode length: 177.90 +/- 24.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 178        |
|    mean_reward          | 18.1       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.07418914 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.484      |
|    learning_rate        | 0.00105    |
|    loss                 | -0.0784    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0587    |
|    value_loss           | 0.148      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 5        |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 183         |
|    ep_rew_mean          | 14.9        |
| time/                   |             |
|    fps                  | 793         |
|    iterations           | 6           |
|    time_elapsed         | 30          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.092475496 |
|    clip_fraction        | 0.487       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.484       |
|    learning_rate        | 0.00105     |
|    loss                 | -0.0609     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0602     |
|    value_loss           | 0.146       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=18.70 +/- 3.72
Episode length: 183.80 +/- 37.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 184       |
|    mean_reward          | 18.7      |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.1035174 |
|    clip_fraction        | 0.528     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.13     |
|    explained_variance   | 0.533     |
|    learning_rate        | 0.00105   |
|    loss                 | -0.0815   |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0645   |
|    value_loss           | 0.131     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 7        |
|    time_elapsed    | 36       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=18.40 +/- 6.48
Episode length: 185.20 +/- 66.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 185        |
|    mean_reward          | 18.4       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.11842871 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.00105    |
|    loss                 | -0.0453    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0642    |
|    value_loss           | 0.131      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 769      |
|    iterations      | 8        |
|    time_elapsed    | 42       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=21.20 +/- 6.68
Episode length: 198.60 +/- 59.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 199        |
|    mean_reward          | 21.2       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.14647463 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.00105    |
|    loss                 | -0.0679    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0653    |
|    value_loss           | 0.123      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 759      |
|    iterations      | 9        |
|    time_elapsed    | 48       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=24.90 +/- 5.80
Episode length: 227.70 +/- 68.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 24.9       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.14580882 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.987     |
|    explained_variance   | 0.641      |
|    learning_rate        | 0.00105    |
|    loss                 | -0.0716    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0659    |
|    value_loss           | 0.125      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 750      |
|    iterations      | 10       |
|    time_elapsed    | 54       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=22.20 +/- 6.43
Episode length: 208.70 +/- 63.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 209       |
|    mean_reward          | 22.2      |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.1668368 |
|    clip_fraction        | 0.546     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.918    |
|    explained_variance   | 0.661     |
|    learning_rate        | 0.00105   |
|    loss                 | -0.0806   |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0642   |
|    value_loss           | 0.13      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 743      |
|    iterations      | 11       |
|    time_elapsed    | 60       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 197        |
|    ep_rew_mean          | 19.2       |
| time/                   |            |
|    fps                  | 754        |
|    iterations           | 12         |
|    time_elapsed         | 65         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.20005456 |
|    clip_fraction        | 0.54       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.842     |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.00105    |
|    loss                 | -0.0431    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0604    |
|    value_loss           | 0.124      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=25.30 +/- 6.86
Episode length: 236.90 +/- 72.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 237       |
|    mean_reward          | 25.3      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.2191746 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.847    |
|    explained_variance   | 0.694     |
|    learning_rate        | 0.00105   |
|    loss                 | -0.0572   |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.0631   |
|    value_loss           | 0.122     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    fps             | 746      |
|    iterations      | 13       |
|    time_elapsed    | 71       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=24.30 +/- 4.50
Episode length: 241.20 +/- 40.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 241        |
|    mean_reward          | 24.3       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.22057323 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.814     |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.00105    |
|    loss                 | -0.0417    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0614    |
|    value_loss           | 0.121      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 21       |
| time/              |          |
|    fps             | 739      |
|    iterations      | 14       |
|    time_elapsed    | 77       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=22.00 +/- 5.60
Episode length: 209.00 +/- 46.32
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 209      |
|    mean_reward          | 22       |
| time/                   |          |
|    total_timesteps      | 60000    |
| train/                  |          |
|    approx_kl            | 0.225916 |
|    clip_fraction        | 0.562    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.83    |
|    explained_variance   | 0.648    |
|    learning_rate        | 0.00105  |
|    loss                 | -0.0976  |
|    n_updates            | 140      |
|    policy_gradient_loss | -0.0599  |
|    value_loss           | 0.126    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 21.4     |
| time/              |          |
|    fps             | 736      |
|    iterations      | 15       |
|    time_elapsed    | 83       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:09:33,204] Trial 20 finished with value: 20.8 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.0010510301130129951, 'gamma': 0.9525026676157196, 'gae_lambda': 0.8830237646585937}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 8.07     |
| time/              |          |
|    fps             | 1229     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 132         |
|    ep_rew_mean          | 7.6         |
| time/                   |             |
|    fps                  | 1116        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009376352 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.0247      |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0707      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.25        |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=9.00 +/- 3.07
Episode length: 138.10 +/- 42.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 138         |
|    mean_reward          | 9           |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010693023 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0666      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.23        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 8.3      |
| time/              |          |
|    fps             | 928      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 143         |
|    ep_rew_mean          | 9           |
| time/                   |             |
|    fps                  | 948         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012047845 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0413      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 0.201       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=8.30 +/- 2.87
Episode length: 140.00 +/- 36.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 140         |
|    mean_reward          | 8.3         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.013501942 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0536      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.219       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 9.83     |
| time/              |          |
|    fps             | 882      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 153        |
|    ep_rew_mean          | 10.5       |
| time/                   |            |
|    fps                  | 902        |
|    iterations           | 6          |
|    time_elapsed         | 13         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01254284 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | 0.494      |
|    learning_rate        | 0.000237   |
|    loss                 | 0.0326     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0276    |
|    value_loss           | 0.234      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 156         |
|    ep_rew_mean          | 11          |
| time/                   |             |
|    fps                  | 917         |
|    iterations           | 7           |
|    time_elapsed         | 15          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009983916 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.511       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0127      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.209       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=13.40 +/- 4.27
Episode length: 141.80 +/- 44.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 142          |
|    mean_reward          | 13.4         |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0123842945 |
|    clip_fraction        | 0.127        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.558        |
|    learning_rate        | 0.000237     |
|    loss                 | 0.0364       |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.031       |
|    value_loss           | 0.219        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 11.7     |
| time/              |          |
|    fps             | 880      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 167         |
|    ep_rew_mean          | 12.7        |
| time/                   |             |
|    fps                  | 894         |
|    iterations           | 9           |
|    time_elapsed         | 20          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.015030492 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0171      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0302     |
|    value_loss           | 0.236       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=16.70 +/- 2.83
Episode length: 164.70 +/- 24.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 165        |
|    mean_reward          | 16.7       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.01518579 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.000237   |
|    loss                 | 0.042      |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0316    |
|    value_loss           | 0.197      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 176         |
|    ep_rew_mean          | 14.8        |
| time/                   |             |
|    fps                  | 873         |
|    iterations           | 11          |
|    time_elapsed         | 25          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.019030254 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0392      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.252       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | 16          |
| time/                   |             |
|    fps                  | 884         |
|    iterations           | 12          |
|    time_elapsed         | 27          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.019341763 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0163      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.037      |
|    value_loss           | 0.234       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=21.50 +/- 5.10
Episode length: 214.60 +/- 60.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 215         |
|    mean_reward          | 21.5        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.016763017 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0354      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.033      |
|    value_loss           | 0.215       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 851      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | 17.2        |
| time/                   |             |
|    fps                  | 861         |
|    iterations           | 14          |
|    time_elapsed         | 33          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.018431265 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.969      |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0288      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0365     |
|    value_loss           | 0.253       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=17.40 +/- 4.25
Episode length: 173.00 +/- 51.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 173         |
|    mean_reward          | 17.4        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.024512615 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.993      |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0258      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.035      |
|    value_loss           | 0.187       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 842      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 18.5        |
| time/                   |             |
|    fps                  | 851         |
|    iterations           | 16          |
|    time_elapsed         | 38          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.023034407 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.985      |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0109      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0399     |
|    value_loss           | 0.201       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 197         |
|    ep_rew_mean          | 19          |
| time/                   |             |
|    fps                  | 860         |
|    iterations           | 17          |
|    time_elapsed         | 40          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.026368823 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.95       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0254      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0401     |
|    value_loss           | 0.211       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=23.20 +/- 6.19
Episode length: 236.70 +/- 60.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 237         |
|    mean_reward          | 23.2        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.025240071 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.883      |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.000237    |
|    loss                 | -0.0129     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0368     |
|    value_loss           | 0.182       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 836      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 196         |
|    ep_rew_mean          | 19.5        |
| time/                   |             |
|    fps                  | 844         |
|    iterations           | 19          |
|    time_elapsed         | 46          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.027062852 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.864      |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0162      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0397     |
|    value_loss           | 0.208       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=19.00 +/- 5.33
Episode length: 174.00 +/- 56.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 174        |
|    mean_reward          | 19         |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.02228762 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.851     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.000237   |
|    loss                 | 0.0207     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0351    |
|    value_loss           | 0.203      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 831      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 196        |
|    ep_rew_mean          | 19.7       |
| time/                   |            |
|    fps                  | 838        |
|    iterations           | 21         |
|    time_elapsed         | 51         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.02654631 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.777     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.000237   |
|    loss                 | 0.0261     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.217      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=22.10 +/- 7.11
Episode length: 206.10 +/- 73.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | 22.1        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.022702908 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0143      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0324     |
|    value_loss           | 0.201       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 19.8     |
| time/              |          |
|    fps             | 823      |
|    iterations      | 22       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 197         |
|    ep_rew_mean          | 20.1        |
| time/                   |             |
|    fps                  | 830         |
|    iterations           | 23          |
|    time_elapsed         | 56          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.028519757 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.00749     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0386     |
|    value_loss           | 0.217       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 199         |
|    ep_rew_mean          | 20.5        |
| time/                   |             |
|    fps                  | 837         |
|    iterations           | 24          |
|    time_elapsed         | 58          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.031018171 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0375      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 0.227       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=22.20 +/- 6.14
Episode length: 200.50 +/- 65.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 200         |
|    mean_reward          | 22.2        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.035830215 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.00558     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0361     |
|    value_loss           | 0.236       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 824      |
|    iterations      | 25       |
|    time_elapsed    | 62       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | 20.8        |
| time/                   |             |
|    fps                  | 830         |
|    iterations           | 26          |
|    time_elapsed         | 64          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.031725053 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.687      |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.000237    |
|    loss                 | -0.00287    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0352     |
|    value_loss           | 0.189       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=22.50 +/- 5.68
Episode length: 208.00 +/- 60.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 208         |
|    mean_reward          | 22.5        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.029063208 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0321      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0368     |
|    value_loss           | 0.214       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 818      |
|    iterations      | 27       |
|    time_elapsed    | 67       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 204         |
|    ep_rew_mean          | 21.4        |
| time/                   |             |
|    fps                  | 824         |
|    iterations           | 28          |
|    time_elapsed         | 69          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.033799477 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.0311      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0374     |
|    value_loss           | 0.203       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 208        |
|    ep_rew_mean          | 21.7       |
| time/                   |            |
|    fps                  | 830        |
|    iterations           | 29         |
|    time_elapsed         | 71         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.03174723 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.667     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.000237   |
|    loss                 | 0.0379     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0351    |
|    value_loss           | 0.214      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=23.10 +/- 4.09
Episode length: 201.40 +/- 34.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 201         |
|    mean_reward          | 23.1        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.025757384 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.000237    |
|    loss                 | 0.00803     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0355     |
|    value_loss           | 0.192       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | 21.9     |
| time/              |          |
|    fps             | 820      |
|    iterations      | 30       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:10:50,123] Trial 21 finished with value: 23.6 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0002367839361856935, 'gamma': 0.9608957992893132, 'gae_lambda': 0.926393370098383}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 8.43     |
| time/              |          |
|    fps             | 1227     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 130          |
|    ep_rew_mean          | 7.87         |
| time/                   |              |
|    fps                  | 1112         |
|    iterations           | 2            |
|    time_elapsed         | 3            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0068609454 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | 0.000968     |
|    learning_rate        | 0.00031      |
|    loss                 | 0.114        |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00878     |
|    value_loss           | 0.378        |
------------------------------------------
Eval num_timesteps=5000, episode_reward=11.40 +/- 5.08
Episode length: 142.90 +/- 52.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | 11.4        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010324967 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.119       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.404       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 8.55     |
| time/              |          |
|    fps             | 922      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | 9.57        |
| time/                   |             |
|    fps                  | 944         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008792214 |
|    clip_fraction        | 0.0976      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.465       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0763      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.338       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=14.60 +/- 3.83
Episode length: 170.50 +/- 48.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 170         |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009004256 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.317       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.104       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.391       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    fps             | 864      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | 11.5        |
| time/                   |             |
|    fps                  | 886         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011828679 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.443       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0558      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.347       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | 11.8        |
| time/                   |             |
|    fps                  | 903         |
|    iterations           | 7           |
|    time_elapsed         | 15          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.012705293 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0724      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.325       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=14.00 +/- 5.53
Episode length: 164.70 +/- 69.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | 14          |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012513402 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0915      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.312       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 12.2     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 169         |
|    ep_rew_mean          | 13.1        |
| time/                   |             |
|    fps                  | 876         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.014857065 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0532      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.032      |
|    value_loss           | 0.345       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=12.10 +/- 3.70
Episode length: 133.60 +/- 40.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 12.1        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.015230166 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0498      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0312     |
|    value_loss           | 0.343       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 854      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 179         |
|    ep_rew_mean          | 14.9        |
| time/                   |             |
|    fps                  | 867         |
|    iterations           | 11          |
|    time_elapsed         | 25          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.015373012 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0326      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0304     |
|    value_loss           | 0.311       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | 15.6        |
| time/                   |             |
|    fps                  | 877         |
|    iterations           | 12          |
|    time_elapsed         | 27          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.020400262 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0288      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.284       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=15.60 +/- 4.54
Episode length: 163.60 +/- 50.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 164         |
|    mean_reward          | 15.6        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.018319655 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0279      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0377     |
|    value_loss           | 0.32        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 854      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 174         |
|    ep_rew_mean          | 15.5        |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 14          |
|    time_elapsed         | 33          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.023617607 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0347      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0412     |
|    value_loss           | 0.313       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=17.50 +/- 2.42
Episode length: 182.60 +/- 28.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 183        |
|    mean_reward          | 17.5       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.02019139 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.571      |
|    learning_rate        | 0.00031    |
|    loss                 | 0.068      |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0397    |
|    value_loss           | 0.345      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 843      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 16.3       |
| time/                   |            |
|    fps                  | 852        |
|    iterations           | 16         |
|    time_elapsed         | 38         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.01814311 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.00031    |
|    loss                 | 0.039      |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0369    |
|    value_loss           | 0.314      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | 16.3        |
| time/                   |             |
|    fps                  | 859         |
|    iterations           | 17          |
|    time_elapsed         | 40          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.023276951 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0379      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0409     |
|    value_loss           | 0.257       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=16.00 +/- 3.97
Episode length: 160.20 +/- 38.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 160         |
|    mean_reward          | 16          |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.026375093 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0243      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0419     |
|    value_loss           | 0.344       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 845      |
|    iterations      | 18       |
|    time_elapsed    | 43       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | 17          |
| time/                   |             |
|    fps                  | 853         |
|    iterations           | 19          |
|    time_elapsed         | 45          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.030551162 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.991      |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.00031     |
|    loss                 | 0.00322     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0426     |
|    value_loss           | 0.242       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=14.60 +/- 5.70
Episode length: 157.30 +/- 58.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 157         |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.027586747 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.933      |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0452      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0398     |
|    value_loss           | 0.292       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 841      |
|    iterations      | 20       |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 17.6       |
| time/                   |            |
|    fps                  | 848        |
|    iterations           | 21         |
|    time_elapsed         | 50         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.02482476 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.942     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.00031    |
|    loss                 | 0.00474    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0394    |
|    value_loss           | 0.277      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=17.80 +/- 2.09
Episode length: 188.10 +/- 23.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 188         |
|    mean_reward          | 17.8        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.031377696 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.951      |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.00475     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0489     |
|    value_loss           | 0.3         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 17.9     |
| time/              |          |
|    fps             | 834      |
|    iterations      | 22       |
|    time_elapsed    | 53       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 18.2        |
| time/                   |             |
|    fps                  | 840         |
|    iterations           | 23          |
|    time_elapsed         | 56          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.027501393 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.952      |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.117       |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0408     |
|    value_loss           | 0.352       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 18          |
| time/                   |             |
|    fps                  | 846         |
|    iterations           | 24          |
|    time_elapsed         | 58          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.037921574 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.946      |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.00748     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0486     |
|    value_loss           | 0.283       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=18.50 +/- 4.74
Episode length: 191.10 +/- 54.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 191         |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.032299623 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.921      |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.00031     |
|    loss                 | -0.00606    |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0474     |
|    value_loss           | 0.314       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 834      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 192         |
|    ep_rew_mean          | 18          |
| time/                   |             |
|    fps                  | 840         |
|    iterations           | 26          |
|    time_elapsed         | 63          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.030922856 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.895      |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0172      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.048      |
|    value_loss           | 0.287       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=23.50 +/- 5.54
Episode length: 230.10 +/- 64.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 230        |
|    mean_reward          | 23.5       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.02916513 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.88      |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.00031    |
|    loss                 | 0.0203     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0414    |
|    value_loss           | 0.295      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 27       |
|    time_elapsed    | 66       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 18.1       |
| time/                   |            |
|    fps                  | 831        |
|    iterations           | 28         |
|    time_elapsed         | 68         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.03042275 |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.856     |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.00031    |
|    loss                 | 0.0532     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 0.324      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 18.4       |
| time/                   |            |
|    fps                  | 836        |
|    iterations           | 29         |
|    time_elapsed         | 70         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.03815253 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.871     |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.00031    |
|    loss                 | 0.0484     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0417    |
|    value_loss           | 0.315      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=21.40 +/- 7.27
Episode length: 208.00 +/- 79.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 208         |
|    mean_reward          | 21.4        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.040951334 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.844      |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.0265      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0468     |
|    value_loss           | 0.371       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 30       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:12:06,502] Trial 22 finished with value: 20.9 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0003104065827307336, 'gamma': 0.9690496565864317, 'gae_lambda': 0.9620394633586398}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 8.67     |
| time/              |          |
|    fps             | 1227     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 145         |
|    ep_rew_mean          | 8.21        |
| time/                   |             |
|    fps                  | 1111        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008807665 |
|    clip_fraction        | 0.0578      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.111      |
|    learning_rate        | 0.000111    |
|    loss                 | 0.103       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00808    |
|    value_loss           | 0.227       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=9.00 +/- 1.73
Episode length: 117.10 +/- 22.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 117        |
|    mean_reward          | 9          |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01087768 |
|    clip_fraction        | 0.0684     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.174      |
|    learning_rate        | 0.000111   |
|    loss                 | 0.0585     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00921   |
|    value_loss           | 0.239      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 8.57     |
| time/              |          |
|    fps             | 945      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 149        |
|    ep_rew_mean          | 9.31       |
| time/                   |            |
|    fps                  | 961        |
|    iterations           | 4          |
|    time_elapsed         | 8          |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.01123555 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.000111   |
|    loss                 | 0.0753     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 0.252      |
----------------------------------------
Eval num_timesteps=10000, episode_reward=10.40 +/- 3.14
Episode length: 137.00 +/- 39.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 137          |
|    mean_reward          | 10.4         |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0070872502 |
|    clip_fraction        | 0.0432       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.461        |
|    learning_rate        | 0.000111     |
|    loss                 | 0.0713       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00782     |
|    value_loss           | 0.238        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 9.94     |
| time/              |          |
|    fps             | 892      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | 10.5        |
| time/                   |             |
|    fps                  | 910         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.009123239 |
|    clip_fraction        | 0.0513      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.102       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.252       |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | 10.8         |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 7            |
|    time_elapsed         | 15           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0114809675 |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.449        |
|    learning_rate        | 0.000111     |
|    loss                 | 0.107        |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0158      |
|    value_loss           | 0.253        |
------------------------------------------
Eval num_timesteps=15000, episode_reward=15.00 +/- 4.73
Episode length: 168.30 +/- 54.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 168         |
|    mean_reward          | 15          |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012057875 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0908      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.259       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 876      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | 11.9        |
| time/                   |             |
|    fps                  | 889         |
|    iterations           | 9           |
|    time_elapsed         | 20          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.006328566 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0655      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.247       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=13.50 +/- 4.10
Episode length: 150.80 +/- 39.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 151          |
|    mean_reward          | 13.5         |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0068617696 |
|    clip_fraction        | 0.0647       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0.424        |
|    learning_rate        | 0.000111     |
|    loss                 | 0.0843       |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 0.242        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 12.9     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | 13.7        |
| time/                   |             |
|    fps                  | 873         |
|    iterations           | 11          |
|    time_elapsed         | 25          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.009283861 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0603      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.255       |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 181          |
|    ep_rew_mean          | 14.4         |
| time/                   |              |
|    fps                  | 884          |
|    iterations           | 12           |
|    time_elapsed         | 27           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0069548422 |
|    clip_fraction        | 0.0744       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.541        |
|    learning_rate        | 0.000111     |
|    loss                 | 0.0556       |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.0155      |
|    value_loss           | 0.23         |
------------------------------------------
Eval num_timesteps=25000, episode_reward=14.50 +/- 4.94
Episode length: 158.40 +/- 45.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 158          |
|    mean_reward          | 14.5         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0057294113 |
|    clip_fraction        | 0.071        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.989       |
|    explained_variance   | 0.508        |
|    learning_rate        | 0.000111     |
|    loss                 | 0.0509       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0143      |
|    value_loss           | 0.26         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 862      |
|    iterations      | 13       |
|    time_elapsed    | 30       |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 183          |
|    ep_rew_mean          | 15.3         |
| time/                   |              |
|    fps                  | 871          |
|    iterations           | 14           |
|    time_elapsed         | 32           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0038928133 |
|    clip_fraction        | 0.075        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.95        |
|    explained_variance   | 0.626        |
|    learning_rate        | 0.000111     |
|    loss                 | 0.0614       |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00957     |
|    value_loss           | 0.218        |
------------------------------------------
Eval num_timesteps=30000, episode_reward=14.70 +/- 5.25
Episode length: 163.70 +/- 53.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 164          |
|    mean_reward          | 14.7         |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0071655605 |
|    clip_fraction        | 0.0976       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.941       |
|    explained_variance   | 0.649        |
|    learning_rate        | 0.000111     |
|    loss                 | 0.0969       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.013       |
|    value_loss           | 0.234        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 852      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 182          |
|    ep_rew_mean          | 16.1         |
| time/                   |              |
|    fps                  | 860          |
|    iterations           | 16           |
|    time_elapsed         | 38           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0069975285 |
|    clip_fraction        | 0.0771       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.888       |
|    explained_variance   | 0.536        |
|    learning_rate        | 0.000111     |
|    loss                 | 0.0986       |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.234        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | 16.5        |
| time/                   |             |
|    fps                  | 868         |
|    iterations           | 17          |
|    time_elapsed         | 40          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.007929362 |
|    clip_fraction        | 0.0734      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.896      |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0903      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.218       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=14.40 +/- 5.68
Episode length: 163.50 +/- 64.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 164         |
|    mean_reward          | 14.4        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.007589469 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.847      |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.057       |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 853      |
|    iterations      | 18       |
|    time_elapsed    | 43       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 181         |
|    ep_rew_mean          | 16.7        |
| time/                   |             |
|    fps                  | 860         |
|    iterations           | 19          |
|    time_elapsed         | 45          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.007123637 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0768      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.218       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=14.30 +/- 4.22
Episode length: 160.20 +/- 49.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 160        |
|    mean_reward          | 14.3       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.00855689 |
|    clip_fraction        | 0.0922     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.803     |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.000111   |
|    loss                 | 0.0645     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0161    |
|    value_loss           | 0.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 847      |
|    iterations      | 20       |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 17.4       |
| time/                   |            |
|    fps                  | 853        |
|    iterations           | 21         |
|    time_elapsed         | 50         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.00874096 |
|    clip_fraction        | 0.0907     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.753     |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.000111   |
|    loss                 | 0.0714     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 0.21       |
----------------------------------------
Eval num_timesteps=45000, episode_reward=14.30 +/- 3.72
Episode length: 142.10 +/- 41.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 142          |
|    mean_reward          | 14.3         |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0062281173 |
|    clip_fraction        | 0.0857       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.711       |
|    explained_variance   | 0.62         |
|    learning_rate        | 0.000111     |
|    loss                 | 0.0583       |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 0.226        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 844      |
|    iterations      | 22       |
|    time_elapsed    | 53       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 17.9        |
| time/                   |             |
|    fps                  | 850         |
|    iterations           | 23          |
|    time_elapsed         | 55          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.009403728 |
|    clip_fraction        | 0.097       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.721      |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0791      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.234       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 18.2        |
| time/                   |             |
|    fps                  | 856         |
|    iterations           | 24          |
|    time_elapsed         | 57          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.008099031 |
|    clip_fraction        | 0.0955      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0836      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.223       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=17.90 +/- 3.62
Episode length: 177.50 +/- 37.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 178          |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0077705956 |
|    clip_fraction        | 0.0867       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.708       |
|    explained_variance   | 0.654        |
|    learning_rate        | 0.000111     |
|    loss                 | 0.0514       |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.0156      |
|    value_loss           | 0.231        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 844      |
|    iterations      | 25       |
|    time_elapsed    | 60       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 18.4        |
| time/                   |             |
|    fps                  | 850         |
|    iterations           | 26          |
|    time_elapsed         | 62          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.010385919 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0651      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.26        |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=18.00 +/- 5.16
Episode length: 176.10 +/- 60.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 176         |
|    mean_reward          | 18          |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.008681491 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0672      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.21        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 839      |
|    iterations      | 27       |
|    time_elapsed    | 65       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 192         |
|    ep_rew_mean          | 18.8        |
| time/                   |             |
|    fps                  | 844         |
|    iterations           | 28          |
|    time_elapsed         | 67          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.009283949 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0739      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.202       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 191         |
|    ep_rew_mean          | 18.7        |
| time/                   |             |
|    fps                  | 849         |
|    iterations           | 29          |
|    time_elapsed         | 69          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.007890696 |
|    clip_fraction        | 0.0899      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.0837      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.249       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=19.80 +/- 4.26
Episode length: 195.00 +/- 50.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 195         |
|    mean_reward          | 19.8        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.009848146 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.628      |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.000111    |
|    loss                 | 0.071       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.207       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 838      |
|    iterations      | 30       |
|    time_elapsed    | 73       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:13:21,705] Trial 23 finished with value: 21.3 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00011148726882419649, 'gamma': 0.9577696654845765, 'gae_lambda': 0.922711693982957}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 7.92     |
| time/              |          |
|    fps             | 1231     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 146         |
|    ep_rew_mean          | 8.18        |
| time/                   |             |
|    fps                  | 1112        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.007146026 |
|    clip_fraction        | 0.0725      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0153     |
|    learning_rate        | 0.000372    |
|    loss                 | 0.0664      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.205       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=12.60 +/- 5.14
Episode length: 164.80 +/- 55.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | 12.6        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010491453 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.137       |
|    learning_rate        | 0.000372    |
|    loss                 | 0.0376      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.189       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 8.61     |
| time/              |          |
|    fps             | 903      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 156         |
|    ep_rew_mean          | 9.6         |
| time/                   |             |
|    fps                  | 928         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012549569 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.000372    |
|    loss                 | 0.021       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0242     |
|    value_loss           | 0.169       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=15.50 +/- 5.41
Episode length: 172.40 +/- 59.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 172         |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.015593881 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.00977    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0408     |
|    value_loss           | 0.167       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 852      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 164        |
|    ep_rew_mean          | 11.1       |
| time/                   |            |
|    fps                  | 876        |
|    iterations           | 6          |
|    time_elapsed         | 14         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.02119989 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.414      |
|    learning_rate        | 0.000372   |
|    loss                 | -0.00848   |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0408    |
|    value_loss           | 0.169      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 168         |
|    ep_rew_mean          | 11.8        |
| time/                   |             |
|    fps                  | 893         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.023668623 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.0435     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0444     |
|    value_loss           | 0.155       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=17.20 +/- 7.30
Episode length: 175.10 +/- 81.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 175         |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.022179956 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.0139     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0475     |
|    value_loss           | 0.158       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 849      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 179         |
|    ep_rew_mean          | 13.8        |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.025324978 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.015      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0451     |
|    value_loss           | 0.168       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=20.90 +/- 5.70
Episode length: 201.90 +/- 60.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 20.9        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.024810612 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.0314     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0478     |
|    value_loss           | 0.181       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 188         |
|    ep_rew_mean          | 15.8        |
| time/                   |             |
|    fps                  | 841         |
|    iterations           | 11          |
|    time_elapsed         | 26          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.039082862 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.000372    |
|    loss                 | 0.0294      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0483     |
|    value_loss           | 0.173       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 16.8        |
| time/                   |             |
|    fps                  | 853         |
|    iterations           | 12          |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.028900383 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.000372    |
|    loss                 | 0.0114      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0453     |
|    value_loss           | 0.18        |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=20.80 +/- 5.15
Episode length: 190.90 +/- 55.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 191         |
|    mean_reward          | 20.8        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.032982845 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.997      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.0221     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0458     |
|    value_loss           | 0.172       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 829      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 192         |
|    ep_rew_mean          | 18.1        |
| time/                   |             |
|    fps                  | 840         |
|    iterations           | 14          |
|    time_elapsed         | 34          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.040140904 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.949      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.000372    |
|    loss                 | -0.0258     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0496     |
|    value_loss           | 0.16        |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=23.90 +/- 6.79
Episode length: 226.20 +/- 67.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.032281637 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.927      |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.0351     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0457     |
|    value_loss           | 0.156       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 816      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 19.1        |
| time/                   |             |
|    fps                  | 826         |
|    iterations           | 16          |
|    time_elapsed         | 39          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.054259144 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.904      |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.000569   |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0494     |
|    value_loss           | 0.151       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 19.1        |
| time/                   |             |
|    fps                  | 835         |
|    iterations           | 17          |
|    time_elapsed         | 41          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.046264894 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.898      |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.00587    |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0475     |
|    value_loss           | 0.154       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=23.20 +/- 8.08
Episode length: 208.50 +/- 68.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 208         |
|    mean_reward          | 23.2        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.049320973 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.842      |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.0104     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0499     |
|    value_loss           | 0.173       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 19.3     |
| time/              |          |
|    fps             | 817      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 19.6        |
| time/                   |             |
|    fps                  | 825         |
|    iterations           | 19          |
|    time_elapsed         | 47          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.055491008 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.797      |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.0161     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0473     |
|    value_loss           | 0.176       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=25.20 +/- 5.15
Episode length: 227.10 +/- 56.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 227         |
|    mean_reward          | 25.2        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.058066122 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.026      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0456     |
|    value_loss           | 0.14        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 808      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 193         |
|    ep_rew_mean          | 20.4        |
| time/                   |             |
|    fps                  | 816         |
|    iterations           | 21          |
|    time_elapsed         | 52          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.052078456 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.00848    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0459     |
|    value_loss           | 0.162       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=22.30 +/- 5.57
Episode length: 201.40 +/- 53.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 201         |
|    mean_reward          | 22.3        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.058069587 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.00159    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.041      |
|    value_loss           | 0.191       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 803      |
|    iterations      | 22       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 21.3       |
| time/                   |            |
|    fps                  | 811        |
|    iterations           | 23         |
|    time_elapsed         | 58         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.06032631 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.699     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.000372   |
|    loss                 | -0.0392    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.168      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 21.1       |
| time/                   |            |
|    fps                  | 818        |
|    iterations           | 24         |
|    time_elapsed         | 60         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.06412046 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.677     |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.000372   |
|    loss                 | -0.00224   |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.175      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=23.70 +/- 3.90
Episode length: 215.20 +/- 34.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 23.7       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.06366981 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.717     |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.000372   |
|    loss                 | -0.0186    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 0.173      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 805      |
|    iterations      | 25       |
|    time_elapsed    | 63       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 201         |
|    ep_rew_mean          | 21.9        |
| time/                   |             |
|    fps                  | 812         |
|    iterations           | 26          |
|    time_elapsed         | 65          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.072036564 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.695      |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.015      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0461     |
|    value_loss           | 0.154       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=24.30 +/- 6.71
Episode length: 217.60 +/- 66.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 218        |
|    mean_reward          | 24.3       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.06749051 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.000372   |
|    loss                 | -0.0323    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.16       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 22.3     |
| time/              |          |
|    fps             | 801      |
|    iterations      | 27       |
|    time_elapsed    | 69       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | 22.3        |
| time/                   |             |
|    fps                  | 807         |
|    iterations           | 28          |
|    time_elapsed         | 71          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.068772845 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.000372    |
|    loss                 | -0.00713    |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0446     |
|    value_loss           | 0.167       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 22.3       |
| time/                   |            |
|    fps                  | 813        |
|    iterations           | 29         |
|    time_elapsed         | 73         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.07476077 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.62      |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.000372   |
|    loss                 | 0.00797    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0397    |
|    value_loss           | 0.147      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=23.40 +/- 5.08
Episode length: 205.80 +/- 63.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 206        |
|    mean_reward          | 23.4       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.06691604 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.692      |
|    learning_rate        | 0.000372   |
|    loss                 | -0.00346   |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.154      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 22.3     |
| time/              |          |
|    fps             | 803      |
|    iterations      | 30       |
|    time_elapsed    | 76       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:14:40,251] Trial 24 finished with value: 25.4 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0003722616514961587, 'gamma': 0.9408698784665287, 'gae_lambda': 0.9242960688290474}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 10       |
| time/              |          |
|    fps             | 1231     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | 9.15        |
| time/                   |             |
|    fps                  | 1115        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.014423132 |
|    clip_fraction        | 0.0952      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.00409    |
|    learning_rate        | 0.00215     |
|    loss                 | 0.0265      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.227       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=15.70 +/- 5.40
Episode length: 191.90 +/- 66.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 192         |
|    mean_reward          | 15.7        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.035991658 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.00215     |
|    loss                 | -0.000294   |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0453     |
|    value_loss           | 0.155       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 9.92     |
| time/              |          |
|    fps             | 880      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | 10.7       |
| time/                   |            |
|    fps                  | 909        |
|    iterations           | 4          |
|    time_elapsed         | 9          |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.07491492 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 0.256      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0718    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0742    |
|    value_loss           | 0.147      |
----------------------------------------
Eval num_timesteps=10000, episode_reward=15.90 +/- 4.91
Episode length: 176.90 +/- 64.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 177         |
|    mean_reward          | 15.9        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.102132455 |
|    clip_fraction        | 0.519       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.00215     |
|    loss                 | -0.0631     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0733     |
|    value_loss           | 0.117       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 834      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 11.8       |
| time/                   |            |
|    fps                  | 859        |
|    iterations           | 6          |
|    time_elapsed         | 14         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.10225348 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.502      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0893    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0721    |
|    value_loss           | 0.136      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | 12.3       |
| time/                   |            |
|    fps                  | 878        |
|    iterations           | 7          |
|    time_elapsed         | 16         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.15132234 |
|    clip_fraction        | 0.584      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.421      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0848    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0734    |
|    value_loss           | 0.132      |
----------------------------------------
Eval num_timesteps=15000, episode_reward=19.30 +/- 5.46
Episode length: 194.70 +/- 56.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 195        |
|    mean_reward          | 19.3       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.17048377 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.315      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.071     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0699    |
|    value_loss           | 0.137      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 12.9     |
| time/              |          |
|    fps             | 834      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | 13.6       |
| time/                   |            |
|    fps                  | 850        |
|    iterations           | 9          |
|    time_elapsed         | 21         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.16389585 |
|    clip_fraction        | 0.561      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.994     |
|    explained_variance   | 0.461      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.073     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0767    |
|    value_loss           | 0.117      |
----------------------------------------
Eval num_timesteps=20000, episode_reward=17.10 +/- 4.61
Episode length: 176.60 +/- 50.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 177       |
|    mean_reward          | 17.1      |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.1863211 |
|    clip_fraction        | 0.564     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.942    |
|    explained_variance   | 0.302     |
|    learning_rate        | 0.00215   |
|    loss                 | -0.0886   |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.0782   |
|    value_loss           | 0.127     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 823      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | 15.2       |
| time/                   |            |
|    fps                  | 838        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.21010026 |
|    clip_fraction        | 0.563      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.918     |
|    explained_variance   | 0.457      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0869    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0734    |
|    value_loss           | 0.115      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 177       |
|    ep_rew_mean          | 15.6      |
| time/                   |           |
|    fps                  | 850       |
|    iterations           | 12        |
|    time_elapsed         | 28        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.2442897 |
|    clip_fraction        | 0.573     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.918    |
|    explained_variance   | 0.563     |
|    learning_rate        | 0.00215   |
|    loss                 | -0.0723   |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0699   |
|    value_loss           | 0.12      |
---------------------------------------
Eval num_timesteps=25000, episode_reward=21.50 +/- 4.65
Episode length: 207.00 +/- 49.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 207        |
|    mean_reward          | 21.5       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.24821004 |
|    clip_fraction        | 0.569      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.871     |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0855    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0768    |
|    value_loss           | 0.103      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 15.8     |
| time/              |          |
|    fps             | 823      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 15.9       |
| time/                   |            |
|    fps                  | 835        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.28504705 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.829     |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.111     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0814    |
|    value_loss           | 0.107      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=17.50 +/- 7.12
Episode length: 174.50 +/- 70.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 174        |
|    mean_reward          | 17.5       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.29799175 |
|    clip_fraction        | 0.578      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.00215    |
|    loss                 | -0.104     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0751    |
|    value_loss           | 0.104      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 818      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 828        |
|    iterations           | 16         |
|    time_elapsed         | 39         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.26972145 |
|    clip_fraction        | 0.579      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.811     |
|    explained_variance   | 0.553      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.114     |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0731    |
|    value_loss           | 0.113      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 178        |
|    ep_rew_mean          | 16.5       |
| time/                   |            |
|    fps                  | 837        |
|    iterations           | 17         |
|    time_elapsed         | 41         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.27687407 |
|    clip_fraction        | 0.559      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.745     |
|    explained_variance   | 0.53       |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0902    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0733    |
|    value_loss           | 0.108      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=21.60 +/- 4.45
Episode length: 226.70 +/- 51.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 227        |
|    mean_reward          | 21.6       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.32018626 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.705     |
|    explained_variance   | 0.519      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0874    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0685    |
|    value_loss           | 0.106      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 817      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 178        |
|    ep_rew_mean          | 16.7       |
| time/                   |            |
|    fps                  | 826        |
|    iterations           | 19         |
|    time_elapsed         | 47         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.35225898 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.107     |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.072     |
|    value_loss           | 0.0976     |
----------------------------------------
Eval num_timesteps=40000, episode_reward=16.40 +/- 5.44
Episode length: 163.20 +/- 46.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 163        |
|    mean_reward          | 16.4       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.34214193 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.679     |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0856    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.065     |
|    value_loss           | 0.115      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 815      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | 17.6       |
| time/                   |            |
|    fps                  | 823        |
|    iterations           | 21         |
|    time_elapsed         | 52         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.37539703 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.663     |
|    explained_variance   | 0.545      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0734    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0722    |
|    value_loss           | 0.116      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=18.90 +/- 5.30
Episode length: 188.80 +/- 48.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 189       |
|    mean_reward          | 18.9      |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.3817408 |
|    clip_fraction        | 0.554     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.656    |
|    explained_variance   | 0.615     |
|    learning_rate        | 0.00215   |
|    loss                 | -0.0804   |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.0665   |
|    value_loss           | 0.105     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 811      |
|    iterations      | 22       |
|    time_elapsed    | 55       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 195       |
|    ep_rew_mean          | 18.7      |
| time/                   |           |
|    fps                  | 818       |
|    iterations           | 23        |
|    time_elapsed         | 57        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.3875757 |
|    clip_fraction        | 0.565     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.682    |
|    explained_variance   | 0.65      |
|    learning_rate        | 0.00215   |
|    loss                 | -0.0941   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0688   |
|    value_loss           | 0.101     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 197        |
|    ep_rew_mean          | 19         |
| time/                   |            |
|    fps                  | 825        |
|    iterations           | 24         |
|    time_elapsed         | 59         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.34257478 |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0866    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0701    |
|    value_loss           | 0.0974     |
----------------------------------------
Eval num_timesteps=50000, episode_reward=17.70 +/- 5.92
Episode length: 174.00 +/- 53.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 174        |
|    mean_reward          | 17.7       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.34314984 |
|    clip_fraction        | 0.555      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.664     |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0959    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0627    |
|    value_loss           | 0.109      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 19.2     |
| time/              |          |
|    fps             | 816      |
|    iterations      | 25       |
|    time_elapsed    | 62       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 19.4       |
| time/                   |            |
|    fps                  | 822        |
|    iterations           | 26         |
|    time_elapsed         | 64         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.39731032 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.69      |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0906    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0734    |
|    value_loss           | 0.107      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=20.50 +/- 5.12
Episode length: 194.50 +/- 53.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 194        |
|    mean_reward          | 20.5       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.48449448 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.612     |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.089     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0718    |
|    value_loss           | 0.0981     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 812      |
|    iterations      | 27       |
|    time_elapsed    | 68       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 20.2       |
| time/                   |            |
|    fps                  | 818        |
|    iterations           | 28         |
|    time_elapsed         | 70         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.39654392 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.619     |
|    explained_variance   | 0.498      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.103     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0686    |
|    value_loss           | 0.101      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 202        |
|    ep_rew_mean          | 20.1       |
| time/                   |            |
|    fps                  | 824        |
|    iterations           | 29         |
|    time_elapsed         | 72         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.50993174 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.57      |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0873    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0657    |
|    value_loss           | 0.116      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=21.10 +/- 6.59
Episode length: 206.90 +/- 72.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 207        |
|    mean_reward          | 21.1       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.46203512 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.588     |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.00215    |
|    loss                 | -0.0989    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0687    |
|    value_loss           | 0.111      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 20       |
| time/              |          |
|    fps             | 814      |
|    iterations      | 30       |
|    time_elapsed    | 75       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:15:57,811] Trial 25 finished with value: 22.5 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.002148034821370048, 'gamma': 0.9407644403022364, 'gae_lambda': 0.8931734915254032}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 7.69     |
| time/              |          |
|    fps             | 1225     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 141         |
|    ep_rew_mean          | 7.61        |
| time/                   |             |
|    fps                  | 1112        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008421352 |
|    clip_fraction        | 0.0913      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.0104      |
|    learning_rate        | 0.000427    |
|    loss                 | 0.0398      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.173       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=11.60 +/- 4.80
Episode length: 167.40 +/- 57.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 167         |
|    mean_reward          | 11.6        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011744026 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.000427    |
|    loss                 | 0.000521    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.026      |
|    value_loss           | 0.151       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 8.21     |
| time/              |          |
|    fps             | 902      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 152         |
|    ep_rew_mean          | 9.04        |
| time/                   |             |
|    fps                  | 928         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.017872479 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.000427    |
|    loss                 | -0.0252     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0389     |
|    value_loss           | 0.163       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=18.00 +/- 5.51
Episode length: 181.70 +/- 51.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 182         |
|    mean_reward          | 18          |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.021973114 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.245       |
|    learning_rate        | 0.000427    |
|    loss                 | -0.0237     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0507     |
|    value_loss           | 0.178       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 9.86     |
| time/              |          |
|    fps             | 848      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | 10.7        |
| time/                   |             |
|    fps                  | 872         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.027955137 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.209       |
|    learning_rate        | 0.000427    |
|    loss                 | -0.04       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0494     |
|    value_loss           | 0.187       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 164         |
|    ep_rew_mean          | 11.4        |
| time/                   |             |
|    fps                  | 889         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.028872985 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.226       |
|    learning_rate        | 0.000427    |
|    loss                 | -0.0424     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0549     |
|    value_loss           | 0.184       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=15.50 +/- 5.90
Episode length: 153.70 +/- 54.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 154         |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.028340297 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.196       |
|    learning_rate        | 0.000427    |
|    loss                 | -0.00965    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.054      |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 11.7     |
| time/              |          |
|    fps             | 854      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | 12.8        |
| time/                   |             |
|    fps                  | 869         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.035365444 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.000427    |
|    loss                 | -0.0471     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0605     |
|    value_loss           | 0.176       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=20.40 +/- 5.85
Episode length: 192.40 +/- 56.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 192        |
|    mean_reward          | 20.4       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.03961681 |
|    clip_fraction        | 0.348      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.318      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0233    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0598    |
|    value_loss           | 0.179      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 13.8     |
| time/              |          |
|    fps             | 835      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | 14.8       |
| time/                   |            |
|    fps                  | 849        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.05004274 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.439      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0506    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.061     |
|    value_loss           | 0.178      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 178        |
|    ep_rew_mean          | 15.6       |
| time/                   |            |
|    fps                  | 860        |
|    iterations           | 12         |
|    time_elapsed         | 28         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.04409613 |
|    clip_fraction        | 0.348      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.409      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0262    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.16       |
----------------------------------------
Eval num_timesteps=25000, episode_reward=18.70 +/- 5.24
Episode length: 174.60 +/- 44.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 175        |
|    mean_reward          | 18.7       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.04915076 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.989     |
|    explained_variance   | 0.423      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0311    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.178      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 838      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | 17.3        |
| time/                   |             |
|    fps                  | 849         |
|    iterations           | 14          |
|    time_elapsed         | 33          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.051350415 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.977      |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.000427    |
|    loss                 | -0.0319     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0514     |
|    value_loss           | 0.179       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=21.80 +/- 6.94
Episode length: 211.10 +/- 63.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 211        |
|    mean_reward          | 21.8       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.05384843 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.944     |
|    explained_variance   | 0.419      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0547    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.174      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | 17.8       |
| time/                   |            |
|    fps                  | 835        |
|    iterations           | 16         |
|    time_elapsed         | 39         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.05258746 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.916     |
|    explained_variance   | 0.436      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0159    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0527    |
|    value_loss           | 0.176      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 18.6       |
| time/                   |            |
|    fps                  | 844        |
|    iterations           | 17         |
|    time_elapsed         | 41         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.06381568 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.888     |
|    explained_variance   | 0.451      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0189    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.169      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=22.10 +/- 5.92
Episode length: 199.20 +/- 56.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 22.1        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.066247866 |
|    clip_fraction        | 0.36        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.841      |
|    explained_variance   | 0.494       |
|    learning_rate        | 0.000427    |
|    loss                 | -0.0197     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0482     |
|    value_loss           | 0.163       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 827      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 19.4       |
| time/                   |            |
|    fps                  | 835        |
|    iterations           | 19         |
|    time_elapsed         | 46         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.06460269 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.428      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0201    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0483    |
|    value_loss           | 0.18       |
----------------------------------------
Eval num_timesteps=40000, episode_reward=23.10 +/- 5.75
Episode length: 207.60 +/- 55.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 208         |
|    mean_reward          | 23.1        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.061947025 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.796      |
|    explained_variance   | 0.466       |
|    learning_rate        | 0.000427    |
|    loss                 | -0.0127     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0479     |
|    value_loss           | 0.174       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 819      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 204       |
|    ep_rew_mean          | 20.6      |
| time/                   |           |
|    fps                  | 827       |
|    iterations           | 21        |
|    time_elapsed         | 51        |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.0755083 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.798    |
|    explained_variance   | 0.517     |
|    learning_rate        | 0.000427  |
|    loss                 | -0.0296   |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0493   |
|    value_loss           | 0.153     |
---------------------------------------
Eval num_timesteps=45000, episode_reward=24.10 +/- 3.96
Episode length: 215.20 +/- 34.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 24.1       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.06836164 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.778     |
|    explained_variance   | 0.417      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0332    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0549    |
|    value_loss           | 0.155      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 21.4     |
| time/              |          |
|    fps             | 812      |
|    iterations      | 22       |
|    time_elapsed    | 55       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 208        |
|    ep_rew_mean          | 21.2       |
| time/                   |            |
|    fps                  | 819        |
|    iterations           | 23         |
|    time_elapsed         | 57         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.07815855 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.422      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0153    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0517    |
|    value_loss           | 0.182      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 21.2       |
| time/                   |            |
|    fps                  | 825        |
|    iterations           | 24         |
|    time_elapsed         | 59         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.09257897 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.467      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0233    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0523    |
|    value_loss           | 0.178      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=25.50 +/- 6.96
Episode length: 238.50 +/- 84.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 238       |
|    mean_reward          | 25.5      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0894168 |
|    clip_fraction        | 0.361     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.719    |
|    explained_variance   | 0.439     |
|    learning_rate        | 0.000427  |
|    loss                 | -0.0135   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0488   |
|    value_loss           | 0.175     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 21.1     |
| time/              |          |
|    fps             | 810      |
|    iterations      | 25       |
|    time_elapsed    | 63       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 206        |
|    ep_rew_mean          | 21.7       |
| time/                   |            |
|    fps                  | 817        |
|    iterations           | 26         |
|    time_elapsed         | 65         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.09945665 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.459      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0411    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.174      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=26.20 +/- 4.21
Episode length: 228.20 +/- 33.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 26.2       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.07571465 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.707     |
|    explained_variance   | 0.506      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.00466   |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 0.166      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 22.2     |
| time/              |          |
|    fps             | 804      |
|    iterations      | 27       |
|    time_elapsed    | 68       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 212        |
|    ep_rew_mean          | 22.3       |
| time/                   |            |
|    fps                  | 810        |
|    iterations           | 28         |
|    time_elapsed         | 70         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.08635076 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.478      |
|    learning_rate        | 0.000427   |
|    loss                 | 0.0129     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0479    |
|    value_loss           | 0.167      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 211        |
|    ep_rew_mean          | 22.2       |
| time/                   |            |
|    fps                  | 816        |
|    iterations           | 29         |
|    time_elapsed         | 72         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.08061159 |
|    clip_fraction        | 0.348      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.691     |
|    explained_variance   | 0.505      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0486    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0524    |
|    value_loss           | 0.166      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=22.90 +/- 6.59
Episode length: 202.20 +/- 63.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 22.9       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.09967345 |
|    clip_fraction        | 0.367      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.491      |
|    learning_rate        | 0.000427   |
|    loss                 | -0.0139    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0501    |
|    value_loss           | 0.164      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 22.4     |
| time/              |          |
|    fps             | 807      |
|    iterations      | 30       |
|    time_elapsed    | 76       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:17:16,186] Trial 26 finished with value: 25.9 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00042715539270229746, 'gamma': 0.9284483374214826, 'gae_lambda': 0.9558680449143301}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 9.62     |
| time/              |          |
|    fps             | 1228     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 142         |
|    ep_rew_mean          | 8.89        |
| time/                   |             |
|    fps                  | 1113        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012774518 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0407     |
|    learning_rate        | 0.000747    |
|    loss                 | 0.0312      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.198       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=15.20 +/- 3.82
Episode length: 169.20 +/- 34.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 15.2        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.023318026 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.188       |
|    learning_rate        | 0.000747    |
|    loss                 | -0.0203     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0409     |
|    value_loss           | 0.192       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 10       |
| time/              |          |
|    fps             | 897      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 157         |
|    ep_rew_mean          | 11.1        |
| time/                   |             |
|    fps                  | 924         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.028616818 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.203       |
|    learning_rate        | 0.000747    |
|    loss                 | -0.0453     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.053      |
|    value_loss           | 0.156       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=16.30 +/- 5.48
Episode length: 183.80 +/- 60.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 184        |
|    mean_reward          | 16.3       |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.04262927 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.389      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.053     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0685    |
|    value_loss           | 0.151      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 12       |
| time/              |          |
|    fps             | 835      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | 12.7        |
| time/                   |             |
|    fps                  | 860         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.045957718 |
|    clip_fraction        | 0.383       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000747    |
|    loss                 | -0.0746     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0677     |
|    value_loss           | 0.158       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 13         |
| time/                   |            |
|    fps                  | 879        |
|    iterations           | 7          |
|    time_elapsed         | 16         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.04574751 |
|    clip_fraction        | 0.381      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.293      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0519    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.194      |
----------------------------------------
Eval num_timesteps=15000, episode_reward=20.70 +/- 5.93
Episode length: 210.00 +/- 66.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 20.7       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.06077631 |
|    clip_fraction        | 0.408      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.299      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0273    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.168      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 830      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 14.1       |
| time/                   |            |
|    fps                  | 846        |
|    iterations           | 9          |
|    time_elapsed         | 21         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.06538959 |
|    clip_fraction        | 0.449      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.36       |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0564    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0697    |
|    value_loss           | 0.147      |
----------------------------------------
Eval num_timesteps=20000, episode_reward=22.20 +/- 5.42
Episode length: 224.20 +/- 61.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 22.2       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.06364725 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.232      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0582    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0671    |
|    value_loss           | 0.171      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 809      |
|    iterations      | 10       |
|    time_elapsed    | 25       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 184       |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 824       |
|    iterations           | 11        |
|    time_elapsed         | 27        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.0783788 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.04     |
|    explained_variance   | 0.313     |
|    learning_rate        | 0.000747  |
|    loss                 | -0.0609   |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.072    |
|    value_loss           | 0.153     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 16.8        |
| time/                   |             |
|    fps                  | 837         |
|    iterations           | 12          |
|    time_elapsed         | 29          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.080933094 |
|    clip_fraction        | 0.451       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.994      |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.000747    |
|    loss                 | -0.0453     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.067      |
|    value_loss           | 0.178       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=22.40 +/- 3.85
Episode length: 210.50 +/- 37.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 22.4       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.10280207 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.95      |
|    explained_variance   | 0.398      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0608    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0694    |
|    value_loss           | 0.162      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 812      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 17.9       |
| time/                   |            |
|    fps                  | 824        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.11167089 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.879     |
|    explained_variance   | 0.388      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0643    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0598    |
|    value_loss           | 0.162      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=23.90 +/- 7.27
Episode length: 221.00 +/- 75.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 221         |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.104108766 |
|    clip_fraction        | 0.422       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.000747    |
|    loss                 | -0.0458     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0598     |
|    value_loss           | 0.163       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 802      |
|    iterations      | 15       |
|    time_elapsed    | 38       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 193        |
|    ep_rew_mean          | 18.8       |
| time/                   |            |
|    fps                  | 812        |
|    iterations           | 16         |
|    time_elapsed         | 40         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.10552661 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.804     |
|    explained_variance   | 0.44       |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0616    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0625    |
|    value_loss           | 0.158      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 19.5       |
| time/                   |            |
|    fps                  | 822        |
|    iterations           | 17         |
|    time_elapsed         | 42         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.13210563 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.771     |
|    explained_variance   | 0.428      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0567    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0625    |
|    value_loss           | 0.145      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=26.40 +/- 7.00
Episode length: 251.90 +/- 74.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 252        |
|    mean_reward          | 26.4       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.14006506 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.74      |
|    explained_variance   | 0.428      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0423    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.059     |
|    value_loss           | 0.141      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 18       |
|    time_elapsed    | 46       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 202        |
|    ep_rew_mean          | 20.8       |
| time/                   |            |
|    fps                  | 809        |
|    iterations           | 19         |
|    time_elapsed         | 48         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.12933722 |
|    clip_fraction        | 0.433      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.751     |
|    explained_variance   | 0.426      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0813    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0552    |
|    value_loss           | 0.149      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=24.70 +/- 4.98
Episode length: 219.20 +/- 46.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 219        |
|    mean_reward          | 24.7       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.14563347 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.408      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.055     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.155      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 21.4     |
| time/              |          |
|    fps             | 794      |
|    iterations      | 20       |
|    time_elapsed    | 51       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 212        |
|    ep_rew_mean          | 22.1       |
| time/                   |            |
|    fps                  | 802        |
|    iterations           | 21         |
|    time_elapsed         | 53         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.14379176 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | 0.411      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.053     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0588    |
|    value_loss           | 0.153      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=24.10 +/- 6.74
Episode length: 216.00 +/- 73.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 24.1       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.15041329 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.705     |
|    explained_variance   | 0.438      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0638    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0656    |
|    value_loss           | 0.144      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 22.3     |
| time/              |          |
|    fps             | 790      |
|    iterations      | 22       |
|    time_elapsed    | 57       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 215        |
|    ep_rew_mean          | 22.7       |
| time/                   |            |
|    fps                  | 797        |
|    iterations           | 23         |
|    time_elapsed         | 59         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.16117594 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.522      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0476    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0539    |
|    value_loss           | 0.162      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 214        |
|    ep_rew_mean          | 22.8       |
| time/                   |            |
|    fps                  | 804        |
|    iterations           | 24         |
|    time_elapsed         | 61         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.15249026 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.633     |
|    explained_variance   | 0.406      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.056     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.057     |
|    value_loss           | 0.149      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=28.60 +/- 5.18
Episode length: 279.20 +/- 58.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 279       |
|    mean_reward          | 28.6      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.1420484 |
|    clip_fraction        | 0.412     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.627    |
|    explained_variance   | 0.406     |
|    learning_rate        | 0.000747  |
|    loss                 | -0.0539   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0553   |
|    value_loss           | 0.175     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | 22.7     |
| time/              |          |
|    fps             | 788      |
|    iterations      | 25       |
|    time_elapsed    | 64       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 23         |
| time/                   |            |
|    fps                  | 795        |
|    iterations           | 26         |
|    time_elapsed         | 66         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.16449665 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.474      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0652    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.157      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=24.40 +/- 4.92
Episode length: 225.40 +/- 53.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 225       |
|    mean_reward          | 24.4      |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.1741772 |
|    clip_fraction        | 0.44      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.646    |
|    explained_variance   | 0.421     |
|    learning_rate        | 0.000747  |
|    loss                 | -0.0856   |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.0662   |
|    value_loss           | 0.14      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 784      |
|    iterations      | 27       |
|    time_elapsed    | 70       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 23.3       |
| time/                   |            |
|    fps                  | 790        |
|    iterations           | 28         |
|    time_elapsed         | 72         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.16673121 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.606     |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0598    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0559    |
|    value_loss           | 0.164      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 23         |
| time/                   |            |
|    fps                  | 796        |
|    iterations           | 29         |
|    time_elapsed         | 74         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.15158363 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.592     |
|    explained_variance   | 0.407      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0376    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.171      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=24.40 +/- 5.97
Episode length: 231.50 +/- 61.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 24.4       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.19317326 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.596     |
|    explained_variance   | 0.433      |
|    learning_rate        | 0.000747   |
|    loss                 | -0.0664    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0576    |
|    value_loss           | 0.158      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 22.8     |
| time/              |          |
|    fps             | 786      |
|    iterations      | 30       |
|    time_elapsed    | 78       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:18:36,301] Trial 27 finished with value: 22.4 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0007467474659426946, 'gamma': 0.9242554684133415, 'gae_lambda': 0.9573198659088207}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 8.14     |
| time/              |          |
|    fps             | 1223     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=7.30 +/- 2.37
Episode length: 131.40 +/- 45.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 7.3         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.008175897 |
|    clip_fraction        | 0.0548      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.00309    |
|    learning_rate        | 0.000143    |
|    loss                 | 0.119       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00974    |
|    value_loss           | 0.272       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 9.21     |
| time/              |          |
|    fps             | 935      |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=13.90 +/- 5.63
Episode length: 168.80 +/- 63.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 13.9        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010413138 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000143    |
|    loss                 | 0.0504      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 0.229       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 9.77     |
| time/              |          |
|    fps             | 843      |
|    iterations      | 3        |
|    time_elapsed    | 14       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=17.70 +/- 4.34
Episode length: 180.80 +/- 41.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 181         |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.011194587 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000143    |
|    loss                 | 0.0691      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 0.235       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 805      |
|    iterations      | 4        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=17.70 +/- 4.10
Episode length: 182.80 +/- 46.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011806857 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.000143    |
|    loss                 | 0.0676      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.215       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 5        |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 168          |
|    ep_rew_mean          | 13.5         |
| time/                   |              |
|    fps                  | 800          |
|    iterations           | 6            |
|    time_elapsed         | 30           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0132835815 |
|    clip_fraction        | 0.173        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.284        |
|    learning_rate        | 0.000143     |
|    loss                 | 0.0505       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0318      |
|    value_loss           | 0.243        |
------------------------------------------
Eval num_timesteps=25000, episode_reward=15.90 +/- 4.16
Episode length: 156.70 +/- 44.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 157         |
|    mean_reward          | 15.9        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.014538022 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000143    |
|    loss                 | 0.0555      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.229       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 789      |
|    iterations      | 7        |
|    time_elapsed    | 36       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=16.30 +/- 2.33
Episode length: 159.70 +/- 20.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 160         |
|    mean_reward          | 16.3        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.016743027 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.000143    |
|    loss                 | 0.0225      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.246       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 780      |
|    iterations      | 8        |
|    time_elapsed    | 41       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=17.50 +/- 3.61
Episode length: 178.00 +/- 34.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 178         |
|    mean_reward          | 17.5        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.017782763 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.000143    |
|    loss                 | 0.0119      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0359     |
|    value_loss           | 0.232       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 772      |
|    iterations      | 9        |
|    time_elapsed    | 47       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=17.00 +/- 2.83
Episode length: 158.70 +/- 29.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 159         |
|    mean_reward          | 17          |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.019595113 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.000143    |
|    loss                 | 0.0379      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.221       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 767      |
|    iterations      | 10       |
|    time_elapsed    | 53       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=20.10 +/- 3.73
Episode length: 190.20 +/- 37.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 190        |
|    mean_reward          | 20.1       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.02110102 |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.423      |
|    learning_rate        | 0.000143   |
|    loss                 | -0.00238   |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0385    |
|    value_loss           | 0.22       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 761      |
|    iterations      | 11       |
|    time_elapsed    | 59       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 193         |
|    ep_rew_mean          | 18.6        |
| time/                   |             |
|    fps                  | 770         |
|    iterations           | 12          |
|    time_elapsed         | 63          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.019967142 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.394       |
|    learning_rate        | 0.000143    |
|    loss                 | 0.039       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.238       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=21.40 +/- 6.83
Episode length: 208.60 +/- 71.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 21.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.022864375 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.956      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000143    |
|    loss                 | 0.0375      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0405     |
|    value_loss           | 0.215       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 763      |
|    iterations      | 13       |
|    time_elapsed    | 69       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=22.80 +/- 7.57
Episode length: 212.30 +/- 71.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 212         |
|    mean_reward          | 22.8        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.023811262 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.953      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000143    |
|    loss                 | -0.0107     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0399     |
|    value_loss           | 0.214       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 757      |
|    iterations      | 14       |
|    time_elapsed    | 75       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=22.20 +/- 6.00
Episode length: 200.50 +/- 62.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 200         |
|    mean_reward          | 22.2        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.024914308 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.906      |
|    explained_variance   | 0.408       |
|    learning_rate        | 0.000143    |
|    loss                 | 0.00788     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0361     |
|    value_loss           | 0.234       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 210      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 752      |
|    iterations      | 15       |
|    time_elapsed    | 81       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:20:00,835] Trial 28 finished with value: 23.0 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.00014262320456437833, 'gamma': 0.9274056821937156, 'gae_lambda': 0.9824844020208299}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 9.77     |
| time/              |          |
|    fps             | 1230     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 157         |
|    ep_rew_mean          | 10.5        |
| time/                   |             |
|    fps                  | 1112        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.019544423 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.00415    |
|    learning_rate        | 0.00433     |
|    loss                 | 0.0909      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 1.29        |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=16.00 +/- 2.65
Episode length: 182.50 +/- 36.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 182         |
|    mean_reward          | 16          |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.038933568 |
|    clip_fraction        | 0.33        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.173       |
|    learning_rate        | 0.00433     |
|    loss                 | 0.0302      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0357     |
|    value_loss           | 0.212       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    fps             | 887      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 164        |
|    ep_rew_mean          | 12.2       |
| time/                   |            |
|    fps                  | 915        |
|    iterations           | 4          |
|    time_elapsed         | 8          |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.06406751 |
|    clip_fraction        | 0.392      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.255      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.00869   |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0443    |
|    value_loss           | 0.198      |
----------------------------------------
Eval num_timesteps=10000, episode_reward=18.50 +/- 5.37
Episode length: 196.60 +/- 59.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 197        |
|    mean_reward          | 18.5       |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.09607732 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.253      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0536    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0557    |
|    value_loss           | 0.188      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 12.9     |
| time/              |          |
|    fps             | 832      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 13.2       |
| time/                   |            |
|    fps                  | 857        |
|    iterations           | 6          |
|    time_elapsed         | 14         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.13362443 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.168      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0701    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0661    |
|    value_loss           | 0.163      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 13.4       |
| time/                   |            |
|    fps                  | 875        |
|    iterations           | 7          |
|    time_elapsed         | 16         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.16837338 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.31       |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0572    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0696    |
|    value_loss           | 0.144      |
----------------------------------------
Eval num_timesteps=15000, episode_reward=19.00 +/- 4.02
Episode length: 211.20 +/- 58.82
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 211      |
|    mean_reward          | 19       |
| time/                   |          |
|    total_timesteps      | 15000    |
| train/                  |          |
|    approx_kl            | 0.202458 |
|    clip_fraction        | 0.563    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.06    |
|    explained_variance   | 0.457    |
|    learning_rate        | 0.00433  |
|    loss                 | -0.0364  |
|    n_updates            | 70       |
|    policy_gradient_loss | -0.0726  |
|    value_loss           | 0.141    |
--------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 827      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 14.5       |
| time/                   |            |
|    fps                  | 844        |
|    iterations           | 9          |
|    time_elapsed         | 21         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.26055133 |
|    clip_fraction        | 0.583      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.979     |
|    explained_variance   | 0.379      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0749    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0738    |
|    value_loss           | 0.131      |
----------------------------------------
Eval num_timesteps=20000, episode_reward=17.00 +/- 5.42
Episode length: 174.80 +/- 59.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 175        |
|    mean_reward          | 17         |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.27076474 |
|    clip_fraction        | 0.588      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.949     |
|    explained_variance   | 0.373      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0824    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0724    |
|    value_loss           | 0.148      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 818      |
|    iterations      | 10       |
|    time_elapsed    | 25       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 184       |
|    ep_rew_mean          | 16        |
| time/                   |           |
|    fps                  | 833       |
|    iterations           | 11        |
|    time_elapsed         | 27        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.3245881 |
|    clip_fraction        | 0.616     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.886    |
|    explained_variance   | 0.361     |
|    learning_rate        | 0.00433   |
|    loss                 | -0.0862   |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0731   |
|    value_loss           | 0.133     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 845        |
|    iterations           | 12         |
|    time_elapsed         | 29         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.33805776 |
|    clip_fraction        | 0.612      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.859     |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0836    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0785    |
|    value_loss           | 0.136      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=20.00 +/- 4.96
Episode length: 194.80 +/- 49.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 195        |
|    mean_reward          | 20         |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.34694144 |
|    clip_fraction        | 0.62       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.846     |
|    explained_variance   | 0.263      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0819    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0647    |
|    value_loss           | 0.147      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 821      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 187        |
|    ep_rew_mean          | 16.9       |
| time/                   |            |
|    fps                  | 833        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.36598414 |
|    clip_fraction        | 0.625      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.84      |
|    explained_variance   | 0.249      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0573    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0549    |
|    value_loss           | 0.154      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=18.50 +/- 4.65
Episode length: 198.00 +/- 50.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 198        |
|    mean_reward          | 18.5       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.42588603 |
|    clip_fraction        | 0.623      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.817     |
|    explained_variance   | 0.231      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0898    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0667    |
|    value_loss           | 0.145      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 813      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 823        |
|    iterations           | 16         |
|    time_elapsed         | 39         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.43570912 |
|    clip_fraction        | 0.629      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.828     |
|    explained_variance   | 0.312      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0649    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0681    |
|    value_loss           | 0.127      |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 188      |
|    ep_rew_mean          | 17       |
| time/                   |          |
|    fps                  | 833      |
|    iterations           | 17       |
|    time_elapsed         | 41       |
|    total_timesteps      | 34816    |
| train/                  |          |
|    approx_kl            | 0.605773 |
|    clip_fraction        | 0.654    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.777   |
|    explained_variance   | 0.245    |
|    learning_rate        | 0.00433  |
|    loss                 | -0.0774  |
|    n_updates            | 160      |
|    policy_gradient_loss | -0.0688  |
|    value_loss           | 0.136    |
--------------------------------------
Eval num_timesteps=35000, episode_reward=21.20 +/- 4.38
Episode length: 226.20 +/- 57.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 226        |
|    mean_reward          | 21.2       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.49178064 |
|    clip_fraction        | 0.644      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.788     |
|    explained_variance   | 0.258      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0802    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0623    |
|    value_loss           | 0.133      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 813      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 822        |
|    iterations           | 19         |
|    time_elapsed         | 47         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.52651554 |
|    clip_fraction        | 0.629      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.748     |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0959    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0675    |
|    value_loss           | 0.127      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=16.80 +/- 5.47
Episode length: 193.00 +/- 59.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 193       |
|    mean_reward          | 16.8      |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.6055684 |
|    clip_fraction        | 0.634     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.763    |
|    explained_variance   | 0.335     |
|    learning_rate        | 0.00433   |
|    loss                 | -0.108    |
|    n_updates            | 190       |
|    policy_gradient_loss | -0.0814   |
|    value_loss           | 0.124     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 808      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 190      |
|    ep_rew_mean          | 17.2     |
| time/                   |          |
|    fps                  | 816      |
|    iterations           | 21       |
|    time_elapsed         | 52       |
|    total_timesteps      | 43008    |
| train/                  |          |
|    approx_kl            | 0.650528 |
|    clip_fraction        | 0.622    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.697   |
|    explained_variance   | 0.271    |
|    learning_rate        | 0.00433  |
|    loss                 | -0.085   |
|    n_updates            | 200      |
|    policy_gradient_loss | -0.0698  |
|    value_loss           | 0.147    |
--------------------------------------
Eval num_timesteps=45000, episode_reward=22.90 +/- 8.30
Episode length: 243.70 +/- 105.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 244       |
|    mean_reward          | 22.9      |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.5947375 |
|    clip_fraction        | 0.618     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.689    |
|    explained_variance   | 0.314     |
|    learning_rate        | 0.00433   |
|    loss                 | -0.101    |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.0741   |
|    value_loss           | 0.119     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 799      |
|    iterations      | 22       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 181       |
|    ep_rew_mean          | 16.2      |
| time/                   |           |
|    fps                  | 807       |
|    iterations           | 23        |
|    time_elapsed         | 58        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.7584199 |
|    clip_fraction        | 0.64      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.695    |
|    explained_variance   | 0.442     |
|    learning_rate        | 0.00433   |
|    loss                 | -0.0471   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0693   |
|    value_loss           | 0.127     |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 183      |
|    ep_rew_mean          | 16.4     |
| time/                   |          |
|    fps                  | 814      |
|    iterations           | 24       |
|    time_elapsed         | 60       |
|    total_timesteps      | 49152    |
| train/                  |          |
|    approx_kl            | 0.640673 |
|    clip_fraction        | 0.652    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.726   |
|    explained_variance   | 0.368    |
|    learning_rate        | 0.00433  |
|    loss                 | -0.0675  |
|    n_updates            | 230      |
|    policy_gradient_loss | -0.0587  |
|    value_loss           | 0.139    |
--------------------------------------
Eval num_timesteps=50000, episode_reward=20.20 +/- 5.29
Episode length: 217.90 +/- 54.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 218       |
|    mean_reward          | 20.2      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.6373675 |
|    clip_fraction        | 0.634     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.716    |
|    explained_variance   | 0.402     |
|    learning_rate        | 0.00433   |
|    loss                 | -0.107    |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.07     |
|    value_loss           | 0.136     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 801      |
|    iterations      | 25       |
|    time_elapsed    | 63       |
|    total_timesteps | 51200    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 180       |
|    ep_rew_mean          | 16.1      |
| time/                   |           |
|    fps                  | 808       |
|    iterations           | 26        |
|    time_elapsed         | 65        |
|    total_timesteps      | 53248     |
| train/                  |           |
|    approx_kl            | 0.7390954 |
|    clip_fraction        | 0.671     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.754    |
|    explained_variance   | 0.272     |
|    learning_rate        | 0.00433   |
|    loss                 | -0.114    |
|    n_updates            | 250       |
|    policy_gradient_loss | -0.07     |
|    value_loss           | 0.133     |
---------------------------------------
Eval num_timesteps=55000, episode_reward=15.60 +/- 4.86
Episode length: 167.80 +/- 52.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 168        |
|    mean_reward          | 15.6       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.68431795 |
|    clip_fraction        | 0.657      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.741     |
|    explained_variance   | 0.286      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0569    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0555    |
|    value_loss           | 0.148      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 15.7     |
| time/              |          |
|    fps             | 801      |
|    iterations      | 27       |
|    time_elapsed    | 69       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 172        |
|    ep_rew_mean          | 15.4       |
| time/                   |            |
|    fps                  | 807        |
|    iterations           | 28         |
|    time_elapsed         | 71         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.63313854 |
|    clip_fraction        | 0.674      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.759     |
|    explained_variance   | 0.412      |
|    learning_rate        | 0.00433    |
|    loss                 | -0.0614    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0567    |
|    value_loss           | 0.152      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 170       |
|    ep_rew_mean          | 15.2      |
| time/                   |           |
|    fps                  | 812       |
|    iterations           | 29        |
|    time_elapsed         | 73        |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 0.6696581 |
|    clip_fraction        | 0.669     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.734    |
|    explained_variance   | 0.359     |
|    learning_rate        | 0.00433   |
|    loss                 | -0.0859   |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0634   |
|    value_loss           | 0.154     |
---------------------------------------
Eval num_timesteps=60000, episode_reward=21.60 +/- 6.95
Episode length: 218.20 +/- 66.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 218       |
|    mean_reward          | 21.6      |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.6660503 |
|    clip_fraction        | 0.645     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.686    |
|    explained_variance   | 0.218     |
|    learning_rate        | 0.00433   |
|    loss                 | -0.0565   |
|    n_updates            | 290       |
|    policy_gradient_loss | -0.0647   |
|    value_loss           | 0.158     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 802      |
|    iterations      | 30       |
|    time_elapsed    | 76       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:21:19,260] Trial 29 finished with value: 18.7 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.004326818019615788, 'gamma': 0.916542177784925, 'gae_lambda': 0.9539250854049903}. Best is trial 17 with value: 28.1.
Using cuda device
Eval num_timesteps=5000, episode_reward=3.20 +/- 3.19
Episode length: 100.40 +/- 45.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 3.2      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 1117     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=14.50 +/- 4.06
Episode length: 160.70 +/- 42.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 161         |
|    mean_reward          | 14.5        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.016492456 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.00304    |
|    learning_rate        | 0.00185     |
|    loss                 | 0.215       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.535       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=13.10 +/- 3.24
Episode length: 144.70 +/- 28.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 145      |
|    mean_reward     | 13.1     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 9.34     |
| time/              |          |
|    fps             | 884      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=14.00 +/- 3.69
Episode length: 166.80 +/- 42.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 167         |
|    mean_reward          | 14          |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.019884255 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.456       |
|    learning_rate        | 0.00185     |
|    loss                 | 0.25        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.508       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 11.9     |
| time/              |          |
|    fps             | 854      |
|    iterations      | 3        |
|    time_elapsed    | 28       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=12.10 +/- 3.18
Episode length: 150.00 +/- 31.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | 12.1        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.030248534 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.00185     |
|    loss                 | 0.251       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.511       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=14.60 +/- 4.15
Episode length: 177.80 +/- 53.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 14.6     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 816      |
|    iterations      | 4        |
|    time_elapsed    | 40       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=15.80 +/- 6.69
Episode length: 172.80 +/- 75.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 173       |
|    mean_reward          | 15.8      |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0422821 |
|    clip_fraction        | 0.338     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.3      |
|    explained_variance   | 0.675     |
|    learning_rate        | 0.00185   |
|    loss                 | 0.0866    |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0276   |
|    value_loss           | 0.5       |
---------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=17.00 +/- 3.46
Episode length: 189.50 +/- 47.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 791      |
|    iterations      | 5        |
|    time_elapsed    | 51       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=15.40 +/- 5.99
Episode length: 165.30 +/- 70.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | 15.4        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.050820827 |
|    clip_fraction        | 0.363       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.00185     |
|    loss                 | 0.075       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.525       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 792      |
|    iterations      | 6        |
|    time_elapsed    | 61       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=18.30 +/- 6.54
Episode length: 202.90 +/- 70.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 203        |
|    mean_reward          | 18.3       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.06713535 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.00185    |
|    loss                 | 0.151      |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0332    |
|    value_loss           | 0.563      |
----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=16.40 +/- 3.17
Episode length: 177.50 +/- 37.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 16.4     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 7        |
|    time_elapsed    | 73       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=17.10 +/- 2.95
Episode length: 178.70 +/- 33.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 179        |
|    mean_reward          | 17.1       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.07885322 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.735      |
|    learning_rate        | 0.00185    |
|    loss                 | 0.0966     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0332    |
|    value_loss           | 0.504      |
----------------------------------------
Eval num_timesteps=65000, episode_reward=16.00 +/- 3.41
Episode length: 174.80 +/- 36.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 175      |
|    mean_reward     | 16       |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 769      |
|    iterations      | 8        |
|    time_elapsed    | 85       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-16 19:22:48,493] Trial 30 finished with value: 18.9 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.001853992003550411, 'gamma': 0.9834234044498898, 'gae_lambda': 0.9701594537768389}. Best is trial 17 with value: 28.1.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 1226     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 140         |
|    ep_rew_mean          | 8.07        |
| time/                   |             |
|    fps                  | 1115        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008297423 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0311     |
|    learning_rate        | 0.000381    |
|    loss                 | 0.0494      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.182       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=11.90 +/- 4.09
Episode length: 145.10 +/- 51.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 145         |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009222306 |
|    clip_fraction        | 0.095       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.216       |
|    learning_rate        | 0.000381    |
|    loss                 | 0.0185      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.166       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 8.86     |
| time/              |          |
|    fps             | 920      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 153         |
|    ep_rew_mean          | 10.2        |
| time/                   |             |
|    fps                  | 943         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.014860306 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.000381    |
|    loss                 | -0.00453    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0348     |
|    value_loss           | 0.167       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=16.50 +/- 4.39
Episode length: 188.00 +/- 49.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 188         |
|    mean_reward          | 16.5        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.018251231 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.000381    |
|    loss                 | 0.000818    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0423     |
|    value_loss           | 0.162       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 11.3     |
| time/              |          |
|    fps             | 854      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | 12          |
| time/                   |             |
|    fps                  | 878         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.020461513 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.000381    |
|    loss                 | -0.0156     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0456     |
|    value_loss           | 0.165       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | 12.8        |
| time/                   |             |
|    fps                  | 895         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.026369475 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.36        |
|    learning_rate        | 0.000381    |
|    loss                 | -0.0392     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0469     |
|    value_loss           | 0.163       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=14.80 +/- 2.18
Episode length: 155.00 +/- 24.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 155         |
|    mean_reward          | 14.8        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.025772993 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000381    |
|    loss                 | -0.0342     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0547     |
|    value_loss           | 0.168       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 859      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 174         |
|    ep_rew_mean          | 14          |
| time/                   |             |
|    fps                  | 874         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.028537333 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.444       |
|    learning_rate        | 0.000381    |
|    loss                 | -0.0236     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0536     |
|    value_loss           | 0.148       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=20.90 +/- 4.72
Episode length: 215.50 +/- 50.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 216         |
|    mean_reward          | 20.9        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.035726108 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.484       |
|    learning_rate        | 0.000381    |
|    loss                 | -0.0396     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0528     |
|    value_loss           | 0.159       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 834      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 185        |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 848        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.03762925 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.402      |
|    learning_rate        | 0.000381   |
|    loss                 | -0.0407    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.177      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 17.5       |
| time/                   |            |
|    fps                  | 860        |
|    iterations           | 12         |
|    time_elapsed         | 28         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.04053458 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.372      |
|    learning_rate        | 0.000381   |
|    loss                 | -0.0388    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.181      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=20.20 +/- 6.45
Episode length: 199.30 +/- 71.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 20.2        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.043894213 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.984      |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.000381    |
|    loss                 | -0.0242     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0523     |
|    value_loss           | 0.167       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 834      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 17.9        |
| time/                   |             |
|    fps                  | 845         |
|    iterations           | 14          |
|    time_elapsed         | 33          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.049967494 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.993      |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.000381    |
|    loss                 | -0.0198     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0554     |
|    value_loss           | 0.17        |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=26.00 +/- 3.87
Episode length: 248.60 +/- 45.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 249        |
|    mean_reward          | 26         |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.03630062 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.995     |
|    explained_variance   | 0.512      |
|    learning_rate        | 0.000381   |
|    loss                 | -0.0231    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0457    |
|    value_loss           | 0.16       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 817      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 18.8       |
| time/                   |            |
|    fps                  | 827        |
|    iterations           | 16         |
|    time_elapsed         | 39         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.04787845 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.942     |
|    explained_variance   | 0.443      |
|    learning_rate        | 0.000381   |
|    loss                 | -0.00626   |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.195      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 195       |
|    ep_rew_mean          | 19.4      |
| time/                   |           |
|    fps                  | 837       |
|    iterations           | 17        |
|    time_elapsed         | 41        |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.0523645 |
|    clip_fraction        | 0.343     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.911    |
|    explained_variance   | 0.467     |
|    learning_rate        | 0.000381  |
|    loss                 | -0.0202   |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.053    |
|    value_loss           | 0.164     |
---------------------------------------
Eval num_timesteps=35000, episode_reward=24.60 +/- 4.22
Episode length: 230.80 +/- 41.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 231         |
|    mean_reward          | 24.6        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.052075732 |
|    clip_fraction        | 0.347       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.9        |
|    explained_variance   | 0.501       |
|    learning_rate        | 0.000381    |
|    loss                 | -0.0404     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0496     |
|    value_loss           | 0.152       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 816      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 19.8       |
| time/                   |            |
|    fps                  | 825        |
|    iterations           | 19         |
|    time_elapsed         | 47         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.06468257 |
|    clip_fraction        | 0.347      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.827     |
|    explained_variance   | 0.487      |
|    learning_rate        | 0.000381   |
|    loss                 | -0.0287    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0501    |
|    value_loss           | 0.168      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=23.50 +/- 5.82
Episode length: 209.20 +/- 60.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 23.5        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.058328073 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.823      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.000381    |
|    loss                 | -0.0383     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.054      |
|    value_loss           | 0.141       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 810      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | 20.4        |
| time/                   |             |
|    fps                  | 818         |
|    iterations           | 21          |
|    time_elapsed         | 52          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.061941575 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.794      |
|    explained_variance   | 0.503       |
|    learning_rate        | 0.000381    |
|    loss                 | -0.043      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0548     |
|    value_loss           | 0.148       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=25.30 +/- 7.16
Episode length: 241.10 +/- 85.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 25.3        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.062077947 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.000381    |
|    loss                 | -0.0248     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0488     |
|    value_loss           | 0.155       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 20.6     |
| time/              |          |
|    fps             | 801      |
|    iterations      | 22       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 206        |
|    ep_rew_mean          | 21.2       |
| time/                   |            |
|    fps                  | 809        |
|    iterations           | 23         |
|    time_elapsed         | 58         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.07335533 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.701     |
|    explained_variance   | 0.485      |
|    learning_rate        | 0.000381   |
|    loss                 | -0.00422   |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.174      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 211        |
|    ep_rew_mean          | 21.8       |
| time/                   |            |
|    fps                  | 816        |
|    iterations           | 24         |
|    time_elapsed         | 60         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.06392875 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.67      |
|    explained_variance   | 0.445      |
|    learning_rate        | 0.000381   |
|    loss                 | -0.0203    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0434    |
|    value_loss           | 0.185      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=22.60 +/- 7.38
Episode length: 196.20 +/- 72.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 196        |
|    mean_reward          | 22.6       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.06183165 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.622     |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.000381   |
|    loss                 | -0.00703   |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.156      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 22.1     |
| time/              |          |
|    fps             | 805      |
|    iterations      | 25       |
|    time_elapsed    | 63       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 215        |
|    ep_rew_mean          | 22.3       |
| time/                   |            |
|    fps                  | 812        |
|    iterations           | 26         |
|    time_elapsed         | 65         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.05974216 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.614     |
|    explained_variance   | 0.481      |
|    learning_rate        | 0.000381   |
|    loss                 | -0.0112    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0396    |
|    value_loss           | 0.178      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=26.10 +/- 3.96
Episode length: 236.40 +/- 44.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 236        |
|    mean_reward          | 26.1       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.05979226 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.619     |
|    explained_variance   | 0.544      |
|    learning_rate        | 0.000381   |
|    loss                 | -0.0109    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0383    |
|    value_loss           | 0.154      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 22.1     |
| time/              |          |
|    fps             | 799      |
|    iterations      | 27       |
|    time_elapsed    | 69       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 216       |
|    ep_rew_mean          | 22.7      |
| time/                   |           |
|    fps                  | 805       |
|    iterations           | 28        |
|    time_elapsed         | 71        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.0702981 |
|    clip_fraction        | 0.297     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.638    |
|    explained_variance   | 0.542     |
|    learning_rate        | 0.000381  |
|    loss                 | -0.0225   |
|    n_updates            | 270       |
|    policy_gradient_loss | -0.0473   |
|    value_loss           | 0.151     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 218         |
|    ep_rew_mean          | 23          |
| time/                   |             |
|    fps                  | 811         |
|    iterations           | 29          |
|    time_elapsed         | 73          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.077624835 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.621      |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.000381    |
|    loss                 | -0.00919    |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0469     |
|    value_loss           | 0.148       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=26.50 +/- 6.00
Episode length: 245.50 +/- 68.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 246        |
|    mean_reward          | 26.5       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.08143854 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.641     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.000381   |
|    loss                 | -0.0261    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0474    |
|    value_loss           | 0.168      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 23.3     |
| time/              |          |
|    fps             | 799      |
|    iterations      | 30       |
|    time_elapsed    | 76       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:24:07,732] Trial 31 finished with value: 29.6 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00038097029254553663, 'gamma': 0.9360172856913327, 'gae_lambda': 0.9290500244305719}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 8.07     |
| time/              |          |
|    fps             | 1228     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 144         |
|    ep_rew_mean          | 8.36        |
| time/                   |             |
|    fps                  | 1115        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009191687 |
|    clip_fraction        | 0.0501      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0339     |
|    learning_rate        | 0.000394    |
|    loss                 | 0.0102      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.188       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=11.90 +/- 6.11
Episode length: 161.00 +/- 48.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 161         |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010138438 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.000394    |
|    loss                 | 0.0113      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.166       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 9.07     |
| time/              |          |
|    fps             | 910      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 151         |
|    ep_rew_mean          | 9.59        |
| time/                   |             |
|    fps                  | 936         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.014445836 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000394    |
|    loss                 | 0.048       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0305     |
|    value_loss           | 0.186       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=18.50 +/- 6.65
Episode length: 200.80 +/- 76.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 201        |
|    mean_reward          | 18.5       |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01871951 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.000394   |
|    loss                 | -0.00423   |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0421    |
|    value_loss           | 0.181      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 845      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | 11.3        |
| time/                   |             |
|    fps                  | 870         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.019926075 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0275     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 0.168       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | 11.8        |
| time/                   |             |
|    fps                  | 889         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.020676393 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.22        |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0175     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0419     |
|    value_loss           | 0.184       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=18.60 +/- 5.43
Episode length: 184.30 +/- 50.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 184         |
|    mean_reward          | 18.6        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.024953656 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0218     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0511     |
|    value_loss           | 0.159       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 12.3     |
| time/              |          |
|    fps             | 840      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 172         |
|    ep_rew_mean          | 13.1        |
| time/                   |             |
|    fps                  | 856         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.027458496 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0196     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0491     |
|    value_loss           | 0.172       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=21.60 +/- 5.33
Episode length: 213.30 +/- 57.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 213         |
|    mean_reward          | 21.6        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.030209206 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0378     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0522     |
|    value_loss           | 0.165       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 821      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 181         |
|    ep_rew_mean          | 15          |
| time/                   |             |
|    fps                  | 835         |
|    iterations           | 11          |
|    time_elapsed         | 26          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.035938565 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.00645    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0468     |
|    value_loss           | 0.18        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | 15.9        |
| time/                   |             |
|    fps                  | 848         |
|    iterations           | 12          |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.040183373 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.51        |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0179     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.055      |
|    value_loss           | 0.154       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=18.50 +/- 4.20
Episode length: 167.70 +/- 42.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 168         |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.044238582 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.504       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0371     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0508     |
|    value_loss           | 0.167       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 16.8     |
| time/              |          |
|    fps             | 829      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 17.3        |
| time/                   |             |
|    fps                  | 840         |
|    iterations           | 14          |
|    time_elapsed         | 34          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.044455387 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.992      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0274     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0492     |
|    value_loss           | 0.167       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=22.00 +/- 6.71
Episode length: 196.40 +/- 60.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 196        |
|    mean_reward          | 22         |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.04866842 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.938     |
|    explained_variance   | 0.46       |
|    learning_rate        | 0.000394   |
|    loss                 | -0.0191    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0511    |
|    value_loss           | 0.164      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.7     |
| time/              |          |
|    fps             | 820      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 18.1       |
| time/                   |            |
|    fps                  | 830        |
|    iterations           | 16         |
|    time_elapsed         | 39         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.04814314 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.924     |
|    explained_variance   | 0.542      |
|    learning_rate        | 0.000394   |
|    loss                 | -0.0431    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0511    |
|    value_loss           | 0.162      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 197        |
|    ep_rew_mean          | 19.1       |
| time/                   |            |
|    fps                  | 839        |
|    iterations           | 17         |
|    time_elapsed         | 41         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.05984008 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.877     |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.000394   |
|    loss                 | -0.0342    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.053     |
|    value_loss           | 0.161      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=25.00 +/- 6.20
Episode length: 243.40 +/- 57.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 243         |
|    mean_reward          | 25          |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.051183455 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.849      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0414     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0416     |
|    value_loss           | 0.152       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 19.3     |
| time/              |          |
|    fps             | 817      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 197         |
|    ep_rew_mean          | 19.6        |
| time/                   |             |
|    fps                  | 826         |
|    iterations           | 19          |
|    time_elapsed         | 47          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.060185935 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.848      |
|    explained_variance   | 0.538       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0163     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0512     |
|    value_loss           | 0.177       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=23.00 +/- 5.10
Episode length: 211.40 +/- 51.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | 23          |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.053497236 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.831      |
|    explained_variance   | 0.531       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0335     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0514     |
|    value_loss           | 0.163       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 810      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 204         |
|    ep_rew_mean          | 20.8        |
| time/                   |             |
|    fps                  | 818         |
|    iterations           | 21          |
|    time_elapsed         | 52          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.054344833 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.814      |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0358     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.048      |
|    value_loss           | 0.153       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=23.20 +/- 8.20
Episode length: 215.80 +/- 88.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 23.2       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.06721637 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.802     |
|    explained_variance   | 0.551      |
|    learning_rate        | 0.000394   |
|    loss                 | -0.0433    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.17       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | 21.2     |
| time/              |          |
|    fps             | 804      |
|    iterations      | 22       |
|    time_elapsed    | 55       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 211         |
|    ep_rew_mean          | 21.8        |
| time/                   |             |
|    fps                  | 812         |
|    iterations           | 23          |
|    time_elapsed         | 57          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.057649173 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0394     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0428     |
|    value_loss           | 0.146       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 213         |
|    ep_rew_mean          | 22.1        |
| time/                   |             |
|    fps                  | 819         |
|    iterations           | 24          |
|    time_elapsed         | 59          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.073127225 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0143     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0413     |
|    value_loss           | 0.163       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=24.20 +/- 6.55
Episode length: 216.20 +/- 62.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 24.2       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.07306793 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.698     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.000394   |
|    loss                 | -0.0164    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0434    |
|    value_loss           | 0.158      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | 22       |
| time/              |          |
|    fps             | 806      |
|    iterations      | 25       |
|    time_elapsed    | 63       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 22.3       |
| time/                   |            |
|    fps                  | 813        |
|    iterations           | 26         |
|    time_elapsed         | 65         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.07698527 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.692     |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.000394   |
|    loss                 | -0.0121    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0485    |
|    value_loss           | 0.168      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=23.20 +/- 5.08
Episode length: 217.10 +/- 54.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 217        |
|    mean_reward          | 23.2       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.07782405 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.633     |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.000394   |
|    loss                 | -0.0245    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.154      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 22       |
| time/              |          |
|    fps             | 801      |
|    iterations      | 27       |
|    time_elapsed    | 68       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | 22.2        |
| time/                   |             |
|    fps                  | 808         |
|    iterations           | 28          |
|    time_elapsed         | 70          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.075877346 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.674      |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.00968    |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0506     |
|    value_loss           | 0.163       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 217        |
|    ep_rew_mean          | 22.5       |
| time/                   |            |
|    fps                  | 813        |
|    iterations           | 29         |
|    time_elapsed         | 72         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.08380215 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.622     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.000394   |
|    loss                 | -0.0257    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0419    |
|    value_loss           | 0.15       |
----------------------------------------
Eval num_timesteps=60000, episode_reward=25.40 +/- 4.52
Episode length: 222.40 +/- 41.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 222         |
|    mean_reward          | 25.4        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.088370726 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.000394    |
|    loss                 | -0.0294     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0428     |
|    value_loss           | 0.152       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 22.6     |
| time/              |          |
|    fps             | 803      |
|    iterations      | 30       |
|    time_elapsed    | 76       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:25:26,278] Trial 32 finished with value: 23.6 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0003939977455602532, 'gamma': 0.93712366324944, 'gae_lambda': 0.9308242912449338}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 8.14     |
| time/              |          |
|    fps             | 1233     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 133         |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 1116        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010382818 |
|    clip_fraction        | 0.0724      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0799     |
|    learning_rate        | 0.000802    |
|    loss                 | 0.0229      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.192       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=15.20 +/- 4.21
Episode length: 179.00 +/- 39.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 179         |
|    mean_reward          | 15.2        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.024076022 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.159       |
|    learning_rate        | 0.000802    |
|    loss                 | 0.00356     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.148       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 8.24     |
| time/              |          |
|    fps             | 893      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 139        |
|    ep_rew_mean          | 9.17       |
| time/                   |            |
|    fps                  | 921        |
|    iterations           | 4          |
|    time_elapsed         | 8          |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.03641659 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.204      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0638    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0601    |
|    value_loss           | 0.14       |
----------------------------------------
Eval num_timesteps=10000, episode_reward=18.60 +/- 5.16
Episode length: 203.20 +/- 52.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 18.6        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.041464325 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.245       |
|    learning_rate        | 0.000802    |
|    loss                 | -0.0606     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0627     |
|    value_loss           | 0.148       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    fps             | 834      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 149         |
|    ep_rew_mean          | 10.8        |
| time/                   |             |
|    fps                  | 860         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.054874904 |
|    clip_fraction        | 0.405       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.000802    |
|    loss                 | -0.0356     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0687     |
|    value_loss           | 0.128       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 154        |
|    ep_rew_mean          | 11.6       |
| time/                   |            |
|    fps                  | 880        |
|    iterations           | 7          |
|    time_elapsed         | 16         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.06332404 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0779    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0738    |
|    value_loss           | 0.128      |
----------------------------------------
Eval num_timesteps=15000, episode_reward=20.90 +/- 4.18
Episode length: 214.10 +/- 43.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 214        |
|    mean_reward          | 20.9       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.07243508 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0603    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0746    |
|    value_loss           | 0.138      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 830      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | 13.1       |
| time/                   |            |
|    fps                  | 848        |
|    iterations           | 9          |
|    time_elapsed         | 21         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.08925897 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.356      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0583    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0703    |
|    value_loss           | 0.131      |
----------------------------------------
Eval num_timesteps=20000, episode_reward=18.00 +/- 5.37
Episode length: 185.20 +/- 50.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 185        |
|    mean_reward          | 18         |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.09151616 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.477      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0745    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0675    |
|    value_loss           | 0.109      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 819      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | 15.5       |
| time/                   |            |
|    fps                  | 834        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.10963312 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.484      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0698    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0704    |
|    value_loss           | 0.119      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | 16.7        |
| time/                   |             |
|    fps                  | 847         |
|    iterations           | 12          |
|    time_elapsed         | 29          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.104083374 |
|    clip_fraction        | 0.489       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.995      |
|    explained_variance   | 0.477       |
|    learning_rate        | 0.000802    |
|    loss                 | -0.083      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0692     |
|    value_loss           | 0.122       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=19.10 +/- 6.24
Episode length: 182.90 +/- 72.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | 19.1        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.096677676 |
|    clip_fraction        | 0.46        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.946      |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.000802    |
|    loss                 | -0.0662     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0667     |
|    value_loss           | 0.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 193        |
|    ep_rew_mean          | 18         |
| time/                   |            |
|    fps                  | 837        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.13249838 |
|    clip_fraction        | 0.486      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.894     |
|    explained_variance   | 0.461      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0797    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0705    |
|    value_loss           | 0.118      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=22.70 +/- 6.07
Episode length: 217.90 +/- 68.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 218        |
|    mean_reward          | 22.7       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.12260923 |
|    clip_fraction        | 0.486      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.903     |
|    explained_variance   | 0.499      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0507    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0709    |
|    value_loss           | 0.116      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 813      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | 19.2       |
| time/                   |            |
|    fps                  | 823        |
|    iterations           | 16         |
|    time_elapsed         | 39         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.15941226 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.846     |
|    explained_variance   | 0.478      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0611    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0613    |
|    value_loss           | 0.125      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 202        |
|    ep_rew_mean          | 19.6       |
| time/                   |            |
|    fps                  | 832        |
|    iterations           | 17         |
|    time_elapsed         | 41         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.12717807 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.842     |
|    explained_variance   | 0.464      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0657    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.062     |
|    value_loss           | 0.116      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=23.10 +/- 3.75
Episode length: 223.00 +/- 48.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 223        |
|    mean_reward          | 23.1       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.15142922 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.81      |
|    explained_variance   | 0.451      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0834    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0717    |
|    value_loss           | 0.109      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 19.8     |
| time/              |          |
|    fps             | 813      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 20.5       |
| time/                   |            |
|    fps                  | 822        |
|    iterations           | 19         |
|    time_elapsed         | 47         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.16925141 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.801     |
|    explained_variance   | 0.401      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0865    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0655    |
|    value_loss           | 0.123      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=23.70 +/- 7.21
Episode length: 213.30 +/- 78.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 213       |
|    mean_reward          | 23.7      |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.1848205 |
|    clip_fraction        | 0.486     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.763    |
|    explained_variance   | 0.437     |
|    learning_rate        | 0.000802  |
|    loss                 | -0.072    |
|    n_updates            | 190       |
|    policy_gradient_loss | -0.0651   |
|    value_loss           | 0.115     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 807      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 21         |
| time/                   |            |
|    fps                  | 815        |
|    iterations           | 21         |
|    time_elapsed         | 52         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.16639665 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.764     |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.068     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0624    |
|    value_loss           | 0.121      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=23.70 +/- 4.65
Episode length: 214.40 +/- 50.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 214        |
|    mean_reward          | 23.7       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.19200715 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.695     |
|    explained_variance   | 0.485      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0715    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0595    |
|    value_loss           | 0.126      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 801      |
|    iterations      | 22       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 202        |
|    ep_rew_mean          | 20.9       |
| time/                   |            |
|    fps                  | 809        |
|    iterations           | 23         |
|    time_elapsed         | 58         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.16393581 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.481      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0778    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0613    |
|    value_loss           | 0.11       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | 20.8       |
| time/                   |            |
|    fps                  | 816        |
|    iterations           | 24         |
|    time_elapsed         | 60         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.18493593 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.512      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0619    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0611    |
|    value_loss           | 0.119      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=21.70 +/- 4.80
Episode length: 209.00 +/- 48.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 209        |
|    mean_reward          | 21.7       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.21658185 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.655     |
|    explained_variance   | 0.443      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0666    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0581    |
|    value_loss           | 0.126      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 805      |
|    iterations      | 25       |
|    time_elapsed    | 63       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 20.8       |
| time/                   |            |
|    fps                  | 811        |
|    iterations           | 26         |
|    time_elapsed         | 65         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.23390117 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.654     |
|    explained_variance   | 0.454      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0827    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.125      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=23.30 +/- 5.59
Episode length: 209.60 +/- 57.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 210       |
|    mean_reward          | 23.3      |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.2366478 |
|    clip_fraction        | 0.437     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.574    |
|    explained_variance   | 0.484     |
|    learning_rate        | 0.000802  |
|    loss                 | -0.0411   |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.0506   |
|    value_loss           | 0.125     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 21.2     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 27       |
|    time_elapsed    | 69       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 202        |
|    ep_rew_mean          | 21.2       |
| time/                   |            |
|    fps                  | 807        |
|    iterations           | 28         |
|    time_elapsed         | 71         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.22235602 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.592     |
|    explained_variance   | 0.525      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0591    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.121      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 21.4       |
| time/                   |            |
|    fps                  | 812        |
|    iterations           | 29         |
|    time_elapsed         | 73         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.21684751 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.559     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0629    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.114      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=23.80 +/- 4.92
Episode length: 224.30 +/- 52.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 23.8       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.24834242 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.564     |
|    explained_variance   | 0.469      |
|    learning_rate        | 0.000802   |
|    loss                 | -0.0261    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.057     |
|    value_loss           | 0.11       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 21.7     |
| time/              |          |
|    fps             | 802      |
|    iterations      | 30       |
|    time_elapsed    | 76       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:26:44,654] Trial 33 finished with value: 18.8 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0008024462256112292, 'gamma': 0.9179264269949944, 'gae_lambda': 0.9134258307295814}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 9.75     |
| time/              |          |
|    fps             | 1232     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 164          |
|    ep_rew_mean          | 9.83         |
| time/                   |              |
|    fps                  | 1123         |
|    iterations           | 2            |
|    time_elapsed         | 3            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0066301795 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.02        |
|    learning_rate        | 0.000287     |
|    loss                 | 0.0649       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0116      |
|    value_loss           | 0.276        |
------------------------------------------
Eval num_timesteps=5000, episode_reward=8.60 +/- 2.97
Episode length: 141.80 +/- 51.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 142         |
|    mean_reward          | 8.6         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009446364 |
|    clip_fraction        | 0.0754      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0533      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.22        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 933      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 169         |
|    ep_rew_mean          | 10.9        |
| time/                   |             |
|    fps                  | 953         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012844514 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000287    |
|    loss                 | 0.00171     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 0.22        |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=14.80 +/- 5.04
Episode length: 169.90 +/- 58.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 170        |
|    mean_reward          | 14.8       |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01386689 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.417      |
|    learning_rate        | 0.000287   |
|    loss                 | 0.0268     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0312    |
|    value_loss           | 0.208      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 11.5     |
| time/              |          |
|    fps             | 869      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 169         |
|    ep_rew_mean          | 12          |
| time/                   |             |
|    fps                  | 888         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.017013546 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.364       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0128      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.222       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | 12.7        |
| time/                   |             |
|    fps                  | 904         |
|    iterations           | 7           |
|    time_elapsed         | 15          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.015046609 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0248      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.222       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=15.30 +/- 4.36
Episode length: 166.60 +/- 49.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 167        |
|    mean_reward          | 15.3       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01755397 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.391      |
|    learning_rate        | 0.000287   |
|    loss                 | 0.0338     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0391    |
|    value_loss           | 0.219      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 12.9     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 174         |
|    ep_rew_mean          | 13.7        |
| time/                   |             |
|    fps                  | 877         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.017782101 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.000287    |
|    loss                 | 0.00284     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0383     |
|    value_loss           | 0.208       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=14.70 +/- 3.10
Episode length: 147.40 +/- 29.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 147         |
|    mean_reward          | 14.7        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.029865175 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.48        |
|    learning_rate        | 0.000287    |
|    loss                 | -0.00896    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0432     |
|    value_loss           | 0.201       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 852      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | 15.1       |
| time/                   |            |
|    fps                  | 865        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.02626869 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.44       |
|    learning_rate        | 0.000287   |
|    loss                 | 0.0178     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0377    |
|    value_loss           | 0.21       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | 15.9        |
| time/                   |             |
|    fps                  | 875         |
|    iterations           | 12          |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.024794694 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.00118     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0432     |
|    value_loss           | 0.203       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=18.20 +/- 2.44
Episode length: 191.60 +/- 28.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 192         |
|    mean_reward          | 18.2        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.023319885 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.00612     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0433     |
|    value_loss           | 0.204       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 849      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 16.9       |
| time/                   |            |
|    fps                  | 859        |
|    iterations           | 14         |
|    time_elapsed         | 33         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.03209257 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.000287   |
|    loss                 | -0.00694   |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.217      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=20.50 +/- 6.33
Episode length: 205.70 +/- 75.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | 20.5        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.034426577 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.00114     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0483     |
|    value_loss           | 0.19        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 835      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 17.5       |
| time/                   |            |
|    fps                  | 844        |
|    iterations           | 16         |
|    time_elapsed         | 38         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.03033043 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.968     |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.000287   |
|    loss                 | -0.0157    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0479    |
|    value_loss           | 0.18       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | 18.1        |
| time/                   |             |
|    fps                  | 853         |
|    iterations           | 17          |
|    time_elapsed         | 40          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.028268378 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.978      |
|    explained_variance   | 0.468       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0216      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0474     |
|    value_loss           | 0.211       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=22.70 +/- 6.02
Episode length: 221.80 +/- 66.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 222         |
|    mean_reward          | 22.7        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.030022241 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.922      |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.000287    |
|    loss                 | -0.00902    |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0444     |
|    value_loss           | 0.193       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 832      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 187        |
|    ep_rew_mean          | 18.9       |
| time/                   |            |
|    fps                  | 840        |
|    iterations           | 19         |
|    time_elapsed         | 46         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.03654701 |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.873     |
|    explained_variance   | 0.481      |
|    learning_rate        | 0.000287   |
|    loss                 | 0.00713    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0461    |
|    value_loss           | 0.211      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=21.90 +/- 6.09
Episode length: 202.60 +/- 55.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 203        |
|    mean_reward          | 21.9       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.03550665 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.893     |
|    explained_variance   | 0.555      |
|    learning_rate        | 0.000287   |
|    loss                 | -0.0117    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.039     |
|    value_loss           | 0.217      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 19.3     |
| time/              |          |
|    fps             | 823      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 191         |
|    ep_rew_mean          | 19.6        |
| time/                   |             |
|    fps                  | 831         |
|    iterations           | 21          |
|    time_elapsed         | 51          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.035080142 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.821      |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.000287    |
|    loss                 | -0.00649    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0412     |
|    value_loss           | 0.194       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=28.40 +/- 5.77
Episode length: 256.50 +/- 67.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 256        |
|    mean_reward          | 28.4       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.03271903 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.813     |
|    explained_variance   | 0.501      |
|    learning_rate        | 0.000287   |
|    loss                 | 0.0288     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.227      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 812      |
|    iterations      | 22       |
|    time_elapsed    | 55       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 193        |
|    ep_rew_mean          | 20.2       |
| time/                   |            |
|    fps                  | 819        |
|    iterations           | 23         |
|    time_elapsed         | 57         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.03877356 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.785     |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.000287   |
|    loss                 | -0.0172    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0461    |
|    value_loss           | 0.208      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 196        |
|    ep_rew_mean          | 20.7       |
| time/                   |            |
|    fps                  | 826        |
|    iterations           | 24         |
|    time_elapsed         | 59         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.03381635 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.757     |
|    explained_variance   | 0.549      |
|    learning_rate        | 0.000287   |
|    loss                 | -0.00809   |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0423    |
|    value_loss           | 0.219      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=23.60 +/- 7.38
Episode length: 210.80 +/- 67.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | 23.6        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.037725024 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.746      |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0285      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0413     |
|    value_loss           | 0.244       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 21.1     |
| time/              |          |
|    fps             | 814      |
|    iterations      | 25       |
|    time_elapsed    | 62       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 201         |
|    ep_rew_mean          | 21.5        |
| time/                   |             |
|    fps                  | 820         |
|    iterations           | 26          |
|    time_elapsed         | 64          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.037767604 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.749      |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.0169      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0419     |
|    value_loss           | 0.233       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=26.00 +/- 4.56
Episode length: 238.20 +/- 61.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 238        |
|    mean_reward          | 26         |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.04030119 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.754     |
|    explained_variance   | 0.498      |
|    learning_rate        | 0.000287   |
|    loss                 | -0.0037    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0394    |
|    value_loss           | 0.219      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 21.6     |
| time/              |          |
|    fps             | 807      |
|    iterations      | 27       |
|    time_elapsed    | 68       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 204       |
|    ep_rew_mean          | 21.9      |
| time/                   |           |
|    fps                  | 813       |
|    iterations           | 28        |
|    time_elapsed         | 70        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.0384874 |
|    clip_fraction        | 0.268     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.72     |
|    explained_variance   | 0.557     |
|    learning_rate        | 0.000287  |
|    loss                 | 0.00693   |
|    n_updates            | 270       |
|    policy_gradient_loss | -0.0418   |
|    value_loss           | 0.226     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | 22.2        |
| time/                   |             |
|    fps                  | 818         |
|    iterations           | 29          |
|    time_elapsed         | 72          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.042886846 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.000294    |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0426     |
|    value_loss           | 0.21        |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=24.10 +/- 3.83
Episode length: 222.20 +/- 37.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 222         |
|    mean_reward          | 24.1        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.043537196 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.632      |
|    explained_variance   | 0.586       |
|    learning_rate        | 0.000287    |
|    loss                 | 0.00355     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0386     |
|    value_loss           | 0.208       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 21.9     |
| time/              |          |
|    fps             | 807      |
|    iterations      | 30       |
|    time_elapsed    | 76       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:28:02,884] Trial 34 finished with value: 25.0 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00028656621378345717, 'gamma': 0.9452343351737708, 'gae_lambda': 0.9524069240705034}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 7.8      |
| time/              |          |
|    fps             | 1231     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 132          |
|    ep_rew_mean          | 7.81         |
| time/                   |              |
|    fps                  | 912          |
|    iterations           | 2            |
|    time_elapsed         | 4            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0121786725 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.002       |
|    learning_rate        | 6.54e-05     |
|    loss                 | 0.0133       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0124      |
|    value_loss           | 0.158        |
------------------------------------------
Eval num_timesteps=5000, episode_reward=9.60 +/- 2.54
Episode length: 128.70 +/- 33.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 129         |
|    mean_reward          | 9.6         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.007000345 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.209       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0187      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00534    |
|    value_loss           | 0.167       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 8.16     |
| time/              |          |
|    fps             | 749      |
|    iterations      | 3        |
|    time_elapsed    | 8        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 139         |
|    ep_rew_mean          | 8.6         |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 4           |
|    time_elapsed         | 11          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011707291 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.303       |
|    learning_rate        | 6.54e-05    |
|    loss                 | -0.0208     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.161       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=9.20 +/- 1.17
Episode length: 125.40 +/- 13.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 9.2         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.006438135 |
|    clip_fraction        | 0.0483      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.428       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0371      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.135       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 9.29     |
| time/              |          |
|    fps             | 693      |
|    iterations      | 5        |
|    time_elapsed    | 14       |
|    total_timesteps | 10240    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 146       |
|    ep_rew_mean          | 9.37      |
| time/                   |           |
|    fps                  | 698       |
|    iterations           | 6         |
|    time_elapsed         | 17        |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 0.0100633 |
|    clip_fraction        | 0.0908    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.24     |
|    explained_variance   | 0.398     |
|    learning_rate        | 6.54e-05  |
|    loss                 | 0.0327    |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0139   |
|    value_loss           | 0.165     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 151        |
|    ep_rew_mean          | 10.2       |
| time/                   |            |
|    fps                  | 701        |
|    iterations           | 7          |
|    time_elapsed         | 20         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.01115508 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.496      |
|    learning_rate        | 6.54e-05   |
|    loss                 | 0.0296     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.156      |
----------------------------------------
Eval num_timesteps=15000, episode_reward=10.00 +/- 2.19
Episode length: 134.40 +/- 23.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 10          |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.008164025 |
|    clip_fraction        | 0.0852      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.424       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0246      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.165       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 11       |
| time/              |          |
|    fps             | 676      |
|    iterations      | 8        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | 12.2         |
| time/                   |              |
|    fps                  | 681          |
|    iterations           | 9            |
|    time_elapsed         | 27           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0083446745 |
|    clip_fraction        | 0.0788       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.472        |
|    learning_rate        | 6.54e-05     |
|    loss                 | 0.0297       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.0159      |
|    value_loss           | 0.165        |
------------------------------------------
Eval num_timesteps=20000, episode_reward=9.50 +/- 2.97
Episode length: 112.50 +/- 29.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 9.5         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.005578272 |
|    clip_fraction        | 0.0791      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.971      |
|    explained_variance   | 0.405       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.05        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.168       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 668      |
|    iterations      | 10       |
|    time_elapsed    | 30       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 172         |
|    ep_rew_mean          | 14.2        |
| time/                   |             |
|    fps                  | 673         |
|    iterations           | 11          |
|    time_elapsed         | 33          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.010801373 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.937      |
|    explained_variance   | 0.369       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0225      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.167       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 176         |
|    ep_rew_mean          | 15          |
| time/                   |             |
|    fps                  | 677         |
|    iterations           | 12          |
|    time_elapsed         | 36          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009172518 |
|    clip_fraction        | 0.0967      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.954      |
|    explained_variance   | 0.502       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0186      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.162       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=14.20 +/- 5.60
Episode length: 166.40 +/- 67.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 14.2        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.005993044 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.924      |
|    explained_variance   | 0.462       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.00349     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.175       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 15.5     |
| time/              |          |
|    fps             | 662      |
|    iterations      | 13       |
|    time_elapsed    | 40       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 176         |
|    ep_rew_mean          | 16.1        |
| time/                   |             |
|    fps                  | 666         |
|    iterations           | 14          |
|    time_elapsed         | 43          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.011221163 |
|    clip_fraction        | 0.0882      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.878      |
|    explained_variance   | 0.439       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0708      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.165       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=14.90 +/- 4.61
Episode length: 169.20 +/- 54.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 14.9        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.009155162 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.789      |
|    explained_variance   | 0.517       |
|    learning_rate        | 6.54e-05    |
|    loss                 | -0.00617    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.157       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 653      |
|    iterations      | 15       |
|    time_elapsed    | 47       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 180          |
|    ep_rew_mean          | 17.2         |
| time/                   |              |
|    fps                  | 657          |
|    iterations           | 16           |
|    time_elapsed         | 49           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0053059175 |
|    clip_fraction        | 0.0675       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.734       |
|    explained_variance   | 0.536        |
|    learning_rate        | 6.54e-05     |
|    loss                 | 0.0453       |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.0127      |
|    value_loss           | 0.151        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | 17.6        |
| time/                   |             |
|    fps                  | 661         |
|    iterations           | 17          |
|    time_elapsed         | 52          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.011879837 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.79       |
|    explained_variance   | 0.526       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0852      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.158       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=17.10 +/- 4.95
Episode length: 176.70 +/- 55.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 177         |
|    mean_reward          | 17.1        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.009381782 |
|    clip_fraction        | 0.0972      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.818      |
|    explained_variance   | 0.489       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.061       |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.162       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 650      |
|    iterations      | 18       |
|    time_elapsed    | 56       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | 17.5        |
| time/                   |             |
|    fps                  | 654         |
|    iterations           | 19          |
|    time_elapsed         | 59          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.008715011 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.87       |
|    explained_variance   | 0.563       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0715      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.143       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=16.10 +/- 3.67
Episode length: 168.10 +/- 36.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 168         |
|    mean_reward          | 16.1        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.008804154 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.525       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.015       |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.142       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 645      |
|    iterations      | 20       |
|    time_elapsed    | 63       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 177         |
|    ep_rew_mean          | 17.5        |
| time/                   |             |
|    fps                  | 649         |
|    iterations           | 21          |
|    time_elapsed         | 66          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.013062349 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.521       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.00117     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.158       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=17.80 +/- 5.78
Episode length: 179.30 +/- 68.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 179        |
|    mean_reward          | 17.8       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.00931778 |
|    clip_fraction        | 0.0931     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.534      |
|    learning_rate        | 6.54e-05   |
|    loss                 | 0.0121     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.023     |
|    value_loss           | 0.158      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 640      |
|    iterations      | 22       |
|    time_elapsed    | 70       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 179         |
|    ep_rew_mean          | 17.7        |
| time/                   |             |
|    fps                  | 644         |
|    iterations           | 23          |
|    time_elapsed         | 73          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.012519669 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.771      |
|    explained_variance   | 0.534       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0333      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.156       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 182         |
|    ep_rew_mean          | 18.1        |
| time/                   |             |
|    fps                  | 646         |
|    iterations           | 24          |
|    time_elapsed         | 75          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.009662895 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.756      |
|    explained_variance   | 0.534       |
|    learning_rate        | 6.54e-05    |
|    loss                 | -0.000372   |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.15        |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=21.10 +/- 4.06
Episode length: 208.80 +/- 37.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 209        |
|    mean_reward          | 21.1       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.01207543 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.509      |
|    learning_rate        | 6.54e-05   |
|    loss                 | 0.0361     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.021     |
|    value_loss           | 0.152      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 18.1     |
| time/              |          |
|    fps             | 637      |
|    iterations      | 25       |
|    time_elapsed    | 80       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | 18.1        |
| time/                   |             |
|    fps                  | 640         |
|    iterations           | 26          |
|    time_elapsed         | 83          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.013642605 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.501       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.059       |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.156       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=20.30 +/- 6.54
Episode length: 203.20 +/- 72.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 20.3        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.013345412 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.574       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.00237     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.16        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 633      |
|    iterations      | 27       |
|    time_elapsed    | 87       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 18.9        |
| time/                   |             |
|    fps                  | 635         |
|    iterations           | 28          |
|    time_elapsed         | 90          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.011049235 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.576       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.00603     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.153       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 19.5       |
| time/                   |            |
|    fps                  | 638        |
|    iterations           | 29         |
|    time_elapsed         | 93         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.01209998 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.622     |
|    explained_variance   | 0.523      |
|    learning_rate        | 6.54e-05   |
|    loss                 | 0.0199     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0195    |
|    value_loss           | 0.147      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=22.50 +/- 5.87
Episode length: 207.20 +/- 53.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | 22.5        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.014555916 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.633      |
|    explained_variance   | 0.528       |
|    learning_rate        | 6.54e-05    |
|    loss                 | 0.0282      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.153       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    fps             | 631      |
|    iterations      | 30       |
|    time_elapsed    | 97       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:29:43,004] Trial 35 finished with value: 21.1 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 6.537971772867228e-05, 'gamma': 0.9308758480007022, 'gae_lambda': 0.8969198355723582}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 7.36     |
| time/              |          |
|    fps             | 1226     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 133        |
|    ep_rew_mean          | 7          |
| time/                   |            |
|    fps                  | 1108       |
|    iterations           | 2          |
|    time_elapsed         | 3          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.01269819 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -0.0686    |
|    learning_rate        | 0.000909   |
|    loss                 | 0.0236     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 0.168      |
----------------------------------------
Eval num_timesteps=5000, episode_reward=10.80 +/- 4.73
Episode length: 151.10 +/- 55.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 151         |
|    mean_reward          | 10.8        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.023514755 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.123       |
|    learning_rate        | 0.000909    |
|    loss                 | 0.0108      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.162       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 8.46     |
| time/              |          |
|    fps             | 911      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 151       |
|    ep_rew_mean          | 9.57      |
| time/                   |           |
|    fps                  | 933       |
|    iterations           | 4         |
|    time_elapsed         | 8         |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0351308 |
|    clip_fraction        | 0.28      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.33     |
|    explained_variance   | 0.24      |
|    learning_rate        | 0.000909  |
|    loss                 | -0.0502   |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.0505   |
|    value_loss           | 0.138     |
---------------------------------------
Eval num_timesteps=10000, episode_reward=15.00 +/- 3.85
Episode length: 164.10 +/- 38.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 164        |
|    mean_reward          | 15         |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.03893667 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.266      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0413    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.15       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 858      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 160        |
|    ep_rew_mean          | 11.2       |
| time/                   |            |
|    fps                  | 880        |
|    iterations           | 6          |
|    time_elapsed         | 13         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.05439557 |
|    clip_fraction        | 0.392      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.253      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0505    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0638    |
|    value_loss           | 0.147      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 163       |
|    ep_rew_mean          | 11.9      |
| time/                   |           |
|    fps                  | 896       |
|    iterations           | 7         |
|    time_elapsed         | 15        |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 0.0568937 |
|    clip_fraction        | 0.409     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.17     |
|    explained_variance   | 0.236     |
|    learning_rate        | 0.000909  |
|    loss                 | -0.0502   |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0601   |
|    value_loss           | 0.137     |
---------------------------------------
Eval num_timesteps=15000, episode_reward=24.40 +/- 5.08
Episode length: 241.70 +/- 54.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 242       |
|    mean_reward          | 24.4      |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.0816994 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.11     |
|    explained_variance   | 0.279     |
|    learning_rate        | 0.000909  |
|    loss                 | -0.039    |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.0632   |
|    value_loss           | 0.132     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 12.7     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 13.9       |
| time/                   |            |
|    fps                  | 849        |
|    iterations           | 9          |
|    time_elapsed         | 21         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.08752996 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.301      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0209    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.062     |
|    value_loss           | 0.141      |
----------------------------------------
Eval num_timesteps=20000, episode_reward=18.00 +/- 5.33
Episode length: 175.50 +/- 51.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 176        |
|    mean_reward          | 18         |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.09828229 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.997     |
|    explained_variance   | 0.356      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0693    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0628    |
|    value_loss           | 0.125      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 822      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 16.6       |
| time/                   |            |
|    fps                  | 836        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.10380881 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.971     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000909   |
|    loss                 | -0.077     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0667    |
|    value_loss           | 0.128      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 17.7       |
| time/                   |            |
|    fps                  | 848        |
|    iterations           | 12         |
|    time_elapsed         | 28         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.11476457 |
|    clip_fraction        | 0.463      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.899     |
|    explained_variance   | 0.269      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0734    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0623    |
|    value_loss           | 0.125      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=25.00 +/- 3.29
Episode length: 232.40 +/- 34.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 25         |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.13407001 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.86      |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0637    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0656    |
|    value_loss           | 0.116      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 18.3     |
| time/              |          |
|    fps             | 817      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 197        |
|    ep_rew_mean          | 18.9       |
| time/                   |            |
|    fps                  | 828        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.14185339 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.804     |
|    explained_variance   | 0.312      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0675    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.06      |
|    value_loss           | 0.122      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=22.80 +/- 5.69
Episode length: 222.80 +/- 64.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 223        |
|    mean_reward          | 22.8       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.12260446 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.821     |
|    explained_variance   | 0.276      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0737    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.133      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 805      |
|    iterations      | 15       |
|    time_elapsed    | 38       |
|    total_timesteps | 30720    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 203       |
|    ep_rew_mean          | 20.2      |
| time/                   |           |
|    fps                  | 815       |
|    iterations           | 16        |
|    time_elapsed         | 40        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 0.1454893 |
|    clip_fraction        | 0.488     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.825    |
|    explained_variance   | 0.405     |
|    learning_rate        | 0.000909  |
|    loss                 | -0.068    |
|    n_updates            | 150       |
|    policy_gradient_loss | -0.0661   |
|    value_loss           | 0.115     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 201       |
|    ep_rew_mean          | 20.3      |
| time/                   |           |
|    fps                  | 824       |
|    iterations           | 17        |
|    time_elapsed         | 42        |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.1541831 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.785    |
|    explained_variance   | 0.221     |
|    learning_rate        | 0.000909  |
|    loss                 | -0.0569   |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0614   |
|    value_loss           | 0.123     |
---------------------------------------
Eval num_timesteps=35000, episode_reward=22.00 +/- 3.19
Episode length: 207.60 +/- 30.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 208        |
|    mean_reward          | 22         |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.17746699 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.823     |
|    explained_variance   | 0.334      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0681    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0643    |
|    value_loss           | 0.115      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 807      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 196        |
|    ep_rew_mean          | 20         |
| time/                   |            |
|    fps                  | 816        |
|    iterations           | 19         |
|    time_elapsed         | 47         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.17384285 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.758     |
|    explained_variance   | 0.265      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0704    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0627    |
|    value_loss           | 0.124      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=21.50 +/- 4.63
Episode length: 207.40 +/- 58.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 207        |
|    mean_reward          | 21.5       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.19395895 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.76      |
|    explained_variance   | 0.312      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0498    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0585    |
|    value_loss           | 0.12       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    fps             | 801      |
|    iterations      | 20       |
|    time_elapsed    | 51       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 20.4       |
| time/                   |            |
|    fps                  | 809        |
|    iterations           | 21         |
|    time_elapsed         | 53         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.16935089 |
|    clip_fraction        | 0.502      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0779    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0686    |
|    value_loss           | 0.112      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=18.80 +/- 5.17
Episode length: 171.60 +/- 47.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 172        |
|    mean_reward          | 18.8       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.17262982 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.802     |
|    explained_variance   | 0.296      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0701    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0687    |
|    value_loss           | 0.114      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    fps             | 799      |
|    iterations      | 22       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 20.2       |
| time/                   |            |
|    fps                  | 807        |
|    iterations           | 23         |
|    time_elapsed         | 58         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.19411223 |
|    clip_fraction        | 0.508      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.788     |
|    explained_variance   | 0.41       |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0945    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0686    |
|    value_loss           | 0.0972     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | 20.2       |
| time/                   |            |
|    fps                  | 813        |
|    iterations           | 24         |
|    time_elapsed         | 60         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.23843229 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.423      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0794    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0659    |
|    value_loss           | 0.106      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=19.60 +/- 4.92
Episode length: 187.20 +/- 54.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 187        |
|    mean_reward          | 19.6       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.20511268 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0959    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0675    |
|    value_loss           | 0.101      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 20.6     |
| time/              |          |
|    fps             | 804      |
|    iterations      | 25       |
|    time_elapsed    | 63       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | 20.2       |
| time/                   |            |
|    fps                  | 810        |
|    iterations           | 26         |
|    time_elapsed         | 65         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.22505471 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.719     |
|    explained_variance   | 0.401      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0804    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.069     |
|    value_loss           | 0.101      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=24.00 +/- 4.10
Episode length: 221.20 +/- 45.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 221        |
|    mean_reward          | 24         |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.22342137 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.355      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0802    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0676    |
|    value_loss           | 0.103      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 20.3     |
| time/              |          |
|    fps             | 799      |
|    iterations      | 27       |
|    time_elapsed    | 69       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 20.6       |
| time/                   |            |
|    fps                  | 805        |
|    iterations           | 28         |
|    time_elapsed         | 71         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.22610241 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.692     |
|    explained_variance   | 0.317      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.0809    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0653    |
|    value_loss           | 0.109      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 207        |
|    ep_rew_mean          | 20.8       |
| time/                   |            |
|    fps                  | 811        |
|    iterations           | 29         |
|    time_elapsed         | 73         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.32833993 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.69      |
|    explained_variance   | 0.391      |
|    learning_rate        | 0.000909   |
|    loss                 | -0.079     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0671    |
|    value_loss           | 0.102      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=23.10 +/- 5.30
Episode length: 222.80 +/- 59.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 223       |
|    mean_reward          | 23.1      |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.2550717 |
|    clip_fraction        | 0.484     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.656    |
|    explained_variance   | 0.328     |
|    learning_rate        | 0.000909  |
|    loss                 | -0.0759   |
|    n_updates            | 290       |
|    policy_gradient_loss | -0.0648   |
|    value_loss           | 0.0992    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 20.5     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 30       |
|    time_elapsed    | 76       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:31:01,963] Trial 36 finished with value: 24.4 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0009086151515196796, 'gamma': 0.9035389820096066, 'gae_lambda': 0.9350777873079723}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 8.07     |
| time/              |          |
|    fps             | 1229     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=19.20 +/- 4.75
Episode length: 198.90 +/- 50.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 19.2        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010849653 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0856     |
|    learning_rate        | 0.000514    |
|    loss                 | 0.00814     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.172       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 9.04     |
| time/              |          |
|    fps             | 942      |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=14.30 +/- 3.55
Episode length: 150.50 +/- 39.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | 14.3        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.020291101 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.000514    |
|    loss                 | -0.0101     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0373     |
|    value_loss           | 0.177       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 9.95     |
| time/              |          |
|    fps             | 895      |
|    iterations      | 3        |
|    time_elapsed    | 13       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=13.50 +/- 5.45
Episode length: 146.30 +/- 49.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 146         |
|    mean_reward          | 13.5        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.027912114 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.309       |
|    learning_rate        | 0.000514    |
|    loss                 | -0.0171     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0487     |
|    value_loss           | 0.192       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 873      |
|    iterations      | 4        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=18.10 +/- 4.28
Episode length: 176.20 +/- 47.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 176         |
|    mean_reward          | 18.1        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.034560956 |
|    clip_fraction        | 0.33        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.431       |
|    learning_rate        | 0.000514    |
|    loss                 | 0.000721    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0551     |
|    value_loss           | 0.177       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 12.8     |
| time/              |          |
|    fps             | 852      |
|    iterations      | 5        |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | 14.7       |
| time/                   |            |
|    fps                  | 874        |
|    iterations           | 6          |
|    time_elapsed         | 28         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.04021591 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.000514   |
|    loss                 | -0.0478    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.177      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=22.60 +/- 6.00
Episode length: 224.70 +/- 56.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 225        |
|    mean_reward          | 22.6       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.04469467 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | 0.492      |
|    learning_rate        | 0.000514   |
|    loss                 | -0.0104    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0549    |
|    value_loss           | 0.175      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 16.3     |
| time/              |          |
|    fps             | 851      |
|    iterations      | 7        |
|    time_elapsed    | 33       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=23.30 +/- 5.16
Episode length: 233.70 +/- 63.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 234        |
|    mean_reward          | 23.3       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.05039272 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.000514   |
|    loss                 | -0.0436    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0612    |
|    value_loss           | 0.163      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 834      |
|    iterations      | 8        |
|    time_elapsed    | 39       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=18.70 +/- 5.06
Episode length: 175.20 +/- 52.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 175        |
|    mean_reward          | 18.7       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.05915458 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.617      |
|    learning_rate        | 0.000514   |
|    loss                 | -0.0526    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0576    |
|    value_loss           | 0.16       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 829      |
|    iterations      | 9        |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=21.60 +/- 6.71
Episode length: 198.20 +/- 69.73
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 198      |
|    mean_reward          | 21.6     |
| time/                   |          |
|    total_timesteps      | 40000    |
| train/                  |          |
|    approx_kl            | 0.06734  |
|    clip_fraction        | 0.402    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1       |
|    explained_variance   | 0.62     |
|    learning_rate        | 0.000514 |
|    loss                 | -0.0338  |
|    n_updates            | 90       |
|    policy_gradient_loss | -0.0549  |
|    value_loss           | 0.168    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 19.8     |
| time/              |          |
|    fps             | 822      |
|    iterations      | 10       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=24.00 +/- 5.08
Episode length: 218.70 +/- 58.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 219        |
|    mean_reward          | 24         |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.07288097 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.963     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.000514   |
|    loss                 | -0.0359    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.18       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 813      |
|    iterations      | 11       |
|    time_elapsed    | 55       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 20.4       |
| time/                   |            |
|    fps                  | 827        |
|    iterations           | 12         |
|    time_elapsed         | 59         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.06898624 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.936     |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.000514   |
|    loss                 | -0.0333    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0562    |
|    value_loss           | 0.165      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=20.60 +/- 5.70
Episode length: 184.70 +/- 57.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 185        |
|    mean_reward          | 20.6       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.08028974 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.925     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.000514   |
|    loss                 | -0.0421    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0559    |
|    value_loss           | 0.162      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 822      |
|    iterations      | 13       |
|    time_elapsed    | 64       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=22.10 +/- 4.30
Episode length: 200.50 +/- 37.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 200        |
|    mean_reward          | 22.1       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.07988191 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.853     |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.000514   |
|    loss                 | -0.0244    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.183      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 21.5     |
| time/              |          |
|    fps             | 817      |
|    iterations      | 14       |
|    time_elapsed    | 70       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=23.50 +/- 5.73
Episode length: 215.20 +/- 60.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 23.5       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.07740434 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.000514   |
|    loss                 | -0.0213    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0481    |
|    value_loss           | 0.176      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 22.3     |
| time/              |          |
|    fps             | 812      |
|    iterations      | 15       |
|    time_elapsed    | 75       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:32:20,074] Trial 37 finished with value: 25.8 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.0005137788869275317, 'gamma': 0.9551579218616448, 'gae_lambda': 0.9127643465375311}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 8.54     |
| time/              |          |
|    fps             | 1231     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 156        |
|    ep_rew_mean          | 9.81       |
| time/                   |            |
|    fps                  | 914        |
|    iterations           | 2          |
|    time_elapsed         | 4          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.02425316 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -0.0164    |
|    learning_rate        | 0.00158    |
|    loss                 | 0.0727     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0222    |
|    value_loss           | 0.215      |
----------------------------------------
Eval num_timesteps=5000, episode_reward=16.60 +/- 4.45
Episode length: 195.40 +/- 55.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 195         |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.053176828 |
|    clip_fraction        | 0.382       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.172       |
|    learning_rate        | 0.00158     |
|    loss                 | -0.011      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0392     |
|    value_loss           | 0.239       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 713      |
|    iterations      | 3        |
|    time_elapsed    | 8        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | 11.9       |
| time/                   |            |
|    fps                  | 716        |
|    iterations           | 4          |
|    time_elapsed         | 11         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.07519731 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.192      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.00829   |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0469    |
|    value_loss           | 0.226      |
----------------------------------------
Eval num_timesteps=10000, episode_reward=18.00 +/- 5.69
Episode length: 190.70 +/- 61.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 191        |
|    mean_reward          | 18         |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.08051217 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.333      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.0405    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.21       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 12.7     |
| time/              |          |
|    fps             | 658      |
|    iterations      | 5        |
|    time_elapsed    | 15       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | 13.4       |
| time/                   |            |
|    fps                  | 668        |
|    iterations           | 6          |
|    time_elapsed         | 18         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.10912366 |
|    clip_fraction        | 0.524      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.388      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.0295    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0618    |
|    value_loss           | 0.202      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | 14.1       |
| time/                   |            |
|    fps                  | 676        |
|    iterations           | 7          |
|    time_elapsed         | 21         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.12128181 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.325      |
|    learning_rate        | 0.00158    |
|    loss                 | 0.0658     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.063     |
|    value_loss           | 0.201      |
----------------------------------------
Eval num_timesteps=15000, episode_reward=19.40 +/- 7.36
Episode length: 192.80 +/- 72.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 193        |
|    mean_reward          | 19.4       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.16653362 |
|    clip_fraction        | 0.579      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.0819    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0687    |
|    value_loss           | 0.158      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 647      |
|    iterations      | 8        |
|    time_elapsed    | 25       |
|    total_timesteps | 16384    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 183       |
|    ep_rew_mean          | 14.7      |
| time/                   |           |
|    fps                  | 655       |
|    iterations           | 9         |
|    time_elapsed         | 28        |
|    total_timesteps      | 18432     |
| train/                  |           |
|    approx_kl            | 0.1715849 |
|    clip_fraction        | 0.602     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.05     |
|    explained_variance   | 0.532     |
|    learning_rate        | 0.00158   |
|    loss                 | -0.0533   |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.0695   |
|    value_loss           | 0.18      |
---------------------------------------
Eval num_timesteps=20000, episode_reward=21.00 +/- 7.51
Episode length: 216.80 +/- 74.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 217        |
|    mean_reward          | 21         |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.19648546 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.999     |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.0748    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.166      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 632      |
|    iterations      | 10       |
|    time_elapsed    | 32       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 194       |
|    ep_rew_mean          | 16.7      |
| time/                   |           |
|    fps                  | 639       |
|    iterations           | 11        |
|    time_elapsed         | 35        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.2036059 |
|    clip_fraction        | 0.582     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.981    |
|    explained_variance   | 0.473     |
|    learning_rate        | 0.00158   |
|    loss                 | -0.0149   |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0721   |
|    value_loss           | 0.163     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 197       |
|    ep_rew_mean          | 17.4      |
| time/                   |           |
|    fps                  | 646       |
|    iterations           | 12        |
|    time_elapsed         | 38        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.2777468 |
|    clip_fraction        | 0.604     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.931    |
|    explained_variance   | 0.407     |
|    learning_rate        | 0.00158   |
|    loss                 | -0.103    |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0692   |
|    value_loss           | 0.147     |
---------------------------------------
Eval num_timesteps=25000, episode_reward=17.50 +/- 4.36
Episode length: 185.90 +/- 50.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 186        |
|    mean_reward          | 17.5       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.30705371 |
|    clip_fraction        | 0.633      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.887     |
|    explained_variance   | 0.453      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.0793    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0603    |
|    value_loss           | 0.159      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 632      |
|    iterations      | 13       |
|    time_elapsed    | 42       |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 192       |
|    ep_rew_mean          | 17.5      |
| time/                   |           |
|    fps                  | 638       |
|    iterations           | 14        |
|    time_elapsed         | 44        |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.2941441 |
|    clip_fraction        | 0.624     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.873    |
|    explained_variance   | 0.559     |
|    learning_rate        | 0.00158   |
|    loss                 | -0.0368   |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.0705   |
|    value_loss           | 0.153     |
---------------------------------------
Eval num_timesteps=30000, episode_reward=19.80 +/- 6.01
Episode length: 202.60 +/- 68.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 203        |
|    mean_reward          | 19.8       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.30570933 |
|    clip_fraction        | 0.626      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.829     |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.0765    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0689    |
|    value_loss           | 0.142      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 625      |
|    iterations      | 15       |
|    time_elapsed    | 49       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | 17.2       |
| time/                   |            |
|    fps                  | 631        |
|    iterations           | 16         |
|    time_elapsed         | 51         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.31601852 |
|    clip_fraction        | 0.641      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.865     |
|    explained_variance   | 0.453      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.0845    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0711    |
|    value_loss           | 0.147      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 17.5       |
| time/                   |            |
|    fps                  | 636        |
|    iterations           | 17         |
|    time_elapsed         | 54         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.41028434 |
|    clip_fraction        | 0.624      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.816     |
|    explained_variance   | 0.387      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.113     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0696    |
|    value_loss           | 0.16       |
----------------------------------------
Eval num_timesteps=35000, episode_reward=21.70 +/- 3.80
Episode length: 206.40 +/- 36.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 206       |
|    mean_reward          | 21.7      |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.3674053 |
|    clip_fraction        | 0.638     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.841    |
|    explained_variance   | 0.446     |
|    learning_rate        | 0.00158   |
|    loss                 | -0.095    |
|    n_updates            | 170       |
|    policy_gradient_loss | -0.0669   |
|    value_loss           | 0.163     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 625      |
|    iterations      | 18       |
|    time_elapsed    | 58       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 17.8       |
| time/                   |            |
|    fps                  | 629        |
|    iterations           | 19         |
|    time_elapsed         | 61         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.38430935 |
|    clip_fraction        | 0.655      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.829     |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.0807    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0791    |
|    value_loss           | 0.136      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=19.90 +/- 4.76
Episode length: 204.60 +/- 40.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 205        |
|    mean_reward          | 19.9       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.41565764 |
|    clip_fraction        | 0.631      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.797     |
|    explained_variance   | 0.438      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.0325    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0685    |
|    value_loss           | 0.166      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 620      |
|    iterations      | 20       |
|    time_elapsed    | 65       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 188        |
|    ep_rew_mean          | 17.5       |
| time/                   |            |
|    fps                  | 624        |
|    iterations           | 21         |
|    time_elapsed         | 68         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.48332176 |
|    clip_fraction        | 0.658      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.819     |
|    explained_variance   | 0.392      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.107     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.156      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=16.90 +/- 4.09
Episode length: 171.30 +/- 39.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 171       |
|    mean_reward          | 16.9      |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.5041631 |
|    clip_fraction        | 0.67      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.769    |
|    explained_variance   | 0.48      |
|    learning_rate        | 0.00158   |
|    loss                 | -0.0947   |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.0751   |
|    value_loss           | 0.144     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 619      |
|    iterations      | 22       |
|    time_elapsed    | 72       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 17.3       |
| time/                   |            |
|    fps                  | 623        |
|    iterations           | 23         |
|    time_elapsed         | 75         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.51536244 |
|    clip_fraction        | 0.675      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.817     |
|    explained_variance   | 0.484      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.089     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0663    |
|    value_loss           | 0.165      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 191       |
|    ep_rew_mean          | 17.4      |
| time/                   |           |
|    fps                  | 627       |
|    iterations           | 24        |
|    time_elapsed         | 78        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.5308156 |
|    clip_fraction        | 0.657     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.755    |
|    explained_variance   | 0.52      |
|    learning_rate        | 0.00158   |
|    loss                 | -0.107    |
|    n_updates            | 230       |
|    policy_gradient_loss | -0.075    |
|    value_loss           | 0.148     |
---------------------------------------
Eval num_timesteps=50000, episode_reward=16.50 +/- 3.35
Episode length: 169.80 +/- 39.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 170       |
|    mean_reward          | 16.5      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.5159663 |
|    clip_fraction        | 0.654     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.754    |
|    explained_variance   | 0.556     |
|    learning_rate        | 0.00158   |
|    loss                 | -0.0854   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0772   |
|    value_loss           | 0.134     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 17.3     |
| time/              |          |
|    fps             | 622      |
|    iterations      | 25       |
|    time_elapsed    | 82       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 17.1       |
| time/                   |            |
|    fps                  | 626        |
|    iterations           | 26         |
|    time_elapsed         | 85         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.54121697 |
|    clip_fraction        | 0.666      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.73      |
|    explained_variance   | 0.465      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.105     |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0776    |
|    value_loss           | 0.148      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=17.10 +/- 6.61
Episode length: 181.10 +/- 71.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 181        |
|    mean_reward          | 17.1       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.57984287 |
|    clip_fraction        | 0.656      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.737     |
|    explained_variance   | 0.452      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.0889    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0798    |
|    value_loss           | 0.138      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 621      |
|    iterations      | 27       |
|    time_elapsed    | 89       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 16.4       |
| time/                   |            |
|    fps                  | 624        |
|    iterations           | 28         |
|    time_elapsed         | 91         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.52409303 |
|    clip_fraction        | 0.666      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.776     |
|    explained_variance   | 0.439      |
|    learning_rate        | 0.00158    |
|    loss                 | -0.107     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0778    |
|    value_loss           | 0.151      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 187       |
|    ep_rew_mean          | 16.5      |
| time/                   |           |
|    fps                  | 628       |
|    iterations           | 29        |
|    time_elapsed         | 94        |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 0.5795411 |
|    clip_fraction        | 0.663     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.758    |
|    explained_variance   | 0.49      |
|    learning_rate        | 0.00158   |
|    loss                 | -0.109    |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0819   |
|    value_loss           | 0.136     |
---------------------------------------
Eval num_timesteps=60000, episode_reward=13.60 +/- 3.56
Episode length: 151.50 +/- 39.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 152       |
|    mean_reward          | 13.6      |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.5465672 |
|    clip_fraction        | 0.677     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.802    |
|    explained_variance   | 0.522     |
|    learning_rate        | 0.00158   |
|    loss                 | -0.113    |
|    n_updates            | 290       |
|    policy_gradient_loss | -0.0784   |
|    value_loss           | 0.141     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    fps             | 624      |
|    iterations      | 30       |
|    time_elapsed    | 98       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:34:00,717] Trial 38 finished with value: 12.2 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 0.001575868473073573, 'gamma': 0.9499176970860953, 'gae_lambda': 0.950262147752696}. Best is trial 31 with value: 29.6.
Using cuda device
Eval num_timesteps=5000, episode_reward=4.70 +/- 1.73
Episode length: 119.70 +/- 24.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 4.7      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 8.09     |
| time/              |          |
|    fps             | 1099     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=9.20 +/- 2.32
Episode length: 119.10 +/- 32.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 119         |
|    mean_reward          | 9.2         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008890345 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0311     |
|    learning_rate        | 4.32e-05    |
|    loss                 | 0.211       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 0.52        |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=8.50 +/- 1.91
Episode length: 116.10 +/- 24.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 8.5      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 8.59     |
| time/              |          |
|    fps             | 901      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=12.40 +/- 3.44
Episode length: 152.30 +/- 37.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 152         |
|    mean_reward          | 12.4        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.008519869 |
|    clip_fraction        | 0.0692      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.42        |
|    learning_rate        | 4.32e-05    |
|    loss                 | 0.222       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00791    |
|    value_loss           | 0.561       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 9.28     |
| time/              |          |
|    fps             | 866      |
|    iterations      | 3        |
|    time_elapsed    | 28       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=9.60 +/- 3.83
Episode length: 130.90 +/- 57.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 9.6         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010871592 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.553       |
|    learning_rate        | 4.32e-05    |
|    loss                 | 0.176       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.497       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=9.60 +/- 2.65
Episode length: 121.80 +/- 24.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 9.6      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 835      |
|    iterations      | 4        |
|    time_elapsed    | 39       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=12.90 +/- 4.74
Episode length: 159.90 +/- 52.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 160          |
|    mean_reward          | 12.9         |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0069932723 |
|    clip_fraction        | 0.0526       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.507        |
|    learning_rate        | 4.32e-05     |
|    loss                 | 0.189        |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00667     |
|    value_loss           | 0.588        |
------------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=9.90 +/- 2.98
Episode length: 129.60 +/- 38.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 9.9      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 12       |
| time/              |          |
|    fps             | 814      |
|    iterations      | 5        |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=11.30 +/- 2.49
Episode length: 134.20 +/- 24.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 11.3        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.010344842 |
|    clip_fraction        | 0.0806      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.55        |
|    learning_rate        | 4.32e-05    |
|    loss                 | 0.227       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00774    |
|    value_loss           | 0.624       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 12.1     |
| time/              |          |
|    fps             | 814      |
|    iterations      | 6        |
|    time_elapsed    | 60       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=13.80 +/- 6.95
Episode length: 167.70 +/- 78.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 168          |
|    mean_reward          | 13.8         |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0093253525 |
|    clip_fraction        | 0.0842       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 0.613        |
|    learning_rate        | 4.32e-05     |
|    loss                 | 0.283        |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 0.593        |
------------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=12.70 +/- 4.50
Episode length: 163.30 +/- 56.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 163      |
|    mean_reward     | 12.7     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 798      |
|    iterations      | 7        |
|    time_elapsed    | 71       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=11.40 +/- 5.68
Episode length: 132.40 +/- 59.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 132         |
|    mean_reward          | 11.4        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.007004164 |
|    clip_fraction        | 0.0656      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.571       |
|    learning_rate        | 4.32e-05    |
|    loss                 | 0.203       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 0.592       |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=15.90 +/- 5.70
Episode length: 173.80 +/- 63.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 15.9     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 789      |
|    iterations      | 8        |
|    time_elapsed    | 83       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-16 19:35:27,569] Trial 39 finished with value: 13.5 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 4.319353998612794e-05, 'gamma': 0.9742506782513921, 'gae_lambda': 0.9762451218438435}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 6.69     |
| time/              |          |
|    fps             | 1230     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 134          |
|    ep_rew_mean          | 7.73         |
| time/                   |              |
|    fps                  | 1114         |
|    iterations           | 2            |
|    time_elapsed         | 3            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0072500445 |
|    clip_fraction        | 0.0773       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.0428      |
|    learning_rate        | 0.000148     |
|    loss                 | 0.0283       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0092      |
|    value_loss           | 0.139        |
------------------------------------------
Eval num_timesteps=5000, episode_reward=11.10 +/- 3.18
Episode length: 150.90 +/- 46.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 151         |
|    mean_reward          | 11.1        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010763554 |
|    clip_fraction        | 0.0606      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.18        |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0466      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.137       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 8.18     |
| time/              |          |
|    fps             | 913      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 145          |
|    ep_rew_mean          | 9.2          |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 4            |
|    time_elapsed         | 8            |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0076078456 |
|    clip_fraction        | 0.0528       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.306        |
|    learning_rate        | 0.000148     |
|    loss                 | 0.0219       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0129      |
|    value_loss           | 0.129        |
------------------------------------------
Eval num_timesteps=10000, episode_reward=10.40 +/- 4.98
Episode length: 136.10 +/- 64.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 136         |
|    mean_reward          | 10.4        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009049099 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0489      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.136       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 9.55     |
| time/              |          |
|    fps             | 877      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 153        |
|    ep_rew_mean          | 10         |
| time/                   |            |
|    fps                  | 899        |
|    iterations           | 6          |
|    time_elapsed         | 13         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01120089 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.343      |
|    learning_rate        | 0.000148   |
|    loss                 | 0.0207     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.023     |
|    value_loss           | 0.129      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 156         |
|    ep_rew_mean          | 10.6        |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 7           |
|    time_elapsed         | 15          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.008605854 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0255      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.129       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=14.70 +/- 5.10
Episode length: 189.10 +/- 63.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 189        |
|    mean_reward          | 14.7       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01204305 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | 0.336      |
|    learning_rate        | 0.000148   |
|    loss                 | 0.0453     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0232    |
|    value_loss           | 0.146      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    fps             | 863      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | 12.3        |
| time/                   |             |
|    fps                  | 878         |
|    iterations           | 9           |
|    time_elapsed         | 20          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.012176895 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0174      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.148       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=19.50 +/- 2.84
Episode length: 231.20 +/- 50.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 231          |
|    mean_reward          | 19.5         |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0088306535 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.02        |
|    explained_variance   | 0.37         |
|    learning_rate        | 0.000148     |
|    loss                 | 0.0163       |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.0206      |
|    value_loss           | 0.161        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 173         |
|    ep_rew_mean          | 14.2        |
| time/                   |             |
|    fps                  | 847         |
|    iterations           | 11          |
|    time_elapsed         | 26          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.008157289 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.985      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0452      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 0.149       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 181         |
|    ep_rew_mean          | 15.2        |
| time/                   |             |
|    fps                  | 858         |
|    iterations           | 12          |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009074315 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.915      |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0198      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.138       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=16.80 +/- 5.34
Episode length: 171.60 +/- 64.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 172         |
|    mean_reward          | 16.8        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010907103 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.831      |
|    explained_variance   | 0.431       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.023       |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.14        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 837      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 16.6        |
| time/                   |             |
|    fps                  | 848         |
|    iterations           | 14          |
|    time_elapsed         | 33          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.013038954 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.898      |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.00674     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.135       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=19.30 +/- 4.27
Episode length: 201.70 +/- 49.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 19.3        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.012460831 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.895      |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0415      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.15        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 186          |
|    ep_rew_mean          | 17.7         |
| time/                   |              |
|    fps                  | 835          |
|    iterations           | 16           |
|    time_elapsed         | 39           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0110376105 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.867       |
|    explained_variance   | 0.5          |
|    learning_rate        | 0.000148     |
|    loss                 | 0.0213       |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.0227      |
|    value_loss           | 0.136        |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 18.2       |
| time/                   |            |
|    fps                  | 844        |
|    iterations           | 17         |
|    time_elapsed         | 41         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.01318526 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.901     |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.000148   |
|    loss                 | 0.0462     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0252    |
|    value_loss           | 0.152      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=22.50 +/- 3.35
Episode length: 222.60 +/- 37.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | 22.5        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.015057323 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.865      |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0164      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.147       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    fps             | 823      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 19.1        |
| time/                   |             |
|    fps                  | 832         |
|    iterations           | 19          |
|    time_elapsed         | 46          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.019192971 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.826      |
|    explained_variance   | 0.499       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0133     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.125       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=16.60 +/- 4.10
Episode length: 155.50 +/- 41.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.016212977 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.016       |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.138       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 19.2     |
| time/              |          |
|    fps             | 822      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 195         |
|    ep_rew_mean          | 19.6        |
| time/                   |             |
|    fps                  | 829         |
|    iterations           | 21          |
|    time_elapsed         | 51          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.017758217 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.842      |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0231      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0291     |
|    value_loss           | 0.132       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=22.70 +/- 5.53
Episode length: 215.40 +/- 60.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 215         |
|    mean_reward          | 22.7        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.017385976 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.00604     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 0.132       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 814      |
|    iterations      | 22       |
|    time_elapsed    | 55       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 19.4        |
| time/                   |             |
|    fps                  | 821         |
|    iterations           | 23          |
|    time_elapsed         | 57          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.017952936 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.00874     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.032      |
|    value_loss           | 0.128       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 19.7        |
| time/                   |             |
|    fps                  | 828         |
|    iterations           | 24          |
|    time_elapsed         | 59          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.027559586 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.00112    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.149       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=22.60 +/- 5.16
Episode length: 207.10 +/- 59.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | 22.6        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.021229228 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.735      |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0135      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.135       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 20       |
| time/              |          |
|    fps             | 815      |
|    iterations      | 25       |
|    time_elapsed    | 62       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 20.1        |
| time/                   |             |
|    fps                  | 821         |
|    iterations           | 26          |
|    time_elapsed         | 64          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.018577263 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.56        |
|    learning_rate        | 0.000148    |
|    loss                 | 0.00249     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0261     |
|    value_loss           | 0.135       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=22.70 +/- 8.46
Episode length: 203.10 +/- 77.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 22.7        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.019895062 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.00914    |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 810      |
|    iterations      | 27       |
|    time_elapsed    | 68       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 193         |
|    ep_rew_mean          | 20.4        |
| time/                   |             |
|    fps                  | 816         |
|    iterations           | 28          |
|    time_elapsed         | 70          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.024493609 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.00655     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0328     |
|    value_loss           | 0.135       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 195         |
|    ep_rew_mean          | 20.8        |
| time/                   |             |
|    fps                  | 821         |
|    iterations           | 29          |
|    time_elapsed         | 72          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.018389877 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.651      |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.00529     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 0.128       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=25.60 +/- 3.47
Episode length: 233.50 +/- 45.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | 25.6        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.021892648 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.6         |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0138      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 0.121       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 21.1     |
| time/              |          |
|    fps             | 809      |
|    iterations      | 30       |
|    time_elapsed    | 75       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:36:45,828] Trial 40 finished with value: 29.1 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00014815379149886153, 'gamma': 0.9193201149623467, 'gae_lambda': 0.8604390048336967}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 8.07     |
| time/              |          |
|    fps             | 1231     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 140         |
|    ep_rew_mean          | 8.62        |
| time/                   |             |
|    fps                  | 1114        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.015267866 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0393     |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0251      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.12        |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=9.60 +/- 1.96
Episode length: 126.40 +/- 31.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 126          |
|    mean_reward          | 9.6          |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0076349443 |
|    clip_fraction        | 0.0652       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.000146     |
|    loss                 | 0.043        |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0123      |
|    value_loss           | 0.129        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 8.64     |
| time/              |          |
|    fps             | 939      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 139         |
|    ep_rew_mean          | 8.69        |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.009638667 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.249       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0618      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.122       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=10.40 +/- 2.33
Episode length: 143.00 +/- 32.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | 10.4        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008613551 |
|    clip_fraction        | 0.0699      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0383      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.126       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 9.88     |
| time/              |          |
|    fps             | 888      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 155         |
|    ep_rew_mean          | 10.6        |
| time/                   |             |
|    fps                  | 907         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010854997 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.291       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0411      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.128       |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 156          |
|    ep_rew_mean          | 11.1         |
| time/                   |              |
|    fps                  | 921          |
|    iterations           | 7            |
|    time_elapsed         | 15           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0130292885 |
|    clip_fraction        | 0.203        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.000146     |
|    loss                 | 0.00307      |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.026       |
|    value_loss           | 0.135        |
------------------------------------------
Eval num_timesteps=15000, episode_reward=13.00 +/- 3.52
Episode length: 161.30 +/- 37.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 161         |
|    mean_reward          | 13          |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.013523732 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.433       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.00656     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 0.133       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 11.7     |
| time/              |          |
|    fps             | 877      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 167         |
|    ep_rew_mean          | 12.9        |
| time/                   |             |
|    fps                  | 891         |
|    iterations           | 9           |
|    time_elapsed         | 20          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.012525003 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0322      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.127       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=15.30 +/- 5.80
Episode length: 182.20 +/- 67.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 182         |
|    mean_reward          | 15.3        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011623977 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.458       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.00466     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.116       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 855      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 176         |
|    ep_rew_mean          | 14.7        |
| time/                   |             |
|    fps                  | 867         |
|    iterations           | 11          |
|    time_elapsed         | 25          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.011462879 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0313      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.121       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | 15.6        |
| time/                   |             |
|    fps                  | 878         |
|    iterations           | 12          |
|    time_elapsed         | 27          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.011398999 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.882      |
|    explained_variance   | 0.421       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.03        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.131       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=11.90 +/- 3.67
Episode length: 130.60 +/- 38.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 131        |
|    mean_reward          | 11.9       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.00949932 |
|    clip_fraction        | 0.0905     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.865     |
|    explained_variance   | 0.422      |
|    learning_rate        | 0.000146   |
|    loss                 | 0.0312     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0203    |
|    value_loss           | 0.135      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 861      |
|    iterations      | 13       |
|    time_elapsed    | 30       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | 17          |
| time/                   |             |
|    fps                  | 870         |
|    iterations           | 14          |
|    time_elapsed         | 32          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.010854319 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.87       |
|    explained_variance   | 0.449       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0188      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.126       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=18.30 +/- 4.00
Episode length: 182.10 +/- 47.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 182         |
|    mean_reward          | 18.3        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.013455173 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0138      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.128       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 848      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 188         |
|    ep_rew_mean          | 18          |
| time/                   |             |
|    fps                  | 856         |
|    iterations           | 16          |
|    time_elapsed         | 38          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.011988435 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.834      |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0161      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.132       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 18.2        |
| time/                   |             |
|    fps                  | 863         |
|    iterations           | 17          |
|    time_elapsed         | 40          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.010078231 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.806      |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0363      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.136       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=16.60 +/- 4.39
Episode length: 162.40 +/- 48.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 162         |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014500484 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0214      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.127       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 848      |
|    iterations      | 18       |
|    time_elapsed    | 43       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | 18.4        |
| time/                   |             |
|    fps                  | 855         |
|    iterations           | 19          |
|    time_elapsed         | 45          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.014554814 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.000146    |
|    loss                 | -0.00986    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.117       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=18.60 +/- 4.13
Episode length: 173.50 +/- 45.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 174        |
|    mean_reward          | 18.6       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.01625074 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.483      |
|    learning_rate        | 0.000146   |
|    loss                 | 0.0287     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0267    |
|    value_loss           | 0.129      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    fps             | 841      |
|    iterations      | 20       |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | 19.3        |
| time/                   |             |
|    fps                  | 847         |
|    iterations           | 21          |
|    time_elapsed         | 50          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.019025173 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.736      |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0321      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.134       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=19.00 +/- 3.71
Episode length: 191.20 +/- 39.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 191         |
|    mean_reward          | 19          |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.016640212 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.531       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0148      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.112       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 22       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 195         |
|    ep_rew_mean          | 20          |
| time/                   |             |
|    fps                  | 839         |
|    iterations           | 23          |
|    time_elapsed         | 56          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.015861794 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.676      |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.031       |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 0.124       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 20.1        |
| time/                   |             |
|    fps                  | 845         |
|    iterations           | 24          |
|    time_elapsed         | 58          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.012650881 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.677      |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.00482     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.123       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=21.40 +/- 3.44
Episode length: 199.90 +/- 35.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 200         |
|    mean_reward          | 21.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015638003 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.443       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.00745     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0259     |
|    value_loss           | 0.128       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 832      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 201         |
|    ep_rew_mean          | 20.9        |
| time/                   |             |
|    fps                  | 838         |
|    iterations           | 26          |
|    time_elapsed         | 63          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.016472913 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.00955     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.117       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=21.70 +/- 4.31
Episode length: 192.00 +/- 34.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 192        |
|    mean_reward          | 21.7       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.01696287 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.604     |
|    explained_variance   | 0.45       |
|    learning_rate        | 0.000146   |
|    loss                 | 0.026      |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0236    |
|    value_loss           | 0.127      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 21.3     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 27       |
|    time_elapsed    | 66       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 205         |
|    ep_rew_mean          | 21.5        |
| time/                   |             |
|    fps                  | 833         |
|    iterations           | 28          |
|    time_elapsed         | 68          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.023656467 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0228      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.121       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 208         |
|    ep_rew_mean          | 21.8        |
| time/                   |             |
|    fps                  | 838         |
|    iterations           | 29          |
|    time_elapsed         | 70          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.018078059 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.0252      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.121       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=22.20 +/- 5.25
Episode length: 201.30 +/- 52.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 201         |
|    mean_reward          | 22.2        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.015371023 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.554      |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.000146    |
|    loss                 | 0.00474     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.132       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 210      |
|    ep_rew_mean     | 22.2     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 30       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:38:02,235] Trial 41 finished with value: 25.3 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00014577870573679105, 'gamma': 0.9097471480647021, 'gae_lambda': 0.8359005295936032}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 8.21     |
| time/              |          |
|    fps             | 1224     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 131         |
|    ep_rew_mean          | 7.39        |
| time/                   |             |
|    fps                  | 1111        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012895009 |
|    clip_fraction        | 0.0746      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0384     |
|    learning_rate        | 0.000302    |
|    loss                 | 0.0417      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.121       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=19.30 +/- 3.85
Episode length: 229.20 +/- 45.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 229         |
|    mean_reward          | 19.3        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009531144 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.117       |
|    learning_rate        | 0.000302    |
|    loss                 | 0.00376     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.114       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 844      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 141       |
|    ep_rew_mean          | 8.72      |
| time/                   |           |
|    fps                  | 878       |
|    iterations           | 4         |
|    time_elapsed         | 9         |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0145879 |
|    clip_fraction        | 0.202     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.34     |
|    explained_variance   | 0.311     |
|    learning_rate        | 0.000302  |
|    loss                 | -0.0329   |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.0368   |
|    value_loss           | 0.117     |
---------------------------------------
Eval num_timesteps=10000, episode_reward=18.40 +/- 3.64
Episode length: 181.70 +/- 36.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 182         |
|    mean_reward          | 18.4        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.017148726 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.0145     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.043      |
|    value_loss           | 0.124       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 9.82     |
| time/              |          |
|    fps             | 813      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | 10.8        |
| time/                   |             |
|    fps                  | 836         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.019004948 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.0433     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0441     |
|    value_loss           | 0.12        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | 11.7        |
| time/                   |             |
|    fps                  | 857         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.020358281 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.0428     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0465     |
|    value_loss           | 0.125       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=19.40 +/- 4.94
Episode length: 176.40 +/- 55.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 176        |
|    mean_reward          | 19.4       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.02345835 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.000302   |
|    loss                 | -0.0308    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0482    |
|    value_loss           | 0.122      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 821      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | 13.6        |
| time/                   |             |
|    fps                  | 839         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.025861625 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.0125     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0523     |
|    value_loss           | 0.127       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=21.60 +/- 6.76
Episode length: 199.00 +/- 67.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 199        |
|    mean_reward          | 21.6       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.03061856 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.383      |
|    learning_rate        | 0.000302   |
|    loss                 | -0.0188    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0517    |
|    value_loss           | 0.114      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 808      |
|    iterations      | 10       |
|    time_elapsed    | 25       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | 16.3       |
| time/                   |            |
|    fps                  | 823        |
|    iterations           | 11         |
|    time_elapsed         | 27         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.03439025 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.419      |
|    learning_rate        | 0.000302   |
|    loss                 | -0.0248    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.049     |
|    value_loss           | 0.132      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 195         |
|    ep_rew_mean          | 17.6        |
| time/                   |             |
|    fps                  | 836         |
|    iterations           | 12          |
|    time_elapsed         | 29          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.031693563 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.984      |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.0418     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0465     |
|    value_loss           | 0.122       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=20.50 +/- 6.10
Episode length: 188.30 +/- 47.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 188         |
|    mean_reward          | 20.5        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.035974752 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.927      |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.0348     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0488     |
|    value_loss           | 0.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 815      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 204         |
|    ep_rew_mean          | 19.8        |
| time/                   |             |
|    fps                  | 827         |
|    iterations           | 14          |
|    time_elapsed         | 34          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.035543807 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.874      |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.0263     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0441     |
|    value_loss           | 0.12        |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=23.00 +/- 3.58
Episode length: 211.80 +/- 35.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 23         |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.04950738 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.841     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.000302   |
|    loss                 | -0.0282    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0492    |
|    value_loss           | 0.112      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 806      |
|    iterations      | 15       |
|    time_elapsed    | 38       |
|    total_timesteps | 30720    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 207       |
|    ep_rew_mean          | 20.7      |
| time/                   |           |
|    fps                  | 816       |
|    iterations           | 16        |
|    time_elapsed         | 40        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 0.0331598 |
|    clip_fraction        | 0.267     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.816    |
|    explained_variance   | 0.539     |
|    learning_rate        | 0.000302  |
|    loss                 | -0.0273   |
|    n_updates            | 150       |
|    policy_gradient_loss | -0.0445   |
|    value_loss           | 0.112     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 208        |
|    ep_rew_mean          | 20.8       |
| time/                   |            |
|    fps                  | 826        |
|    iterations           | 17         |
|    time_elapsed         | 42         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.04118265 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.831     |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.000302   |
|    loss                 | -0.0254    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0469    |
|    value_loss           | 0.131      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=21.80 +/- 6.54
Episode length: 191.40 +/- 51.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 191         |
|    mean_reward          | 21.8        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.038455196 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.767      |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.036      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0436     |
|    value_loss           | 0.128       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 21       |
| time/              |          |
|    fps             | 811      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 21.2       |
| time/                   |            |
|    fps                  | 819        |
|    iterations           | 19         |
|    time_elapsed         | 47         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.04955575 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.705     |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.000302   |
|    loss                 | -0.00997   |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0395    |
|    value_loss           | 0.124      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=24.00 +/- 4.12
Episode length: 205.30 +/- 42.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 205        |
|    mean_reward          | 24         |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.05057586 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.662     |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.000302   |
|    loss                 | -0.00139   |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.041     |
|    value_loss           | 0.12       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 21.3     |
| time/              |          |
|    fps             | 804      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | 21.5        |
| time/                   |             |
|    fps                  | 813         |
|    iterations           | 21          |
|    time_elapsed         | 52          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.059864648 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.629      |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.000302    |
|    loss                 | 0.00464     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0403     |
|    value_loss           | 0.127       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=22.50 +/- 4.01
Episode length: 202.40 +/- 42.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 22.5        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.053367775 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.0182     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0373     |
|    value_loss           | 0.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 21.6     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 22       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | 21.5       |
| time/                   |            |
|    fps                  | 807        |
|    iterations           | 23         |
|    time_elapsed         | 58         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.05412138 |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.547     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.000302   |
|    loss                 | -0.00145   |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0337    |
|    value_loss           | 0.126      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 201         |
|    ep_rew_mean          | 21.8        |
| time/                   |             |
|    fps                  | 814         |
|    iterations           | 24          |
|    time_elapsed         | 60          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.050796874 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.537      |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.000302    |
|    loss                 | -0.0206     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0398     |
|    value_loss           | 0.118       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=25.60 +/- 4.52
Episode length: 240.10 +/- 45.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 240       |
|    mean_reward          | 25.6      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0652928 |
|    clip_fraction        | 0.255     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.528    |
|    explained_variance   | 0.507     |
|    learning_rate        | 0.000302  |
|    loss                 | 0.00528   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.033    |
|    value_loss           | 0.138     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 22.1     |
| time/              |          |
|    fps             | 799      |
|    iterations      | 25       |
|    time_elapsed    | 64       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 22.4       |
| time/                   |            |
|    fps                  | 805        |
|    iterations           | 26         |
|    time_elapsed         | 66         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.05598581 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.513     |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.000302   |
|    loss                 | -0.0162    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0357    |
|    value_loss           | 0.119      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=25.80 +/- 4.77
Episode length: 230.20 +/- 53.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 230         |
|    mean_reward          | 25.8        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.053225607 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.496      |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.00346    |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.133       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 22.8     |
| time/              |          |
|    fps             | 793      |
|    iterations      | 27       |
|    time_elapsed    | 69       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 207         |
|    ep_rew_mean          | 23          |
| time/                   |             |
|    fps                  | 799         |
|    iterations           | 28          |
|    time_elapsed         | 71          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.056614354 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.484      |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.0253     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.12        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 208         |
|    ep_rew_mean          | 23.2        |
| time/                   |             |
|    fps                  | 805         |
|    iterations           | 29          |
|    time_elapsed         | 73          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.059021793 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.475      |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.000302    |
|    loss                 | -0.00209    |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.124       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=25.40 +/- 3.50
Episode length: 215.40 +/- 29.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 25.4       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.06722106 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.458     |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.000302   |
|    loss                 | -0.0192    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0354    |
|    value_loss           | 0.123      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 210      |
|    ep_rew_mean     | 23.5     |
| time/              |          |
|    fps             | 795      |
|    iterations      | 30       |
|    time_elapsed    | 77       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:39:21,590] Trial 42 finished with value: 26.3 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0003016898718477928, 'gamma': 0.918125426408215, 'gae_lambda': 0.8535814959735822}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 8.21     |
| time/              |          |
|    fps             | 1227     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 146         |
|    ep_rew_mean          | 9.14        |
| time/                   |             |
|    fps                  | 1113        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010726943 |
|    clip_fraction        | 0.0865      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.057      |
|    learning_rate        | 0.000297    |
|    loss                 | 0.034       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.127       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=14.50 +/- 4.32
Episode length: 187.30 +/- 50.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 187         |
|    mean_reward          | 14.5        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009759232 |
|    clip_fraction        | 0.0761      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.161       |
|    learning_rate        | 0.000297    |
|    loss                 | 0.00606     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.117       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 8.62     |
| time/              |          |
|    fps             | 884      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | 9.31        |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.013711768 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.242       |
|    learning_rate        | 0.000297    |
|    loss                 | 0.00477     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.101       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=18.90 +/- 4.72
Episode length: 200.00 +/- 60.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 200         |
|    mean_reward          | 18.9        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014502328 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.00419    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0382     |
|    value_loss           | 0.115       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 831      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 161         |
|    ep_rew_mean          | 11.3        |
| time/                   |             |
|    fps                  | 857         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.019042045 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.03       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0447     |
|    value_loss           | 0.118       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 164         |
|    ep_rew_mean          | 12.1        |
| time/                   |             |
|    fps                  | 876         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.021686798 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0246     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0469     |
|    value_loss           | 0.117       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=18.90 +/- 3.96
Episode length: 196.30 +/- 45.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 196        |
|    mean_reward          | 18.9       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.02427507 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.286      |
|    learning_rate        | 0.000297   |
|    loss                 | -0.0113    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0478    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 12.6     |
| time/              |          |
|    fps             | 832      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 168         |
|    ep_rew_mean          | 13.3        |
| time/                   |             |
|    fps                  | 849         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.024219837 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0236     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.043      |
|    value_loss           | 0.127       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=18.50 +/- 2.20
Episode length: 179.90 +/- 21.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 180         |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.026435895 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0188     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0479     |
|    value_loss           | 0.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 821      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 15.4       |
| time/                   |            |
|    fps                  | 836        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.02916405 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.000297   |
|    loss                 | -0.0299    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.117      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 186       |
|    ep_rew_mean          | 16.6      |
| time/                   |           |
|    fps                  | 848       |
|    iterations           | 12        |
|    time_elapsed         | 28        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.0377004 |
|    clip_fraction        | 0.297     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.979    |
|    explained_variance   | 0.368     |
|    learning_rate        | 0.000297  |
|    loss                 | -0.0281   |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0497   |
|    value_loss           | 0.134     |
---------------------------------------
Eval num_timesteps=25000, episode_reward=21.10 +/- 6.36
Episode length: 205.30 +/- 66.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 205         |
|    mean_reward          | 21.1        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.040065788 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.915      |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0252     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0434     |
|    value_loss           | 0.124       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 822      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 18.3       |
| time/                   |            |
|    fps                  | 833        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.03245627 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.818     |
|    explained_variance   | 0.484      |
|    learning_rate        | 0.000297   |
|    loss                 | -0.025     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.123      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=24.70 +/- 6.15
Episode length: 226.10 +/- 60.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 226        |
|    mean_reward          | 24.7       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.03612119 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.796     |
|    explained_variance   | 0.484      |
|    learning_rate        | 0.000297   |
|    loss                 | -0.00905   |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.04      |
|    value_loss           | 0.124      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 810      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 194        |
|    ep_rew_mean          | 19.3       |
| time/                   |            |
|    fps                  | 820        |
|    iterations           | 16         |
|    time_elapsed         | 39         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.03406164 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.729     |
|    explained_variance   | 0.403      |
|    learning_rate        | 0.000297   |
|    loss                 | -0.00522   |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0343    |
|    value_loss           | 0.128      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 19.6       |
| time/                   |            |
|    fps                  | 830        |
|    iterations           | 17         |
|    time_elapsed         | 41         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.03990588 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.75      |
|    explained_variance   | 0.466      |
|    learning_rate        | 0.000297   |
|    loss                 | -0.0142    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0414    |
|    value_loss           | 0.12       |
----------------------------------------
Eval num_timesteps=35000, episode_reward=24.50 +/- 5.99
Episode length: 216.10 +/- 59.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 216         |
|    mean_reward          | 24.5        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.039814085 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.534       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0019     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0403     |
|    value_loss           | 0.123       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 811      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | 21.3        |
| time/                   |             |
|    fps                  | 820         |
|    iterations           | 19          |
|    time_elapsed         | 47          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.038672082 |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.665      |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.00777    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0377     |
|    value_loss           | 0.116       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=23.50 +/- 4.76
Episode length: 211.10 +/- 40.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | 23.5        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.042975277 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.633      |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0176     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.111       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 21.6     |
| time/              |          |
|    fps             | 805      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 207         |
|    ep_rew_mean          | 21.9        |
| time/                   |             |
|    fps                  | 813         |
|    iterations           | 21          |
|    time_elapsed         | 52          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.046802323 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.628      |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0252     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0378     |
|    value_loss           | 0.121       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=28.00 +/- 4.34
Episode length: 254.90 +/- 45.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 255        |
|    mean_reward          | 28         |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.04601842 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.617     |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.000297   |
|    loss                 | -0.0189    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0387    |
|    value_loss           | 0.117      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 22.4     |
| time/              |          |
|    fps             | 796      |
|    iterations      | 22       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 212         |
|    ep_rew_mean          | 22.6        |
| time/                   |             |
|    fps                  | 803         |
|    iterations           | 23          |
|    time_elapsed         | 58          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.047616176 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0133     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0356     |
|    value_loss           | 0.115       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 213         |
|    ep_rew_mean          | 22.7        |
| time/                   |             |
|    fps                  | 810         |
|    iterations           | 24          |
|    time_elapsed         | 60          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.055517614 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.549      |
|    explained_variance   | 0.518       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0148     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.119       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=23.90 +/- 2.66
Episode length: 201.40 +/- 32.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 201         |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.056385897 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.54       |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0206     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0331     |
|    value_loss           | 0.124       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 25       |
|    time_elapsed    | 63       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 216         |
|    ep_rew_mean          | 23.1        |
| time/                   |             |
|    fps                  | 807         |
|    iterations           | 26          |
|    time_elapsed         | 65          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.057507522 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.567      |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0145     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0375     |
|    value_loss           | 0.119       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=25.20 +/- 4.38
Episode length: 227.20 +/- 61.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 227        |
|    mean_reward          | 25.2       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.05562451 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.506     |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.000297   |
|    loss                 | 0.000498   |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0307    |
|    value_loss           | 0.126      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 23.2     |
| time/              |          |
|    fps             | 795      |
|    iterations      | 27       |
|    time_elapsed    | 69       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 216         |
|    ep_rew_mean          | 23.3        |
| time/                   |             |
|    fps                  | 801         |
|    iterations           | 28          |
|    time_elapsed         | 71          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.045628194 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.561      |
|    explained_variance   | 0.56        |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0227     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.111       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | 23.2        |
| time/                   |             |
|    fps                  | 807         |
|    iterations           | 29          |
|    time_elapsed         | 73          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.062386516 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.542      |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0374     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0404     |
|    value_loss           | 0.104       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=27.00 +/- 4.15
Episode length: 231.30 +/- 40.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 231         |
|    mean_reward          | 27          |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.065067686 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.536      |
|    explained_variance   | 0.586       |
|    learning_rate        | 0.000297    |
|    loss                 | -0.0247     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0355     |
|    value_loss           | 0.118       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 795      |
|    iterations      | 30       |
|    time_elapsed    | 77       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:40:40,782] Trial 43 finished with value: 23.6 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0002969083302096338, 'gamma': 0.9107161080472815, 'gae_lambda': 0.856092610953756}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 7.93     |
| time/              |          |
|    fps             | 1231     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 132         |
|    ep_rew_mean          | 7.03        |
| time/                   |             |
|    fps                  | 1114        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.013315362 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.041      |
|    learning_rate        | 0.0002      |
|    loss                 | 0.0386      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.147       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=10.00 +/- 2.76
Episode length: 123.90 +/- 27.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 124         |
|    mean_reward          | 10          |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.008887464 |
|    clip_fraction        | 0.0682      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.291       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.033       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.119       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 940      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 139        |
|    ep_rew_mean          | 8.14       |
| time/                   |            |
|    fps                  | 958        |
|    iterations           | 4          |
|    time_elapsed         | 8          |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.01303285 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.0002     |
|    loss                 | 0.0106     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0214    |
|    value_loss           | 0.134      |
----------------------------------------
Eval num_timesteps=10000, episode_reward=9.20 +/- 2.96
Episode length: 121.50 +/- 29.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 122        |
|    mean_reward          | 9.2        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.00882329 |
|    clip_fraction        | 0.0799     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.376      |
|    learning_rate        | 0.0002     |
|    loss                 | 0.00628    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0197    |
|    value_loss           | 0.128      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 8.94     |
| time/              |          |
|    fps             | 898      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 148        |
|    ep_rew_mean          | 9.85       |
| time/                   |            |
|    fps                  | 916        |
|    iterations           | 6          |
|    time_elapsed         | 13         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01295213 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.287      |
|    learning_rate        | 0.0002     |
|    loss                 | -0.00811   |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0313    |
|    value_loss           | 0.139      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 150         |
|    ep_rew_mean          | 10.2        |
| time/                   |             |
|    fps                  | 929         |
|    iterations           | 7           |
|    time_elapsed         | 15          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.013324004 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.032       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.14        |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=15.60 +/- 5.20
Episode length: 175.10 +/- 61.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 175        |
|    mean_reward          | 15.6       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01580618 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.504      |
|    learning_rate        | 0.0002     |
|    loss                 | 0.0224     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0299    |
|    value_loss           | 0.128      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 878      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 157         |
|    ep_rew_mean          | 11.9        |
| time/                   |             |
|    fps                  | 892         |
|    iterations           | 9           |
|    time_elapsed         | 20          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.015576439 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.00436     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.139       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=16.40 +/- 4.82
Episode length: 170.00 +/- 53.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 170         |
|    mean_reward          | 16.4        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.016604984 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.00638     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0352     |
|    value_loss           | 0.139       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 859      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | 14.3        |
| time/                   |             |
|    fps                  | 871         |
|    iterations           | 11          |
|    time_elapsed         | 25          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.018946003 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0002      |
|    loss                 | -0.0231     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0382     |
|    value_loss           | 0.138       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 176         |
|    ep_rew_mean          | 15.3        |
| time/                   |             |
|    fps                  | 882         |
|    iterations           | 12          |
|    time_elapsed         | 27          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.020272834 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.0002      |
|    loss                 | -0.0237     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0369     |
|    value_loss           | 0.136       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=20.70 +/- 5.31
Episode length: 197.50 +/- 52.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 198         |
|    mean_reward          | 20.7        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.018592872 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0002      |
|    loss                 | -0.00172    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0373     |
|    value_loss           | 0.13        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 853      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 17.2        |
| time/                   |             |
|    fps                  | 862         |
|    iterations           | 14          |
|    time_elapsed         | 33          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.022990141 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.947      |
|    explained_variance   | 0.55        |
|    learning_rate        | 0.0002      |
|    loss                 | 0.000378    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.132       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=19.50 +/- 4.76
Episode length: 183.00 +/- 38.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | 19.5        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.025529642 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.916      |
|    explained_variance   | 0.533       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.0125      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.138       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 842      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 195         |
|    ep_rew_mean          | 18.8        |
| time/                   |             |
|    fps                  | 851         |
|    iterations           | 16          |
|    time_elapsed         | 38          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.024953203 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.892      |
|    explained_variance   | 0.447       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.00328     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.143       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 19.2        |
| time/                   |             |
|    fps                  | 859         |
|    iterations           | 17          |
|    time_elapsed         | 40          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.022370623 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.88       |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.0337      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0335     |
|    value_loss           | 0.137       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=23.90 +/- 3.48
Episode length: 220.80 +/- 30.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 221         |
|    mean_reward          | 23.9        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.022743238 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.861      |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0002      |
|    loss                 | -0.0041     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0371     |
|    value_loss           | 0.145       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 837      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 198         |
|    ep_rew_mean          | 20.2        |
| time/                   |             |
|    fps                  | 845         |
|    iterations           | 19          |
|    time_elapsed         | 46          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.022722285 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.833      |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.00646     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.121       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=23.50 +/- 3.98
Episode length: 206.60 +/- 39.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 207        |
|    mean_reward          | 23.5       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.02417315 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.79      |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0002     |
|    loss                 | 0.0146     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0353    |
|    value_loss           | 0.127      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 20.6     |
| time/              |          |
|    fps             | 828      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | 21          |
| time/                   |             |
|    fps                  | 835         |
|    iterations           | 21          |
|    time_elapsed         | 51          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.031360228 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.544       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.00203     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.129       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=24.20 +/- 5.13
Episode length: 213.20 +/- 46.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 213         |
|    mean_reward          | 24.2        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.022363372 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.527       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.0179      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.03       |
|    value_loss           | 0.135       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 21.4     |
| time/              |          |
|    fps             | 820      |
|    iterations      | 22       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 203        |
|    ep_rew_mean          | 21.8       |
| time/                   |            |
|    fps                  | 827        |
|    iterations           | 23         |
|    time_elapsed         | 56         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.02847146 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.702     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.0002     |
|    loss                 | -0.00738   |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0332    |
|    value_loss           | 0.12       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | 21.8        |
| time/                   |             |
|    fps                  | 833         |
|    iterations           | 24          |
|    time_elapsed         | 58          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.027686328 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.0002      |
|    loss                 | 0.00541     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0283     |
|    value_loss           | 0.126       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=25.80 +/- 4.38
Episode length: 224.70 +/- 42.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 225        |
|    mean_reward          | 25.8       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.02745539 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.621     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.0002     |
|    loss                 | 0.0147     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0314    |
|    value_loss           | 0.139      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 21.8     |
| time/              |          |
|    fps             | 819      |
|    iterations      | 25       |
|    time_elapsed    | 62       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 201         |
|    ep_rew_mean          | 22          |
| time/                   |             |
|    fps                  | 825         |
|    iterations           | 26          |
|    time_elapsed         | 64          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.024987467 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.605      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.0131      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.131       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=26.90 +/- 6.99
Episode length: 233.10 +/- 63.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 233         |
|    mean_reward          | 26.9        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.030390114 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.0099      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0306     |
|    value_loss           | 0.12        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 22.1     |
| time/              |          |
|    fps             | 811      |
|    iterations      | 27       |
|    time_elapsed    | 68       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 206        |
|    ep_rew_mean          | 22.6       |
| time/                   |            |
|    fps                  | 817        |
|    iterations           | 28         |
|    time_elapsed         | 70         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.03401841 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.581     |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.0002     |
|    loss                 | -0.00199   |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0283    |
|    value_loss           | 0.131      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 204         |
|    ep_rew_mean          | 22.7        |
| time/                   |             |
|    fps                  | 823         |
|    iterations           | 29          |
|    time_elapsed         | 72          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.030420851 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0002      |
|    loss                 | 0.0217      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0282     |
|    value_loss           | 0.124       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=25.80 +/- 5.25
Episode length: 225.40 +/- 59.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 225        |
|    mean_reward          | 25.8       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.04076095 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.53      |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.0002     |
|    loss                 | 0.00141    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0306    |
|    value_loss           | 0.129      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 811      |
|    iterations      | 30       |
|    time_elapsed    | 75       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:41:58,696] Trial 44 finished with value: 27.8 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00019980656183021776, 'gamma': 0.9225645212342366, 'gae_lambda': 0.8550818874102561}. Best is trial 31 with value: 29.6.
Using cuda device
Eval num_timesteps=5000, episode_reward=4.80 +/- 1.89
Episode length: 110.60 +/- 22.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 4.8      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 1106     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=13.50 +/- 4.20
Episode length: 167.20 +/- 45.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 167        |
|    mean_reward          | 13.5       |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.00962962 |
|    clip_fraction        | 0.0986     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | -0.116     |
|    learning_rate        | 0.000186   |
|    loss                 | 0.0682     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0121    |
|    value_loss           | 0.165      |
----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=11.50 +/- 2.69
Episode length: 138.40 +/- 32.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 8.3      |
| time/              |          |
|    fps             | 933      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=15.30 +/- 5.53
Episode length: 184.60 +/- 73.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 185         |
|    mean_reward          | 15.3        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012440439 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.249       |
|    learning_rate        | 0.000186    |
|    loss                 | -0.00295    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.149       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 914      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=13.70 +/- 2.19
Episode length: 155.90 +/- 22.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 156        |
|    mean_reward          | 13.7       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01351991 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.386      |
|    learning_rate        | 0.000186   |
|    loss                 | 0.0184     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0327    |
|    value_loss           | 0.14       |
----------------------------------------
Eval num_timesteps=30000, episode_reward=11.70 +/- 4.47
Episode length: 133.70 +/- 51.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 11.7     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 887      |
|    iterations      | 4        |
|    time_elapsed    | 36       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=17.30 +/- 5.68
Episode length: 191.10 +/- 71.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 191          |
|    mean_reward          | 17.3         |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0155614205 |
|    clip_fraction        | 0.225        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.424        |
|    learning_rate        | 0.000186     |
|    loss                 | 0.00157      |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0354      |
|    value_loss           | 0.139        |
------------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=15.10 +/- 3.45
Episode length: 163.70 +/- 39.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 15.4     |
| time/              |          |
|    fps             | 864      |
|    iterations      | 5        |
|    time_elapsed    | 47       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=17.60 +/- 5.71
Episode length: 173.90 +/- 63.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 174         |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.018923067 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.000186    |
|    loss                 | -0.00606    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0384     |
|    value_loss           | 0.136       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 16.9     |
| time/              |          |
|    fps             | 867      |
|    iterations      | 6        |
|    time_elapsed    | 56       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=21.20 +/- 6.69
Episode length: 203.40 +/- 83.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 21.2        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.019464873 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.48        |
|    learning_rate        | 0.000186    |
|    loss                 | -0.00455    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0374     |
|    value_loss           | 0.134       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=23.30 +/- 5.29
Episode length: 231.30 +/- 59.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 847      |
|    iterations      | 7        |
|    time_elapsed    | 67       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=20.30 +/- 6.74
Episode length: 189.60 +/- 62.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 190         |
|    mean_reward          | 20.3        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.022598892 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.000186    |
|    loss                 | 0.0315      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0381     |
|    value_loss           | 0.138       |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=21.30 +/- 3.41
Episode length: 189.80 +/- 33.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | 21.3     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 837      |
|    iterations      | 8        |
|    time_elapsed    | 78       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-16 19:43:20,172] Trial 45 finished with value: 25.5 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 0.00018630932632128586, 'gamma': 0.9208782478621298, 'gae_lambda': 0.8735006392577086}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 1221     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 151         |
|    ep_rew_mean          | 8.67        |
| time/                   |             |
|    fps                  | 910         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.015589466 |
|    clip_fraction        | 0.083       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0773     |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0633      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.122       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=8.60 +/- 1.80
Episode length: 115.20 +/- 23.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 115         |
|    mean_reward          | 8.6         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010277733 |
|    clip_fraction        | 0.0821      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.126       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.101       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.123       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 8.9      |
| time/              |          |
|    fps             | 759      |
|    iterations      | 3        |
|    time_elapsed    | 8        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 153         |
|    ep_rew_mean          | 8.96        |
| time/                   |             |
|    fps                  | 751         |
|    iterations           | 4           |
|    time_elapsed         | 10          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.008372936 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.299       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0365      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.117       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=11.90 +/- 5.09
Episode length: 155.90 +/- 52.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008075569 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.372       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.00599     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.124       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 9.58     |
| time/              |          |
|    fps             | 695      |
|    iterations      | 5        |
|    time_elapsed    | 14       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 157         |
|    ep_rew_mean          | 9.82        |
| time/                   |             |
|    fps                  | 701         |
|    iterations           | 6           |
|    time_elapsed         | 17          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011186017 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.4         |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0191      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.13        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 155         |
|    ep_rew_mean          | 9.87        |
| time/                   |             |
|    fps                  | 704         |
|    iterations           | 7           |
|    time_elapsed         | 20          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.008182974 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.425       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0119      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 0.142       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=9.50 +/- 3.23
Episode length: 136.90 +/- 51.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 137         |
|    mean_reward          | 9.5         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.009964114 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.51        |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0564      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.122       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 680      |
|    iterations      | 8        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 167          |
|    ep_rew_mean          | 11.6         |
| time/                   |              |
|    fps                  | 685          |
|    iterations           | 9            |
|    time_elapsed         | 26           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0068172934 |
|    clip_fraction        | 0.0647       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.377        |
|    learning_rate        | 6.09e-05     |
|    loss                 | -0.00613     |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.0166      |
|    value_loss           | 0.133        |
------------------------------------------
Eval num_timesteps=20000, episode_reward=12.80 +/- 3.97
Episode length: 140.30 +/- 46.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 140        |
|    mean_reward          | 12.8       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.01014847 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.379      |
|    learning_rate        | 6.09e-05   |
|    loss                 | 0.0148     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0238    |
|    value_loss           | 0.149      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 667      |
|    iterations      | 10       |
|    time_elapsed    | 30       |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 173          |
|    ep_rew_mean          | 13.2         |
| time/                   |              |
|    fps                  | 672          |
|    iterations           | 11           |
|    time_elapsed         | 33           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0138609335 |
|    clip_fraction        | 0.144        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.403        |
|    learning_rate        | 6.09e-05     |
|    loss                 | -0.019       |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.0285      |
|    value_loss           | 0.144        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | 14.2        |
| time/                   |             |
|    fps                  | 676         |
|    iterations           | 12          |
|    time_elapsed         | 36          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009583105 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.48        |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0177      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 0.144       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=21.70 +/- 7.10
Episode length: 223.90 +/- 84.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 21.7       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01056891 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.979     |
|    explained_variance   | 0.433      |
|    learning_rate        | 6.09e-05   |
|    loss                 | 0.0602     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.022     |
|    value_loss           | 0.16       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 655      |
|    iterations      | 13       |
|    time_elapsed    | 40       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | 15.9        |
| time/                   |             |
|    fps                  | 660         |
|    iterations           | 14          |
|    time_elapsed         | 43          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.009080173 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.988      |
|    explained_variance   | 0.496       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0526      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.147       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=17.00 +/- 3.13
Episode length: 175.40 +/- 43.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 175         |
|    mean_reward          | 17          |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.009537276 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.965      |
|    explained_variance   | 0.456       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0299      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.15        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 16.5     |
| time/              |          |
|    fps             | 647      |
|    iterations      | 15       |
|    time_elapsed    | 47       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 191          |
|    ep_rew_mean          | 17.6         |
| time/                   |              |
|    fps                  | 653          |
|    iterations           | 16           |
|    time_elapsed         | 50           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0114018805 |
|    clip_fraction        | 0.129        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.898       |
|    explained_variance   | 0.521        |
|    learning_rate        | 6.09e-05     |
|    loss                 | 0.0132       |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.0207      |
|    value_loss           | 0.148        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 191         |
|    ep_rew_mean          | 17.9        |
| time/                   |             |
|    fps                  | 658         |
|    iterations           | 17          |
|    time_elapsed         | 52          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.011539858 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.836      |
|    explained_variance   | 0.469       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0146      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.149       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=16.10 +/- 2.88
Episode length: 159.00 +/- 28.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 159         |
|    mean_reward          | 16.1        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012694481 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.776      |
|    explained_variance   | 0.538       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0118      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 0.135       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 649      |
|    iterations      | 18       |
|    time_elapsed    | 56       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 192         |
|    ep_rew_mean          | 18.8        |
| time/                   |             |
|    fps                  | 653         |
|    iterations           | 19          |
|    time_elapsed         | 59          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.011150161 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.529       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.033       |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.14        |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=19.50 +/- 2.62
Episode length: 183.20 +/- 32.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | 19.5        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.010784229 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.722      |
|    explained_variance   | 0.57        |
|    learning_rate        | 6.09e-05    |
|    loss                 | -0.00832    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.132       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 645      |
|    iterations      | 20       |
|    time_elapsed    | 63       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 192         |
|    ep_rew_mean          | 19.2        |
| time/                   |             |
|    fps                  | 650         |
|    iterations           | 21          |
|    time_elapsed         | 66          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.014111828 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.726      |
|    explained_variance   | 0.59        |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.00162     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.125       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=18.10 +/- 5.92
Episode length: 179.70 +/- 58.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 180         |
|    mean_reward          | 18.1        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.012951523 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.545       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0658      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 0.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 642      |
|    iterations      | 22       |
|    time_elapsed    | 70       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 19         |
| time/                   |            |
|    fps                  | 646        |
|    iterations           | 23         |
|    time_elapsed         | 72         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.01398832 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.728     |
|    explained_variance   | 0.615      |
|    learning_rate        | 6.09e-05   |
|    loss                 | 0.0322     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0246    |
|    value_loss           | 0.128      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 188         |
|    ep_rew_mean          | 19.1        |
| time/                   |             |
|    fps                  | 649         |
|    iterations           | 24          |
|    time_elapsed         | 75          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.014642069 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.714      |
|    explained_variance   | 0.703       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0221      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.118       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=18.00 +/- 4.58
Episode length: 168.60 +/- 45.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 18          |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.014469114 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.708      |
|    explained_variance   | 0.619       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0451      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.138       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 19.1     |
| time/              |          |
|    fps             | 644      |
|    iterations      | 25       |
|    time_elapsed    | 79       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | 19          |
| time/                   |             |
|    fps                  | 647         |
|    iterations           | 26          |
|    time_elapsed         | 82          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.011568825 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.679       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0169      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.132       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=20.00 +/- 3.44
Episode length: 187.40 +/- 32.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 187          |
|    mean_reward          | 20           |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0146345645 |
|    clip_fraction        | 0.138        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.693       |
|    explained_variance   | 0.664        |
|    learning_rate        | 6.09e-05     |
|    loss                 | -0.0164      |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.0216      |
|    value_loss           | 0.125        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 640      |
|    iterations      | 27       |
|    time_elapsed    | 86       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 188         |
|    ep_rew_mean          | 19.3        |
| time/                   |             |
|    fps                  | 643         |
|    iterations           | 28          |
|    time_elapsed         | 89          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.012576528 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.668      |
|    explained_variance   | 0.656       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0207      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 0.133       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 188         |
|    ep_rew_mean          | 19.4        |
| time/                   |             |
|    fps                  | 646         |
|    iterations           | 29          |
|    time_elapsed         | 91          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.012305496 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.59        |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0582      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.143       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=23.30 +/- 6.65
Episode length: 206.40 +/- 63.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | 23.3        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.012128539 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.636       |
|    learning_rate        | 6.09e-05    |
|    loss                 | 0.0247      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.133       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 19.7     |
| time/              |          |
|    fps             | 639      |
|    iterations      | 30       |
|    time_elapsed    | 96       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:44:59,162] Trial 46 finished with value: 24.1 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 6.085527686522235e-05, 'gamma': 0.9363059600820791, 'gae_lambda': 0.8357838561248804}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 7.47     |
| time/              |          |
|    fps             | 1231     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 129         |
|    ep_rew_mean          | 7.1         |
| time/                   |             |
|    fps                  | 1115        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009795943 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0333     |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0329      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.128       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=9.50 +/- 3.64
Episode length: 162.20 +/- 54.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 162         |
|    mean_reward          | 9.5         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009020306 |
|    clip_fraction        | 0.0652      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.165       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0459      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.127       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 7.28     |
| time/              |          |
|    fps             | 908      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 133         |
|    ep_rew_mean          | 7.78        |
| time/                   |             |
|    fps                  | 933         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012220362 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0597      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.129       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=11.80 +/- 3.03
Episode length: 144.50 +/- 39.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 11.8        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.006216392 |
|    clip_fraction        | 0.0574      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0312      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.148       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 8.55     |
| time/              |          |
|    fps             | 869      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 142         |
|    ep_rew_mean          | 9.26        |
| time/                   |             |
|    fps                  | 891         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012306944 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0287      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.142       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | 10.1        |
| time/                   |             |
|    fps                  | 907         |
|    iterations           | 7           |
|    time_elapsed         | 15          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.011829783 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0241      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.028      |
|    value_loss           | 0.172       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=13.60 +/- 4.67
Episode length: 140.40 +/- 60.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 140         |
|    mean_reward          | 13.6        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.013818161 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0374      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.154       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 11.3     |
| time/              |          |
|    fps             | 873      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 159         |
|    ep_rew_mean          | 12.2        |
| time/                   |             |
|    fps                  | 887         |
|    iterations           | 9           |
|    time_elapsed         | 20          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.010547271 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0573      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.147       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=13.60 +/- 2.15
Episode length: 142.40 +/- 26.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 142         |
|    mean_reward          | 13.6        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.008751638 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0238      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.163       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 175         |
|    ep_rew_mean          | 14.7        |
| time/                   |             |
|    fps                  | 874         |
|    iterations           | 11          |
|    time_elapsed         | 25          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.011165895 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.406       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0109      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.151       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 181         |
|    ep_rew_mean          | 15.7        |
| time/                   |             |
|    fps                  | 884         |
|    iterations           | 12          |
|    time_elapsed         | 27          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.012217319 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0114      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.144       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=17.60 +/- 3.77
Episode length: 176.10 +/- 34.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 176         |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009428644 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0144      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.146       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 858      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | 17          |
| time/                   |             |
|    fps                  | 867         |
|    iterations           | 14          |
|    time_elapsed         | 33          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.012009907 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0258      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.136       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=17.00 +/- 6.21
Episode length: 175.20 +/- 71.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 175         |
|    mean_reward          | 17          |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.012271033 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.94       |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0213      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.157       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 847      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 17.8        |
| time/                   |             |
|    fps                  | 856         |
|    iterations           | 16          |
|    time_elapsed         | 38          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.010094366 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.923      |
|    explained_variance   | 0.538       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0128      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.144       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 18.1        |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 17          |
|    time_elapsed         | 40          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.013253322 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.875      |
|    explained_variance   | 0.498       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0129      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.155       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=17.30 +/- 2.10
Episode length: 166.00 +/- 26.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 17.3        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014213121 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.889      |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.028       |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.145       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 849      |
|    iterations      | 18       |
|    time_elapsed    | 43       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 188         |
|    ep_rew_mean          | 18.6        |
| time/                   |             |
|    fps                  | 857         |
|    iterations           | 19          |
|    time_elapsed         | 45          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.013587232 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.854      |
|    explained_variance   | 0.444       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0317      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.161       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=18.40 +/- 4.98
Episode length: 177.70 +/- 46.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 178        |
|    mean_reward          | 18.4       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.01227097 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.817     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.000136   |
|    loss                 | 0.0239     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0241    |
|    value_loss           | 0.139      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 842      |
|    iterations      | 20       |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | 18.8        |
| time/                   |             |
|    fps                  | 849         |
|    iterations           | 21          |
|    time_elapsed         | 50          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.013745266 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.777      |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.00642     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.134       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=19.60 +/- 5.02
Episode length: 179.50 +/- 45.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 180         |
|    mean_reward          | 19.6        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.012433143 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.759      |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.033       |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.146       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 19       |
| time/              |          |
|    fps             | 837      |
|    iterations      | 22       |
|    time_elapsed    | 53       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 192         |
|    ep_rew_mean          | 19.5        |
| time/                   |             |
|    fps                  | 843         |
|    iterations           | 23          |
|    time_elapsed         | 55          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.012230857 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.000136    |
|    loss                 | 0.00952     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 0.135       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 198         |
|    ep_rew_mean          | 20.1        |
| time/                   |             |
|    fps                  | 849         |
|    iterations           | 24          |
|    time_elapsed         | 57          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.021000557 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0205      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.134       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=20.90 +/- 4.06
Episode length: 198.20 +/- 41.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 198         |
|    mean_reward          | 20.9        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.014345461 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0273      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0267     |
|    value_loss           | 0.137       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 836      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | 20.6        |
| time/                   |             |
|    fps                  | 842         |
|    iterations           | 26          |
|    time_elapsed         | 63          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.016624702 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.681      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.000136    |
|    loss                 | -0.00581    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.115       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=22.80 +/- 4.62
Episode length: 214.30 +/- 48.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 214         |
|    mean_reward          | 22.8        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.018832214 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.684      |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.000136    |
|    loss                 | -0.00793    |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.135       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 20.7     |
| time/              |          |
|    fps             | 828      |
|    iterations      | 27       |
|    time_elapsed    | 66       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | 20.9        |
| time/                   |             |
|    fps                  | 834         |
|    iterations           | 28          |
|    time_elapsed         | 68          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.016813964 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.695      |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0165      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0267     |
|    value_loss           | 0.136       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | 20.9        |
| time/                   |             |
|    fps                  | 839         |
|    iterations           | 29          |
|    time_elapsed         | 70          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.015674066 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.683      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0127      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.146       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=21.50 +/- 6.39
Episode length: 194.90 +/- 66.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 195         |
|    mean_reward          | 21.5        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.015659895 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.68       |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.000136    |
|    loss                 | 0.0368      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.026      |
|    value_loss           | 0.145       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 21.6     |
| time/              |          |
|    fps             | 829      |
|    iterations      | 30       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:46:15,103] Trial 47 finished with value: 20.4 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00013619456477790006, 'gamma': 0.9236762657322847, 'gae_lambda': 0.8643555007282098}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 9.46     |
| time/              |          |
|    fps             | 1233     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=12.40 +/- 4.34
Episode length: 158.00 +/- 47.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 12.4        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012867053 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0772     |
|    learning_rate        | 0.000636    |
|    loss                 | 0.0316      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.121       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 9.04     |
| time/              |          |
|    fps             | 972      |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=13.30 +/- 4.90
Episode length: 142.00 +/- 48.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 142         |
|    mean_reward          | 13.3        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.026527142 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.15        |
|    learning_rate        | 0.000636    |
|    loss                 | -0.0161     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0452     |
|    value_loss           | 0.112       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 912      |
|    iterations      | 3        |
|    time_elapsed    | 13       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=21.40 +/- 3.67
Episode length: 219.20 +/- 46.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 219         |
|    mean_reward          | 21.4        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.039248865 |
|    clip_fraction        | 0.375       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.000636    |
|    loss                 | -0.0447     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0604     |
|    value_loss           | 0.113       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 11.6     |
| time/              |          |
|    fps             | 861      |
|    iterations      | 4        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=22.00 +/- 5.31
Episode length: 210.50 +/- 52.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | 22          |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.044510286 |
|    clip_fraction        | 0.402       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000636    |
|    loss                 | -0.0606     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0617     |
|    value_loss           | 0.111       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 835      |
|    iterations      | 5        |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | 15.1       |
| time/                   |            |
|    fps                  | 859        |
|    iterations           | 6          |
|    time_elapsed         | 28         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.05353208 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.000636   |
|    loss                 | -0.0582    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0644    |
|    value_loss           | 0.108      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=21.80 +/- 4.96
Episode length: 198.70 +/- 48.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 21.8        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.065433815 |
|    clip_fraction        | 0.444       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.418       |
|    learning_rate        | 0.000636    |
|    loss                 | -0.0515     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0643     |
|    value_loss           | 0.106       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 842      |
|    iterations      | 7        |
|    time_elapsed    | 34       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=19.60 +/- 3.72
Episode length: 184.00 +/- 33.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 184        |
|    mean_reward          | 19.6       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.06409818 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.442      |
|    learning_rate        | 0.000636   |
|    loss                 | -0.0577    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0609    |
|    value_loss           | 0.0985     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 8        |
|    time_elapsed    | 39       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=23.20 +/- 5.62
Episode length: 213.70 +/- 54.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 214        |
|    mean_reward          | 23.2       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.07497677 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.487      |
|    learning_rate        | 0.000636   |
|    loss                 | -0.0608    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0622    |
|    value_loss           | 0.102      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 821      |
|    iterations      | 9        |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=22.30 +/- 5.50
Episode length: 202.30 +/- 53.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 202       |
|    mean_reward          | 22.3      |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0869052 |
|    clip_fraction        | 0.455     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.968    |
|    explained_variance   | 0.517     |
|    learning_rate        | 0.000636  |
|    loss                 | -0.063    |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.0621   |
|    value_loss           | 0.0973    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 19.7     |
| time/              |          |
|    fps             | 814      |
|    iterations      | 10       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=25.80 +/- 4.73
Episode length: 240.00 +/- 48.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 240        |
|    mean_reward          | 25.8       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.09564844 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.914     |
|    explained_variance   | 0.482      |
|    learning_rate        | 0.000636   |
|    loss                 | -0.059     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0627    |
|    value_loss           | 0.099      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 210      |
|    ep_rew_mean     | 20.5     |
| time/              |          |
|    fps             | 805      |
|    iterations      | 11       |
|    time_elapsed    | 55       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | 20.7        |
| time/                   |             |
|    fps                  | 818         |
|    iterations           | 12          |
|    time_elapsed         | 60          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.109870255 |
|    clip_fraction        | 0.452       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.872      |
|    explained_variance   | 0.524       |
|    learning_rate        | 0.000636    |
|    loss                 | -0.0752     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.06       |
|    value_loss           | 0.0913      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=24.80 +/- 5.83
Episode length: 212.30 +/- 60.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 24.8       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.11506416 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.827     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.000636   |
|    loss                 | -0.0711    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0567    |
|    value_loss           | 0.0984     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | 21.7     |
| time/              |          |
|    fps             | 812      |
|    iterations      | 13       |
|    time_elapsed    | 65       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=24.00 +/- 3.55
Episode length: 225.80 +/- 38.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 226        |
|    mean_reward          | 24         |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.10828595 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.776     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.000636   |
|    loss                 | -0.0566    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.0898     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 22.6     |
| time/              |          |
|    fps             | 806      |
|    iterations      | 14       |
|    time_elapsed    | 71       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=26.10 +/- 8.03
Episode length: 241.10 +/- 82.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 241        |
|    mean_reward          | 26.1       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.12747744 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.756     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.000636   |
|    loss                 | -0.0533    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0506    |
|    value_loss           | 0.09       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 15       |
|    time_elapsed    | 76       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:47:34,307] Trial 48 finished with value: 24.0 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.0006356799897984346, 'gamma': 0.9074249455150161, 'gae_lambda': 0.8208200755195938}. Best is trial 31 with value: 29.6.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 7.44     |
| time/              |          |
|    fps             | 1210     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 137         |
|    ep_rew_mean          | 8.52        |
| time/                   |             |
|    fps                  | 1098        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.006364928 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0913     |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.0795      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 0.161       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=9.50 +/- 2.54
Episode length: 124.70 +/- 36.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 9.5         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.008265529 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.103       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.067       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00466    |
|    value_loss           | 0.162       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 8.45     |
| time/              |          |
|    fps             | 929      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 137       |
|    ep_rew_mean          | 8.61      |
| time/                   |           |
|    fps                  | 946       |
|    iterations           | 4         |
|    time_elapsed         | 8         |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0110137 |
|    clip_fraction        | 0.0516    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.32     |
|    explained_variance   | 0.158     |
|    learning_rate        | 3.52e-05  |
|    loss                 | 0.0547    |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.00651  |
|    value_loss           | 0.176     |
---------------------------------------
Eval num_timesteps=10000, episode_reward=10.20 +/- 2.09
Episode length: 138.00 +/- 28.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 138         |
|    mean_reward          | 10.2        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014959218 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.234       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.0595      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.196       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 8.68     |
| time/              |          |
|    fps             | 880      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 143          |
|    ep_rew_mean          | 9.05         |
| time/                   |              |
|    fps                  | 899          |
|    iterations           | 6            |
|    time_elapsed         | 13           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0088841915 |
|    clip_fraction        | 0.0851       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.287        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0998       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00888     |
|    value_loss           | 0.19         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 146         |
|    ep_rew_mean          | 9.43        |
| time/                   |             |
|    fps                  | 913         |
|    iterations           | 7           |
|    time_elapsed         | 15          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.015755855 |
|    clip_fraction        | 0.0975      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.403       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.0589      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.185       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=10.30 +/- 4.88
Episode length: 140.30 +/- 76.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 140         |
|    mean_reward          | 10.3        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.008183511 |
|    clip_fraction        | 0.0601      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.469       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.0725      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00677    |
|    value_loss           | 0.17        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 9.91     |
| time/              |          |
|    fps             | 875      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 151         |
|    ep_rew_mean          | 10.2        |
| time/                   |             |
|    fps                  | 888         |
|    iterations           | 9           |
|    time_elapsed         | 20          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.011421901 |
|    clip_fraction        | 0.0763      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.95       |
|    explained_variance   | 0.434       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.0838      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00742    |
|    value_loss           | 0.213       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=9.10 +/- 2.34
Episode length: 126.60 +/- 29.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 127          |
|    mean_reward          | 9.1          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0025737428 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.893       |
|    explained_variance   | 0.457        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0647       |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00228     |
|    value_loss           | 0.218        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 11       |
| time/              |          |
|    fps             | 865      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 160          |
|    ep_rew_mean          | 11.5         |
| time/                   |              |
|    fps                  | 876          |
|    iterations           | 11           |
|    time_elapsed         | 25           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0049816817 |
|    clip_fraction        | 0.0443       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.914       |
|    explained_variance   | 0.424        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.076        |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00523     |
|    value_loss           | 0.201        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | 12.1        |
| time/                   |             |
|    fps                  | 886         |
|    iterations           | 12          |
|    time_elapsed         | 27          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.005722056 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.864      |
|    explained_variance   | 0.546       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.0762      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00452    |
|    value_loss           | 0.207       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=10.40 +/- 4.69
Episode length: 139.60 +/- 61.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 140          |
|    mean_reward          | 10.4         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0030135135 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.781       |
|    explained_variance   | 0.625        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0575       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00378     |
|    value_loss           | 0.189        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 12.6     |
| time/              |          |
|    fps             | 867      |
|    iterations      | 13       |
|    time_elapsed    | 30       |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 175          |
|    ep_rew_mean          | 13.1         |
| time/                   |              |
|    fps                  | 876          |
|    iterations           | 14           |
|    time_elapsed         | 32           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0035046404 |
|    clip_fraction        | 0.0313       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.788       |
|    explained_variance   | 0.639        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0733       |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00386     |
|    value_loss           | 0.188        |
------------------------------------------
Eval num_timesteps=30000, episode_reward=10.00 +/- 1.95
Episode length: 134.70 +/- 29.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 135          |
|    mean_reward          | 10           |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0049310224 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.759       |
|    explained_variance   | 0.633        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0774       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 0.187        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 861      |
|    iterations      | 15       |
|    time_elapsed    | 35       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 175          |
|    ep_rew_mean          | 13.6         |
| time/                   |              |
|    fps                  | 868          |
|    iterations           | 16           |
|    time_elapsed         | 37           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0024769902 |
|    clip_fraction        | 0.0337       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.757       |
|    explained_variance   | 0.584        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0899       |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00399     |
|    value_loss           | 0.201        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 174         |
|    ep_rew_mean          | 13.6        |
| time/                   |             |
|    fps                  | 875         |
|    iterations           | 17          |
|    time_elapsed         | 39          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.002218528 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.794      |
|    explained_variance   | 0.542       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.0884      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00359    |
|    value_loss           | 0.226       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=9.80 +/- 2.79
Episode length: 129.50 +/- 38.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 130          |
|    mean_reward          | 9.8          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0034356746 |
|    clip_fraction        | 0.0444       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.817       |
|    explained_variance   | 0.609        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0854       |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00675     |
|    value_loss           | 0.208        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 863      |
|    iterations      | 18       |
|    time_elapsed    | 42       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 173          |
|    ep_rew_mean          | 13.8         |
| time/                   |              |
|    fps                  | 869          |
|    iterations           | 19           |
|    time_elapsed         | 44           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0023207827 |
|    clip_fraction        | 0.0343       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.784       |
|    explained_variance   | 0.581        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0712       |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00476     |
|    value_loss           | 0.197        |
------------------------------------------
Eval num_timesteps=40000, episode_reward=10.50 +/- 4.06
Episode length: 140.10 +/- 53.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 140          |
|    mean_reward          | 10.5         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0017527835 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.822       |
|    explained_variance   | 0.618        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0796       |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 0.211        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 857      |
|    iterations      | 20       |
|    time_elapsed    | 47       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 173          |
|    ep_rew_mean          | 14           |
| time/                   |              |
|    fps                  | 863          |
|    iterations           | 21           |
|    time_elapsed         | 49           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0044326866 |
|    clip_fraction        | 0.0536       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.824       |
|    explained_variance   | 0.684        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0617       |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00697     |
|    value_loss           | 0.184        |
------------------------------------------
Eval num_timesteps=45000, episode_reward=7.80 +/- 1.89
Episode length: 108.40 +/- 27.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 108          |
|    mean_reward          | 7.8          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0031454675 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.75        |
|    explained_variance   | 0.625        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0717       |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00315     |
|    value_loss           | 0.203        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 857      |
|    iterations      | 22       |
|    time_elapsed    | 52       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 175          |
|    ep_rew_mean          | 14.5         |
| time/                   |              |
|    fps                  | 862          |
|    iterations           | 23           |
|    time_elapsed         | 54           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0012503132 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.754       |
|    explained_variance   | 0.595        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0918       |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 0.202        |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 174          |
|    ep_rew_mean          | 14.4         |
| time/                   |              |
|    fps                  | 868          |
|    iterations           | 24           |
|    time_elapsed         | 56           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0021966684 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.739       |
|    explained_variance   | 0.62         |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0977       |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00464     |
|    value_loss           | 0.215        |
------------------------------------------
Eval num_timesteps=50000, episode_reward=9.40 +/- 2.37
Episode length: 123.80 +/- 27.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 124         |
|    mean_reward          | 9.4         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.003297762 |
|    clip_fraction        | 0.0468      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.702       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.0651      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00539    |
|    value_loss           | 0.211       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 860      |
|    iterations      | 25       |
|    time_elapsed    | 59       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 174          |
|    ep_rew_mean          | 14.5         |
| time/                   |              |
|    fps                  | 864          |
|    iterations           | 26           |
|    time_elapsed         | 61           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0047352905 |
|    clip_fraction        | 0.0633       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.799       |
|    explained_variance   | 0.632        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0768       |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00625     |
|    value_loss           | 0.207        |
------------------------------------------
Eval num_timesteps=55000, episode_reward=9.70 +/- 2.69
Episode length: 128.40 +/- 36.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 9.7         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.003546509 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.795      |
|    explained_variance   | 0.713       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.0665      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 0.186       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 14.9     |
| time/              |          |
|    fps             | 857      |
|    iterations      | 27       |
|    time_elapsed    | 64       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 180          |
|    ep_rew_mean          | 15.2         |
| time/                   |              |
|    fps                  | 861          |
|    iterations           | 28           |
|    time_elapsed         | 66           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0024895717 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.808       |
|    explained_variance   | 0.757        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0737       |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00584     |
|    value_loss           | 0.173        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | 15.5        |
| time/                   |             |
|    fps                  | 865         |
|    iterations           | 29          |
|    time_elapsed         | 68          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.003850701 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.682       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.0974      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00581    |
|    value_loss           | 0.205       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=9.70 +/- 3.23
Episode length: 132.10 +/- 45.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 132          |
|    mean_reward          | 9.7          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0015081514 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.793       |
|    explained_variance   | 0.723        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0673       |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00362     |
|    value_loss           | 0.18         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 15.6     |
| time/              |          |
|    fps             | 858      |
|    iterations      | 30       |
|    time_elapsed    | 71       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-16 19:48:47,378] Trial 49 finished with value: 9.7 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 3.5231755782018765e-05, 'gamma': 0.9677788946509693, 'gae_lambda': 0.8038231604958214}. Best is trial 31 with value: 29.6.
PPO Best trial: 29.6
PPO Best hyperparameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00038097029254553663, 'gamma': 0.9360172856913327, 'gae_lambda': 0.9290500244305719}
