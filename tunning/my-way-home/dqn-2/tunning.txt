[I 2024-07-21 04:16:37,251] A new study created in memory with name: no-name-e38f5015-6deb-4fd7-ab9f-3a14e70643b0
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4209     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.83     |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4218     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.798    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.746    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1475     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6300     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.681    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1699     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7898     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1945     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9998     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.596    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.16    |
|    exploration_rate | 0.511    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1248     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12098    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 8.62e-07 |
|    n_updates        | 524      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.427    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1242     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14198    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 9.06e-07 |
|    n_updates        | 1049     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.394    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.24e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.172   |
|    exploration_rate | 0.342    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1024     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16298    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 9.29e-06 |
|    n_updates        | 1574     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.257    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1038     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18398    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.64e-06 |
|    n_updates        | 2099     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.11 +/- 0.31
Episode length: 520.10 +/- 14.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 520      |
|    mean_reward      | -0.108   |
| rollout/            |          |
|    exploration_rate | 0.192    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 8.47e-07 |
|    n_updates        | 2499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.18    |
|    exploration_rate | 0.172    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 920      |
|    time_elapsed     | 22       |
|    total_timesteps  | 20498    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 7.17e-07 |
|    n_updates        | 2624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.183   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 936      |
|    time_elapsed     | 24       |
|    total_timesteps  | 22598    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 8.54e-07 |
|    n_updates        | 3149     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 515      |
|    ep_rew_mean      | -0.185   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 946      |
|    time_elapsed     | 26       |
|    total_timesteps  | 24698    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.2e-05  |
|    n_updates        | 3674     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0886   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 6.19e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 515      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 872      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26798    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 9.73e-06 |
|    n_updates        | 4199     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 516      |
|    ep_rew_mean      | -0.189   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 885      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28898    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 4.26e-07 |
|    n_updates        | 4724     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0886   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.49e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.172   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 829      |
|    time_elapsed     | 37       |
|    total_timesteps  | 30849    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.04e-07 |
|    n_updates        | 5212     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.156   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 839      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32474    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 3.43e-07 |
|    n_updates        | 5618     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.159   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 851      |
|    time_elapsed     | 40       |
|    total_timesteps  | 34574    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 6.22e-07 |
|    n_updates        | 6143     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0886   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.47e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.162   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 809      |
|    time_elapsed     | 45       |
|    total_timesteps  | 36674    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.69e-05 |
|    n_updates        | 6668     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 820      |
|    time_elapsed     | 47       |
|    total_timesteps  | 38774    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 8.16e-06 |
|    n_updates        | 7193     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0886   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 5.97e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 785      |
|    time_elapsed     | 51       |
|    total_timesteps  | 40387    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 3.86e-06 |
|    n_updates        | 7596     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 796      |
|    time_elapsed     | 53       |
|    total_timesteps  | 42487    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.08e-05 |
|    n_updates        | 8121     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 806      |
|    time_elapsed     | 55       |
|    total_timesteps  | 44587    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.74e-07 |
|    n_updates        | 8646     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0886   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 5.19e-07 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.16    |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 779      |
|    time_elapsed     | 59       |
|    total_timesteps  | 46687    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 7.6e-07  |
|    n_updates        | 9171     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.162   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 789      |
|    time_elapsed     | 61       |
|    total_timesteps  | 48787    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.17e-06 |
|    n_updates        | 9696     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 478.20 +/- 140.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | -0.0912  |
| rollout/            |          |
|    exploration_rate | 0.0886   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 6.81e-07 |
|    n_updates        | 9999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 766      |
|    time_elapsed     | 65       |
|    total_timesteps  | 50383    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 6.09e-06 |
|    n_updates        | 10095    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 774      |
|    time_elapsed     | 67       |
|    total_timesteps  | 52157    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 3.35e-07 |
|    n_updates        | 10539    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 783      |
|    time_elapsed     | 69       |
|    total_timesteps  | 54257    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 8.23e-07 |
|    n_updates        | 11064    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0886   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 0.00024  |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 760      |
|    time_elapsed     | 73       |
|    total_timesteps  | 56104    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 4.45e-07 |
|    n_updates        | 11525    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 768      |
|    time_elapsed     | 75       |
|    total_timesteps  | 58146    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 7.36e-06 |
|    n_updates        | 12036    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0886   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 4.21e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:18:00,678] Trial 0 finished with value: -0.21000000000000002 and parameters: {'batch_size': 128, 'learning_rate': 0.0001805623701526311, 'buffer_size': 44371, 'gamma': 0.9616676411370321, 'exploration_fraction': 0.37612031526850476, 'exploration_final_eps': 0.0886436265155585}. Best is trial 0 with value: -0.21000000000000002.
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 446      |
|    ep_rew_mean      | 0.0715   |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4170     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1785     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0692  |
|    exploration_rate | 0.819    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4209     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3885     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.767    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.721    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1454     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5985     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.624    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1752     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8085     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.535    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.528    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1283     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10130    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 7.86e-06 |
|    n_updates        | 32       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.431    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1278     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12230    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.17e-06 |
|    n_updates        | 557      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.333    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1263     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14330    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 2.38e-06 |
|    n_updates        | 1082     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.302    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.86e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.257    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1035     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15956    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.45e-06 |
|    n_updates        | 1488     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.16     |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1047     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18056    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 3.1e-05  |
|    n_updates        | 2013     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0691   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 3.72e-05 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 924      |
|    time_elapsed     | 21       |
|    total_timesteps  | 20156    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 3.03e-05 |
|    n_updates        | 2538     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 937      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22256    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 2.72e-05 |
|    n_updates        | 3063     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.096   |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 946      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24020    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 3.07e-05 |
|    n_updates        | 3504     |
----------------------------------
Eval num_timesteps=25000, episode_reward=0.03 +/- 0.47
Episode length: 437.20 +/- 176.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 437      |
|    mean_reward      | 0.0252   |
| rollout/            |          |
|    exploration_rate | 0.0664   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 0.000557 |
|    n_updates        | 3749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 882      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26120    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 5.89e-05 |
|    n_updates        | 4029     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 894      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28220    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.69e-05 |
|    n_updates        | 4554     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0664   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 2.16e-05 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 838      |
|    time_elapsed     | 36       |
|    total_timesteps  | 30320    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 7.09e-06 |
|    n_updates        | 5079     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 849      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32129    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 8.14e-06 |
|    n_updates        | 5532     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0958  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 858      |
|    time_elapsed     | 39       |
|    total_timesteps  | 33783    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 7.04e-06 |
|    n_updates        | 5945     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0664   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 7.1e-06  |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0858  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 812      |
|    time_elapsed     | 43       |
|    total_timesteps  | 35453    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 5.9e-06  |
|    n_updates        | 6363     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0627  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 821      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36918    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.28e-05 |
|    n_updates        | 6729     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0701  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 831      |
|    time_elapsed     | 46       |
|    total_timesteps  | 39018    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 2.13e-05 |
|    n_updates        | 7254     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 473.90 +/- 153.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0895  |
| rollout/            |          |
|    exploration_rate | 0.0664   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.99e-05 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0767  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 801      |
|    time_elapsed     | 51       |
|    total_timesteps  | 41118    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 4.05e-05 |
|    n_updates        | 7779     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0828  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 812      |
|    time_elapsed     | 53       |
|    total_timesteps  | 43218    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.5e-05  |
|    n_updates        | 8304     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0664   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.19e-05 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0883  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 784      |
|    time_elapsed     | 57       |
|    total_timesteps  | 45318    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.41e-05 |
|    n_updates        | 8829     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0809  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 791      |
|    time_elapsed     | 59       |
|    total_timesteps  | 46914    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.8e-05  |
|    n_updates        | 9228     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.086   |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 801      |
|    time_elapsed     | 61       |
|    total_timesteps  | 49014    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 3.84e-05 |
|    n_updates        | 9753     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 474.80 +/- 150.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 475      |
|    mean_reward      | -0.0899  |
| rollout/            |          |
|    exploration_rate | 0.0664   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 3.19e-05 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0744  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 776      |
|    time_elapsed     | 64       |
|    total_timesteps  | 50393    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.86e-05 |
|    n_updates        | 10098    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0744  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 785      |
|    time_elapsed     | 66       |
|    total_timesteps  | 52493    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.36e-05 |
|    n_updates        | 10623    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0744  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 794      |
|    time_elapsed     | 68       |
|    total_timesteps  | 54593    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.03e-05 |
|    n_updates        | 11148    |
----------------------------------
Eval num_timesteps=55000, episode_reward=0.03 +/- 0.48
Episode length: 422.30 +/- 205.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 422      |
|    mean_reward      | 0.0311   |
| rollout/            |          |
|    exploration_rate | 0.0664   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 2.13e-05 |
|    n_updates        | 11249    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 477      |
|    ep_rew_mean      | -0.0509  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 774      |
|    time_elapsed     | 72       |
|    total_timesteps  | 55819    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 2.97e-05 |
|    n_updates        | 11454    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0611  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 783      |
|    time_elapsed     | 73       |
|    total_timesteps  | 57919    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 6.22e-06 |
|    n_updates        | 11979    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.0493  |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 124      |
|    fps              | 789      |
|    time_elapsed     | 75       |
|    total_timesteps  | 59564    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.23e-05 |
|    n_updates        | 12390    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0664   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 9.42e-05 |
|    loss             | 1.21e-05 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:19:22,278] Trial 1 finished with value: -0.21000000000000002 and parameters: {'batch_size': 128, 'learning_rate': 9.415752254883096e-05, 'buffer_size': 26486, 'gamma': 0.9130166221127844, 'exploration_fraction': 0.33430384001936586, 'exploration_final_eps': 0.06635948624582538}. Best is trial 0 with value: -0.21000000000000002.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.914    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4243     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.827    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4257     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.794    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.76     |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1430     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5846     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 445      |
|    ep_rew_mean      | 0.00963  |
|    exploration_rate | 0.707    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1621     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7116     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 461      |
|    ep_rew_mean      | -0.0343  |
|    exploration_rate | 0.621    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1887     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9216     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.589    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | -0.0636  |
|    exploration_rate | 0.535    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1284     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11316    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 7.51e-07 |
|    n_updates        | 328      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | -0.0419  |
|    exploration_rate | 0.468    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1277     |
|    time_elapsed     | 10       |
|    total_timesteps  | 12935    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 9.18e-07 |
|    n_updates        | 733      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.383    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.57e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 470      |
|    ep_rew_mean      | -0.0629  |
|    exploration_rate | 0.382    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1030     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15035    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 5.81e-06 |
|    n_updates        | 1258     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0793  |
|    exploration_rate | 0.296    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1046     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17135    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 7.13e-06 |
|    n_updates        | 1783     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0668  |
|    exploration_rate | 0.211    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1055     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19185    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 5.34e-06 |
|    n_updates        | 2296     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.178    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.26e-05 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0547  |
|    exploration_rate | 0.136    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 933      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21021    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 3.24e-06 |
|    n_updates        | 2755     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0677  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 946      |
|    time_elapsed     | 24       |
|    total_timesteps  | 23121    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 3.05e-06 |
|    n_updates        | 3280     |
----------------------------------
Eval num_timesteps=25000, episode_reward=0.01 +/- 0.45
Episode length: 465.90 +/- 151.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 466      |
|    mean_reward      | 0.0137   |
| rollout/            |          |
|    exploration_rate | 0.0731   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 5.38e-06 |
|    n_updates        | 3749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0786  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 878      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25221    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 7.24e-06 |
|    n_updates        | 3805     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0675  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 888      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26952    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 2.06e-05 |
|    n_updates        | 4237     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.077   |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 900      |
|    time_elapsed     | 32       |
|    total_timesteps  | 29052    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 7.81e-06 |
|    n_updates        | 4762     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0731   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.91e-05 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0853  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 844      |
|    time_elapsed     | 36       |
|    total_timesteps  | 31152    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 0.000838 |
|    n_updates        | 5287     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0749  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 853      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32739    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 7.67e-06 |
|    n_updates        | 5684     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0824  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 864      |
|    time_elapsed     | 40       |
|    total_timesteps  | 34839    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.77e-05 |
|    n_updates        | 6209     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0731   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 2.68e-05 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0739  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 819      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36540    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.48e-05 |
|    n_updates        | 6634     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 471      |
|    ep_rew_mean      | -0.0509  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 826      |
|    time_elapsed     | 45       |
|    total_timesteps  | 37675    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 2.58e-05 |
|    n_updates        | 6918     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 466      |
|    ep_rew_mean      | -0.0317  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 834      |
|    time_elapsed     | 46       |
|    total_timesteps  | 39170    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 2.01e-05 |
|    n_updates        | 7292     |
----------------------------------
Eval num_timesteps=40000, episode_reward=0.03 +/- 0.48
Episode length: 428.90 +/- 192.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 0.0285   |
| rollout/            |          |
|    exploration_rate | 0.0731   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 3.29e-05 |
|    n_updates        | 7499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | -0.0398  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 806      |
|    time_elapsed     | 51       |
|    total_timesteps  | 41270    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 2.81e-05 |
|    n_updates        | 7817     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 471      |
|    ep_rew_mean      | -0.0472  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 817      |
|    time_elapsed     | 53       |
|    total_timesteps  | 43370    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.08e-05 |
|    n_updates        | 8342     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0731   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 4.06e-05 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.054   |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 787      |
|    time_elapsed     | 57       |
|    total_timesteps  | 45470    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.29e-05 |
|    n_updates        | 8867     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.0493  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 796      |
|    time_elapsed     | 59       |
|    total_timesteps  | 47322    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.46e-05 |
|    n_updates        | 9330     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.0493  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 805      |
|    time_elapsed     | 61       |
|    total_timesteps  | 49422    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.78e-05 |
|    n_updates        | 9855     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.10 +/- 0.34
Episode length: 488.80 +/- 108.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 489      |
|    mean_reward      | -0.0955  |
| rollout/            |          |
|    exploration_rate | 0.0731   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 6.09e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.0493  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 783      |
|    time_elapsed     | 65       |
|    total_timesteps  | 51522    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.15e-05 |
|    n_updates        | 10380    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0498  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 791      |
|    time_elapsed     | 67       |
|    total_timesteps  | 53293    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 2.06e-05 |
|    n_updates        | 10823    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.0615  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 798      |
|    time_elapsed     | 68       |
|    total_timesteps  | 54995    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.33e-05 |
|    n_updates        | 11248    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0731   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 6.3e-06  |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | -0.0278  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 774      |
|    time_elapsed     | 72       |
|    total_timesteps  | 56165    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 1.1e-05  |
|    n_updates        | 11541    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 466      |
|    ep_rew_mean      | -0.0165  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 124      |
|    fps              | 781      |
|    time_elapsed     | 74       |
|    total_timesteps  | 57959    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 2.32e-05 |
|    n_updates        | 11989    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | -0.0169  |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 128      |
|    fps              | 788      |
|    time_elapsed     | 75       |
|    total_timesteps  | 59674    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 3.42e-05 |
|    n_updates        | 12418    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0731   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 7.85e-05 |
|    loss             | 2.8e-05  |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:20:43,993] Trial 2 finished with value: -0.21000000000000002 and parameters: {'batch_size': 128, 'learning_rate': 7.853456717077294e-05, 'buffer_size': 29685, 'gamma': 0.9126777033174477, 'exploration_fraction': 0.37577126701520747, 'exploration_final_eps': 0.07310408621515645}. Best is trial 0 with value: -0.21000000000000002.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 0.0603   |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4200     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1897     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.0748  |
|    exploration_rate | 0.814    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4222     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3997     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.768    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.029   |
|    exploration_rate | 0.727    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1430     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5870     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.0742  |
|    exploration_rate | 0.629    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1724     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7970     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.535    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.0504  |
|    exploration_rate | 0.534    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1276     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10022    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 0.00122  |
|    n_updates        | 5        |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.077   |
|    exploration_rate | 0.436    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1283     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12122    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 8.06e-06 |
|    n_updates        | 530      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.096   |
|    exploration_rate | 0.339    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1281     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14222    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 1.51e-06 |
|    n_updates        | 1055     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.303    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 2.28e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0403  |
|    exploration_rate | 0.269    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1046     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15724    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 9.59e-06 |
|    n_updates        | 1430     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0591  |
|    exploration_rate | 0.171    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1063     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17824    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 1.28e-06 |
|    n_updates        | 1955     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.0742  |
|    exploration_rate | 0.0736   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1074     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19924    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 8.66e-06 |
|    n_updates        | 2480     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0701   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 3.46e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.0866  |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 955      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22024    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 4.31e-08 |
|    n_updates        | 3005     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.0969  |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 968      |
|    time_elapsed     | 24       |
|    total_timesteps  | 24124    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 4.6e-07  |
|    n_updates        | 3530     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0137   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 5.57e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 889      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26224    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 2.64e-07 |
|    n_updates        | 4055     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.0938  |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 902      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28131    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 1.89e-07 |
|    n_updates        | 4532     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0137   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 1.37e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.0839  |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 845      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30087    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 5.52e-08 |
|    n_updates        | 5021     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.0918  |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 861      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32187    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 6.54e-07 |
|    n_updates        | 5546     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.0822  |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 873      |
|    time_elapsed     | 38       |
|    total_timesteps  | 33972    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 6.71e-09 |
|    n_updates        | 5992     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0137   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 2.21e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.0893  |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 830      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36072    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 4.19e-07 |
|    n_updates        | 6517     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.0956  |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 843      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38172    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 7.23e-05 |
|    n_updates        | 7042     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0137   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 1.11e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 809      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40272    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 4.31e-08 |
|    n_updates        | 7567     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 821      |
|    time_elapsed     | 51       |
|    total_timesteps  | 42372    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 4.62e-05 |
|    n_updates        | 8092     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 832      |
|    time_elapsed     | 53       |
|    total_timesteps  | 44472    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 6.06e-07 |
|    n_updates        | 8617     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0137   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 6.59e-09 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 800      |
|    time_elapsed     | 57       |
|    total_timesteps  | 46065    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 1.35e-07 |
|    n_updates        | 9016     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 811      |
|    time_elapsed     | 59       |
|    total_timesteps  | 48165    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 2.26e-07 |
|    n_updates        | 9541     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0137   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 1.69e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 786      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50265    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 5.3e-05  |
|    n_updates        | 10066    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 796      |
|    time_elapsed     | 65       |
|    total_timesteps  | 52365    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 5.09e-07 |
|    n_updates        | 10591    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 805      |
|    time_elapsed     | 67       |
|    total_timesteps  | 54465    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 3.68e-08 |
|    n_updates        | 11116    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0137   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 2.41e-05 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 782      |
|    time_elapsed     | 72       |
|    total_timesteps  | 56565    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 1.65e-07 |
|    n_updates        | 11641    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0137   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 791      |
|    time_elapsed     | 74       |
|    total_timesteps  | 58665    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 1.42e-07 |
|    n_updates        | 12166    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0137   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.00445  |
|    loss             | 1.08e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:22:04,995] Trial 3 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.0044543260166382175, 'buffer_size': 14561, 'gamma': 0.9924198827479164, 'exploration_fraction': 0.35354352006091805, 'exploration_final_eps': 0.013674222660297318}. Best is trial 0 with value: -0.21000000000000002.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.9      |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4201     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.799    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4215     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.761    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.703    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1483     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6215     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 520      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.602    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1776     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8315     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.522    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.51     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1326     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10257    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 3.67e-06 |
|    n_updates        | 64       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 515      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.409    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1322     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12357    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 3.99e-06 |
|    n_updates        | 589      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 516      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.309    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1312     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14457    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 3.22e-06 |
|    n_updates        | 1114     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.283    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 5.91e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.223    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1069     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16261    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 3.11e-06 |
|    n_updates        | 1565     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.122    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1084     |
|    time_elapsed     | 16       |
|    total_timesteps  | 18361    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 3.41e-06 |
|    n_updates        | 2090     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0439   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 2.83e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 953      |
|    time_elapsed     | 21       |
|    total_timesteps  | 20261    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 2.79e-08 |
|    n_updates        | 2565     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 969      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22361    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 1.34e-09 |
|    n_updates        | 3090     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 981      |
|    time_elapsed     | 24       |
|    total_timesteps  | 24461    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 2.5e-09  |
|    n_updates        | 3615     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0334   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 7.9e-08  |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 901      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26561    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 3.4e-08  |
|    n_updates        | 4140     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 915      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28661    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 8.24e-08 |
|    n_updates        | 4665     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0334   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 3.92e-08 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 854      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30761    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 1.25e-08 |
|    n_updates        | 5190     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 868      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32756    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 5.11e-06 |
|    n_updates        | 5688     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 881      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34856    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 1.43e-07 |
|    n_updates        | 6213     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0334   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 9.49e-09 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 837      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36956    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 2.14e-07 |
|    n_updates        | 6738     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 849      |
|    time_elapsed     | 45       |
|    total_timesteps  | 39056    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 1.1e-08  |
|    n_updates        | 7263     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0334   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 3.44e-05 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 814      |
|    time_elapsed     | 50       |
|    total_timesteps  | 41156    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 1.29e-06 |
|    n_updates        | 7788     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 515      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 825      |
|    time_elapsed     | 52       |
|    total_timesteps  | 43256    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 8.54e-07 |
|    n_updates        | 8313     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0334   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 2.83e-09 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 795      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45072    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 5.47e-07 |
|    n_updates        | 8767     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 806      |
|    time_elapsed     | 58       |
|    total_timesteps  | 47172    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 3.9e-08  |
|    n_updates        | 9292     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 814      |
|    time_elapsed     | 59       |
|    total_timesteps  | 48880    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 6.18e-07 |
|    n_updates        | 9719     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0334   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 2.77e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 789      |
|    time_elapsed     | 64       |
|    total_timesteps  | 50980    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 2.89e-07 |
|    n_updates        | 10244    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 798      |
|    time_elapsed     | 66       |
|    total_timesteps  | 52977    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 2.2e-07  |
|    n_updates        | 10744    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0334   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 2.76e-09 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 775      |
|    time_elapsed     | 70       |
|    total_timesteps  | 55077    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 1.28e-09 |
|    n_updates        | 11269    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 783      |
|    time_elapsed     | 72       |
|    total_timesteps  | 56805    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 3.01e-06 |
|    n_updates        | 11701    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0334   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 789      |
|    time_elapsed     | 74       |
|    total_timesteps  | 58474    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 1.37e-06 |
|    n_updates        | 12118    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0334   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.00639  |
|    loss             | 4.21e-08 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:23:26,188] Trial 4 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.006393078014607747, 'buffer_size': 79211, 'gamma': 0.9223572170710699, 'exploration_fraction': 0.3369792962141043, 'exploration_final_eps': 0.03338838355608089}. Best is trial 0 with value: -0.21000000000000002.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 411      |
|    ep_rew_mean      | 0.0857   |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4211     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1644     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 468      |
|    ep_rew_mean      | -0.0622  |
|    exploration_rate | 0.826    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4200     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3744     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.768    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.729    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1424     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5844     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.632    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1727     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7944     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.536    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.151   |
|    exploration_rate | 0.534    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1283     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10044    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 0.000328 |
|    n_updates        | 10       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.161   |
|    exploration_rate | 0.437    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1282     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12144    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 3.25e-06 |
|    n_updates        | 535      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.361    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1278     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13772    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 2.25e-06 |
|    n_updates        | 942      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.304    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 3.24e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.279    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1039     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15536    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 3.12e-06 |
|    n_updates        | 1383     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.182    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1052     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17636    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 6.76e-06 |
|    n_updates        | 1908     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0847   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1060     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19736    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 1.78e-06 |
|    n_updates        | 2433     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0725   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 2.6e-06  |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 938      |
|    time_elapsed     | 23       |
|    total_timesteps  | 21836    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 1.45e-06 |
|    n_updates        | 2958     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 949      |
|    time_elapsed     | 25       |
|    total_timesteps  | 23936    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 2.68e-06 |
|    n_updates        | 3483     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.36
Episode length: 476.40 +/- 145.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | -0.0905  |
| rollout/            |          |
|    exploration_rate | 0.0299   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 6.25e-07 |
|    n_updates        | 3749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 881      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26036    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 4.36e-07 |
|    n_updates        | 4008     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.147   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 896      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28136    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 6.26e-07 |
|    n_updates        | 4533     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0299   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 8.4e-07  |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 840      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30236    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 0.00768  |
|    n_updates        | 5058     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 854      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32336    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 5.32e-07 |
|    n_updates        | 5583     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 865      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34114    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 2.96e-07 |
|    n_updates        | 6028     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.10 +/- 0.32
Episode length: 506.40 +/- 55.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | -0.103   |
| rollout/            |          |
|    exploration_rate | 0.0299   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 6.43e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 822      |
|    time_elapsed     | 43       |
|    total_timesteps  | 35766    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 2.47e-06 |
|    n_updates        | 6441     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 834      |
|    time_elapsed     | 45       |
|    total_timesteps  | 37866    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 4.72e-07 |
|    n_updates        | 6966     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 845      |
|    time_elapsed     | 47       |
|    total_timesteps  | 39966    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 1.98e-06 |
|    n_updates        | 7491     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0299   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 7.67e-06 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 811      |
|    time_elapsed     | 51       |
|    total_timesteps  | 42066    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 3.67e-07 |
|    n_updates        | 8016     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 822      |
|    time_elapsed     | 53       |
|    total_timesteps  | 44166    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 2.88e-07 |
|    n_updates        | 8541     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0299   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 3.97e-07 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.147   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 793      |
|    time_elapsed     | 58       |
|    total_timesteps  | 46266    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 4.29e-07 |
|    n_updates        | 9066     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 804      |
|    time_elapsed     | 60       |
|    total_timesteps  | 48366    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 6.12e-07 |
|    n_updates        | 9591     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0299   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 5.53e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 780      |
|    time_elapsed     | 64       |
|    total_timesteps  | 50466    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 1.87e-07 |
|    n_updates        | 10116    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 790      |
|    time_elapsed     | 66       |
|    total_timesteps  | 52566    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 5.78e-07 |
|    n_updates        | 10641    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 799      |
|    time_elapsed     | 68       |
|    total_timesteps  | 54666    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 3.96e-07 |
|    n_updates        | 11166    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0299   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 3.3e-07  |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 777      |
|    time_elapsed     | 72       |
|    total_timesteps  | 56766    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 5.57e-06 |
|    n_updates        | 11691    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.0299   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 786      |
|    time_elapsed     | 74       |
|    total_timesteps  | 58866    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 1.6e-07  |
|    n_updates        | 12216    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0299   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.77e-05 |
|    loss             | 4.42e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:24:47,695] Trial 5 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 1.7692611735242695e-05, 'buffer_size': 28970, 'gamma': 0.9988104389540475, 'exploration_fraction': 0.34862502562458353, 'exploration_final_eps': 0.029921715538545675}. Best is trial 0 with value: -0.21000000000000002.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 517      |
|    ep_rew_mean      | 0.0432   |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4232     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2069     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 521      |
|    ep_rew_mean      | -0.0834  |
|    exploration_rate | 0.846    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4243     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4169     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.815    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 522      |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.769    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1493     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6269     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 517      |
|    ep_rew_mean      | -0.0819  |
|    exploration_rate | 0.694    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1770     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8278     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.631    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 519      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.617    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1284     |
|    time_elapsed     | 8        |
|    total_timesteps  | 10378    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.36e-06 |
|    n_updates        | 94       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 515      |
|    ep_rew_mean      | -0.0809  |
|    exploration_rate | 0.544    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1296     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12357    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.47e-06 |
|    n_updates        | 589      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.0216  |
|    exploration_rate | 0.483    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1303     |
|    time_elapsed     | 10       |
|    total_timesteps  | 14015    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.21e-06 |
|    n_updates        | 1003     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.446    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.06e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.0452  |
|    exploration_rate | 0.405    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1071     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16115    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.47e-06 |
|    n_updates        | 1528     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.0635  |
|    exploration_rate | 0.328    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1092     |
|    time_elapsed     | 16       |
|    total_timesteps  | 18215    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.99e-05 |
|    n_updates        | 2053     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.262    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.39e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.0781  |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 963      |
|    time_elapsed     | 21       |
|    total_timesteps  | 20315    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 6.52e-08 |
|    n_updates        | 2578     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.0901  |
|    exploration_rate | 0.173    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 982      |
|    time_elapsed     | 22       |
|    total_timesteps  | 22415    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 3.49e-07 |
|    n_updates        | 3103     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.0773  |
|    exploration_rate | 0.104    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 995      |
|    time_elapsed     | 24       |
|    total_timesteps  | 24276    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 2.52e-05 |
|    n_updates        | 3568     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0772   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 6.66e-08 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.0875  |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 911      |
|    time_elapsed     | 28       |
|    total_timesteps  | 26376    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.07e-07 |
|    n_updates        | 4093     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.0757  |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 922      |
|    time_elapsed     | 30       |
|    total_timesteps  | 28098    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 6.74e-08 |
|    n_updates        | 4524     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0662  |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 934      |
|    time_elapsed     | 32       |
|    total_timesteps  | 29930    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 3.05e-07 |
|    n_updates        | 4982     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0624   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.59e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.0752  |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 876      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32030    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 2.76e-08 |
|    n_updates        | 5507     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.0831  |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 889      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34130    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 9.3e-08  |
|    n_updates        | 6032     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0624   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 5.03e-10 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.0901  |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 843      |
|    time_elapsed     | 42       |
|    total_timesteps  | 36230    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 4.33e-10 |
|    n_updates        | 6557     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.0965  |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 855      |
|    time_elapsed     | 44       |
|    total_timesteps  | 38330    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 4.53e-09 |
|    n_updates        | 7082     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0624   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 6.78e-08 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 819      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40430    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 2.56e-07 |
|    n_updates        | 7607     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.0953  |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 831      |
|    time_elapsed     | 51       |
|    total_timesteps  | 42512    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.87e-06 |
|    n_updates        | 8127     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 842      |
|    time_elapsed     | 52       |
|    total_timesteps  | 44612    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 8.81e-06 |
|    n_updates        | 8652     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0624   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.38e-07 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 813      |
|    time_elapsed     | 57       |
|    total_timesteps  | 46712    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 4.05e-08 |
|    n_updates        | 9177     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.0974  |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 821      |
|    time_elapsed     | 58       |
|    total_timesteps  | 48372    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.87e-05 |
|    n_updates        | 9592     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0624   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.27e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.091   |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 794      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50256    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 4.4e-07  |
|    n_updates        | 10063    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 804      |
|    time_elapsed     | 65       |
|    total_timesteps  | 52356    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 9.22e-06 |
|    n_updates        | 10588    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.091   |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 813      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54436    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 6.89e-08 |
|    n_updates        | 11108    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0624   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 2.04e-05 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.091   |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 791      |
|    time_elapsed     | 71       |
|    total_timesteps  | 56536    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 2.06e-05 |
|    n_updates        | 11633    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.091   |
|    exploration_rate | 0.0624   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 799      |
|    time_elapsed     | 73       |
|    total_timesteps  | 58526    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 8.61e-10 |
|    n_updates        | 12131    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0624   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000336 |
|    loss             | 1.61e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:26:07,753] Trial 6 finished with value: -0.08937999999999999 and parameters: {'batch_size': 32, 'learning_rate': 0.0003359692723199087, 'buffer_size': 92749, 'gamma': 0.9077279275141995, 'exploration_fraction': 0.42330545130885, 'exploration_final_eps': 0.06244877243974493}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.766    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4248     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.531    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4266     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.442    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.336    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1448     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5951     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0688  |
|    exploration_rate | 0.135    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1710     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7753     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.097   |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1960     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9853     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0683  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1256     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11597    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 4.41e-06 |
|    n_updates        | 399      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0476  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1236     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13335    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 2.36e-06 |
|    n_updates        | 833      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 2.62e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0679  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1005     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15435    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 7.35e-06 |
|    n_updates        | 1358     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0837  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1014     |
|    time_elapsed     | 17       |
|    total_timesteps  | 17535    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.92e-06 |
|    n_updates        | 1883     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0674  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1022     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19241    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.57e-05 |
|    n_updates        | 2310     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 4.8e-06  |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0804  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 909      |
|    time_elapsed     | 23       |
|    total_timesteps  | 21341    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.3e-08  |
|    n_updates        | 2835     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0696  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 924      |
|    time_elapsed     | 25       |
|    total_timesteps  | 23359    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.17e-07 |
|    n_updates        | 3339     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 2.21e-08 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0804  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 853      |
|    time_elapsed     | 29       |
|    total_timesteps  | 25459    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.89e-07 |
|    n_updates        | 3864     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0897  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 870      |
|    time_elapsed     | 31       |
|    total_timesteps  | 27559    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 2.48e-08 |
|    n_updates        | 4389     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0977  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 883      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29659    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 6.88e-08 |
|    n_updates        | 4914     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 7.43e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 832      |
|    time_elapsed     | 38       |
|    total_timesteps  | 31759    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.44e-08 |
|    n_updates        | 5439     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 845      |
|    time_elapsed     | 40       |
|    total_timesteps  | 33859    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.9e-07  |
|    n_updates        | 5964     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 8.83e-09 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 803      |
|    time_elapsed     | 44       |
|    total_timesteps  | 35658    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 4.76e-07 |
|    n_updates        | 6414     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 816      |
|    time_elapsed     | 46       |
|    total_timesteps  | 37758    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 2.59e-06 |
|    n_updates        | 6939     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0975  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 826      |
|    time_elapsed     | 47       |
|    total_timesteps  | 39505    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 3.56e-07 |
|    n_updates        | 7376     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 5.28e-08 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0901  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 794      |
|    time_elapsed     | 52       |
|    total_timesteps  | 41421    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 7.53e-06 |
|    n_updates        | 7855     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0955  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 805      |
|    time_elapsed     | 54       |
|    total_timesteps  | 43521    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 2.12e-06 |
|    n_updates        | 8380     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.39e-07 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0886  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 777      |
|    time_elapsed     | 58       |
|    total_timesteps  | 45373    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 2.75e-07 |
|    n_updates        | 8843     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0828  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 787      |
|    time_elapsed     | 60       |
|    total_timesteps  | 47380    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 3.24e-07 |
|    n_updates        | 9344     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0879  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 797      |
|    time_elapsed     | 62       |
|    total_timesteps  | 49480    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 7.27e-06 |
|    n_updates        | 9869     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 9.46e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0772  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 773      |
|    time_elapsed     | 66       |
|    total_timesteps  | 51411    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 3.04e-07 |
|    n_updates        | 10352    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0659  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 781      |
|    time_elapsed     | 68       |
|    total_timesteps  | 53181    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.49e-07 |
|    n_updates        | 10795    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 4.78e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0773  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 761      |
|    time_elapsed     | 72       |
|    total_timesteps  | 55281    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 0.00726  |
|    n_updates        | 11320    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0885  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 769      |
|    time_elapsed     | 74       |
|    total_timesteps  | 57381    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 8.39e-07 |
|    n_updates        | 11845    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0885  |
|    exploration_rate | 0.0144   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 777      |
|    time_elapsed     | 76       |
|    total_timesteps  | 59481    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.09e-05 |
|    n_updates        | 12370    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0144   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.00298  |
|    loss             | 1.94e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:27:30,401] Trial 7 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 0.0029826279207428663, 'buffer_size': 79162, 'gamma': 0.9095338804554939, 'exploration_fraction': 0.1471590376559195, 'exploration_final_eps': 0.014376764855325417}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 439      |
|    ep_rew_mean      | 0.0745   |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4218     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1755     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0677  |
|    exploration_rate | 0.798    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4233     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3855     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.739    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.688    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1441     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5955     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0679  |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1695     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7716     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.0391  |
|    exploration_rate | 0.505    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1902     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9458     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.477    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 466      |
|    ep_rew_mean      | -0.0197  |
|    exploration_rate | 0.415    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1275     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11182    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 8.09e-06 |
|    n_updates        | 295      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0469  |
|    exploration_rate | 0.305    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1260     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13282    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.57e-06 |
|    n_updates        | 820      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.215    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.16e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0673  |
|    exploration_rate | 0.195    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1018     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15382    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.85e-06 |
|    n_updates        | 1345     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0831  |
|    exploration_rate | 0.0855   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1029     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17482    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.42e-06 |
|    n_updates        | 1870     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0671  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1034     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19214    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 8.86e-06 |
|    n_updates        | 2303     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.10 +/- 0.32
Episode length: 505.90 +/- 57.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 506      |
|    mean_reward      | -0.102   |
| rollout/            |          |
|    exploration_rate | 0.0696   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.26e-05 |
|    n_updates        | 2499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0801  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 920      |
|    time_elapsed     | 23       |
|    total_timesteps  | 21314    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.46e-05 |
|    n_updates        | 2828     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0909  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 931      |
|    time_elapsed     | 25       |
|    total_timesteps  | 23414    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.27e-05 |
|    n_updates        | 3353     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.0569  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 938      |
|    time_elapsed     | 26       |
|    total_timesteps  | 24902    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 9.5e-06  |
|    n_updates        | 3725     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0696   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 7.04e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0476  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 864      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26662    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 3.12e-05 |
|    n_updates        | 4165     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 465      |
|    ep_rew_mean      | -0.0195  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 872      |
|    time_elapsed     | 32       |
|    total_timesteps  | 27925    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 3.22e-05 |
|    n_updates        | 4481     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | -0.0151  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 884      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29919    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.93e-05 |
|    n_updates        | 4979     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0696   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.74e-05 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 471      |
|    ep_rew_mean      | -0.0266  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 833      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32019    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.34e-05 |
|    n_updates        | 5504     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0367  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 846      |
|    time_elapsed     | 40       |
|    total_timesteps  | 34119    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 3.98e-06 |
|    n_updates        | 6029     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0696   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 9.86e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 477      |
|    ep_rew_mean      | -0.0459  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 805      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36219    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 9.99e-06 |
|    n_updates        | 6554     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0395  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 814      |
|    time_elapsed     | 46       |
|    total_timesteps  | 37902    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.25e-05 |
|    n_updates        | 6975     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0696   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.6e-06  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0476  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 782      |
|    time_elapsed     | 51       |
|    total_timesteps  | 40002    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 4.67e-06 |
|    n_updates        | 7500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.055   |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 792      |
|    time_elapsed     | 53       |
|    total_timesteps  | 42102    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 3.61e-06 |
|    n_updates        | 8025     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0617  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 802      |
|    time_elapsed     | 55       |
|    total_timesteps  | 44202    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 7.51e-06 |
|    n_updates        | 8550     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0696   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.04e-05 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0679  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 775      |
|    time_elapsed     | 59       |
|    total_timesteps  | 46302    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.02e-05 |
|    n_updates        | 9075     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0622  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 783      |
|    time_elapsed     | 61       |
|    total_timesteps  | 48066    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 5.77e-06 |
|    n_updates        | 9516     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0622  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 790      |
|    time_elapsed     | 62       |
|    total_timesteps  | 49812    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.37e-05 |
|    n_updates        | 9952     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0696   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 9.98e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0622  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 767      |
|    time_elapsed     | 67       |
|    total_timesteps  | 51912    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 4.1e-05  |
|    n_updates        | 10477    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 475      |
|    ep_rew_mean      | -0.0401  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 774      |
|    time_elapsed     | 69       |
|    total_timesteps  | 53493    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 4.39e-05 |
|    n_updates        | 10873    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0696   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.42e-05 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0413  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 753      |
|    time_elapsed     | 73       |
|    total_timesteps  | 55546    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 3.2e-05  |
|    n_updates        | 11386    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 477      |
|    ep_rew_mean      | -0.0409  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 760      |
|    time_elapsed     | 75       |
|    total_timesteps  | 57192    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 2.3e-05  |
|    n_updates        | 11797    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.0414  |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 124      |
|    fps              | 767      |
|    time_elapsed     | 76       |
|    total_timesteps  | 59046    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 3.6e-05  |
|    n_updates        | 12261    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0696   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000181 |
|    loss             | 1.69e-05 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:28:53,982] Trial 8 finished with value: -0.21000000000000002 and parameters: {'batch_size': 128, 'learning_rate': 0.00018141602616479722, 'buffer_size': 48276, 'gamma': 0.9306898163249344, 'exploration_fraction': 0.29643514279657956, 'exploration_final_eps': 0.06961175461624279}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 320      |
|    ep_rew_mean      | 0.372    |
|    exploration_rate | 0.879    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4187     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1280     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 422      |
|    ep_rew_mean      | 0.0811   |
|    exploration_rate | 0.681    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4235     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3380     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.528    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 420      |
|    ep_rew_mean      | 0.0821   |
|    exploration_rate | 0.524    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1249     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5039     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 446      |
|    ep_rew_mean      | 0.00908  |
|    exploration_rate | 0.326    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1575     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7139     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 455      |
|    ep_rew_mean      | 0.0181   |
|    exploration_rate | 0.141    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1819     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9097     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0561   |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | -0.0199  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1235     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11197    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 5.73e-06 |
|    n_updates        | 299      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 475      |
|    ep_rew_mean      | -0.0471  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1212     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13297    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 5.8e-07  |
|    n_updates        | 824      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 4.23e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0674  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 985      |
|    time_elapsed     | 15       |
|    total_timesteps  | 15397    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 3.94e-07 |
|    n_updates        | 1349     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0833  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 998      |
|    time_elapsed     | 17       |
|    total_timesteps  | 17497    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 6.59e-06 |
|    n_updates        | 1874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.0667  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1006     |
|    time_elapsed     | 19       |
|    total_timesteps  | 19178    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 4.95e-07 |
|    n_updates        | 2294     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 6.23e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0798  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 899      |
|    time_elapsed     | 23       |
|    total_timesteps  | 21278    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 4.17e-09 |
|    n_updates        | 2819     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0662  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 912      |
|    time_elapsed     | 25       |
|    total_timesteps  | 22953    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 5.13e-07 |
|    n_updates        | 3238     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 3.19e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0773  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 842      |
|    time_elapsed     | 29       |
|    total_timesteps  | 25053    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 1.07e-05 |
|    n_updates        | 3763     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 477      |
|    ep_rew_mean      | -0.0656  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 855      |
|    time_elapsed     | 31       |
|    total_timesteps  | 26693    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 8.81e-07 |
|    n_updates        | 4173     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0753  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 869      |
|    time_elapsed     | 33       |
|    total_timesteps  | 28793    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 1.56e-07 |
|    n_updates        | 4698     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 2.43e-08 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0837  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 819      |
|    time_elapsed     | 37       |
|    total_timesteps  | 30893    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 3.17e-07 |
|    n_updates        | 5223     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0736  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 830      |
|    time_elapsed     | 39       |
|    total_timesteps  | 32510    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 5.37e-08 |
|    n_updates        | 5627     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0811  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 843      |
|    time_elapsed     | 41       |
|    total_timesteps  | 34610    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 3.36e-06 |
|    n_updates        | 6152     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 9.05e-09 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0879  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 804      |
|    time_elapsed     | 45       |
|    total_timesteps  | 36710    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 4.5e-06  |
|    n_updates        | 6677     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.094   |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 817      |
|    time_elapsed     | 47       |
|    total_timesteps  | 38810    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 3.22e-07 |
|    n_updates        | 7202     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 4.4e-08  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0996  |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 785      |
|    time_elapsed     | 52       |
|    total_timesteps  | 40910    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 7.28e-09 |
|    n_updates        | 7727     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 796      |
|    time_elapsed     | 53       |
|    total_timesteps  | 43010    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 2.82e-07 |
|    n_updates        | 8252     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 1.81e-09 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 769      |
|    time_elapsed     | 58       |
|    total_timesteps  | 45110    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 2.25e-08 |
|    n_updates        | 8777     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 781      |
|    time_elapsed     | 60       |
|    total_timesteps  | 47210    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 1.5e-09  |
|    n_updates        | 9302     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 789      |
|    time_elapsed     | 61       |
|    total_timesteps  | 48904    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 5.68e-09 |
|    n_updates        | 9725     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 4.53e-08 |
|    n_updates        | 9999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 769      |
|    time_elapsed     | 66       |
|    total_timesteps  | 51004    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 6.08e-10 |
|    n_updates        | 10250    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 778      |
|    time_elapsed     | 68       |
|    total_timesteps  | 53104    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 1.1e-07  |
|    n_updates        | 10775    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 8.11e-09 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 757      |
|    time_elapsed     | 72       |
|    total_timesteps  | 55204    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 5.61e-09 |
|    n_updates        | 11300    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 767      |
|    time_elapsed     | 74       |
|    total_timesteps  | 57304    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 3.05e-08 |
|    n_updates        | 11825    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.151   |
|    exploration_rate | 0.0239   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 775      |
|    time_elapsed     | 76       |
|    total_timesteps  | 59404    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 9.33e-07 |
|    n_updates        | 12350    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0239   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000404 |
|    loss             | 8.77e-09 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:30:16,898] Trial 9 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 0.00040383903167151916, 'buffer_size': 68643, 'gamma': 0.9423709580943015, 'exploration_fraction': 0.17232695562367994, 'exploration_final_eps': 0.02394698966888633}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.863    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4196     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.08    |
|    exploration_rate | 0.733    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4198     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4100     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.674    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 517      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1491     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6200     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 519      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.46     |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1783     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8300     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.349    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.344    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1289     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10068    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.01e-05 |
|    n_updates        | 16       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.208    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1281     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12168    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 2.31e-06 |
|    n_updates        | 541      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0895  |
|    exploration_rate | 0.104    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1267     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13766    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.47e-06 |
|    n_updates        | 941      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0509   |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 2.49e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0702  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1026     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15617    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 3.6e-06  |
|    n_updates        | 1404     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0565  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1037     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17584    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 2.69e-06 |
|    n_updates        | 1895     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0718  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1048     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19684    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.03e-06 |
|    n_updates        | 2420     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0509   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.62e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.033   |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 927      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21127    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 5.78e-06 |
|    n_updates        | 2781     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0477  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 943      |
|    time_elapsed     | 24       |
|    total_timesteps  | 23227    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.79e-05 |
|    n_updates        | 3306     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.038   |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 955      |
|    time_elapsed     | 26       |
|    total_timesteps  | 24938    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.93e-07 |
|    n_updates        | 3734     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0509   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 9.32e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0503  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 881      |
|    time_elapsed     | 30       |
|    total_timesteps  | 27038    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 2.11e-05 |
|    n_updates        | 4259     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0609  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 896      |
|    time_elapsed     | 32       |
|    total_timesteps  | 29138    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 3.07e-07 |
|    n_updates        | 4784     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0509   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 4.11e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0524  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 840      |
|    time_elapsed     | 36       |
|    total_timesteps  | 30893    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 7.87e-06 |
|    n_updates        | 5223     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0617  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 854      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32993    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 7.01e-07 |
|    n_updates        | 5748     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0538  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 865      |
|    time_elapsed     | 40       |
|    total_timesteps  | 34689    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 7.18e-06 |
|    n_updates        | 6172     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.36
Episode length: 474.20 +/- 152.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0897  |
| rollout/            |          |
|    exploration_rate | 0.0509   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 2.62e-07 |
|    n_updates        | 6249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.062   |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 829      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36789    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.89e-07 |
|    n_updates        | 6697     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0554  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 840      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38576    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 2.78e-05 |
|    n_updates        | 7143     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0509   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 4.5e-07  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0627  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 806      |
|    time_elapsed     | 50       |
|    total_timesteps  | 40676    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 5.57e-07 |
|    n_updates        | 7668     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0694  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 818      |
|    time_elapsed     | 52       |
|    total_timesteps  | 42776    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.6e-08  |
|    n_updates        | 8193     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0755  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 829      |
|    time_elapsed     | 54       |
|    total_timesteps  | 44876    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 4.19e-06 |
|    n_updates        | 8718     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0509   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.19e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0811  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 800      |
|    time_elapsed     | 58       |
|    total_timesteps  | 46976    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 9.27e-09 |
|    n_updates        | 9243     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0752  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 810      |
|    time_elapsed     | 60       |
|    total_timesteps  | 48806    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 3.82e-07 |
|    n_updates        | 9701     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0509   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 5.77e-05 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0638  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 783      |
|    time_elapsed     | 64       |
|    total_timesteps  | 50566    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 5.43e-05 |
|    n_updates        | 10141    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0742  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 793      |
|    time_elapsed     | 66       |
|    total_timesteps  | 52666    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 9.2e-08  |
|    n_updates        | 10666    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0742  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 802      |
|    time_elapsed     | 68       |
|    total_timesteps  | 54766    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.06e-08 |
|    n_updates        | 11191    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0509   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 1.99e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0742  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 780      |
|    time_elapsed     | 72       |
|    total_timesteps  | 56866    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 9.98e-08 |
|    n_updates        | 11716    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0856  |
|    exploration_rate | 0.0509   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 788      |
|    time_elapsed     | 74       |
|    total_timesteps  | 58966    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 5.07e-06 |
|    n_updates        | 12241    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0509   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000972 |
|    loss             | 7.48e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:31:38,415] Trial 10 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.0009723080604424385, 'buffer_size': 99987, 'gamma': 0.9689494637364628, 'exploration_fraction': 0.24292907437256017, 'exploration_final_eps': 0.05090450099253852}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4189     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.0779  |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4206     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4058     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.846    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.81     |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1462     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6158     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0703  |
|    exploration_rate | 0.759    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1697     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7813     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | -0.0387  |
|    exploration_rate | 0.709    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1891     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9438     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.692    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0253  |
|    exploration_rate | 0.645    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1282     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11521    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 4.79e-06 |
|    n_updates        | 380      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | -0.00895 |
|    exploration_rate | 0.595    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1294     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13127    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.07e-06 |
|    n_updates        | 781      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.538    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 9.09e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0341  |
|    exploration_rate | 0.531    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1054     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15227    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 2.54e-06 |
|    n_updates        | 1306     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0536  |
|    exploration_rate | 0.466    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1080     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17327    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 9.07e-07 |
|    n_updates        | 1831     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0413  |
|    exploration_rate | 0.41     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1097     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19135    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.49e-06 |
|    n_updates        | 2283     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.384    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 2.16e-05 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0567  |
|    exploration_rate | 0.346    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 973      |
|    time_elapsed     | 21       |
|    total_timesteps  | 21235    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 3.29e-05 |
|    n_updates        | 2808     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0695  |
|    exploration_rate | 0.281    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 992      |
|    time_elapsed     | 23       |
|    total_timesteps  | 23335    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.04e-06 |
|    n_updates        | 3333     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0576  |
|    exploration_rate | 0.23     |
| time/               |          |
|    episodes         | 52       |
|    fps              | 1005     |
|    time_elapsed     | 24       |
|    total_timesteps  | 24983    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 2.84e-06 |
|    n_updates        | 3745     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.23     |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.22e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0684  |
|    exploration_rate | 0.165    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 921      |
|    time_elapsed     | 29       |
|    total_timesteps  | 27083    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 6.63e-06 |
|    n_updates        | 4270     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0578  |
|    exploration_rate | 0.116    |
| time/               |          |
|    episodes         | 60       |
|    fps              | 932      |
|    time_elapsed     | 30       |
|    total_timesteps  | 28676    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 4.17e-07 |
|    n_updates        | 4668     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 6.43e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0673  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 870      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30776    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.75e-08 |
|    n_updates        | 5193     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0757  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 883      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32876    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.33e-06 |
|    n_updates        | 5718     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0681  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 894      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34759    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 2.01e-10 |
|    n_updates        | 6189     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 3.81e-09 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0756  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 850      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36859    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 7.48e-06 |
|    n_updates        | 6714     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0823  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 863      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38959    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 2.02e-09 |
|    n_updates        | 7239     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.72e-08 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0884  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 826      |
|    time_elapsed     | 49       |
|    total_timesteps  | 41059    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 4.35e-12 |
|    n_updates        | 7764     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0939  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 838      |
|    time_elapsed     | 51       |
|    total_timesteps  | 43159    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.04e-08 |
|    n_updates        | 8289     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 4.09e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0989  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 808      |
|    time_elapsed     | 55       |
|    total_timesteps  | 45259    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.95e-07 |
|    n_updates        | 8814     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0917  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 818      |
|    time_elapsed     | 57       |
|    total_timesteps  | 47007    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.31e-08 |
|    n_updates        | 9251     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0964  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 829      |
|    time_elapsed     | 59       |
|    total_timesteps  | 49107    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 8.12e-09 |
|    n_updates        | 9776     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 2.04e-09 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0964  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 803      |
|    time_elapsed     | 63       |
|    total_timesteps  | 51207    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 2.94e-08 |
|    n_updates        | 10301    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 813      |
|    time_elapsed     | 65       |
|    total_timesteps  | 53307    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 3.72e-08 |
|    n_updates        | 10826    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.75e-08 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 791      |
|    time_elapsed     | 70       |
|    total_timesteps  | 55407    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 6.06e-10 |
|    n_updates        | 11351    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 800      |
|    time_elapsed     | 71       |
|    total_timesteps  | 57507    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.45e-07 |
|    n_updates        | 11876    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 808      |
|    time_elapsed     | 73       |
|    total_timesteps  | 59517    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 1.52e-08 |
|    n_updates        | 12379    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000688 |
|    loss             | 4.15e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:32:57,910] Trial 11 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.0006877516684029376, 'buffer_size': 54178, 'gamma': 0.9600044309351979, 'exploration_fraction': 0.48932353064901546, 'exploration_final_eps': 0.09515027349446797}. Best is trial 6 with value: -0.08937999999999999.
New best mean reward!
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4216     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 461      |
|    ep_rew_mean      | -0.0595  |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4223     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3691     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.833    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1407     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5791     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0708  |
|    exploration_rate | 0.738    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1701     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7833     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0986  |
|    exploration_rate | 0.668    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1940     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9933     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.666    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.068   |
|    exploration_rate | 0.613    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1276     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11582    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 4.36e-06 |
|    n_updates        | 395      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0883  |
|    exploration_rate | 0.542    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1277     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13682    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.35e-06 |
|    n_updates        | 920      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.498    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.25e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.472    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1042     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15782    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.71e-06 |
|    n_updates        | 1445     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.402    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1059     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17882    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 4.87e-06 |
|    n_updates        | 1970     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.332    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1071     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19982    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 6.66e-06 |
|    n_updates        | 2495     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.331    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 8.46e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.262    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 951      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22082    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 6.03e-06 |
|    n_updates        | 3020     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.191    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 964      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24182    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.57e-06 |
|    n_updates        | 3545     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.164    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.74e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.121    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 870      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26282    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 7.01e-07 |
|    n_updates        | 4070     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 881      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28382    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.42e-06 |
|    n_updates        | 4595     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0994   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 0.0033   |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 826      |
|    time_elapsed     | 36       |
|    total_timesteps  | 30482    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 4.78e-06 |
|    n_updates        | 5120     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 837      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32394    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.84e-06 |
|    n_updates        | 5598     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 849      |
|    time_elapsed     | 40       |
|    total_timesteps  | 34494    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.58e-06 |
|    n_updates        | 6123     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0994   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 2.12e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 804      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36081    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 4.93e-06 |
|    n_updates        | 6520     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 814      |
|    time_elapsed     | 46       |
|    total_timesteps  | 37825    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 2.21e-06 |
|    n_updates        | 6956     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 822      |
|    time_elapsed     | 48       |
|    total_timesteps  | 39502    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 6.2e-06  |
|    n_updates        | 7375     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0994   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.75e-06 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 788      |
|    time_elapsed     | 52       |
|    total_timesteps  | 41602    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 2.14e-06 |
|    n_updates        | 7900     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 799      |
|    time_elapsed     | 54       |
|    total_timesteps  | 43702    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.59e-06 |
|    n_updates        | 8425     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0994   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 7.24e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 771      |
|    time_elapsed     | 59       |
|    total_timesteps  | 45558    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 2.45e-06 |
|    n_updates        | 8889     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 781      |
|    time_elapsed     | 60       |
|    total_timesteps  | 47658    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.4e-06  |
|    n_updates        | 9414     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 790      |
|    time_elapsed     | 62       |
|    total_timesteps  | 49758    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 3.73e-06 |
|    n_updates        | 9939     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0994   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 2.23e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 767      |
|    time_elapsed     | 67       |
|    total_timesteps  | 51858    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 1.25e-05 |
|    n_updates        | 10464    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 776      |
|    time_elapsed     | 69       |
|    total_timesteps  | 53958    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 4.12e-06 |
|    n_updates        | 10989    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0994   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 9.44e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 755      |
|    time_elapsed     | 74       |
|    total_timesteps  | 56058    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 3.49e-06 |
|    n_updates        | 11514    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.0994   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 762      |
|    time_elapsed     | 75       |
|    total_timesteps  | 57938    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 7.75e-06 |
|    n_updates        | 11984    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.36
Episode length: 477.50 +/- 142.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | -0.091   |
| rollout/            |          |
|    exploration_rate | 0.0994   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 2.97e-05 |
|    loss             | 9.72e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:34:21,308] Trial 12 finished with value: -0.21000000000000002 and parameters: {'batch_size': 128, 'learning_rate': 2.9734417016771402e-05, 'buffer_size': 94491, 'gamma': 0.9735940900649467, 'exploration_fraction': 0.44881880266748486, 'exploration_final_eps': 0.09940686650679069}. Best is trial 6 with value: -0.08937999999999999.
New best mean reward!
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 333      |
|    ep_rew_mean      | 0.367    |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 3940     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1332     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 378      |
|    ep_rew_mean      | 0.224    |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 3951     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3025     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.828    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 427      |
|    ep_rew_mean      | 0.0792   |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1290     |
|    time_elapsed     | 3        |
|    total_timesteps  | 5125     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 452      |
|    ep_rew_mean      | 0.00691  |
|    exploration_rate | 0.751    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1617     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7225     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 466      |
|    ep_rew_mean      | -0.0365  |
|    exploration_rate | 0.679    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1880     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9325     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.656    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0654  |
|    exploration_rate | 0.607    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1287     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11425    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 1.17e-06 |
|    n_updates        | 356      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.086   |
|    exploration_rate | 0.535    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1294     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13525    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 1.31e-06 |
|    n_updates        | 881      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.484    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 2.45e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.462    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1058     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15625    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 2.48e-06 |
|    n_updates        | 1406     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0847  |
|    exploration_rate | 0.394    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1080     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17624    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 1.53e-06 |
|    n_updates        | 1905     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0972  |
|    exploration_rate | 0.321    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1098     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19724    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 1.94e-06 |
|    n_updates        | 2430     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.312    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 8.7e-07  |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0842  |
|    exploration_rate | 0.251    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 972      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21764    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 3.53e-09 |
|    n_updates        | 2940     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0698  |
|    exploration_rate | 0.196    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 985      |
|    time_elapsed     | 23       |
|    total_timesteps  | 23381    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 3.36e-05 |
|    n_updates        | 3345     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.14     |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 4.06e-08 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0806  |
|    exploration_rate | 0.123    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 901      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25481    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 4.36e-08 |
|    n_updates        | 3870     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0705  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 914      |
|    time_elapsed     | 29       |
|    total_timesteps  | 27379    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 1.48e-09 |
|    n_updates        | 4344     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0798  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 929      |
|    time_elapsed     | 31       |
|    total_timesteps  | 29479    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 8.57e-11 |
|    n_updates        | 4869     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0867   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 3.58e-08 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0516  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 865      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30766    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 5.77e-09 |
|    n_updates        | 5191     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.061   |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 878      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32866    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 5.63e-06 |
|    n_updates        | 5716     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0692  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 890      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34966    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 2.33e-09 |
|    n_updates        | 6241     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0867   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 7.03e-10 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0623  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 843      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36846    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 1.33e-12 |
|    n_updates        | 6711     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0697  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 856      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38946    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 4.88e-09 |
|    n_updates        | 7236     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0867   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 7.64e-08 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0764  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 820      |
|    time_elapsed     | 50       |
|    total_timesteps  | 41046    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 3.44e-10 |
|    n_updates        | 7761     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0825  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 832      |
|    time_elapsed     | 51       |
|    total_timesteps  | 43146    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 2.6e-08  |
|    n_updates        | 8286     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0867   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 3.06e-10 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.088   |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 802      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45246    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 2.04e-11 |
|    n_updates        | 8811     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0931  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 813      |
|    time_elapsed     | 58       |
|    total_timesteps  | 47346    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 1.19e-10 |
|    n_updates        | 9336     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0861  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 821      |
|    time_elapsed     | 59       |
|    total_timesteps  | 49032    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 2.75e-05 |
|    n_updates        | 9757     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.0867   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 4.92e-10 |
|    n_updates        | 9999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0978  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 797      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50790    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 5.36e-06 |
|    n_updates        | 10197    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0975  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 806      |
|    time_elapsed     | 65       |
|    total_timesteps  | 52410    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 2.82e-09 |
|    n_updates        | 10602    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0859  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 813      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54104    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 1.46e-06 |
|    n_updates        | 11025    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0867   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 2.27e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0859  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 791      |
|    time_elapsed     | 70       |
|    total_timesteps  | 56204    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 1.67e-08 |
|    n_updates        | 11550    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0859  |
|    exploration_rate | 0.0867   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 800      |
|    time_elapsed     | 72       |
|    total_timesteps  | 58304    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 6.56e-08 |
|    n_updates        | 12075    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.36
Episode length: 474.30 +/- 152.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0897  |
| rollout/            |          |
|    exploration_rate | 0.0867   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.00154  |
|    loss             | 2.48e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:35:41,078] Trial 13 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.0015386214682885987, 'buffer_size': 44670, 'gamma': 0.9454899590527855, 'exploration_fraction': 0.44238667032045137, 'exploration_final_eps': 0.0867156326492258}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4213     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.833    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4254     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.09 +/- 0.36
Episode length: 474.20 +/- 152.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0897  |
| rollout/            |          |
|    exploration_rate | 0.801    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.763    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1489     |
|    time_elapsed     | 3        |
|    total_timesteps  | 5953     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 455      |
|    ep_rew_mean      | 0.00544  |
|    exploration_rate | 0.71     |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1687     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7284     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 459      |
|    ep_rew_mean      | 0.0166   |
|    exploration_rate | 0.635    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1924     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9171     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.602    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 470      |
|    ep_rew_mean      | -0.0212  |
|    exploration_rate | 0.551    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1299     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11271    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 9.64e-06 |
|    n_updates        | 317      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 459      |
|    ep_rew_mean      | -0.00509 |
|    exploration_rate | 0.488    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1296     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12858    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 2.06e-06 |
|    n_updates        | 714      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 453      |
|    ep_rew_mean      | 0.00649  |
|    exploration_rate | 0.423    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1286     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14483    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 4.45e-06 |
|    n_updates        | 1120     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.402    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 4.78e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 461      |
|    ep_rew_mean      | -0.0176  |
|    exploration_rate | 0.339    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1034     |
|    time_elapsed     | 16       |
|    total_timesteps  | 16583    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.48e-05 |
|    n_updates        | 1645     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | -0.0368  |
|    exploration_rate | 0.256    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1046     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18683    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.5e-06  |
|    n_updates        | 2170     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.203    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.19e-05 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | -0.0526  |
|    exploration_rate | 0.172    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 925      |
|    time_elapsed     | 22       |
|    total_timesteps  | 20783    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 9.55e-06 |
|    n_updates        | 2695     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 477      |
|    ep_rew_mean      | -0.0657  |
|    exploration_rate | 0.0882   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 938      |
|    time_elapsed     | 24       |
|    total_timesteps  | 22883    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.07e-05 |
|    n_updates        | 3220     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.055   |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 946      |
|    time_elapsed     | 26       |
|    total_timesteps  | 24648    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.39e-05 |
|    n_updates        | 3661     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0478   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 3.07e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.066   |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 871      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26748    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.78e-05 |
|    n_updates        | 4186     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 477      |
|    ep_rew_mean      | -0.0573  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 881      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28603    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 4.7e-05  |
|    n_updates        | 4650     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0478   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 5.39e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0669  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 826      |
|    time_elapsed     | 37       |
|    total_timesteps  | 30703    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 4.26e-05 |
|    n_updates        | 5175     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0753  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 839      |
|    time_elapsed     | 39       |
|    total_timesteps  | 32803    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 0.000982 |
|    n_updates        | 5700     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0828  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 851      |
|    time_elapsed     | 41       |
|    total_timesteps  | 34903    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.51e-05 |
|    n_updates        | 6225     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0478   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 2.54e-05 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0895  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 810      |
|    time_elapsed     | 45       |
|    total_timesteps  | 37003    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.74e-05 |
|    n_updates        | 6750     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0955  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 821      |
|    time_elapsed     | 47       |
|    total_timesteps  | 39103    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 6.81e-06 |
|    n_updates        | 7275     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0478   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 0.000197 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 788      |
|    time_elapsed     | 52       |
|    total_timesteps  | 41203    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 2.39e-06 |
|    n_updates        | 7800     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0923  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 796      |
|    time_elapsed     | 53       |
|    total_timesteps  | 42799    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 2.58e-05 |
|    n_updates        | 8199     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0864  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 806      |
|    time_elapsed     | 55       |
|    total_timesteps  | 44870    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 8.23e-06 |
|    n_updates        | 8717     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0478   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.26e-05 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0915  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 779      |
|    time_elapsed     | 60       |
|    total_timesteps  | 46970    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 2.52e-06 |
|    n_updates        | 9242     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0963  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 788      |
|    time_elapsed     | 62       |
|    total_timesteps  | 49070    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.8e-05  |
|    n_updates        | 9767     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.10 +/- 0.34
Episode length: 489.30 +/- 107.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 489      |
|    mean_reward      | -0.0957  |
| rollout/            |          |
|    exploration_rate | 0.0478   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.32e-05 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0845  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 764      |
|    time_elapsed     | 66       |
|    total_timesteps  | 50737    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.84e-05 |
|    n_updates        | 10184    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0845  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 773      |
|    time_elapsed     | 68       |
|    total_timesteps  | 52837    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 7.99e-06 |
|    n_updates        | 10709    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0841  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 780      |
|    time_elapsed     | 69       |
|    total_timesteps  | 54482    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 9.61e-05 |
|    n_updates        | 11120    |
----------------------------------
Eval num_timesteps=55000, episode_reward=0.03 +/- 0.48
Episode length: 424.10 +/- 201.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 424      |
|    mean_reward      | 0.0304   |
| rollout/            |          |
|    exploration_rate | 0.0478   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.21e-05 |
|    n_updates        | 11249    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0969  |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 763      |
|    time_elapsed     | 73       |
|    total_timesteps  | 56514    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 1.26e-05 |
|    n_updates        | 11628    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0478   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 771      |
|    time_elapsed     | 75       |
|    total_timesteps  | 58614    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 2.64e-05 |
|    n_updates        | 12153    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0478   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000216 |
|    loss             | 7.14e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:37:04,201] Trial 14 finished with value: -0.21000000000000002 and parameters: {'batch_size': 128, 'learning_rate': 0.0002163950178629793, 'buffer_size': 64052, 'gamma': 0.9811134816671945, 'exploration_fraction': 0.3982497137885804, 'exploration_final_eps': 0.04783807509675417}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 310      |
|    ep_rew_mean      | 0.376    |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4188     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1239     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 417      |
|    ep_rew_mean      | 0.0831   |
|    exploration_rate | 0.803    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4241     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3339     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.706    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 453      |
|    ep_rew_mean      | -0.0146  |
|    exploration_rate | 0.68     |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1360     |
|    time_elapsed     | 3        |
|    total_timesteps  | 5439     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 471      |
|    ep_rew_mean      | -0.0635  |
|    exploration_rate | 0.556    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1678     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7539     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0928  |
|    exploration_rate | 0.433    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1933     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9639     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.411    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.309    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1268     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11739    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 7.28e-06 |
|    n_updates        | 434      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.185    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1251     |
|    time_elapsed     | 11       |
|    total_timesteps  | 13839    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 4.64e-06 |
|    n_updates        | 959      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.117    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 7.26e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1015     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15939    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 9.11e-07 |
|    n_updates        | 1484     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1023     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18039    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 1.09e-06 |
|    n_updates        | 2009     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0816   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 9.17e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.151   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 904      |
|    time_elapsed     | 22       |
|    total_timesteps  | 20139    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 1.25e-06 |
|    n_updates        | 2534     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 920      |
|    time_elapsed     | 24       |
|    total_timesteps  | 22239    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 5.31e-07 |
|    n_updates        | 3059     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.161   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 933      |
|    time_elapsed     | 26       |
|    total_timesteps  | 24339    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 5.25e-06 |
|    n_updates        | 3584     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0816   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 5.23e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 861      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26439    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 6.66e-07 |
|    n_updates        | 4109     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 873      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28322    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 4.93e-06 |
|    n_updates        | 4580     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 881      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29950    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 1e-06    |
|    n_updates        | 4987     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0816   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 0.00344  |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 830      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32039    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 5.8e-06  |
|    n_updates        | 5509     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 842      |
|    time_elapsed     | 40       |
|    total_timesteps  | 34139    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 0.00284  |
|    n_updates        | 6034     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0816   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 2.75e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 801      |
|    time_elapsed     | 45       |
|    total_timesteps  | 36239    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 8.82e-06 |
|    n_updates        | 6559     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 813      |
|    time_elapsed     | 47       |
|    total_timesteps  | 38339    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 7.43e-06 |
|    n_updates        | 7084     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0816   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 6.43e-06 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 781      |
|    time_elapsed     | 51       |
|    total_timesteps  | 40439    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 1.49e-05 |
|    n_updates        | 7609     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 792      |
|    time_elapsed     | 53       |
|    total_timesteps  | 42539    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 1.21e-05 |
|    n_updates        | 8134     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 803      |
|    time_elapsed     | 55       |
|    total_timesteps  | 44639    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 1.43e-05 |
|    n_updates        | 8659     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0816   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 1.97e-05 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 772      |
|    time_elapsed     | 59       |
|    total_timesteps  | 45945    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 2.22e-05 |
|    n_updates        | 8986     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 782      |
|    time_elapsed     | 61       |
|    total_timesteps  | 47899    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 7.89e-06 |
|    n_updates        | 9474     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0998  |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 791      |
|    time_elapsed     | 63       |
|    total_timesteps  | 49949    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 6.58e-06 |
|    n_updates        | 9987     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.35
Episode length: 481.00 +/- 132.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 481      |
|    mean_reward      | -0.0924  |
| rollout/            |          |
|    exploration_rate | 0.0816   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 7.16e-06 |
|    n_updates        | 9999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 770      |
|    time_elapsed     | 67       |
|    total_timesteps  | 52049    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 6.26e-06 |
|    n_updates        | 10512    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 779      |
|    time_elapsed     | 69       |
|    total_timesteps  | 54149    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 8.36e-06 |
|    n_updates        | 11037    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0816   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 2.25e-05 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 757      |
|    time_elapsed     | 73       |
|    total_timesteps  | 55813    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 1.08e-05 |
|    n_updates        | 11453    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 765      |
|    time_elapsed     | 75       |
|    total_timesteps  | 57913    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 3.52e-06 |
|    n_updates        | 11978    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0816   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 772      |
|    time_elapsed     | 77       |
|    total_timesteps  | 59777    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 3.22e-05 |
|    n_updates        | 12444    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.0816   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 5.76e-05 |
|    loss             | 6.97e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:38:27,312] Trial 15 finished with value: -0.21000000000000002 and parameters: {'batch_size': 128, 'learning_rate': 5.758067973053963e-05, 'buffer_size': 40504, 'gamma': 0.9564179081539687, 'exploration_fraction': 0.26001470716951375, 'exploration_final_eps': 0.08157532981898077}. Best is trial 6 with value: -0.08937999999999999.
New best mean reward!
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | 0.043    |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4200     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2070     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 521      |
|    ep_rew_mean      | -0.0835  |
|    exploration_rate | 0.845    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4227     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4170     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.814    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 516      |
|    ep_rew_mean      | -0.0397  |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1479     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6191     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.0823  |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1772     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8291     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.628    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 520      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.613    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1287     |
|    time_elapsed     | 8        |
|    total_timesteps  | 10391    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 3.41e-06 |
|    n_updates        | 97       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 520      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.535    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1305     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12491    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 7.79e-06 |
|    n_updates        | 622      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 521      |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.457    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1308     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14591    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 7.33e-07 |
|    n_updates        | 1147     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.442    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 7.41e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 522      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.379    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1075     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16691    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 4.68e-06 |
|    n_updates        | 1672     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 516      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.309    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1092     |
|    time_elapsed     | 16       |
|    total_timesteps  | 18571    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 6.52e-06 |
|    n_updates        | 2142     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.255    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 3.31e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 517      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.23     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 954      |
|    time_elapsed     | 21       |
|    total_timesteps  | 20671    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 2.57e-07 |
|    n_updates        | 2667     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.152    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 974      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22771    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 1.35e-05 |
|    n_updates        | 3192     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.074    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 988      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24871    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 2.64e-07 |
|    n_updates        | 3717     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0693   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 7.81e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 906      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26923    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 3.21e-07 |
|    n_updates        | 4230     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 921      |
|    time_elapsed     | 31       |
|    total_timesteps  | 29023    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 1.54e-06 |
|    n_updates        | 4755     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 2.12e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 519      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 863      |
|    time_elapsed     | 36       |
|    total_timesteps  | 31123    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 2.23e-08 |
|    n_updates        | 5280     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 519      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 877      |
|    time_elapsed     | 37       |
|    total_timesteps  | 33223    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 3.91e-08 |
|    n_updates        | 5805     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 5.8e-08  |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 831      |
|    time_elapsed     | 42       |
|    total_timesteps  | 35221    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 5.09e-08 |
|    n_updates        | 6305     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 844      |
|    time_elapsed     | 44       |
|    total_timesteps  | 37321    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 8.79e-08 |
|    n_updates        | 6830     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 516      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 855      |
|    time_elapsed     | 45       |
|    total_timesteps  | 39239    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 9.51e-08 |
|    n_updates        | 7309     |
----------------------------------
Eval num_timesteps=40000, episode_reward=0.03 +/- 0.48
Episode length: 421.60 +/- 206.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 422      |
|    mean_reward      | 0.0314   |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 1.53e-07 |
|    n_updates        | 7499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 517      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 828      |
|    time_elapsed     | 49       |
|    total_timesteps  | 41339    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 0.0158   |
|    n_updates        | 7834     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 517      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 839      |
|    time_elapsed     | 51       |
|    total_timesteps  | 43439    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 1.26e-07 |
|    n_updates        | 8359     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 847      |
|    time_elapsed     | 52       |
|    total_timesteps  | 44902    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 1.05e-07 |
|    n_updates        | 8725     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 5.1e-08  |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 816      |
|    time_elapsed     | 57       |
|    total_timesteps  | 47002    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 3.06e-08 |
|    n_updates        | 9250     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.0962  |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 822      |
|    time_elapsed     | 58       |
|    total_timesteps  | 48100    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 1.2e-05  |
|    n_updates        | 9524     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 2.6e-06  |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 796      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50200    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 1.01e-07 |
|    n_updates        | 10049    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0995  |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 804      |
|    time_elapsed     | 64       |
|    total_timesteps  | 51960    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 3.95e-08 |
|    n_updates        | 10489    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0995  |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 813      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54060    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 2.31e-07 |
|    n_updates        | 11014    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 2.57e-08 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 791      |
|    time_elapsed     | 70       |
|    total_timesteps  | 56160    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 1.24e-08 |
|    n_updates        | 11539    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 800      |
|    time_elapsed     | 72       |
|    total_timesteps  | 58260    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 4.52e-08 |
|    n_updates        | 12064    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000351 |
|    loss             | 1.22e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:39:47,534] Trial 16 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.00035101890980966324, 'buffer_size': 84752, 'gamma': 0.9347238969728183, 'exploration_fraction': 0.4204892266228084, 'exploration_final_eps': 0.06068949059537364}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4193     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.0756  |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4205     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4013     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.843    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 468      |
|    ep_rew_mean      | -0.0204  |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1382     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5614     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 455      |
|    ep_rew_mean      | 0.00565  |
|    exploration_rate | 0.772    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1633     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7275     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | -0.0375  |
|    exploration_rate | 0.706    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1896     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9375     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.687    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0662  |
|    exploration_rate | 0.641    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1290     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11475    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 4.2e-06  |
|    n_updates        | 368      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0868  |
|    exploration_rate | 0.575    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1306     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13575    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 4.21e-06 |
|    n_updates        | 893      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.53     |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 2.09e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.509    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1071     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15675    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.7e-06  |
|    n_updates        | 1418     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.443    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1093     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17775    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.75e-06 |
|    n_updates        | 1943     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.378    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1112     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19875    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 9.73e-07 |
|    n_updates        | 2468     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.374    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 2.08e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.321    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 987      |
|    time_elapsed     | 21       |
|    total_timesteps  | 21680    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.17e-05 |
|    n_updates        | 2919     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0709  |
|    exploration_rate | 0.264    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 1003     |
|    time_elapsed     | 23       |
|    total_timesteps  | 23507    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 2.38e-06 |
|    n_updates        | 3376     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.217    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.15e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0816  |
|    exploration_rate | 0.198    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 919      |
|    time_elapsed     | 27       |
|    total_timesteps  | 25607    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 5.6e-07  |
|    n_updates        | 3901     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0907  |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 935      |
|    time_elapsed     | 29       |
|    total_timesteps  | 27707    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 5.09e-07 |
|    n_updates        | 4426     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0987  |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 948      |
|    time_elapsed     | 31       |
|    total_timesteps  | 29807    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 2.87e-06 |
|    n_updates        | 4951     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0809   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.44e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 886      |
|    time_elapsed     | 35       |
|    total_timesteps  | 31907    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.11e-05 |
|    n_updates        | 5476     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 900      |
|    time_elapsed     | 37       |
|    total_timesteps  | 34007    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 9.03e-07 |
|    n_updates        | 6001     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0809   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 5.19e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 853      |
|    time_elapsed     | 42       |
|    total_timesteps  | 36107    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.24e-06 |
|    n_updates        | 6526     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 864      |
|    time_elapsed     | 43       |
|    total_timesteps  | 37947    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 7.06e-07 |
|    n_updates        | 6986     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0809   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 4.45e-06 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 827      |
|    time_elapsed     | 48       |
|    total_timesteps  | 40047    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 5.05e-06 |
|    n_updates        | 7511     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0896  |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 834      |
|    time_elapsed     | 49       |
|    total_timesteps  | 41325    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.13e-06 |
|    n_updates        | 7831     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0951  |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 846      |
|    time_elapsed     | 51       |
|    total_timesteps  | 43425    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 4.46e-07 |
|    n_updates        | 8356     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0809   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.09e-05 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0889  |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 815      |
|    time_elapsed     | 55       |
|    total_timesteps  | 45451    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 3.29e-06 |
|    n_updates        | 8862     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0701  |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 823      |
|    time_elapsed     | 56       |
|    total_timesteps  | 46826    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.19e-06 |
|    n_updates        | 9206     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0643  |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 831      |
|    time_elapsed     | 58       |
|    total_timesteps  | 48589    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 2.47e-06 |
|    n_updates        | 9647     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0809   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 6.81e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0643  |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 805      |
|    time_elapsed     | 62       |
|    total_timesteps  | 50689    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 6.93e-07 |
|    n_updates        | 10172    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.065   |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 814      |
|    time_elapsed     | 64       |
|    total_timesteps  | 52755    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 9.15e-07 |
|    n_updates        | 10688    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.066   |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 822      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54626    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 4.06e-07 |
|    n_updates        | 11156    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.09 +/- 0.36
Episode length: 473.90 +/- 153.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0896  |
| rollout/            |          |
|    exploration_rate | 0.0809   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 1.34e-06 |
|    n_updates        | 11249    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0778  |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 802      |
|    time_elapsed     | 70       |
|    total_timesteps  | 56726    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 2.96e-06 |
|    n_updates        | 11681    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0778  |
|    exploration_rate | 0.0809   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 812      |
|    time_elapsed     | 72       |
|    total_timesteps  | 58826    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 7e-07    |
|    n_updates        | 12206    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.35
Episode length: 486.60 +/- 115.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | -0.0946  |
| rollout/            |          |
|    exploration_rate | 0.0809   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.07e-05 |
|    loss             | 6.68e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:41:06,280] Trial 17 finished with value: -0.09085999999999998 and parameters: {'batch_size': 32, 'learning_rate': 1.0690872540841534e-05, 'buffer_size': 64196, 'gamma': 0.9031480562294714, 'exploration_fraction': 0.48915837358855246, 'exploration_final_eps': 0.08087278289973482}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4159     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0714  |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4150     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3928     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.845    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.813    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1423     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6028     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.748    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1718     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8128     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0996  |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1929     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9980     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.69     |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 0.0133   |
|    exploration_rate | 0.638    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1278     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11704    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.46e-06 |
|    n_updates        | 425      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 475      |
|    ep_rew_mean      | 0.0599   |
|    exploration_rate | 0.588    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1294     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13311    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.47e-06 |
|    n_updates        | 827      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.536    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.94e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | 0.0261   |
|    exploration_rate | 0.523    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1058     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15411    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.74e-06 |
|    n_updates        | 1352     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | 0.0287   |
|    exploration_rate | 0.461    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1081     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17420    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.35e-06 |
|    n_updates        | 1854     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | 0.00482  |
|    exploration_rate | 0.396    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1102     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19520    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.99e-06 |
|    n_updates        | 2379     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.381    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.74e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0147  |
|    exploration_rate | 0.331    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 978      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21620    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.62e-06 |
|    n_updates        | 2904     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.031   |
|    exploration_rate | 0.266    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 998      |
|    time_elapsed     | 23       |
|    total_timesteps  | 23720    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.86e-06 |
|    n_updates        | 3429     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.226    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.05e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0447  |
|    exploration_rate | 0.201    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 913      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25820    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.38e-06 |
|    n_updates        | 3954     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0566  |
|    exploration_rate | 0.136    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 929      |
|    time_elapsed     | 30       |
|    total_timesteps  | 27920    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.23e-06 |
|    n_updates        | 4479     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.79e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.0668  |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 867      |
|    time_elapsed     | 34       |
|    total_timesteps  | 30020    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.11e-06 |
|    n_updates        | 5004     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.0757  |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 882      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32120    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.5e-06  |
|    n_updates        | 5529     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.0836  |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 895      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34220    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.27e-06 |
|    n_updates        | 6054     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.27e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.0907  |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 849      |
|    time_elapsed     | 42       |
|    total_timesteps  | 36320    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.08e-06 |
|    n_updates        | 6579     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.0969  |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 862      |
|    time_elapsed     | 44       |
|    total_timesteps  | 38420    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.53e-06 |
|    n_updates        | 7104     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.94e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 824      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40520    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.57e-06 |
|    n_updates        | 7629     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 836      |
|    time_elapsed     | 50       |
|    total_timesteps  | 42620    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.57e-06 |
|    n_updates        | 8154     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 847      |
|    time_elapsed     | 52       |
|    total_timesteps  | 44720    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.98e-07 |
|    n_updates        | 8679     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 5.32e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 816      |
|    time_elapsed     | 57       |
|    total_timesteps  | 46820    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 9.43e-07 |
|    n_updates        | 9204     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 827      |
|    time_elapsed     | 59       |
|    total_timesteps  | 48920    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 3.05e-06 |
|    n_updates        | 9729     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.81e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 801      |
|    time_elapsed     | 63       |
|    total_timesteps  | 51020    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 2.47e-07 |
|    n_updates        | 10254    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 810      |
|    time_elapsed     | 65       |
|    total_timesteps  | 53120    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.06e-07 |
|    n_updates        | 10779    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.09 +/- 0.36
Episode length: 475.00 +/- 150.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 475      |
|    mean_reward      | -0.09    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.62e-07 |
|    n_updates        | 11249    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 791      |
|    time_elapsed     | 69       |
|    total_timesteps  | 55220    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 1.47e-06 |
|    n_updates        | 11304    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 799      |
|    time_elapsed     | 71       |
|    total_timesteps  | 57320    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 4.9e-07  |
|    n_updates        | 11829    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 808      |
|    time_elapsed     | 73       |
|    total_timesteps  | 59330    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 8.66e-07 |
|    n_updates        | 12332    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1e-05    |
|    loss             | 6.92e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:42:26,112] Trial 18 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 1.0013609293316445e-05, 'buffer_size': 65315, 'gamma': 0.9018219884197378, 'exploration_fraction': 0.49616757834121816, 'exploration_final_eps': 0.07847547901125976}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4198     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0686  |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4203     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3873     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.828    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.794    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1442     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5973     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.722    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1740     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8073     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0396  |
|    exploration_rate | 0.673    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1906     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9481     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.656    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | -0.0182  |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1284     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11094    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 4.15e-06 |
|    n_updates        | 273      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 471      |
|    ep_rew_mean      | -0.0456  |
|    exploration_rate | 0.546    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1289     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13194    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 4.39e-07 |
|    n_updates        | 798      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.483    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 2.96e-05 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0661  |
|    exploration_rate | 0.473    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1053     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15294    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 5.97e-07 |
|    n_updates        | 1323     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0821  |
|    exploration_rate | 0.401    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1076     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17394    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 2.41e-05 |
|    n_updates        | 1848     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0692  |
|    exploration_rate | 0.331    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1091     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19420    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 1.29e-06 |
|    n_updates        | 2354     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.311    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 5.86e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.082   |
|    exploration_rate | 0.259    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 969      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21520    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 1.38e-06 |
|    n_updates        | 2879     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0926  |
|    exploration_rate | 0.187    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 987      |
|    time_elapsed     | 23       |
|    total_timesteps  | 23620    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 3.94e-07 |
|    n_updates        | 3404     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.139    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 2.93e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.114    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 904      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25720    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 4.47e-07 |
|    n_updates        | 3929     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 920      |
|    time_elapsed     | 30       |
|    total_timesteps  | 27820    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 0.0155   |
|    n_updates        | 4454     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 933      |
|    time_elapsed     | 32       |
|    total_timesteps  | 29920    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 3.28e-07 |
|    n_updates        | 4979     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0445   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 3.27e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 874      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32020    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 4.59e-07 |
|    n_updates        | 5504     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 888      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34120    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 9.6e-06  |
|    n_updates        | 6029     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0445   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 3.64e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 843      |
|    time_elapsed     | 42       |
|    total_timesteps  | 36220    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 5.17e-06 |
|    n_updates        | 6554     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 854      |
|    time_elapsed     | 44       |
|    total_timesteps  | 37934    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 5.18e-06 |
|    n_updates        | 6983     |
----------------------------------
Eval num_timesteps=40000, episode_reward=0.14 +/- 0.54
Episode length: 389.20 +/- 209.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 389      |
|    mean_reward      | 0.144    |
| rollout/            |          |
|    exploration_rate | 0.0445   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 3.07e-07 |
|    n_updates        | 7499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 830      |
|    time_elapsed     | 48       |
|    total_timesteps  | 40034    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 1.57e-06 |
|    n_updates        | 7508     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 841      |
|    time_elapsed     | 50       |
|    total_timesteps  | 42134    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 0.0144   |
|    n_updates        | 8033     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 852      |
|    time_elapsed     | 51       |
|    total_timesteps  | 44234    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 2.63e-07 |
|    n_updates        | 8558     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0445   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 2.84e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 818      |
|    time_elapsed     | 55       |
|    total_timesteps  | 45749    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 5.9e-07  |
|    n_updates        | 8937     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 827      |
|    time_elapsed     | 57       |
|    total_timesteps  | 47539    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 9.67e-06 |
|    n_updates        | 9384     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 837      |
|    time_elapsed     | 59       |
|    total_timesteps  | 49639    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 2.03e-07 |
|    n_updates        | 9909     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0445   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 5.21e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 810      |
|    time_elapsed     | 63       |
|    total_timesteps  | 51739    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 1.17e-06 |
|    n_updates        | 10434    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 810      |
|    time_elapsed     | 66       |
|    total_timesteps  | 53839    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 2.01e-07 |
|    n_updates        | 10959    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0445   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 4.17e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 787      |
|    time_elapsed     | 70       |
|    total_timesteps  | 55853    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 4.87e-07 |
|    n_updates        | 11463    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0445   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 796      |
|    time_elapsed     | 72       |
|    total_timesteps  | 57953    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 3.38e-07 |
|    n_updates        | 11988    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0445   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 2.57e-05 |
|    loss             | 6.41e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:43:46,493] Trial 19 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 2.569246337941966e-05, 'buffer_size': 88416, 'gamma': 0.9024719360745844, 'exploration_fraction': 0.4623811964887577, 'exploration_final_eps': 0.044516214058042985}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 456      |
|    ep_rew_mean      | 0.0676   |
|    exploration_rate | 0.931    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4171     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1824     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0712  |
|    exploration_rate | 0.851    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4210     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3924     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.81     |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 447      |
|    ep_rew_mean      | 0.0712   |
|    exploration_rate | 0.796    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1342     |
|    time_elapsed     | 3        |
|    total_timesteps  | 5366     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | 0.000869 |
|    exploration_rate | 0.716    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1662     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7466     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0413  |
|    exploration_rate | 0.636    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1916     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9566     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.62     |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0694  |
|    exploration_rate | 0.556    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1301     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11666    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 7.49e-06 |
|    n_updates        | 416      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0474  |
|    exploration_rate | 0.493    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1309     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13321    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 2.52e-06 |
|    n_updates        | 830      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.429    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 4.15e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0677  |
|    exploration_rate | 0.413    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1066     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15421    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 1.39e-05 |
|    n_updates        | 1355     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0836  |
|    exploration_rate | 0.333    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1085     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17521    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 9.97e-07 |
|    n_updates        | 1880     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0962  |
|    exploration_rate | 0.254    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1102     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19621    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 1.23e-06 |
|    n_updates        | 2405     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.239    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 7.03e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.174    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 977      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21721    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 5.64e-06 |
|    n_updates        | 2930     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0922  |
|    exploration_rate | 0.104    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 991      |
|    time_elapsed     | 23       |
|    total_timesteps  | 23560    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 4.4e-06  |
|    n_updates        | 3389     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 6.94e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 906      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25660    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 5.58e-06 |
|    n_updates        | 3914     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 921      |
|    time_elapsed     | 30       |
|    total_timesteps  | 27760    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 4.84e-06 |
|    n_updates        | 4439     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 935      |
|    time_elapsed     | 31       |
|    total_timesteps  | 29860    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 1.85e-05 |
|    n_updates        | 4964     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.35
Episode length: 479.40 +/- 136.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 479      |
|    mean_reward      | -0.0917  |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 5.74e-06 |
|    n_updates        | 4999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 880      |
|    time_elapsed     | 36       |
|    total_timesteps  | 31960    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 1.06e-07 |
|    n_updates        | 5489     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 894      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34060    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 8.34e-09 |
|    n_updates        | 6014     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 2.01e-08 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 848      |
|    time_elapsed     | 42       |
|    total_timesteps  | 36160    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 7.57e-08 |
|    n_updates        | 6539     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 861      |
|    time_elapsed     | 44       |
|    total_timesteps  | 38260    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 8.53e-08 |
|    n_updates        | 7064     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 870      |
|    time_elapsed     | 45       |
|    total_timesteps  | 39877    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 4.18e-08 |
|    n_updates        | 7469     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 4.19e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 829      |
|    time_elapsed     | 49       |
|    total_timesteps  | 41222    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 1.4e-09  |
|    n_updates        | 7805     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 841      |
|    time_elapsed     | 51       |
|    total_timesteps  | 43322    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 8.51e-08 |
|    n_updates        | 8330     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0976  |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 849      |
|    time_elapsed     | 52       |
|    total_timesteps  | 44954    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 9.11e-07 |
|    n_updates        | 8738     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 4.72e-09 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0902  |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 817      |
|    time_elapsed     | 57       |
|    total_timesteps  | 46658    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 9.39e-08 |
|    n_updates        | 9164     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0843  |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 825      |
|    time_elapsed     | 58       |
|    total_timesteps  | 48590    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 8.14e-09 |
|    n_updates        | 9647     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 2.45e-08 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0836  |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 797      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50233    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 5.28e-08 |
|    n_updates        | 10058    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0836  |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 806      |
|    time_elapsed     | 64       |
|    total_timesteps  | 52333    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 2.27e-09 |
|    n_updates        | 10583    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 815      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54433    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 1.63e-09 |
|    n_updates        | 11108    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 7.03e-08 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0952  |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 791      |
|    time_elapsed     | 71       |
|    total_timesteps  | 56274    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 9e-07    |
|    n_updates        | 11568    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0833  |
|    exploration_rate | 0.0607   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 798      |
|    time_elapsed     | 72       |
|    total_timesteps  | 57906    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 1.83e-05 |
|    n_updates        | 11976    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0607   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.00209  |
|    loss             | 1.19e-08 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:45:06,670] Trial 20 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.0020946937349825404, 'buffer_size': 74427, 'gamma': 0.922356140911482, 'exploration_fraction': 0.41150267901440973, 'exploration_final_eps': 0.06070523975959812}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 407      |
|    ep_rew_mean      | 0.0872   |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4183     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1628     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 466      |
|    ep_rew_mean      | -0.0614  |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4197     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3728     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.808    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.776    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1409     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5828     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.695    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1712     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7928     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.615    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.151   |
|    exploration_rate | 0.614    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1317     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10028    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 4.44e-05 |
|    n_updates        | 6        |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.16    |
|    exploration_rate | 0.533    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1327     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12128    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.7e-06  |
|    n_updates        | 531      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.168   |
|    exploration_rate | 0.452    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1328     |
|    time_elapsed     | 10       |
|    total_timesteps  | 14228    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 9.52e-07 |
|    n_updates        | 1056     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.423    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 9.03e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.389    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1080     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15880    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 7.93e-07 |
|    n_updates        | 1469     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0817  |
|    exploration_rate | 0.332    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1090     |
|    time_elapsed     | 15       |
|    total_timesteps  | 17355    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 5.66e-07 |
|    n_updates        | 1838     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.066   |
|    exploration_rate | 0.265    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1100     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19103    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 9.57e-07 |
|    n_updates        | 2275     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.23     |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 2.68e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0791  |
|    exploration_rate | 0.184    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 970      |
|    time_elapsed     | 21       |
|    total_timesteps  | 21203    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.25e-05 |
|    n_updates        | 2800     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.09    |
|    exploration_rate | 0.103    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 986      |
|    time_elapsed     | 23       |
|    total_timesteps  | 23303    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 3.18e-05 |
|    n_updates        | 3325     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0765  |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 995      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24940    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.2e-06  |
|    n_updates        | 3734     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.089    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 8.59e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.086   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 913      |
|    time_elapsed     | 29       |
|    total_timesteps  | 27040    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.48e-07 |
|    n_updates        | 4259     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0943  |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 60       |
|    fps              | 927      |
|    time_elapsed     | 31       |
|    total_timesteps  | 29140    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 7.3e-08  |
|    n_updates        | 4784     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.089    |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 9.09e-08 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 64       |
|    fps              | 868      |
|    time_elapsed     | 35       |
|    total_timesteps  | 31240    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 5.14e-07 |
|    n_updates        | 5309     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 68       |
|    fps              | 882      |
|    time_elapsed     | 37       |
|    total_timesteps  | 33340    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.51e-07 |
|    n_updates        | 5834     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.089    |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.39e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 72       |
|    fps              | 838      |
|    time_elapsed     | 42       |
|    total_timesteps  | 35440    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.61e-07 |
|    n_updates        | 6359     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 76       |
|    fps              | 851      |
|    time_elapsed     | 43       |
|    total_timesteps  | 37398    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.58e-07 |
|    n_updates        | 6849     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 80       |
|    fps              | 863      |
|    time_elapsed     | 45       |
|    total_timesteps  | 39498    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.23e-07 |
|    n_updates        | 7374     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.089    |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 9.87e-08 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 84       |
|    fps              | 828      |
|    time_elapsed     | 50       |
|    total_timesteps  | 41598    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 5.53e-08 |
|    n_updates        | 7899     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 88       |
|    fps              | 840      |
|    time_elapsed     | 52       |
|    total_timesteps  | 43698    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 8.76e-08 |
|    n_updates        | 8424     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.089    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 8.77e-08 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 92       |
|    fps              | 808      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45443    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 7.27e-08 |
|    n_updates        | 8860     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 96       |
|    fps              | 818      |
|    time_elapsed     | 57       |
|    total_timesteps  | 47471    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 2.1e-07  |
|    n_updates        | 9367     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0962  |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 100      |
|    fps              | 826      |
|    time_elapsed     | 59       |
|    total_timesteps  | 49057    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 4.32e-06 |
|    n_updates        | 9764     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.089    |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 2.04e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 104      |
|    fps              | 800      |
|    time_elapsed     | 63       |
|    total_timesteps  | 51157    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 2.15e-07 |
|    n_updates        | 10289    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0743  |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 108      |
|    fps              | 806      |
|    time_elapsed     | 64       |
|    total_timesteps  | 52300    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.12e-07 |
|    n_updates        | 10574    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0743  |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 112      |
|    fps              | 815      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54400    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 5.67e-07 |
|    n_updates        | 11099    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.089    |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 8.5e-08  |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0629  |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 116      |
|    fps              | 791      |
|    time_elapsed     | 70       |
|    total_timesteps  | 56163    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 6.43e-08 |
|    n_updates        | 11540    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0402  |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 120      |
|    fps              | 798      |
|    time_elapsed     | 72       |
|    total_timesteps  | 57588    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 2.92e-07 |
|    n_updates        | 11896    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0402  |
|    exploration_rate | 0.089    |
| time/               |          |
|    episodes         | 124      |
|    fps              | 807      |
|    time_elapsed     | 73       |
|    total_timesteps  | 59688    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 1.15e-05 |
|    n_updates        | 12421    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.089    |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000144 |
|    loss             | 2.02e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:46:26,695] Trial 21 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.00014402732551781593, 'buffer_size': 36858, 'gamma': 0.9547144536640475, 'exploration_fraction': 0.39448329754567324, 'exploration_final_eps': 0.08904013666546566}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | 0.0522   |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4220     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1979     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.0789  |
|    exploration_rate | 0.866    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4197     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4079     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.836    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 515      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.797    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1480     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6179     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 517      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.728    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1769     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8279     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0986  |
|    exploration_rate | 0.674    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1959     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9929     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.672    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.605    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1282     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12029    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.19e-06 |
|    n_updates        | 507      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.537    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1280     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14129    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.21e-06 |
|    n_updates        | 1032     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.508    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 6.53e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.468    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1049     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16229    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.8e-06  |
|    n_updates        | 1557     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.4      |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1062     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18287    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.53e-06 |
|    n_updates        | 2071     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.344    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 9.17e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.338    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 938      |
|    time_elapsed     | 21       |
|    total_timesteps  | 20196    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 3.83e-07 |
|    n_updates        | 2548     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.0886  |
|    exploration_rate | 0.27     |
| time/               |          |
|    episodes         | 44       |
|    fps              | 954      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22250    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 6.77e-07 |
|    n_updates        | 3062     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.0987  |
|    exploration_rate | 0.201    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 966      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24350    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 3.49e-06 |
|    n_updates        | 3587     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.18     |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 6.53e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 887      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26450    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 4.76e-06 |
|    n_updates        | 4112     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 899      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28550    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 5.11e-07 |
|    n_updates        | 4637     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.2e-05  |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 842      |
|    time_elapsed     | 36       |
|    total_timesteps  | 30650    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 3.17e-07 |
|    n_updates        | 5162     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 853      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32471    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 3.75e-07 |
|    n_updates        | 5617     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 865      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34571    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 3.89e-07 |
|    n_updates        | 6142     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.54e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 822      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36671    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.03e-06 |
|    n_updates        | 6667     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 833      |
|    time_elapsed     | 46       |
|    total_timesteps  | 38771    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.45e-06 |
|    n_updates        | 7192     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 5.3e-07  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 799      |
|    time_elapsed     | 51       |
|    total_timesteps  | 40871    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 6.81e-07 |
|    n_updates        | 7717     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 808      |
|    time_elapsed     | 53       |
|    total_timesteps  | 42851    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.84e-06 |
|    n_updates        | 8212     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 817      |
|    time_elapsed     | 54       |
|    total_timesteps  | 44951    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 8.53e-07 |
|    n_updates        | 8737     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.01e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 789      |
|    time_elapsed     | 59       |
|    total_timesteps  | 47051    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 8.59e-06 |
|    n_updates        | 9262     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 798      |
|    time_elapsed     | 61       |
|    total_timesteps  | 49151    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 8.02e-06 |
|    n_updates        | 9787     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.54e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 774      |
|    time_elapsed     | 66       |
|    total_timesteps  | 51251    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.37e-06 |
|    n_updates        | 10312    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 783      |
|    time_elapsed     | 68       |
|    total_timesteps  | 53351    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 2e-08    |
|    n_updates        | 10837    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.52e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 760      |
|    time_elapsed     | 72       |
|    total_timesteps  | 55060    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 4.2e-07  |
|    n_updates        | 11264    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 769      |
|    time_elapsed     | 74       |
|    total_timesteps  | 57160    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 3.08e-06 |
|    n_updates        | 11789    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0785   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 777      |
|    time_elapsed     | 76       |
|    total_timesteps  | 59260    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.47e-07 |
|    n_updates        | 12314    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000753 |
|    loss             | 1.92e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:47:49,483] Trial 22 finished with value: -0.21000000000000002 and parameters: {'batch_size': 128, 'learning_rate': 0.0007529432882410319, 'buffer_size': 56407, 'gamma': 0.9242952031608094, 'exploration_fraction': 0.46819061968095455, 'exploration_final_eps': 0.07854973820391771}. Best is trial 6 with value: -0.08937999999999999.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4218     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4226     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.827    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.782    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1486     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6300     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 521      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.711    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1767     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8337     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.654    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.65     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1274     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10113    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 7.42e-05 |
|    n_updates        | 28       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.0758  |
|    exploration_rate | 0.583    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1288     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12049    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 3.87e-06 |
|    n_updates        | 512      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.095   |
|    exploration_rate | 0.51     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1294     |
|    time_elapsed     | 10       |
|    total_timesteps  | 14149    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 8.63e-07 |
|    n_updates        | 1037     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.09 +/- 0.36
Episode length: 473.50 +/- 154.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0894  |
| rollout/            |          |
|    exploration_rate | 0.481    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 2.75e-06 |
|    n_updates        | 1249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0722  |
|    exploration_rate | 0.454    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1073     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15780    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 9.81e-07 |
|    n_updates        | 1444     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0875  |
|    exploration_rate | 0.381    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1092     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17880    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.3e-05  |
|    n_updates        | 1969     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.0998  |
|    exploration_rate | 0.308    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1104     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19980    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 8.54e-07 |
|    n_updates        | 2494     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.308    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.33e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.236    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 978      |
|    time_elapsed     | 22       |
|    total_timesteps  | 22080    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.67e-06 |
|    n_updates        | 3019     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.163    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 993      |
|    time_elapsed     | 24       |
|    total_timesteps  | 24180    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 3.05e-06 |
|    n_updates        | 3544     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.134    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.39e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.0945   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 909      |
|    time_elapsed     | 28       |
|    total_timesteps  | 26154    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.56e-06 |
|    n_updates        | 4038     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 924      |
|    time_elapsed     | 30       |
|    total_timesteps  | 28254    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.48e-06 |
|    n_updates        | 4563     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.36
Episode length: 474.40 +/- 151.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0897  |
| rollout/            |          |
|    exploration_rate | 0.0908   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 2.22e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 871      |
|    time_elapsed     | 34       |
|    total_timesteps  | 30354    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.51e-06 |
|    n_updates        | 5088     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 883      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32246    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.15e-05 |
|    n_updates        | 5561     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0967  |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 893      |
|    time_elapsed     | 37       |
|    total_timesteps  | 33948    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 3.66e-06 |
|    n_updates        | 5986     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.0908   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 2.02e-06 |
|    n_updates        | 6249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0864  |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 849      |
|    time_elapsed     | 41       |
|    total_timesteps  | 35557    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 2.47e-05 |
|    n_updates        | 6389     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0929  |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 862      |
|    time_elapsed     | 43       |
|    total_timesteps  | 37657    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 8.41e-07 |
|    n_updates        | 6914     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0846  |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 871      |
|    time_elapsed     | 45       |
|    total_timesteps  | 39420    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 3.7e-06  |
|    n_updates        | 7354     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0908   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 2.96e-06 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0905  |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 834      |
|    time_elapsed     | 49       |
|    total_timesteps  | 41520    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 2.9e-06  |
|    n_updates        | 7879     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.096   |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 845      |
|    time_elapsed     | 51       |
|    total_timesteps  | 43620    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.08e-05 |
|    n_updates        | 8404     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0908   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 3.95e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 814      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45720    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.74e-05 |
|    n_updates        | 8929     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 824      |
|    time_elapsed     | 58       |
|    total_timesteps  | 47820    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.44e-06 |
|    n_updates        | 9454     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 833      |
|    time_elapsed     | 59       |
|    total_timesteps  | 49920    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.71e-06 |
|    n_updates        | 9979     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0908   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 6.15e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 806      |
|    time_elapsed     | 64       |
|    total_timesteps  | 52020    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 7.21e-06 |
|    n_updates        | 10504    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 816      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54120    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 6.73e-06 |
|    n_updates        | 11029    |
----------------------------------
Eval num_timesteps=55000, episode_reward=0.03 +/- 0.47
Episode length: 434.20 +/- 183.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 434      |
|    mean_reward      | 0.0264   |
| rollout/            |          |
|    exploration_rate | 0.0908   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.94e-06 |
|    n_updates        | 11249    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0978  |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 796      |
|    time_elapsed     | 70       |
|    total_timesteps  | 55744    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 3.48e-06 |
|    n_updates        | 11435    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 804      |
|    time_elapsed     | 71       |
|    total_timesteps  | 57844    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 3.88e-06 |
|    n_updates        | 11960    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.0908   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 813      |
|    time_elapsed     | 73       |
|    total_timesteps  | 59944    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 2.02e-05 |
|    n_updates        | 12485    |
----------------------------------
Eval num_timesteps=60000, episode_reward=0.02 +/- 0.47
Episode length: 442.90 +/- 167.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 443      |
|    mean_reward      | 0.0229   |
| rollout/            |          |
|    exploration_rate | 0.0908   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 4.09e-05 |
|    loss             | 1.79e-05 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:49:08,209] Trial 23 finished with value: 0.029390000000000017 and parameters: {'batch_size': 64, 'learning_rate': 4.093510223780286e-05, 'buffer_size': 50519, 'gamma': 0.9645830155352708, 'exploration_fraction': 0.4376412612352858, 'exploration_final_eps': 0.09084847874639472}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4199     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 420      |
|    ep_rew_mean      | 0.0821   |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4200     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3358     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.843    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 455      |
|    ep_rew_mean      | -0.0152  |
|    exploration_rate | 0.829    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1357     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5458     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 471      |
|    ep_rew_mean      | -0.00103 |
|    exploration_rate | 0.764    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1673     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7542     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 466      |
|    ep_rew_mean      | 0.0135   |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1893     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9328     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.686    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 470      |
|    ep_rew_mean      | 0.0203   |
|    exploration_rate | 0.646    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1290     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11284    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.22e-05 |
|    n_updates        | 320      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0126  |
|    exploration_rate | 0.58     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1303     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13384    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 3.3e-06  |
|    n_updates        | 845      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.09 +/- 0.36
Episode length: 473.90 +/- 153.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0895  |
| rollout/            |          |
|    exploration_rate | 0.53     |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 4.33e-06 |
|    n_updates        | 1249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0373  |
|    exploration_rate | 0.515    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1082     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15484    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 3.21e-06 |
|    n_updates        | 1370     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0565  |
|    exploration_rate | 0.449    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1104     |
|    time_elapsed     | 15       |
|    total_timesteps  | 17584    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 3.49e-06 |
|    n_updates        | 1895     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0718  |
|    exploration_rate | 0.383    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1120     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19684    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.36e-06 |
|    n_updates        | 2420     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.373    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 2.06e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0844  |
|    exploration_rate | 0.317    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 992      |
|    time_elapsed     | 21       |
|    total_timesteps  | 21784    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.43e-06 |
|    n_updates        | 2945     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0729  |
|    exploration_rate | 0.255    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 1007     |
|    time_elapsed     | 23       |
|    total_timesteps  | 23756    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.23e-06 |
|    n_updates        | 3438     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.216    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.17e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0835  |
|    exploration_rate | 0.189    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 920      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25856    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.77e-06 |
|    n_updates        | 3963     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0925  |
|    exploration_rate | 0.123    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 934      |
|    time_elapsed     | 29       |
|    total_timesteps  | 27956    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 3.26e-06 |
|    n_updates        | 4488     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0636   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.19e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 872      |
|    time_elapsed     | 34       |
|    total_timesteps  | 30056    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.32e-06 |
|    n_updates        | 5013     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 885      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32156    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.63e-06 |
|    n_updates        | 5538     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 898      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34256    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.01e-06 |
|    n_updates        | 6063     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0636   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.61e-05 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 850      |
|    time_elapsed     | 42       |
|    total_timesteps  | 36356    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 8.84e-07 |
|    n_updates        | 6588     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 860      |
|    time_elapsed     | 44       |
|    total_timesteps  | 38189    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 8.22e-07 |
|    n_updates        | 7047     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.0994  |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 868      |
|    time_elapsed     | 45       |
|    total_timesteps  | 39875    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 9.07e-07 |
|    n_updates        | 7468     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0636   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 9.69e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 831      |
|    time_elapsed     | 50       |
|    total_timesteps  | 41975    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 9.42e-07 |
|    n_updates        | 7993     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.0969  |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 840      |
|    time_elapsed     | 52       |
|    total_timesteps  | 43832    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.29e-06 |
|    n_updates        | 8457     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0636   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 3.38e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 810      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45932    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 3.2e-06  |
|    n_updates        | 8982     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 820      |
|    time_elapsed     | 58       |
|    total_timesteps  | 48032    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 8.55e-07 |
|    n_updates        | 9507     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0985  |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 827      |
|    time_elapsed     | 59       |
|    total_timesteps  | 49632    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 2.55e-06 |
|    n_updates        | 9907     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0636   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.11e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0985  |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 801      |
|    time_elapsed     | 64       |
|    total_timesteps  | 51732    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.64e-06 |
|    n_updates        | 10432    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 810      |
|    time_elapsed     | 66       |
|    total_timesteps  | 53832    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 1.45e-06 |
|    n_updates        | 10957    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0636   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 2.46e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 788      |
|    time_elapsed     | 70       |
|    total_timesteps  | 55932    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 9.18e-06 |
|    n_updates        | 11482    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 796      |
|    time_elapsed     | 72       |
|    total_timesteps  | 58032    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 9.85e-07 |
|    n_updates        | 12007    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 803      |
|    time_elapsed     | 74       |
|    total_timesteps  | 59679    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 6.27e-07 |
|    n_updates        | 12419    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0636   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.11e-05 |
|    loss             | 6.56e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:50:28,523] Trial 24 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 1.113823911218618e-05, 'buffer_size': 57733, 'gamma': 0.9381483120567649, 'exploration_fraction': 0.49777930214275534, 'exploration_final_eps': 0.06356957941131297}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4249     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4258     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.821    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.775    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1491     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6300     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.717    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1718     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7924     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.0993  |
|    exploration_rate | 0.644    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1956     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9965     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.643    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.569    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1289     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12065    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 2.39e-06 |
|    n_updates        | 516      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0884  |
|    exploration_rate | 0.511    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1295     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13692    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 7.82e-07 |
|    n_updates        | 922      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.464    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 9.57e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.436    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1060     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15792    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 5.69e-07 |
|    n_updates        | 1447     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0845  |
|    exploration_rate | 0.371    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1078     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17607    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 7.08e-07 |
|    n_updates        | 1901     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0971  |
|    exploration_rate | 0.296    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1091     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19707    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 1.93e-05 |
|    n_updates        | 2426     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.285    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 1.02e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.221    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 968      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21807    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 3.93e-07 |
|    n_updates        | 2951     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.146    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 983      |
|    time_elapsed     | 24       |
|    total_timesteps  | 23907    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 2.09e-07 |
|    n_updates        | 3476     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.107    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 4.62e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.0788   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 900      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25775    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 7.66e-07 |
|    n_updates        | 3943     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 915      |
|    time_elapsed     | 30       |
|    total_timesteps  | 27875    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 4.04e-07 |
|    n_updates        | 4468     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 927      |
|    time_elapsed     | 32       |
|    total_timesteps  | 29975    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 4.43e-07 |
|    n_updates        | 4993     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0732   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 5.48e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 869      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32075    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 2.45e-06 |
|    n_updates        | 5518     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 882      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34175    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 1.9e-05  |
|    n_updates        | 6043     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0732   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 6.91e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 837      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36275    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 5.35e-06 |
|    n_updates        | 6568     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 848      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38286    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 5.42e-07 |
|    n_updates        | 7071     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0732   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 4.1e-07  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 812      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40386    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 2.58e-07 |
|    n_updates        | 7596     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 822      |
|    time_elapsed     | 51       |
|    total_timesteps  | 42200    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 8.32e-07 |
|    n_updates        | 8049     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 833      |
|    time_elapsed     | 53       |
|    total_timesteps  | 44300    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 4.94e-05 |
|    n_updates        | 8574     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.09 +/- 0.36
Episode length: 476.40 +/- 145.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | -0.0906  |
| rollout/            |          |
|    exploration_rate | 0.0732   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 4.44e-07 |
|    n_updates        | 8749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 804      |
|    time_elapsed     | 57       |
|    total_timesteps  | 46096    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 1.78e-05 |
|    n_updates        | 9023     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 814      |
|    time_elapsed     | 59       |
|    total_timesteps  | 48196    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 4.25e-06 |
|    n_updates        | 9548     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0732   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 3.45e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 789      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50296    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 3.73e-06 |
|    n_updates        | 10073    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 798      |
|    time_elapsed     | 65       |
|    total_timesteps  | 52396    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 7.06e-05 |
|    n_updates        | 10598    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0981  |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 804      |
|    time_elapsed     | 66       |
|    total_timesteps  | 53736    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 1.87e-05 |
|    n_updates        | 10933    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0732   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 4.62e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0981  |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 782      |
|    time_elapsed     | 71       |
|    total_timesteps  | 55836    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 1.97e-06 |
|    n_updates        | 11458    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 791      |
|    time_elapsed     | 73       |
|    total_timesteps  | 57936    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 7.7e-06  |
|    n_updates        | 11983    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0732   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 5.2e-05  |
|    loss             | 2.78e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:51:49,314] Trial 25 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 5.1990492852279653e-05, 'buffer_size': 73248, 'gamma': 0.981676860816633, 'exploration_fraction': 0.4322217911768886, 'exploration_final_eps': 0.07316664435622072}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 521      |
|    ep_rew_mean      | 0.0417   |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4159     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2084     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 523      |
|    ep_rew_mean      | -0.0842  |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4209     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4184     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.744    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.027   |
|    exploration_rate | 0.702    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1409     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5810     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0727  |
|    exploration_rate | 0.595    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1711     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7910     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.488    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.487    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1268     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10010    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 0.00144  |
|    n_updates        | 2        |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.379    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1266     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12110    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 6.1e-06  |
|    n_updates        | 527      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.272    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1259     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14210    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 2.22e-06 |
|    n_updates        | 1052     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.231    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.21e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.179    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1030     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16026    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 3.3e-06  |
|    n_updates        | 1506     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1042     |
|    time_elapsed     | 17       |
|    total_timesteps  | 17859    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 8.26e-06 |
|    n_updates        | 1964     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0996  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1051     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19959    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.46e-06 |
|    n_updates        | 2489     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 473.40 +/- 154.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0894  |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.73e-06 |
|    n_updates        | 2499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 945      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22059    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 8.33e-07 |
|    n_updates        | 3014     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 958      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24159    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 2.39e-06 |
|    n_updates        | 3539     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 6.94e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 881      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26259    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 7.99e-07 |
|    n_updates        | 4064     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 894      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28359    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 6.17e-06 |
|    n_updates        | 4589     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.36
Episode length: 476.20 +/- 146.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | -0.0905  |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 5.53e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 843      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30042    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.7e-06  |
|    n_updates        | 5010     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 856      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32142    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 2.27e-06 |
|    n_updates        | 5535     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 869      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34242    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 6.62e-07 |
|    n_updates        | 6060     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.10 +/- 0.32
Episode length: 510.00 +/- 45.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 510      |
|    mean_reward      | -0.104   |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.91e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 826      |
|    time_elapsed     | 43       |
|    total_timesteps  | 35866    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 2.37e-06 |
|    n_updates        | 6466     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 838      |
|    time_elapsed     | 45       |
|    total_timesteps  | 37966    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.23e-06 |
|    n_updates        | 6991     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 2.01e-06 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 804      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40066    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.84e-06 |
|    n_updates        | 7516     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 816      |
|    time_elapsed     | 51       |
|    total_timesteps  | 42166    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.2e-05  |
|    n_updates        | 8041     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 825      |
|    time_elapsed     | 53       |
|    total_timesteps  | 43941    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.39e-06 |
|    n_updates        | 8485     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 3.37e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 796      |
|    time_elapsed     | 57       |
|    total_timesteps  | 46041    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 9.6e-06  |
|    n_updates        | 9010     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 803      |
|    time_elapsed     | 58       |
|    total_timesteps  | 47357    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.61e-05 |
|    n_updates        | 9339     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 812      |
|    time_elapsed     | 60       |
|    total_timesteps  | 49457    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 2.8e-06  |
|    n_updates        | 9864     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 7.89e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0954  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 785      |
|    time_elapsed     | 64       |
|    total_timesteps  | 50942    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.62e-05 |
|    n_updates        | 10235    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0954  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 795      |
|    time_elapsed     | 66       |
|    total_timesteps  | 53042    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 7.82e-06 |
|    n_updates        | 10760    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.084   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 800      |
|    time_elapsed     | 67       |
|    total_timesteps  | 54313    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 3.98e-06 |
|    n_updates        | 11078    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.87e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.084   |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 778      |
|    time_elapsed     | 72       |
|    total_timesteps  | 56413    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 3.7e-06  |
|    n_updates        | 11603    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0723  |
|    exploration_rate | 0.0952   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 785      |
|    time_elapsed     | 73       |
|    total_timesteps  | 58084    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.2e-05  |
|    n_updates        | 12020    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 3.48e-05 |
|    loss             | 1.08e-05 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:53:10,680] Trial 26 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 3.481888773510428e-05, 'buffer_size': 88629, 'gamma': 0.9003817421510121, 'exploration_fraction': 0.2942849654460971, 'exploration_final_eps': 0.0951543495431809}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4178     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.858    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4210     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.831    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 455      |
|    ep_rew_mean      | -0.0153  |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1357     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5461     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.064   |
|    exploration_rate | 0.744    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1673     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7561     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 463      |
|    ep_rew_mean      | -0.0353  |
|    exploration_rate | 0.687    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1881     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9264     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.662    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | -0.0202  |
|    exploration_rate | 0.621    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1286     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11215    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 2.04e-06 |
|    n_updates        | 303      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.011   |
|    exploration_rate | 0.551    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1296     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13270    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.42e-06 |
|    n_updates        | 817      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.493    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.88e-06 |
|    n_updates        | 1249     |
----------------------------------
-----------------------------------
| rollout/            |           |
|    ep_len_mean      | 470       |
|    ep_rew_mean      | -0.000353 |
|    exploration_rate | 0.492     |
| time/               |           |
|    episodes         | 32        |
|    fps              | 1049      |
|    time_elapsed     | 14        |
|    total_timesteps  | 15031     |
| train/              |           |
|    learning_rate    | 1.7e-05   |
|    loss             | 1.4e-05   |
|    n_updates        | 1257      |
-----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0236  |
|    exploration_rate | 0.421    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1069     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17131    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 4.22e-06 |
|    n_updates        | 1782     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0423  |
|    exploration_rate | 0.35     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1086     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19231    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.96e-06 |
|    n_updates        | 2307     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.324    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 9.83e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0575  |
|    exploration_rate | 0.279    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 962      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21331    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 7.12e-07 |
|    n_updates        | 2832     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0702  |
|    exploration_rate | 0.208    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 978      |
|    time_elapsed     | 23       |
|    total_timesteps  | 23431    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 6.84e-07 |
|    n_updates        | 3357     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.155    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.84e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.081   |
|    exploration_rate | 0.137    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 896      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25531    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 5.78e-07 |
|    n_updates        | 3882     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0902  |
|    exploration_rate | 0.0656   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 910      |
|    time_elapsed     | 30       |
|    total_timesteps  | 27631    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 9.75e-06 |
|    n_updates        | 4407     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0982  |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 923      |
|    time_elapsed     | 32       |
|    total_timesteps  | 29731    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 8.85e-07 |
|    n_updates        | 4932     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0575   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.03e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0863  |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 861      |
|    time_elapsed     | 36       |
|    total_timesteps  | 31318    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 8.82e-07 |
|    n_updates        | 5329     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0936  |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 874      |
|    time_elapsed     | 38       |
|    total_timesteps  | 33418    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 8.72e-06 |
|    n_updates        | 5854     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.35
Episode length: 487.00 +/- 114.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 487      |
|    mean_reward      | -0.0948  |
| rollout/            |          |
|    exploration_rate | 0.0575   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.03e-06 |
|    n_updates        | 6249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.1     |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 834      |
|    time_elapsed     | 42       |
|    total_timesteps  | 35518    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.35e-06 |
|    n_updates        | 6379     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 846      |
|    time_elapsed     | 44       |
|    total_timesteps  | 37618    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 5.09e-07 |
|    n_updates        | 6904     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0965  |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 855      |
|    time_elapsed     | 45       |
|    total_timesteps  | 39313    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 6.89e-07 |
|    n_updates        | 7328     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0575   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 5.98e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 819      |
|    time_elapsed     | 50       |
|    total_timesteps  | 41413    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 7.72e-07 |
|    n_updates        | 7853     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 830      |
|    time_elapsed     | 52       |
|    total_timesteps  | 43513    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.05e-06 |
|    n_updates        | 8378     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.09 +/- 0.35
Episode length: 481.00 +/- 132.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 481      |
|    mean_reward      | -0.0924  |
| rollout/            |          |
|    exploration_rate | 0.0575   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 7.24e-07 |
|    n_updates        | 8749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 803      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45613    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 8.07e-07 |
|    n_updates        | 8903     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 813      |
|    time_elapsed     | 58       |
|    total_timesteps  | 47698    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 8.88e-07 |
|    n_updates        | 9424     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 822      |
|    time_elapsed     | 60       |
|    total_timesteps  | 49798    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.23e-06 |
|    n_updates        | 9949     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0575   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.9e-06  |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0984  |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 796      |
|    time_elapsed     | 64       |
|    total_timesteps  | 51704    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 2.1e-06  |
|    n_updates        | 10425    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0984  |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 805      |
|    time_elapsed     | 66       |
|    total_timesteps  | 53804    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 8.68e-07 |
|    n_updates        | 10950    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0575   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.06e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 782      |
|    time_elapsed     | 71       |
|    total_timesteps  | 55904    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.41e-06 |
|    n_updates        | 11475    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 790      |
|    time_elapsed     | 72       |
|    total_timesteps  | 57665    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 6.95e-06 |
|    n_updates        | 11916    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0575   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 796      |
|    time_elapsed     | 74       |
|    total_timesteps  | 59361    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.44e-06 |
|    n_updates        | 12340    |
----------------------------------
Eval num_timesteps=60000, episode_reward=0.03 +/- 0.47
Episode length: 432.10 +/- 185.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 432      |
|    mean_reward      | 0.0272   |
| rollout/            |          |
|    exploration_rate | 0.0575   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.7e-05  |
|    loss             | 1.15e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:54:31,098] Trial 27 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 1.6986995458939373e-05, 'buffer_size': 62480, 'gamma': 0.9487652643498673, 'exploration_fraction': 0.4644904298555831, 'exploration_final_eps': 0.057489706803295494}. Best is trial 23 with value: 0.029390000000000017.
New best mean reward!
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4198     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 516      |
|    ep_rew_mean      | -0.0813  |
|    exploration_rate | 0.682    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4188     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4126     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.614    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0255  |
|    exploration_rate | 0.555    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1390     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5766     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0716  |
|    exploration_rate | 0.393    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1693     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7866     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0396  |
|    exploration_rate | 0.268    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1888     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9483     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.09 +/- 0.36
Episode length: 473.40 +/- 154.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.228    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0236  |
|    exploration_rate | 0.119    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1290     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11417    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 4.35e-06 |
|    n_updates        | 354      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0502  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1265     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13517    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.61e-06 |
|    n_updates        | 879      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0831   |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.77e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0702  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1024     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15617    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 8.43e-07 |
|    n_updates        | 1404     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0857  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1038     |
|    time_elapsed     | 17       |
|    total_timesteps  | 17717    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.23e-06 |
|    n_updates        | 1929     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0681  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1047     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19310    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.62e-06 |
|    n_updates        | 2327     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 478.70 +/- 138.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 479      |
|    mean_reward      | -0.0915  |
| rollout/            |          |
|    exploration_rate | 0.0831   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.78e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.081   |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 941      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21410    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 6.39e-07 |
|    n_updates        | 2852     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0917  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 958      |
|    time_elapsed     | 24       |
|    total_timesteps  | 23510    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 3.34e-07 |
|    n_updates        | 3377     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0831   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.68e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0811  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 881      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25545    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.18e-06 |
|    n_updates        | 3886     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0689  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 895      |
|    time_elapsed     | 30       |
|    total_timesteps  | 27143    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 5.71e-07 |
|    n_updates        | 4285     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0585  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 907      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28772    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 3.09e-07 |
|    n_updates        | 4692     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0831   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 4.45e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0679  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 852      |
|    time_elapsed     | 36       |
|    total_timesteps  | 30872    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.6e-07  |
|    n_updates        | 5217     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0441  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 864      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32505    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.09e-06 |
|    n_updates        | 5626     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0533  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 878      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34605    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 5.93e-06 |
|    n_updates        | 6151     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.36
Episode length: 473.90 +/- 153.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0896  |
| rollout/            |          |
|    exploration_rate | 0.0831   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 3.7e-06  |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0616  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 840      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36705    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.24e-06 |
|    n_updates        | 6676     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0561  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 852      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38730    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 7.46e-07 |
|    n_updates        | 7182     |
----------------------------------
Eval num_timesteps=40000, episode_reward=0.03 +/- 0.48
Episode length: 427.00 +/- 196.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 427      |
|    mean_reward      | 0.0292   |
| rollout/            |          |
|    exploration_rate | 0.0831   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.2e-06  |
|    n_updates        | 7499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0505  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 823      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40612    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.44e-06 |
|    n_updates        | 7652     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0578  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 835      |
|    time_elapsed     | 51       |
|    total_timesteps  | 42712    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 7.88e-07 |
|    n_updates        | 8177     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0644  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 846      |
|    time_elapsed     | 52       |
|    total_timesteps  | 44812    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 9.39e-07 |
|    n_updates        | 8702     |
----------------------------------
Eval num_timesteps=45000, episode_reward=0.15 +/- 0.55
Episode length: 372.60 +/- 232.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 373      |
|    mean_reward      | 0.151    |
| rollout/            |          |
|    exploration_rate | 0.0831   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 3.32e-07 |
|    n_updates        | 8749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.058   |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 825      |
|    time_elapsed     | 56       |
|    total_timesteps  | 46418    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 5.33e-07 |
|    n_updates        | 9104     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 475      |
|    ep_rew_mean      | -0.0401  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 831      |
|    time_elapsed     | 57       |
|    total_timesteps  | 47533    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 5.46e-06 |
|    n_updates        | 9383     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 466      |
|    ep_rew_mean      | -0.0166  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 837      |
|    time_elapsed     | 58       |
|    total_timesteps  | 48748    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.14e-06 |
|    n_updates        | 9686     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 475.90 +/- 147.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | -0.0903  |
| rollout/            |          |
|    exploration_rate | 0.0831   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 0.0154   |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 467      |
|    ep_rew_mean      | -0.0269  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 813      |
|    time_elapsed     | 62       |
|    total_timesteps  | 50848    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 4.32e-06 |
|    n_updates        | 10211    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | -0.0387  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 823      |
|    time_elapsed     | 64       |
|    total_timesteps  | 52948    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.91e-06 |
|    n_updates        | 10736    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0831   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 5.25e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | -0.0387  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 799      |
|    time_elapsed     | 68       |
|    total_timesteps  | 55048    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.17e-06 |
|    n_updates        | 11261    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 477      |
|    ep_rew_mean      | -0.0506  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 808      |
|    time_elapsed     | 70       |
|    total_timesteps  | 57148    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.87e-06 |
|    n_updates        | 11786    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0613  |
|    exploration_rate | 0.0831   |
| time/               |          |
|    episodes         | 124      |
|    fps              | 817      |
|    time_elapsed     | 72       |
|    total_timesteps  | 59248    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 3.1e-07  |
|    n_updates        | 12311    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0831   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 6.45e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:55:49,802] Trial 28 finished with value: -0.09087999999999999 and parameters: {'batch_size': 32, 'learning_rate': 1.8638377142219964e-05, 'buffer_size': 11397, 'gamma': 0.9081616346727048, 'exploration_fraction': 0.1980361489439141, 'exploration_final_eps': 0.08305413104571957}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 424      |
|    ep_rew_mean      | 0.0806   |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4124     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1694     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 419      |
|    ep_rew_mean      | 0.0826   |
|    exploration_rate | 0.546    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4170     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3349     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.322    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 454      |
|    ep_rew_mean      | -0.0149  |
|    exploration_rate | 0.261    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1333     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5449     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | -0.0637  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1650     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7549     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.093   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1904     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9649     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1242     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11749    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.36e-06 |
|    n_updates        | 437      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1221     |
|    time_elapsed     | 11       |
|    total_timesteps  | 13849    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 2.03e-06 |
|    n_updates        | 962      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.24e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 995      |
|    time_elapsed     | 16       |
|    total_timesteps  | 15949    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.28e-06 |
|    n_updates        | 1487     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1005     |
|    time_elapsed     | 17       |
|    total_timesteps  | 17715    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 4.8e-06  |
|    n_updates        | 1928     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1014     |
|    time_elapsed     | 19       |
|    total_timesteps  | 19815    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.86e-06 |
|    n_updates        | 2453     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.88e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.105   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 903      |
|    time_elapsed     | 23       |
|    total_timesteps  | 21575    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.61e-06 |
|    n_updates        | 2893     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 921      |
|    time_elapsed     | 25       |
|    total_timesteps  | 23675    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.17e-06 |
|    n_updates        | 3418     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.4e-05  |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0999  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 847      |
|    time_elapsed     | 30       |
|    total_timesteps  | 25486    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.67e-06 |
|    n_updates        | 3871     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 864      |
|    time_elapsed     | 31       |
|    total_timesteps  | 27586    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 2.25e-06 |
|    n_updates        | 4396     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0766  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 873      |
|    time_elapsed     | 33       |
|    total_timesteps  | 28998    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 3.08e-06 |
|    n_updates        | 4749     |
----------------------------------
Eval num_timesteps=30000, episode_reward=0.03 +/- 0.48
Episode length: 428.70 +/- 192.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 429      |
|    mean_reward      | 0.0286   |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 7.32e-07 |
|    n_updates        | 4999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0687  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 832      |
|    time_elapsed     | 37       |
|    total_timesteps  | 30989    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.79e-06 |
|    n_updates        | 5247     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.077   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 846      |
|    time_elapsed     | 39       |
|    total_timesteps  | 33089    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 2.41e-05 |
|    n_updates        | 5772     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 6.85e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0844  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 804      |
|    time_elapsed     | 43       |
|    total_timesteps  | 35189    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 2.82e-05 |
|    n_updates        | 6297     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0759  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 815      |
|    time_elapsed     | 45       |
|    total_timesteps  | 36916    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 2.61e-05 |
|    n_updates        | 6728     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0682  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 825      |
|    time_elapsed     | 46       |
|    total_timesteps  | 38632    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 4.22e-06 |
|    n_updates        | 7157     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.10 +/- 0.32
Episode length: 504.60 +/- 61.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 505      |
|    mean_reward      | -0.102   |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.17e-05 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0749  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 793      |
|    time_elapsed     | 51       |
|    total_timesteps  | 40732    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1e-05    |
|    n_updates        | 7682     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0683  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 803      |
|    time_elapsed     | 52       |
|    total_timesteps  | 42533    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.13e-05 |
|    n_updates        | 8133     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0745  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 815      |
|    time_elapsed     | 54       |
|    total_timesteps  | 44633    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 5.25e-06 |
|    n_updates        | 8658     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.62e-05 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0801  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 788      |
|    time_elapsed     | 59       |
|    total_timesteps  | 46733    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 3.94e-05 |
|    n_updates        | 9183     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0853  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 798      |
|    time_elapsed     | 61       |
|    total_timesteps  | 48833    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.12e-05 |
|    n_updates        | 9708     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 2.88e-05 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0969  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 777      |
|    time_elapsed     | 65       |
|    total_timesteps  | 50933    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 4.58e-06 |
|    n_updates        | 10233    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 787      |
|    time_elapsed     | 67       |
|    total_timesteps  | 53033    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 7.82e-06 |
|    n_updates        | 10758    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 6.19e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 766      |
|    time_elapsed     | 71       |
|    total_timesteps  | 55133    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 3.29e-06 |
|    n_updates        | 11283    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0877  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 774      |
|    time_elapsed     | 73       |
|    total_timesteps  | 56964    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 1.03e-05 |
|    n_updates        | 11740    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0877  |
|    exploration_rate | 0.0914   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 783      |
|    time_elapsed     | 75       |
|    total_timesteps  | 59064    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 4.65e-06 |
|    n_updates        | 12265    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0914   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000117 |
|    loss             | 2.74e-05 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:57:11,927] Trial 29 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 0.00011712776154268359, 'buffer_size': 53610, 'gamma': 0.9670129542120361, 'exploration_fraction': 0.1116433113700373, 'exploration_final_eps': 0.09143586846739027}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4201     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 445      |
|    ep_rew_mean      | 0.0722   |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4197     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3557     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.799    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 431      |
|    ep_rew_mean      | 0.0776   |
|    exploration_rate | 0.792    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1300     |
|    time_elapsed     | 3        |
|    total_timesteps  | 5173     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 455      |
|    ep_rew_mean      | 0.00572  |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1626     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7273     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 448      |
|    ep_rew_mean      | 0.0208   |
|    exploration_rate | 0.64     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1838     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8964     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.598    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 461      |
|    ep_rew_mean      | -0.0177  |
|    exploration_rate | 0.556    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1276     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11064    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.51e-06 |
|    n_updates        | 265      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 470      |
|    ep_rew_mean      | -0.0452  |
|    exploration_rate | 0.471    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1287     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13164    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 3.36e-06 |
|    n_updates        | 790      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | -0.0285  |
|    exploration_rate | 0.406    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1285     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14786    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 5.97e-06 |
|    n_updates        | 1196     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.09 +/- 0.36
Episode length: 478.60 +/- 139.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 479      |
|    mean_reward      | -0.0914  |
| rollout/            |          |
|    exploration_rate | 0.398    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.55e-06 |
|    n_updates        | 1249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | -0.0487  |
|    exploration_rate | 0.322    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1081     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16886    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.75e-06 |
|    n_updates        | 1721     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 475      |
|    ep_rew_mean      | -0.0648  |
|    exploration_rate | 0.237    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1096     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18986    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 6.87e-06 |
|    n_updates        | 2246     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.197    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.12e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.078   |
|    exploration_rate | 0.153    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 967      |
|    time_elapsed     | 21       |
|    total_timesteps  | 21086    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 8.5e-07  |
|    n_updates        | 2771     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.089   |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 983      |
|    time_elapsed     | 23       |
|    total_timesteps  | 23186    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 3.85e-07 |
|    n_updates        | 3296     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0739   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 3.34e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0983  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 898      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25286    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 8.7e-07  |
|    n_updates        | 3821     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0676  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 911      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26967    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 4.76e-07 |
|    n_updates        | 4241     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0578  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 922      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28669    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 7.51e-07 |
|    n_updates        | 4667     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.36
Episode length: 474.00 +/- 153.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0896  |
| rollout/            |          |
|    exploration_rate | 0.0739   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 2.75e-07 |
|    n_updates        | 4999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0673  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 870      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30769    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 5.1e-07  |
|    n_updates        | 5192     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0757  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 884      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32869    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 4.55e-07 |
|    n_updates        | 5717     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0831  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 897      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34969    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 7.71e-06 |
|    n_updates        | 6242     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0739   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.21e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0898  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 852      |
|    time_elapsed     | 43       |
|    total_timesteps  | 37069    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 2.79e-06 |
|    n_updates        | 6767     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0813  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 861      |
|    time_elapsed     | 44       |
|    total_timesteps  | 38763    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.96e-06 |
|    n_updates        | 7190     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.0739   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 8.5e-07  |
|    n_updates        | 7499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0874  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 830      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40863    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 5.5e-07  |
|    n_updates        | 7715     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0815  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 840      |
|    time_elapsed     | 51       |
|    total_timesteps  | 42944    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.42e-06 |
|    n_updates        | 8235     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.09 +/- 0.36
Episode length: 475.90 +/- 147.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | -0.0903  |
| rollout/            |          |
|    exploration_rate | 0.0739   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.99e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0871  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 813      |
|    time_elapsed     | 55       |
|    total_timesteps  | 45044    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 2.79e-06 |
|    n_updates        | 8760     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0806  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 822      |
|    time_elapsed     | 56       |
|    total_timesteps  | 46838    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 9.89e-07 |
|    n_updates        | 9209     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0748  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 831      |
|    time_elapsed     | 58       |
|    total_timesteps  | 48706    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.32e-06 |
|    n_updates        | 9676     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 474.70 +/- 150.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 475      |
|    mean_reward      | -0.0899  |
| rollout/            |          |
|    exploration_rate | 0.0739   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.24e-06 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0748  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 806      |
|    time_elapsed     | 62       |
|    total_timesteps  | 50806    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 4.66e-07 |
|    n_updates        | 10201    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0746  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 813      |
|    time_elapsed     | 64       |
|    total_timesteps  | 52202    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.74e-06 |
|    n_updates        | 10550    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0865  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 822      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54302    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 4.69e-07 |
|    n_updates        | 11075    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.11 +/- 0.31
Episode length: 517.90 +/- 21.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 518      |
|    mean_reward      | -0.107   |
| rollout/            |          |
|    exploration_rate | 0.0739   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 4.57e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0633  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 796      |
|    time_elapsed     | 69       |
|    total_timesteps  | 55598    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.16e-06 |
|    n_updates        | 11399    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0749  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 805      |
|    time_elapsed     | 71       |
|    total_timesteps  | 57698    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.17e-05 |
|    n_updates        | 11924    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0534  |
|    exploration_rate | 0.0739   |
| time/               |          |
|    episodes         | 124      |
|    fps              | 812      |
|    time_elapsed     | 73       |
|    total_timesteps  | 59416    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 6.38e-06 |
|    n_updates        | 12353    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0739   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 4.03e-05 |
|    loss             | 1.39e-05 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:58:31,245] Trial 30 finished with value: -0.10062999999999997 and parameters: {'batch_size': 32, 'learning_rate': 4.0299845696827896e-05, 'buffer_size': 35807, 'gamma': 0.9155733965408471, 'exploration_fraction': 0.38429537170718703, 'exploration_final_eps': 0.07386827557159521}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.859    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4221     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.717    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4244     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.664    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 468      |
|    ep_rew_mean      | -0.0203  |
|    exploration_rate | 0.622    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1384     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5611     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0678  |
|    exploration_rate | 0.481    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1694     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7711     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0962  |
|    exploration_rate | 0.34     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1945     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9811     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.327    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.198    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1272     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11911    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.19e-06 |
|    n_updates        | 477      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1248     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14011    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.6e-06  |
|    n_updates        | 1002     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.04e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1021     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16111    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 5.93e-07 |
|    n_updates        | 1527     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.147   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1034     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18211    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.12e-06 |
|    n_updates        | 2052     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 8.62e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 918      |
|    time_elapsed     | 22       |
|    total_timesteps  | 20311    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 9.01e-06 |
|    n_updates        | 2577     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.158   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 936      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22411    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.89e-07 |
|    n_updates        | 3102     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 952      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24511    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 5.2e-07  |
|    n_updates        | 3627     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.28e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 879      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26611    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 4.51e-07 |
|    n_updates        | 4152     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.169   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 896      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28711    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.52e-07 |
|    n_updates        | 4677     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.72e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.172   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 843      |
|    time_elapsed     | 36       |
|    total_timesteps  | 30811    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 3.35e-07 |
|    n_updates        | 5202     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.174   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 858      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32911    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 4.86e-07 |
|    n_updates        | 5727     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.11e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 515      |
|    ep_rew_mean      | -0.177   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 816      |
|    time_elapsed     | 42       |
|    total_timesteps  | 35011    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 4.02e-07 |
|    n_updates        | 6252     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 515      |
|    ep_rew_mean      | -0.178   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 831      |
|    time_elapsed     | 44       |
|    total_timesteps  | 37111    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.96e-07 |
|    n_updates        | 6777     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 516      |
|    ep_rew_mean      | -0.18    |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 844      |
|    time_elapsed     | 46       |
|    total_timesteps  | 39211    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.55e-07 |
|    n_updates        | 7302     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.02e-05 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 808      |
|    time_elapsed     | 50       |
|    total_timesteps  | 40935    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 3.47e-07 |
|    n_updates        | 7733     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.169   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 821      |
|    time_elapsed     | 52       |
|    total_timesteps  | 43035    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.82e-07 |
|    n_updates        | 8258     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.09 +/- 0.36
Episode length: 473.20 +/- 155.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.29e-07 |
|    n_updates        | 8749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.171   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 797      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45135    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 4.62e-07 |
|    n_updates        | 8783     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 513      |
|    ep_rew_mean      | -0.173   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 807      |
|    time_elapsed     | 58       |
|    total_timesteps  | 47235    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 5.71e-07 |
|    n_updates        | 9308     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.174   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 819      |
|    time_elapsed     | 60       |
|    total_timesteps  | 49335    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 4.43e-07 |
|    n_updates        | 9833     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 3.69e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 794      |
|    time_elapsed     | 64       |
|    total_timesteps  | 51435    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.84e-07 |
|    n_updates        | 10358    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 805      |
|    time_elapsed     | 66       |
|    total_timesteps  | 53535    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.24e-07 |
|    n_updates        | 10883    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 2.75e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 783      |
|    time_elapsed     | 70       |
|    total_timesteps  | 55635    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.1e-07  |
|    n_updates        | 11408    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 517      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 791      |
|    time_elapsed     | 72       |
|    total_timesteps  | 57294    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 1.27e-07 |
|    n_updates        | 11823    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 517      |
|    ep_rew_mean      | -0.187   |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 800      |
|    time_elapsed     | 74       |
|    total_timesteps  | 59394    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 3.06e-07 |
|    n_updates        | 12348    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0872   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.86e-05 |
|    loss             | 4.02e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 04:59:51,756] Trial 31 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 1.8608753433413076e-05, 'buffer_size': 19224, 'gamma': 0.9062729023080636, 'exploration_fraction': 0.22603084756182654, 'exploration_final_eps': 0.08720107096331947}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.821    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4099     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.643    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4164     |
|    time_elapsed     | 1        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.575    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.464    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1492     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6300     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.285    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1778     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8400     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.149    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.107    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1272     |
|    time_elapsed     | 8        |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 2.46e-06 |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.161   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1253     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12188    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 6.46e-07 |
|    n_updates        | 546      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.168   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1236     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14288    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 6.06e-07 |
|    n_updates        | 1071     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0827   |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 1.08e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1017     |
|    time_elapsed     | 16       |
|    total_timesteps  | 16387    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 9.36e-07 |
|    n_updates        | 1596     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1031     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18487    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 7.82e-07 |
|    n_updates        | 2121     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0827   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 7.94e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 912      |
|    time_elapsed     | 22       |
|    total_timesteps  | 20223    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 3.52e-07 |
|    n_updates        | 2555     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 931      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22323    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 3.5e-07  |
|    n_updates        | 3080     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 948      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24423    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 2.42e-07 |
|    n_updates        | 3605     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0827   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 3.79e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 876      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26523    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 3.8e-07  |
|    n_updates        | 4130     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 889      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28277    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 7.46e-07 |
|    n_updates        | 4569     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0827   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 3.57e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 835      |
|    time_elapsed     | 36       |
|    total_timesteps  | 30377    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 4.3e-07  |
|    n_updates        | 5094     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 851      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32426    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 4.86e-07 |
|    n_updates        | 5606     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 865      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34526    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 4.84e-07 |
|    n_updates        | 6131     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0827   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 2.78e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 824      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36626    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 2.26e-07 |
|    n_updates        | 6656     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 837      |
|    time_elapsed     | 46       |
|    total_timesteps  | 38726    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 4.99e-07 |
|    n_updates        | 7181     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0827   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 4.95e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 803      |
|    time_elapsed     | 50       |
|    total_timesteps  | 40826    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 9.28e-07 |
|    n_updates        | 7706     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 814      |
|    time_elapsed     | 52       |
|    total_timesteps  | 42690    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 9.34e-07 |
|    n_updates        | 8172     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 824      |
|    time_elapsed     | 53       |
|    total_timesteps  | 44450    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 4.32e-07 |
|    n_updates        | 8612     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.11 +/- 0.30
Episode length: 522.40 +/- 7.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 522      |
|    mean_reward      | -0.109   |
| rollout/            |          |
|    exploration_rate | 0.0827   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 3.71e-07 |
|    n_updates        | 8749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.126   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 796      |
|    time_elapsed     | 58       |
|    total_timesteps  | 46550    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 1.04e-06 |
|    n_updates        | 9137     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 807      |
|    time_elapsed     | 60       |
|    total_timesteps  | 48650    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 6.97e-07 |
|    n_updates        | 9662     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0827   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 4.89e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 782      |
|    time_elapsed     | 64       |
|    total_timesteps  | 50462    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 7.66e-07 |
|    n_updates        | 10115    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 792      |
|    time_elapsed     | 66       |
|    total_timesteps  | 52405    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 1.91e-06 |
|    n_updates        | 10601    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 802      |
|    time_elapsed     | 67       |
|    total_timesteps  | 54505    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 1.18e-06 |
|    n_updates        | 11126    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.10 +/- 0.34
Episode length: 493.40 +/- 94.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 493      |
|    mean_reward      | -0.0973  |
| rollout/            |          |
|    exploration_rate | 0.0827   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 1.66e-06 |
|    n_updates        | 11249    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 783      |
|    time_elapsed     | 72       |
|    total_timesteps  | 56605    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 4.97e-07 |
|    n_updates        | 11651    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.111   |
|    exploration_rate | 0.0827   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 792      |
|    time_elapsed     | 74       |
|    total_timesteps  | 58705    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 1.32e-06 |
|    n_updates        | 12176    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.36
Episode length: 477.50 +/- 142.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 478      |
|    mean_reward      | -0.091   |
| rollout/            |          |
|    exploration_rate | 0.0827   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.25e-05 |
|    loss             | 1.33e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:01:12,503] Trial 32 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 1.2457421532478005e-05, 'buffer_size': 12617, 'gamma': 0.9098601514578118, 'exploration_fraction': 0.179703232693328, 'exploration_final_eps': 0.08274960060990749}. Best is trial 23 with value: 0.029390000000000017.
New best mean reward!
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 446      |
|    ep_rew_mean      | 0.0716   |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4194     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1784     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0692  |
|    exploration_rate | 0.714    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4217     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3884     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.632    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.559    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1446     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5984     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.405    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1747     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8084     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.264    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1277     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10184    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.84e-05 |
|    n_updates        | 45       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1262     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11786    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.25e-06 |
|    n_updates        | 446      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1246     |
|    time_elapsed     | 11       |
|    total_timesteps  | 13886    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.21e-06 |
|    n_updates        | 971      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0679   |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.13e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1016     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15986    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 6.54e-07 |
|    n_updates        | 1496     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1022     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18086    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.5e-06  |
|    n_updates        | 2021     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0679   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 8.01e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 902      |
|    time_elapsed     | 22       |
|    total_timesteps  | 20186    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 3.03e-07 |
|    n_updates        | 2546     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 918      |
|    time_elapsed     | 24       |
|    total_timesteps  | 22085    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.96e-07 |
|    n_updates        | 3021     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 936      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24185    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 9.32e-07 |
|    n_updates        | 3546     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0679   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.04e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 866      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26285    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 2.05e-07 |
|    n_updates        | 4071     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 883      |
|    time_elapsed     | 32       |
|    total_timesteps  | 28385    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 3.09e-07 |
|    n_updates        | 4596     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0679   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 3.06e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 832      |
|    time_elapsed     | 36       |
|    total_timesteps  | 30485    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.75e-07 |
|    n_updates        | 5121     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 845      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32324    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 2.34e-07 |
|    n_updates        | 5580     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 859      |
|    time_elapsed     | 40       |
|    total_timesteps  | 34424    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 3.17e-07 |
|    n_updates        | 6105     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0679   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 2.73e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.147   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 818      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36524    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 3.14e-07 |
|    n_updates        | 6630     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.151   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 832      |
|    time_elapsed     | 46       |
|    total_timesteps  | 38624    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.77e-05 |
|    n_updates        | 7155     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0679   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.4e-07  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 795      |
|    time_elapsed     | 50       |
|    total_timesteps  | 40217    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.19e-07 |
|    n_updates        | 7554     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 807      |
|    time_elapsed     | 52       |
|    total_timesteps  | 42317    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.18e-07 |
|    n_updates        | 8079     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 819      |
|    time_elapsed     | 54       |
|    total_timesteps  | 44417    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.39e-05 |
|    n_updates        | 8604     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0679   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.43e-07 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.148   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 791      |
|    time_elapsed     | 58       |
|    total_timesteps  | 46517    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.3e-07  |
|    n_updates        | 9129     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 803      |
|    time_elapsed     | 60       |
|    total_timesteps  | 48617    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 3.11e-07 |
|    n_updates        | 9654     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0679   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 3.47e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 779      |
|    time_elapsed     | 65       |
|    total_timesteps  | 50717    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 7.09e-07 |
|    n_updates        | 10179    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 790      |
|    time_elapsed     | 66       |
|    total_timesteps  | 52817    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 5.78e-07 |
|    n_updates        | 10704    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 799      |
|    time_elapsed     | 68       |
|    total_timesteps  | 54917    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 4.31e-07 |
|    n_updates        | 11229    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0679   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 3.85e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 778      |
|    time_elapsed     | 73       |
|    total_timesteps  | 57017    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 1.54e-07 |
|    n_updates        | 11754    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.0679   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 787      |
|    time_elapsed     | 75       |
|    total_timesteps  | 59117    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 6.11e-07 |
|    n_updates        | 12279    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0679   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 7.33e-05 |
|    loss             | 5.23e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:02:34,153] Trial 33 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 7.326368166079847e-05, 'buffer_size': 22048, 'gamma': 0.9166637204057345, 'exploration_fraction': 0.21098512816839787, 'exploration_final_eps': 0.06787073627573256}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 423      |
|    ep_rew_mean      | 0.331    |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4151     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1691     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | 0.0605   |
|    exploration_rate | 0.817    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4184     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3791     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.759    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0297  |
|    exploration_rate | 0.715    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1417     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5891     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0748  |
|    exploration_rate | 0.614    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1716     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7991     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | 0.00545  |
|    exploration_rate | 0.53     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1920     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9729     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.517    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0305  |
|    exploration_rate | 0.429    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1279     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11829    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 3.05e-06 |
|    n_updates        | 457      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0196  |
|    exploration_rate | 0.33     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1278     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13873    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 9.8e-07  |
|    n_updates        | 968      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.275    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 1.64e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0434  |
|    exploration_rate | 0.228    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1044     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15973    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 1.64e-06 |
|    n_updates        | 1493     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.0619  |
|    exploration_rate | 0.127    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1060     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18073    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 8.73e-07 |
|    n_updates        | 2018     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0203  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1066     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19534    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 6.32e-07 |
|    n_updates        | 2383     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0828   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 7.2e-06  |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0376  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 946      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21634    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 1.05e-06 |
|    n_updates        | 2908     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0519  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 961      |
|    time_elapsed     | 24       |
|    total_timesteps  | 23734    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 3.81e-07 |
|    n_updates        | 3433     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0828   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 2.04e-06 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0641  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 883      |
|    time_elapsed     | 29       |
|    total_timesteps  | 25834    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 7.2e-07  |
|    n_updates        | 3958     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0745  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 899      |
|    time_elapsed     | 31       |
|    total_timesteps  | 27934    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 3.42e-07 |
|    n_updates        | 4483     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0828   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 1.6e-06  |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.0835  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 842      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30034    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 1.11e-06 |
|    n_updates        | 5008     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.0741  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 855      |
|    time_elapsed     | 37       |
|    total_timesteps  | 31859    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 8.07e-07 |
|    n_updates        | 5464     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0671  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 866      |
|    time_elapsed     | 39       |
|    total_timesteps  | 33914    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 1.34e-05 |
|    n_updates        | 5978     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0828   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 1.86e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.0751  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 825      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36014    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 7.21e-07 |
|    n_updates        | 6503     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0665  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 836      |
|    time_elapsed     | 44       |
|    total_timesteps  | 37636    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 5.79e-07 |
|    n_updates        | 6908     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0737  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 849      |
|    time_elapsed     | 46       |
|    total_timesteps  | 39736    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 3.36e-07 |
|    n_updates        | 7433     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0828   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 3.94e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.0802  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 815      |
|    time_elapsed     | 51       |
|    total_timesteps  | 41836    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 6.09e-07 |
|    n_updates        | 7958     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.0861  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 827      |
|    time_elapsed     | 53       |
|    total_timesteps  | 43936    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 3.94e-07 |
|    n_updates        | 8483     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0828   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 6.12e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.0914  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 798      |
|    time_elapsed     | 57       |
|    total_timesteps  | 46036    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 3.62e-07 |
|    n_updates        | 9008     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.0964  |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 810      |
|    time_elapsed     | 59       |
|    total_timesteps  | 48136    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 2.74e-06 |
|    n_updates        | 9533     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 473.70 +/- 153.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0895  |
| rollout/            |          |
|    exploration_rate | 0.0828   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 2.92e-07 |
|    n_updates        | 9999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.101   |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 789      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50236    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 1.44e-06 |
|    n_updates        | 10058    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 799      |
|    time_elapsed     | 65       |
|    total_timesteps  | 52336    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 5.42e-07 |
|    n_updates        | 10583    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 808      |
|    time_elapsed     | 67       |
|    total_timesteps  | 54436    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 4.15e-07 |
|    n_updates        | 11108    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0828   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 2.79e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 787      |
|    time_elapsed     | 71       |
|    total_timesteps  | 56536    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 3.93e-07 |
|    n_updates        | 11633    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.0828   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 795      |
|    time_elapsed     | 73       |
|    total_timesteps  | 58570    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 4.15e-07 |
|    n_updates        | 12142    |
----------------------------------
Eval num_timesteps=60000, episode_reward=0.01 +/- 0.44
Episode length: 471.10 +/- 134.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 471      |
|    mean_reward      | 0.0116   |
| rollout/            |          |
|    exploration_rate | 0.0828   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.83e-05 |
|    loss             | 2.32e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:03:54,542] Trial 34 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 1.830104139171287e-05, 'buffer_size': 33860, 'gamma': 0.9192143510136883, 'exploration_fraction': 0.31645953962164725, 'exploration_final_eps': 0.08276237001428606}. Best is trial 23 with value: 0.029390000000000017.
New best mean reward!
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4143     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 464      |
|    ep_rew_mean      | -0.0607  |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4159     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3714     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.797    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.0249  |
|    exploration_rate | 0.767    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1390     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5747     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0712  |
|    exploration_rate | 0.682    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1694     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7847     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0989  |
|    exploration_rate | 0.597    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1940     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9947     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.595    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.512    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1284     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12047    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.25e-06 |
|    n_updates        | 511      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.427    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1287     |
|    time_elapsed     | 10       |
|    total_timesteps  | 14147    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 4.32e-07 |
|    n_updates        | 1036     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.392    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 6.84e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.362    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1055     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15736    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 5.31e-07 |
|    n_updates        | 1433     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.277    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1073     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17836    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 9.39e-07 |
|    n_updates        | 1958     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.192    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1086     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19936    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 7.22e-07 |
|    n_updates        | 2483     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.19     |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 2.53e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.118    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 961      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21778    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 4.91e-07 |
|    n_updates        | 2944     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 976      |
|    time_elapsed     | 24       |
|    total_timesteps  | 23878    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 2.44e-06 |
|    n_updates        | 3469     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.099    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 2.66e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 896      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25978    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.03e-06 |
|    n_updates        | 3994     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 907      |
|    time_elapsed     | 30       |
|    total_timesteps  | 27566    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.02e-06 |
|    n_updates        | 4391     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 60       |
|    fps              | 922      |
|    time_elapsed     | 32       |
|    total_timesteps  | 29666    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.44e-06 |
|    n_updates        | 4916     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.099    |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.04e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 64       |
|    fps              | 863      |
|    time_elapsed     | 36       |
|    total_timesteps  | 31440    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.03e-07 |
|    n_updates        | 5359     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 68       |
|    fps              | 877      |
|    time_elapsed     | 38       |
|    total_timesteps  | 33540    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 2.29e-08 |
|    n_updates        | 5884     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.099    |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 3.83e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.115   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 72       |
|    fps              | 833      |
|    time_elapsed     | 42       |
|    total_timesteps  | 35640    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 2.22e-05 |
|    n_updates        | 6409     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 76       |
|    fps              | 846      |
|    time_elapsed     | 44       |
|    total_timesteps  | 37740    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.12e-07 |
|    n_updates        | 6934     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 80       |
|    fps              | 855      |
|    time_elapsed     | 46       |
|    total_timesteps  | 39404    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 4.36e-08 |
|    n_updates        | 7350     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.099    |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 3.09e-09 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 84       |
|    fps              | 819      |
|    time_elapsed     | 50       |
|    total_timesteps  | 41504    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.6e-07  |
|    n_updates        | 7875     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 88       |
|    fps              | 831      |
|    time_elapsed     | 52       |
|    total_timesteps  | 43604    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 2.34e-08 |
|    n_updates        | 8400     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.099    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 2.36e-09 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 92       |
|    fps              | 799      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45188    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 7.89e-09 |
|    n_updates        | 8796     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.114   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 96       |
|    fps              | 810      |
|    time_elapsed     | 58       |
|    total_timesteps  | 47288    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.23e-07 |
|    n_updates        | 9321     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 100      |
|    fps              | 820      |
|    time_elapsed     | 60       |
|    total_timesteps  | 49388    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.76e-05 |
|    n_updates        | 9846     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.099    |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 2.74e-08 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 104      |
|    fps              | 795      |
|    time_elapsed     | 64       |
|    total_timesteps  | 51488    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.93e-05 |
|    n_updates        | 10371    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 108      |
|    fps              | 803      |
|    time_elapsed     | 66       |
|    total_timesteps  | 53217    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.61e-07 |
|    n_updates        | 10804    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.099    |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 2.85e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 112      |
|    fps              | 780      |
|    time_elapsed     | 70       |
|    total_timesteps  | 55317    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 5.29e-07 |
|    n_updates        | 11329    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 116      |
|    fps              | 790      |
|    time_elapsed     | 72       |
|    total_timesteps  | 57417    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 3.66e-09 |
|    n_updates        | 11854    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 120      |
|    fps              | 799      |
|    time_elapsed     | 74       |
|    total_timesteps  | 59517    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.59e-05 |
|    n_updates        | 12379    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.099    |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.00023  |
|    loss             | 1.12e-08 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:05:15,222] Trial 35 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.00023008311603456188, 'buffer_size': 49121, 'gamma': 0.9286261125598325, 'exploration_fraction': 0.37063021147234226, 'exploration_final_eps': 0.09897768114060936}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 419      |
|    ep_rew_mean      | 0.0824   |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4193     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1676     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | -0.0638  |
|    exploration_rate | 0.781    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4215     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3776     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.09 +/- 0.36
Episode length: 474.30 +/- 152.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0897  |
| rollout/            |          |
|    exploration_rate | 0.71     |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.659    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1534     |
|    time_elapsed     | 3        |
|    total_timesteps  | 5876     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 461      |
|    ep_rew_mean      | 0.00318  |
|    exploration_rate | 0.572    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1760     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7373     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0395  |
|    exploration_rate | 0.45     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 2022     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9473     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.419    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0228  |
|    exploration_rate | 0.34     |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1317     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11366    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 2.35e-06 |
|    n_updates        | 341      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0495  |
|    exploration_rate | 0.218    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1301     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13466    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 2.15e-06 |
|    n_updates        | 866      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.09 +/- 0.36
Episode length: 474.50 +/- 151.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0898  |
| rollout/            |          |
|    exploration_rate | 0.128    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 5.03e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0696  |
|    exploration_rate | 0.0955   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1070     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15566    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 1.44e-06 |
|    n_updates        | 1391     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0852  |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1079     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17666    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 9.63e-07 |
|    n_updates        | 1916     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0977  |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1088     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19766    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 6.13e-06 |
|    n_updates        | 2441     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0754   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 6.44e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 965      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21866    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 1.87e-07 |
|    n_updates        | 2966     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 980      |
|    time_elapsed     | 24       |
|    total_timesteps  | 23966    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 1.77e-07 |
|    n_updates        | 3491     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0754   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 3.09e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 899      |
|    time_elapsed     | 28       |
|    total_timesteps  | 26066    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 2.32e-06 |
|    n_updates        | 4016     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 914      |
|    time_elapsed     | 30       |
|    total_timesteps  | 28166    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 2.29e-06 |
|    n_updates        | 4541     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0754   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 2.94e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 857      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30266    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 3.6e-07  |
|    n_updates        | 5066     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 872      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32366    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 1.84e-07 |
|    n_updates        | 5591     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 886      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34466    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 2.1e-06  |
|    n_updates        | 6116     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0754   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 9.4e-07  |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 843      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36566    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 4.66e-06 |
|    n_updates        | 6641     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 857      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38666    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 1.52e-07 |
|    n_updates        | 7166     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0754   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 1.36e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 820      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40730    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 3.87e-07 |
|    n_updates        | 7682     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 832      |
|    time_elapsed     | 51       |
|    total_timesteps  | 42830    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 4.22e-05 |
|    n_updates        | 8207     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 844      |
|    time_elapsed     | 53       |
|    total_timesteps  | 44930    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 1.13e-07 |
|    n_updates        | 8732     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0754   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 2.09e-07 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 814      |
|    time_elapsed     | 57       |
|    total_timesteps  | 46956    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 2.5e-07  |
|    n_updates        | 9238     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 824      |
|    time_elapsed     | 59       |
|    total_timesteps  | 49056    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 7.66e-07 |
|    n_updates        | 9763     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0754   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 2.75e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.123   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 796      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50654    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 7.94e-08 |
|    n_updates        | 10163    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 806      |
|    time_elapsed     | 65       |
|    total_timesteps  | 52754    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 5.83e-05 |
|    n_updates        | 10688    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 816      |
|    time_elapsed     | 67       |
|    total_timesteps  | 54854    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 7.62e-06 |
|    n_updates        | 11213    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0754   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 3.14e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 794      |
|    time_elapsed     | 71       |
|    total_timesteps  | 56954    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 6.26e-07 |
|    n_updates        | 11738    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.0754   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 800      |
|    time_elapsed     | 73       |
|    total_timesteps  | 58545    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 5.65e-07 |
|    n_updates        | 12136    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0754   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 9.2e-05  |
|    loss             | 1.14e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:06:35,475] Trial 36 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 9.197150765417056e-05, 'buffer_size': 23756, 'gamma': 0.9080669791331133, 'exploration_fraction': 0.2652032750228413, 'exploration_final_eps': 0.07536580315805749}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4186     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4205     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.79     |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.735    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1480     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6300     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.647    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1768     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8400     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.58     |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.576    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1272     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10091    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 0.000262 |
|    n_updates        | 22       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0711  |
|    exploration_rate | 0.506    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1279     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11768    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 1.33e-06 |
|    n_updates        | 441      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0496  |
|    exploration_rate | 0.434    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1277     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13471    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 1.23e-06 |
|    n_updates        | 867      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.37     |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 1.8e-06  |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0696  |
|    exploration_rate | 0.346    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1040     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15571    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 1.32e-05 |
|    n_updates        | 1392     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0852  |
|    exploration_rate | 0.258    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1056     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17671    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 1.74e-06 |
|    n_updates        | 1917     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0977  |
|    exploration_rate | 0.169    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1066     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19771    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 1.04e-05 |
|    n_updates        | 2442     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.16     |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 6.79e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 945      |
|    time_elapsed     | 23       |
|    total_timesteps  | 21871    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 4.65e-06 |
|    n_updates        | 2967     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 959      |
|    time_elapsed     | 24       |
|    total_timesteps  | 23971    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 4.72e-06 |
|    n_updates        | 3492     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0919   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 1.03e-05 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 881      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26071    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 5.41e-07 |
|    n_updates        | 4017     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 895      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28171    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 3.43e-06 |
|    n_updates        | 4542     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0919   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 7.56e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 838      |
|    time_elapsed     | 36       |
|    total_timesteps  | 30271    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 3.65e-07 |
|    n_updates        | 5067     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 853      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32371    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 3e-07    |
|    n_updates        | 5592     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 865      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34471    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 7.66e-06 |
|    n_updates        | 6117     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0919   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 2.97e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 821      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36143    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 8.58e-07 |
|    n_updates        | 6535     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 834      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38243    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 4.69e-07 |
|    n_updates        | 7060     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0919   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 5.75e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 800      |
|    time_elapsed     | 50       |
|    total_timesteps  | 40343    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 4.15e-07 |
|    n_updates        | 7585     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 809      |
|    time_elapsed     | 52       |
|    total_timesteps  | 42111    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 0.00765  |
|    n_updates        | 8027     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 820      |
|    time_elapsed     | 53       |
|    total_timesteps  | 44211    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 4.6e-07  |
|    n_updates        | 8552     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0919   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 4.91e-07 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 791      |
|    time_elapsed     | 58       |
|    total_timesteps  | 46311    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 2.34e-07 |
|    n_updates        | 9077     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 801      |
|    time_elapsed     | 60       |
|    total_timesteps  | 48411    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 3.16e-07 |
|    n_updates        | 9602     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0919   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 5.12e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 778      |
|    time_elapsed     | 64       |
|    total_timesteps  | 50511    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 4.48e-07 |
|    n_updates        | 10127    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 788      |
|    time_elapsed     | 66       |
|    total_timesteps  | 52611    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 2.47e-07 |
|    n_updates        | 10652    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 797      |
|    time_elapsed     | 68       |
|    total_timesteps  | 54711    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 2.65e-07 |
|    n_updates        | 11177    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0919   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 3.01e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 776      |
|    time_elapsed     | 73       |
|    total_timesteps  | 56811    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 2.32e-07 |
|    n_updates        | 11702    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.0919   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 785      |
|    time_elapsed     | 75       |
|    total_timesteps  | 58911    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 4e-07    |
|    n_updates        | 12227    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0919   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 2.42e-05 |
|    loss             | 6.67e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:07:57,260] Trial 37 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 2.423573770829691e-05, 'buffer_size': 79303, 'gamma': 0.9144986479566727, 'exploration_fraction': 0.36029963595552167, 'exploration_final_eps': 0.09188658226074309}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | 0.0608   |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4226     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1892     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 446      |
|    ep_rew_mean      | 0.0714   |
|    exploration_rate | 0.883    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4233     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3572     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.837    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.0224  |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1398     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5672     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0693  |
|    exploration_rate | 0.746    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1705     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7772     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0462  |
|    exploration_rate | 0.68     |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1945     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9808     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.674    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0735  |
|    exploration_rate | 0.611    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1299     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11908    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 5.44e-06 |
|    n_updates        | 476      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.0106  |
|    exploration_rate | 0.568    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1305     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13246    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 7.25e-06 |
|    n_updates        | 811      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.511    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 5.02e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0356  |
|    exploration_rate | 0.499    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1066     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15346    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.8e-05  |
|    n_updates        | 1336     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0549  |
|    exploration_rate | 0.431    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1089     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17446    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 2.03e-06 |
|    n_updates        | 1861     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0441  |
|    exploration_rate | 0.367    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1103     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19410    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 3.18e-06 |
|    n_updates        | 2352     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.347    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 4.48e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0335  |
|    exploration_rate | 0.309    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 974      |
|    time_elapsed     | 21       |
|    total_timesteps  | 21184    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.62e-06 |
|    n_updates        | 2795     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 468      |
|    ep_rew_mean      | 0.000327 |
|    exploration_rate | 0.267    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 984      |
|    time_elapsed     | 22       |
|    total_timesteps  | 22463    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.14e-06 |
|    n_updates        | 3115     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 455      |
|    ep_rew_mean      | 0.0294   |
|    exploration_rate | 0.227    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 993      |
|    time_elapsed     | 23       |
|    total_timesteps  | 23675    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 5.9e-06  |
|    n_updates        | 3418     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.36
Episode length: 474.00 +/- 153.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0896  |
| rollout/            |          |
|    exploration_rate | 0.184    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 4.74e-07 |
|    n_updates        | 3749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 457      |
|    ep_rew_mean      | 0.0313   |
|    exploration_rate | 0.164    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 916      |
|    time_elapsed     | 27       |
|    total_timesteps  | 25617    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 6.64e-07 |
|    n_updates        | 3904     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 456      |
|    ep_rew_mean      | 0.0342   |
|    exploration_rate | 0.107    |
| time/               |          |
|    episodes         | 60       |
|    fps              | 927      |
|    time_elapsed     | 29       |
|    total_timesteps  | 27373    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.7e-06  |
|    n_updates        | 4343     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 461      |
|    ep_rew_mean      | 0.0189   |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 938      |
|    time_elapsed     | 31       |
|    total_timesteps  | 29473    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 6.56e-07 |
|    n_updates        | 4868     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0672   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 7.71e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 464      |
|    ep_rew_mean      | 0.00548  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 876      |
|    time_elapsed     | 36       |
|    total_timesteps  | 31573    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.45e-06 |
|    n_updates        | 5393     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | 0.00954  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 886      |
|    time_elapsed     | 37       |
|    total_timesteps  | 33288    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.28e-05 |
|    n_updates        | 5821     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0672   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.38e-05 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 466      |
|    ep_rew_mean      | -0.00202 |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 840      |
|    time_elapsed     | 42       |
|    total_timesteps  | 35388    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.59e-06 |
|    n_updates        | 6346     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | -0.0124  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 852      |
|    time_elapsed     | 43       |
|    total_timesteps  | 37488    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.62e-06 |
|    n_updates        | 6871     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 471      |
|    ep_rew_mean      | -0.0218  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 864      |
|    time_elapsed     | 45       |
|    total_timesteps  | 39588    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 9.34e-07 |
|    n_updates        | 7396     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0672   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.55e-06 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0304  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 828      |
|    time_elapsed     | 50       |
|    total_timesteps  | 41688    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 7.08e-07 |
|    n_updates        | 7921     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0382  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 839      |
|    time_elapsed     | 52       |
|    total_timesteps  | 43788    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 5.62e-06 |
|    n_updates        | 8446     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0672   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 2.33e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0453  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 808      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45888    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 5.48e-07 |
|    n_updates        | 8971     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0519  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 818      |
|    time_elapsed     | 58       |
|    total_timesteps  | 47988    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 2.64e-06 |
|    n_updates        | 9496     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0672   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 9.83e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0628  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 793      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50088    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 8.12e-07 |
|    n_updates        | 10021    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0744  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 802      |
|    time_elapsed     | 65       |
|    total_timesteps  | 52188    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.24e-06 |
|    n_updates        | 10546    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0744  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 811      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54288    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 0.0139   |
|    n_updates        | 11071    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0672   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 9.53e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0744  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 788      |
|    time_elapsed     | 71       |
|    total_timesteps  | 56388    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 1.89e-06 |
|    n_updates        | 11596    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0729  |
|    exploration_rate | 0.0672   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 795      |
|    time_elapsed     | 72       |
|    total_timesteps  | 58048    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 5.46e-07 |
|    n_updates        | 12011    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0672   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 4.25e-05 |
|    loss             | 5.56e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:09:17,917] Trial 38 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 4.246236507893906e-05, 'buffer_size': 71021, 'gamma': 0.905641640101377, 'exploration_fraction': 0.4764507091856726, 'exploration_final_eps': 0.06720518608149852}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 460      |
|    ep_rew_mean      | 0.066    |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4176     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1841     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.072   |
|    exploration_rate | 0.849    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4212     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3941     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.809    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.769    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1467     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6041     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.689    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1765     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8141     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.618    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.155   |
|    exploration_rate | 0.609    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1285     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10241    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 7.34e-07 |
|    n_updates        | 60       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 514      |
|    ep_rew_mean      | -0.164   |
|    exploration_rate | 0.528    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1289     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12341    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 4.21e-07 |
|    n_updates        | 585      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 516      |
|    ep_rew_mean      | -0.171   |
|    exploration_rate | 0.448    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1287     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14441    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 5.12e-07 |
|    n_updates        | 1110     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.427    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 6.94e-07 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.387    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1054     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16045    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 6.29e-07 |
|    n_updates        | 1511     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.306    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1073     |
|    time_elapsed     | 16       |
|    total_timesteps  | 18145    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 4.95e-07 |
|    n_updates        | 2036     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.236    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 6.4e-07  |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.226    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 948      |
|    time_elapsed     | 21       |
|    total_timesteps  | 20245    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 1.32e-07 |
|    n_updates        | 2561     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.158   |
|    exploration_rate | 0.146    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 966      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22345    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 1.63e-07 |
|    n_updates        | 3086     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.162   |
|    exploration_rate | 0.0656   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 979      |
|    time_elapsed     | 24       |
|    total_timesteps  | 24445    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 6.45e-06 |
|    n_updates        | 3611     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.10 +/- 0.34
Episode length: 493.00 +/- 96.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 493      |
|    mean_reward      | -0.0972  |
| rollout/            |          |
|    exploration_rate | 0.0444   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 5.12e-06 |
|    n_updates        | 3749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 52       |
|    fps              | 898      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26051    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 4.59e-06 |
|    n_updates        | 4012     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.147   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 56       |
|    fps              | 910      |
|    time_elapsed     | 30       |
|    total_timesteps  | 28151    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 1.49e-07 |
|    n_updates        | 4537     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.04     |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 3.36e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 60       |
|    fps              | 851      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30251    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 6.34e-08 |
|    n_updates        | 5062     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 64       |
|    fps              | 863      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32132    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 1.13e-06 |
|    n_updates        | 5532     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 68       |
|    fps              | 876      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34232    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 3.67e-07 |
|    n_updates        | 6057     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.04     |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 2.04e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 72       |
|    fps              | 832      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36332    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 1.15e-07 |
|    n_updates        | 6582     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 76       |
|    fps              | 841      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38017    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 2.57e-06 |
|    n_updates        | 7004     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.04     |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 9.09e-08 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 80       |
|    fps              | 804      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40012    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 7.78e-07 |
|    n_updates        | 7502     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 84       |
|    fps              | 815      |
|    time_elapsed     | 51       |
|    total_timesteps  | 42112    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 2.34e-06 |
|    n_updates        | 8027     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 88       |
|    fps              | 826      |
|    time_elapsed     | 53       |
|    total_timesteps  | 44212    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 1.03e-07 |
|    n_updates        | 8552     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.04     |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 3.69e-07 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 92       |
|    fps              | 797      |
|    time_elapsed     | 58       |
|    total_timesteps  | 46312    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 1.28e-06 |
|    n_updates        | 9077     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.128   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 96       |
|    fps              | 806      |
|    time_elapsed     | 59       |
|    total_timesteps  | 48112    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 2.01e-06 |
|    n_updates        | 9527     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.04     |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 6.29e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 100      |
|    fps              | 781      |
|    time_elapsed     | 64       |
|    total_timesteps  | 50212    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 2.26e-07 |
|    n_updates        | 10052    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 104      |
|    fps              | 791      |
|    time_elapsed     | 66       |
|    total_timesteps  | 52312    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 1.47e-05 |
|    n_updates        | 10577    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 108      |
|    fps              | 800      |
|    time_elapsed     | 67       |
|    total_timesteps  | 54412    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 8.14e-06 |
|    n_updates        | 11102    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.04     |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 2.09e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 112      |
|    fps              | 778      |
|    time_elapsed     | 72       |
|    total_timesteps  | 56512    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 4.18e-06 |
|    n_updates        | 11627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 116      |
|    fps              | 787      |
|    time_elapsed     | 74       |
|    total_timesteps  | 58612    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 3.27e-06 |
|    n_updates        | 12152    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.04     |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 6.32e-05 |
|    loss             | 7.25e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:10:39,510] Trial 39 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 6.317951440786908e-05, 'buffer_size': 10040, 'gamma': 0.929407672654423, 'exploration_fraction': 0.41858818404605813, 'exploration_final_eps': 0.03998042128100937}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4181     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0694  |
|    exploration_rate | 0.822    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4186     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3889     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.771    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 458      |
|    ep_rew_mean      | -0.0167  |
|    exploration_rate | 0.748    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1365     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5501     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 450      |
|    ep_rew_mean      | 0.0074   |
|    exploration_rate | 0.67     |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1626     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7206     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 464      |
|    ep_rew_mean      | 0.0143   |
|    exploration_rate | 0.574    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1888     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9290     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.542    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 475      |
|    ep_rew_mean      | -0.0231  |
|    exploration_rate | 0.478    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1280     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11390    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.11e-06 |
|    n_updates        | 347      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0498  |
|    exploration_rate | 0.382    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1281     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13490    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.42e-06 |
|    n_updates        | 872      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.313    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 2.14e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0698  |
|    exploration_rate | 0.285    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1043     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15590    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.05e-06 |
|    n_updates        | 1397     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0521  |
|    exploration_rate | 0.212    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1055     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17193    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 0.0001   |
|    n_updates        | 1798     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0394  |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1065     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18941    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 4.48e-06 |
|    n_updates        | 2235     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 474.40 +/- 151.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0897  |
| rollout/            |          |
|    exploration_rate | 0.0853   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 4.36e-06 |
|    n_updates        | 2499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0549  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 954      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21041    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.16e-06 |
|    n_updates        | 2760     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0445  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 968      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22838    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.25e-05 |
|    n_updates        | 3209     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0572  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 981      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24938    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.87e-08 |
|    n_updates        | 3734     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0853   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.5e-08  |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0681  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 901      |
|    time_elapsed     | 30       |
|    total_timesteps  | 27038    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.94e-06 |
|    n_updates        | 4259     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0776  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 915      |
|    time_elapsed     | 31       |
|    total_timesteps  | 29138    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.36e-08 |
|    n_updates        | 4784     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.10 +/- 0.33
Episode length: 498.70 +/- 78.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 499      |
|    mean_reward      | -0.0995  |
| rollout/            |          |
|    exploration_rate | 0.0853   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.88e-05 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0687  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 859      |
|    time_elapsed     | 36       |
|    total_timesteps  | 30992    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 3.36e-05 |
|    n_updates        | 5247     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.0593  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 870      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32590    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 2.57e-08 |
|    n_updates        | 5647     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0677  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 882      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34690    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.37e-08 |
|    n_updates        | 6172     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0853   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 6.42e-08 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0595  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 823      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36300    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 4.65e-08 |
|    n_updates        | 6574     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.067   |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 837      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38400    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.94e-07 |
|    n_updates        | 7099     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0853   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 7.18e-08 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0738  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 803      |
|    time_elapsed     | 50       |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 7.55e-09 |
|    n_updates        | 7624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.08    |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 815      |
|    time_elapsed     | 52       |
|    total_timesteps  | 42600    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 4.53e-07 |
|    n_updates        | 8149     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0856  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 825      |
|    time_elapsed     | 54       |
|    total_timesteps  | 44700    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 2.42e-09 |
|    n_updates        | 8674     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.09 +/- 0.36
Episode length: 473.40 +/- 154.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0894  |
| rollout/            |          |
|    exploration_rate | 0.0853   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 0.000128 |
|    n_updates        | 8749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0908  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 799      |
|    time_elapsed     | 58       |
|    total_timesteps  | 46800    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 3.52e-08 |
|    n_updates        | 9199     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0847  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 808      |
|    time_elapsed     | 60       |
|    total_timesteps  | 48688    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 3.58e-09 |
|    n_updates        | 9671     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0853   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.56e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0847  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 783      |
|    time_elapsed     | 64       |
|    total_timesteps  | 50788    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 5.55e-05 |
|    n_updates        | 10196    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0847  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 791      |
|    time_elapsed     | 66       |
|    total_timesteps  | 52569    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 4.88e-08 |
|    n_updates        | 10642    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0863  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 800      |
|    time_elapsed     | 68       |
|    total_timesteps  | 54586    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 9.39e-08 |
|    n_updates        | 11146    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0853   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 1.41e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0979  |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 778      |
|    time_elapsed     | 72       |
|    total_timesteps  | 56686    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 7e-08    |
|    n_updates        | 11671    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0853   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 787      |
|    time_elapsed     | 74       |
|    total_timesteps  | 58786    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 2.5e-09  |
|    n_updates        | 12196    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0853   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000535 |
|    loss             | 9.99e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:12:01,101] Trial 40 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.0005352632723602967, 'buffer_size': 95945, 'gamma': 0.9795728535360692, 'exploration_fraction': 0.3326203019014564, 'exploration_final_eps': 0.08533151668634191}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4217     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0687  |
|    exploration_rate | 0.845    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4233     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3875     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.8      |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.761    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1449     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5975     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.139   |
|    exploration_rate | 0.678    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1749     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8075     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.601    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.594    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1282     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10175    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 7.85e-06 |
|    n_updates        | 43       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.121   |
|    exploration_rate | 0.511    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1296     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12243    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 1.54e-05 |
|    n_updates        | 560      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.0946  |
|    exploration_rate | 0.436    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1301     |
|    time_elapsed     | 10       |
|    total_timesteps  | 14123    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 1.02e-06 |
|    n_updates        | 1030     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.09 +/- 0.36
Episode length: 475.50 +/- 148.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | -0.0902  |
| rollout/            |          |
|    exploration_rate | 0.401    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 1.19e-06 |
|    n_updates        | 1249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.352    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1086     |
|    time_elapsed     | 14       |
|    total_timesteps  | 16223    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 1.4e-06  |
|    n_updates        | 1555     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.0905  |
|    exploration_rate | 0.275    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1101     |
|    time_elapsed     | 16       |
|    total_timesteps  | 18149    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 7.91e-07 |
|    n_updates        | 2037     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.35
Episode length: 483.60 +/- 124.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 484      |
|    mean_reward      | -0.0934  |
| rollout/            |          |
|    exploration_rate | 0.201    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 7.85e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.191    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 978      |
|    time_elapsed     | 20       |
|    total_timesteps  | 20249    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 1.36e-06 |
|    n_updates        | 2562     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.108    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 993      |
|    time_elapsed     | 22       |
|    total_timesteps  | 22349    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 1.76e-07 |
|    n_updates        | 3087     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 1005     |
|    time_elapsed     | 24       |
|    total_timesteps  | 24449    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 4.98e-07 |
|    n_updates        | 3612     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0737   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 3.67e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.127   |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 920      |
|    time_elapsed     | 28       |
|    total_timesteps  | 26549    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 4.81e-07 |
|    n_updates        | 4137     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 935      |
|    time_elapsed     | 30       |
|    total_timesteps  | 28649    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 4.75e-07 |
|    n_updates        | 4662     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0737   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 6.15e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 869      |
|    time_elapsed     | 34       |
|    total_timesteps  | 30238    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 4.54e-07 |
|    n_updates        | 5059     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 884      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32338    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 2.86e-07 |
|    n_updates        | 5584     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 894      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34023    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 4.04e-07 |
|    n_updates        | 6005     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.36
Episode length: 475.10 +/- 149.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 475      |
|    mean_reward      | -0.09    |
| rollout/            |          |
|    exploration_rate | 0.0737   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 4.35e-07 |
|    n_updates        | 6249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 852      |
|    time_elapsed     | 42       |
|    total_timesteps  | 36012    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 2.6e-07  |
|    n_updates        | 6502     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 865      |
|    time_elapsed     | 44       |
|    total_timesteps  | 38112    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 3e-07    |
|    n_updates        | 7027     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0985  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 874      |
|    time_elapsed     | 45       |
|    total_timesteps  | 39708    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 5.2e-07  |
|    n_updates        | 7426     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 476.70 +/- 144.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | -0.0907  |
| rollout/            |          |
|    exploration_rate | 0.0737   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 6.16e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0905  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 839      |
|    time_elapsed     | 49       |
|    total_timesteps  | 41507    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 7.5e-07  |
|    n_updates        | 7876     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0832  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 849      |
|    time_elapsed     | 51       |
|    total_timesteps  | 43320    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 6.73e-07 |
|    n_updates        | 8329     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0737   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 1.8e-07  |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0888  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 817      |
|    time_elapsed     | 55       |
|    total_timesteps  | 45420    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 5.66e-06 |
|    n_updates        | 8854     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0938  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 828      |
|    time_elapsed     | 57       |
|    total_timesteps  | 47520    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 8.33e-07 |
|    n_updates        | 9379     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 836      |
|    time_elapsed     | 58       |
|    total_timesteps  | 49323    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 2.92e-06 |
|    n_updates        | 9830     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 474.30 +/- 152.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0897  |
| rollout/            |          |
|    exploration_rate | 0.0737   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 6.09e-07 |
|    n_updates        | 9999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0873  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 813      |
|    time_elapsed     | 63       |
|    total_timesteps  | 51423    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 6.73e-06 |
|    n_updates        | 10355    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0986  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 822      |
|    time_elapsed     | 65       |
|    total_timesteps  | 53523    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 4.92e-07 |
|    n_updates        | 10880    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0737   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 3.66e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0986  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 798      |
|    time_elapsed     | 69       |
|    total_timesteps  | 55623    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 7.54e-07 |
|    n_updates        | 11405    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0986  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 807      |
|    time_elapsed     | 71       |
|    total_timesteps  | 57723    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 4.88e-06 |
|    n_updates        | 11930    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0986  |
|    exploration_rate | 0.0737   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 816      |
|    time_elapsed     | 73       |
|    total_timesteps  | 59823    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 0.0149   |
|    n_updates        | 12455    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0737   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 3.83e-05 |
|    loss             | 1.19e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:13:20,468] Trial 41 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 3.8324155882767334e-05, 'buffer_size': 31125, 'gamma': 0.9129003889589968, 'exploration_fraction': 0.3866387770057779, 'exploration_final_eps': 0.07366734344709028}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4206     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4221     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.828    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.8      |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1411     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5811     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.135   |
|    exploration_rate | 0.727    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1715     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7911     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.655    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.655    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1278     |
|    time_elapsed     | 7        |
|    total_timesteps  | 10011    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 0.000449 |
|    n_updates        | 2        |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.583    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1298     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12099    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 9.69e-07 |
|    n_updates        | 524      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.089   |
|    exploration_rate | 0.527    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1306     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13731    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 8.43e-07 |
|    n_updates        | 932      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.483    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 6.84e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0712  |
|    exploration_rate | 0.459    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1064     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15694    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 1.46e-06 |
|    n_updates        | 1423     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.0866  |
|    exploration_rate | 0.386    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1084     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17794    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 7.42e-07 |
|    n_updates        | 1948     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0989  |
|    exploration_rate | 0.314    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1102     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19894    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 6.85e-07 |
|    n_updates        | 2473     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.31     |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 4.27e-07 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.242    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 977      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21994    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 2.81e-06 |
|    n_updates        | 2998     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.117   |
|    exploration_rate | 0.169    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 992      |
|    time_elapsed     | 24       |
|    total_timesteps  | 24094    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 1.04e-06 |
|    n_updates        | 3523     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.138    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 2.63e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.125   |
|    exploration_rate | 0.0968   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 910      |
|    time_elapsed     | 28       |
|    total_timesteps  | 26194    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 3.71e-07 |
|    n_updates        | 4048     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.131   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 925      |
|    time_elapsed     | 30       |
|    total_timesteps  | 28294    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 3.32e-07 |
|    n_updates        | 4573     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0779   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 3.48e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.136   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 867      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30394    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 7.97e-07 |
|    n_updates        | 5098     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 879      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32494    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 5.06e-07 |
|    n_updates        | 5623     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 509      |
|    ep_rew_mean      | -0.145   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 892      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34594    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 1.98e-07 |
|    n_updates        | 6148     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0779   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 2.71e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.148   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 847      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36694    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 2.24e-07 |
|    n_updates        | 6673     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 860      |
|    time_elapsed     | 45       |
|    total_timesteps  | 38794    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 3.48e-07 |
|    n_updates        | 7198     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0779   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 2.37e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 825      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40894    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 6.3e-07  |
|    n_updates        | 7723     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 837      |
|    time_elapsed     | 51       |
|    total_timesteps  | 42994    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 1.83e-07 |
|    n_updates        | 8248     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0779   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 2e-07    |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.16    |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 807      |
|    time_elapsed     | 55       |
|    total_timesteps  | 45094    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 4.67e-07 |
|    n_updates        | 8773     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.15    |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 816      |
|    time_elapsed     | 57       |
|    total_timesteps  | 46916    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 2.67e-07 |
|    n_updates        | 9228     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 507      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 825      |
|    time_elapsed     | 58       |
|    total_timesteps  | 48671    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 2.55e-07 |
|    n_updates        | 9667     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0779   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 5.87e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 800      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50771    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 4.04e-07 |
|    n_updates        | 10192    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.143   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 810      |
|    time_elapsed     | 65       |
|    total_timesteps  | 52871    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 7.12e-07 |
|    n_updates        | 10717    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 818      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54602    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 4.1e-07  |
|    n_updates        | 11150    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0779   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 2.02e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 794      |
|    time_elapsed     | 70       |
|    total_timesteps  | 56357    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 5.04e-07 |
|    n_updates        | 11589    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.132   |
|    exploration_rate | 0.0779   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 803      |
|    time_elapsed     | 72       |
|    total_timesteps  | 58457    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 3.44e-07 |
|    n_updates        | 12114    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0779   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.42e-05 |
|    loss             | 1.78e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:14:40,527] Trial 42 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 1.42449702460494e-05, 'buffer_size': 40349, 'gamma': 0.9163844879073144, 'exploration_fraction': 0.44572122370415856, 'exploration_final_eps': 0.07788199019173445}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4138     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 470      |
|    ep_rew_mean      | -0.0631  |
|    exploration_rate | 0.862    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4199     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3762     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.09 +/- 0.36
Episode length: 473.20 +/- 155.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.817    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.785    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1532     |
|    time_elapsed     | 3        |
|    total_timesteps  | 5862     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 450      |
|    ep_rew_mean      | 0.00743  |
|    exploration_rate | 0.736    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1739     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7204     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 465      |
|    ep_rew_mean      | -0.0361  |
|    exploration_rate | 0.659    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 2004     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9304     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.634    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 475      |
|    ep_rew_mean      | -0.065   |
|    exploration_rate | 0.583    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1341     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11404    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 9.98e-06 |
|    n_updates        | 350      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0858  |
|    exploration_rate | 0.506    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1342     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13504    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 3.55e-06 |
|    n_updates        | 875      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 457      |
|    ep_rew_mean      | -0.0267  |
|    exploration_rate | 0.464    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1340     |
|    time_elapsed     | 10       |
|    total_timesteps  | 14639    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 3.03e-06 |
|    n_updates        | 1159     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.451    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 3.17e-05 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 465      |
|    ep_rew_mean      | -0.0471  |
|    exploration_rate | 0.387    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1095     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16739    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 2.46e-06 |
|    n_updates        | 1684     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 471      |
|    ep_rew_mean      | -0.0634  |
|    exploration_rate | 0.311    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1111     |
|    time_elapsed     | 16       |
|    total_timesteps  | 18839    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 3.95e-05 |
|    n_updates        | 2209     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.268    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 7.97e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | -0.0512  |
|    exploration_rate | 0.245    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 978      |
|    time_elapsed     | 21       |
|    total_timesteps  | 20638    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 3.08e-08 |
|    n_updates        | 2659     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0645  |
|    exploration_rate | 0.168    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 995      |
|    time_elapsed     | 22       |
|    total_timesteps  | 22738    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 3.73e-07 |
|    n_updates        | 3184     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0756  |
|    exploration_rate | 0.091    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 1007     |
|    time_elapsed     | 24       |
|    total_timesteps  | 24838    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 8.96e-05 |
|    n_updates        | 3709     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0851   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 2.27e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0852  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 923      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26938    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 1.6e-07  |
|    n_updates        | 4234     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0936  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 60       |
|    fps              | 937      |
|    time_elapsed     | 30       |
|    total_timesteps  | 29038    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 1.08e-06 |
|    n_updates        | 4759     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.35
Episode length: 486.40 +/- 115.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | -0.0945  |
| rollout/            |          |
|    exploration_rate | 0.055    |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 3.52e-05 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0846  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 64       |
|    fps              | 880      |
|    time_elapsed     | 35       |
|    total_timesteps  | 31043    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 6.62e-07 |
|    n_updates        | 5260     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0596  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 68       |
|    fps              | 890      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32640    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 6.13e-07 |
|    n_updates        | 5659     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.068   |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 72       |
|    fps              | 901      |
|    time_elapsed     | 38       |
|    total_timesteps  | 34740    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 9.37e-06 |
|    n_updates        | 6184     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.055    |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 0.000105 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0754  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 76       |
|    fps              | 855      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36840    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 3.36e-08 |
|    n_updates        | 6709     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0822  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 80       |
|    fps              | 866      |
|    time_elapsed     | 44       |
|    total_timesteps  | 38940    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 5.54e-08 |
|    n_updates        | 7234     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.055    |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 8.5e-06  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0883  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 84       |
|    fps              | 828      |
|    time_elapsed     | 49       |
|    total_timesteps  | 41040    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 1.26e-06 |
|    n_updates        | 7759     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0938  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 88       |
|    fps              | 838      |
|    time_elapsed     | 51       |
|    total_timesteps  | 43140    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 5.45e-05 |
|    n_updates        | 8284     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0867  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 92       |
|    fps              | 848      |
|    time_elapsed     | 52       |
|    total_timesteps  | 44955    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 5.41e-10 |
|    n_updates        | 8738     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.055    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 1.51e-10 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0919  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 96       |
|    fps              | 817      |
|    time_elapsed     | 57       |
|    total_timesteps  | 47055    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 1.75e-05 |
|    n_updates        | 9263     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0966  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 100      |
|    fps              | 827      |
|    time_elapsed     | 59       |
|    total_timesteps  | 49155    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 5.64e-08 |
|    n_updates        | 9788     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.055    |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 4.32e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0856  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 104      |
|    fps              | 799      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50999    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 4.19e-09 |
|    n_updates        | 10249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0973  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 108      |
|    fps              | 808      |
|    time_elapsed     | 65       |
|    total_timesteps  | 53099    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 4.91e-08 |
|    n_updates        | 10774    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.055    |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 0.000109 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0973  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 112      |
|    fps              | 785      |
|    time_elapsed     | 70       |
|    total_timesteps  | 55199    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 1.4e-06  |
|    n_updates        | 11299    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 116      |
|    fps              | 794      |
|    time_elapsed     | 72       |
|    total_timesteps  | 57207    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 1.15e-08 |
|    n_updates        | 11801    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0983  |
|    exploration_rate | 0.055    |
| time/               |          |
|    episodes         | 120      |
|    fps              | 801      |
|    time_elapsed     | 73       |
|    total_timesteps  | 58874    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 4.25e-07 |
|    n_updates        | 12218    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.055    |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.00984  |
|    loss             | 8.59e-05 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:16:00,919] Trial 43 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.009842754467906238, 'buffer_size': 47024, 'gamma': 0.9107274895148977, 'exploration_fraction': 0.4303553635030867, 'exploration_final_eps': 0.05499495102902037}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4228     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4243     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.808    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.758    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1489     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6300     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.677    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1777     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8400     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.615    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1281     |
|    time_elapsed     | 8        |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 3.21e-06 |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 517      |
|    ep_rew_mean      | -0.165   |
|    exploration_rate | 0.522    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1287     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12412    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.03e-06 |
|    n_updates        | 602      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.172   |
|    exploration_rate | 0.441    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1291     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14512    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 4.2e-07  |
|    n_updates        | 1127     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.423    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 2.53e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 519      |
|    ep_rew_mean      | -0.176   |
|    exploration_rate | 0.361    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1067     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16612    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 6.11e-07 |
|    n_updates        | 1652     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 520      |
|    ep_rew_mean      | -0.18    |
|    exploration_rate | 0.28     |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1086     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18712    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.76e-06 |
|    n_updates        | 2177     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.23     |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.11e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 520      |
|    ep_rew_mean      | -0.183   |
|    exploration_rate | 0.199    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 961      |
|    time_elapsed     | 21       |
|    total_timesteps  | 20812    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 3.54e-07 |
|    n_updates        | 2702     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 510      |
|    ep_rew_mean      | -0.158   |
|    exploration_rate | 0.137    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 975      |
|    time_elapsed     | 22       |
|    total_timesteps  | 22435    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 4.17e-07 |
|    n_updates        | 3108     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 990      |
|    time_elapsed     | 24       |
|    total_timesteps  | 24535    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 4.36e-07 |
|    n_updates        | 3633     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0711   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.73e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.166   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 910      |
|    time_elapsed     | 29       |
|    total_timesteps  | 26635    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 2.61e-07 |
|    n_updates        | 4158     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 505      |
|    ep_rew_mean      | -0.148   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 921      |
|    time_elapsed     | 30       |
|    total_timesteps  | 28277    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 3e-07    |
|    n_updates        | 4569     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0711   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.97e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.153   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 863      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30377    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 9.93e-08 |
|    n_updates        | 5094     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.138   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 874      |
|    time_elapsed     | 36       |
|    total_timesteps  | 32055    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.39e-07 |
|    n_updates        | 5513     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.124   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 884      |
|    time_elapsed     | 38       |
|    total_timesteps  | 33642    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 6.47e-07 |
|    n_updates        | 5910     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0711   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 2.76e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 838      |
|    time_elapsed     | 42       |
|    total_timesteps  | 35742    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 4.12e-07 |
|    n_updates        | 6435     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.133   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 851      |
|    time_elapsed     | 44       |
|    total_timesteps  | 37842    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 5.98e-07 |
|    n_updates        | 6960     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.137   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 863      |
|    time_elapsed     | 46       |
|    total_timesteps  | 39942    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 4.03e-07 |
|    n_updates        | 7485     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0711   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.57e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.141   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 828      |
|    time_elapsed     | 50       |
|    total_timesteps  | 42042    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 5.31e-07 |
|    n_updates        | 8010     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.144   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 839      |
|    time_elapsed     | 52       |
|    total_timesteps  | 44142    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 4.42e-06 |
|    n_updates        | 8535     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0711   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 2.08e-07 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 503      |
|    ep_rew_mean      | -0.147   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 810      |
|    time_elapsed     | 57       |
|    total_timesteps  | 46242    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 3.88e-07 |
|    n_updates        | 9060     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.149   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 821      |
|    time_elapsed     | 58       |
|    total_timesteps  | 48342    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.84e-07 |
|    n_updates        | 9585     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 473.60 +/- 154.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0894  |
| rollout/            |          |
|    exploration_rate | 0.0711   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 2.64e-07 |
|    n_updates        | 9999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 798      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50442    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 2.46e-07 |
|    n_updates        | 10110    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 809      |
|    time_elapsed     | 64       |
|    total_timesteps  | 52542    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.56e-07 |
|    n_updates        | 10635    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.152   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 818      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54642    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 3.11e-07 |
|    n_updates        | 11160    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0711   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 2.72e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 793      |
|    time_elapsed     | 70       |
|    total_timesteps  | 56241    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 2.69e-07 |
|    n_updates        | 11560    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.129   |
|    exploration_rate | 0.0711   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 801      |
|    time_elapsed     | 72       |
|    total_timesteps  | 58069    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.32e-07 |
|    n_updates        | 12017    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0711   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 2.32e-05 |
|    loss             | 1.5e-07  |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:17:20,846] Trial 44 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 2.3151743663465124e-05, 'buffer_size': 15661, 'gamma': 0.9050354084703357, 'exploration_fraction': 0.40224342576885225, 'exploration_final_eps': 0.0711062518874584}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 428      |
|    ep_rew_mean      | 0.0788   |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4169     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1712     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 451      |
|    ep_rew_mean      | 0.0695   |
|    exploration_rate | 0.851    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4216     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3610     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.794    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0236  |
|    exploration_rate | 0.764    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1405     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5710     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.00179 |
|    exploration_rate | 0.688    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1682     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7573     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 466      |
|    ep_rew_mean      | 0.0138   |
|    exploration_rate | 0.616    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1897     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9312     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.588    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 476      |
|    ep_rew_mean      | -0.0235  |
|    exploration_rate | 0.529    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1283     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11412    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.17e-06 |
|    n_updates        | 352      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0501  |
|    exploration_rate | 0.443    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1285     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13512    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 5.36e-07 |
|    n_updates        | 877      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.381    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 3.35e-05 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.0354  |
|    exploration_rate | 0.368    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1043     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15331    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 9.33e-07 |
|    n_updates        | 1332     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 474      |
|    ep_rew_mean      | -0.0228  |
|    exploration_rate | 0.296    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1060     |
|    time_elapsed     | 16       |
|    total_timesteps  | 17055    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.42e-06 |
|    n_updates        | 1763     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.0415  |
|    exploration_rate | 0.21     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1073     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19155    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 6.73e-06 |
|    n_updates        | 2288     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.175    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.31e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0568  |
|    exploration_rate | 0.123    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 949      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21255    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 5.66e-07 |
|    n_updates        | 2813     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0453  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 961      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22941    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.74e-06 |
|    n_updates        | 3235     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.36
Episode length: 474.10 +/- 152.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0896  |
| rollout/            |          |
|    exploration_rate | 0.0652   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 2.92e-06 |
|    n_updates        | 3749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.058   |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 888      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25041    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.31e-06 |
|    n_updates        | 3760     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0689  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 905      |
|    time_elapsed     | 29       |
|    total_timesteps  | 27141    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.27e-07 |
|    n_updates        | 4285     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0602  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 918      |
|    time_elapsed     | 31       |
|    total_timesteps  | 29030    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 4.72e-07 |
|    n_updates        | 4757     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0652   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 6.16e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0696  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 860      |
|    time_elapsed     | 36       |
|    total_timesteps  | 31130    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 5.01e-07 |
|    n_updates        | 5282     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0778  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 872      |
|    time_elapsed     | 38       |
|    total_timesteps  | 33230    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.46e-06 |
|    n_updates        | 5807     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0691  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 882      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34940    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 2.96e-06 |
|    n_updates        | 6234     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0652   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 3.31e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0765  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 838      |
|    time_elapsed     | 44       |
|    total_timesteps  | 37040    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.08e-05 |
|    n_updates        | 6759     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0832  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 850      |
|    time_elapsed     | 46       |
|    total_timesteps  | 39140    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 3.6e-06  |
|    n_updates        | 7284     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0652   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.19e-06 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0892  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 814      |
|    time_elapsed     | 50       |
|    total_timesteps  | 41240    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.6e-07  |
|    n_updates        | 7809     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0818  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 824      |
|    time_elapsed     | 52       |
|    total_timesteps  | 43006    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.21e-07 |
|    n_updates        | 8251     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0652   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 2.78e-05 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0874  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 795      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45106    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 5.46e-08 |
|    n_updates        | 8776     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0806  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 804      |
|    time_elapsed     | 58       |
|    total_timesteps  | 46854    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 2.33e-06 |
|    n_updates        | 9213     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0741  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 812      |
|    time_elapsed     | 59       |
|    total_timesteps  | 48531    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 3.26e-06 |
|    n_updates        | 9632     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0652   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 2.69e-05 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0857  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 787      |
|    time_elapsed     | 64       |
|    total_timesteps  | 50631    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 3.27e-07 |
|    n_updates        | 10157    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0965  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 797      |
|    time_elapsed     | 66       |
|    total_timesteps  | 52731    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 7.67e-07 |
|    n_updates        | 10682    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0863  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 806      |
|    time_elapsed     | 67       |
|    total_timesteps  | 54778    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 9.14e-06 |
|    n_updates        | 11194    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0652   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 3.85e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0972  |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 784      |
|    time_elapsed     | 72       |
|    total_timesteps  | 56878    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 4.39e-07 |
|    n_updates        | 11719    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.109   |
|    exploration_rate | 0.0652   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 792      |
|    time_elapsed     | 74       |
|    total_timesteps  | 58978    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 2.05e-06 |
|    n_updates        | 12244    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0652   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000269 |
|    loss             | 1.78e-05 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:18:42,017] Trial 45 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.000268589350541408, 'buffer_size': 59784, 'gamma': 0.9217916098420612, 'exploration_fraction': 0.37766491246366474, 'exploration_final_eps': 0.06519942234765815}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | 0.0653   |
|    exploration_rate | 0.739    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4139     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1848     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 394      |
|    ep_rew_mean      | 0.217    |
|    exploration_rate | 0.555    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4173     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3152     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.294    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 438      |
|    ep_rew_mean      | 0.075    |
|    exploration_rate | 0.258    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1315     |
|    time_elapsed     | 3        |
|    total_timesteps  | 5252     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 435      |
|    ep_rew_mean      | 0.0759   |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1584     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6967     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 451      |
|    ep_rew_mean      | 0.0696   |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1848     |
|    time_elapsed     | 4        |
|    total_timesteps  | 9021     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 463      |
|    ep_rew_mean      | 0.023    |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1263     |
|    time_elapsed     | 8        |
|    total_timesteps  | 11121    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 4.92e-06 |
|    n_updates        | 280      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | -0.0103  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1242     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13221    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 2.75e-06 |
|    n_updates        | 805      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 8.51e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.00156 |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1006     |
|    time_elapsed     | 15       |
|    total_timesteps  | 15127    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 0.00761  |
|    n_updates        | 1281     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 456      |
|    ep_rew_mean      | 0.04     |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1015     |
|    time_elapsed     | 16       |
|    total_timesteps  | 16400    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 1.51e-06 |
|    n_updates        | 1599     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 462      |
|    ep_rew_mean      | 0.015    |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1028     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18500    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 1.82e-06 |
|    n_updates        | 2124     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 4.19e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 461      |
|    ep_rew_mean      | 0.0201   |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 910      |
|    time_elapsed     | 22       |
|    total_timesteps  | 20292    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 1.32e-06 |
|    n_updates        | 2572     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 466      |
|    ep_rew_mean      | 0.000923 |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 927      |
|    time_elapsed     | 24       |
|    total_timesteps  | 22392    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 7.09e-06 |
|    n_updates        | 3097     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 471      |
|    ep_rew_mean      | -0.0153  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 942      |
|    time_elapsed     | 25       |
|    total_timesteps  | 24492    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 1.61e-06 |
|    n_updates        | 3622     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 1.85e-05 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 468      |
|    ep_rew_mean      | -0.00846 |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 865      |
|    time_elapsed     | 30       |
|    total_timesteps  | 26188    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 4.65e-07 |
|    n_updates        | 4046     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 464      |
|    ep_rew_mean      | -0.00241 |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 878      |
|    time_elapsed     | 31       |
|    total_timesteps  | 27866    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 3.09e-07 |
|    n_updates        | 4466     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 468      |
|    ep_rew_mean      | -0.0154  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 892      |
|    time_elapsed     | 33       |
|    total_timesteps  | 29966    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 2.11e-05 |
|    n_updates        | 4991     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 2.97e-05 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 472      |
|    ep_rew_mean      | -0.0268  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 841      |
|    time_elapsed     | 38       |
|    total_timesteps  | 32066    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 5.68e-05 |
|    n_updates        | 5516     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 475      |
|    ep_rew_mean      | -0.037   |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 855      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34166    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 2.61e-05 |
|    n_updates        | 6041     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.10 +/- 0.33
Episode length: 503.50 +/- 64.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 504      |
|    mean_reward      | -0.101   |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 1.5e-05  |
|    n_updates        | 6249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 477      |
|    ep_rew_mean      | -0.0461  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 816      |
|    time_elapsed     | 44       |
|    total_timesteps  | 36266    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 8.91e-05 |
|    n_updates        | 6566     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0543  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 830      |
|    time_elapsed     | 46       |
|    total_timesteps  | 38366    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 7.79e-05 |
|    n_updates        | 7091     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 474.80 +/- 150.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 475      |
|    mean_reward      | -0.0899  |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 6.8e-05  |
|    n_updates        | 7499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0617  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 800      |
|    time_elapsed     | 50       |
|    total_timesteps  | 40466    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 4.52e-05 |
|    n_updates        | 7616     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0685  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 812      |
|    time_elapsed     | 52       |
|    total_timesteps  | 42566    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 9.15e-05 |
|    n_updates        | 8141     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 480      |
|    ep_rew_mean      | -0.0617  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 820      |
|    time_elapsed     | 53       |
|    total_timesteps  | 44201    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 5.37e-05 |
|    n_updates        | 8550     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 1.39e-05 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0679  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 792      |
|    time_elapsed     | 58       |
|    total_timesteps  | 46301    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 2.43e-05 |
|    n_updates        | 9075     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0736  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 802      |
|    time_elapsed     | 60       |
|    total_timesteps  | 48401    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 8.4e-05  |
|    n_updates        | 9600     |
----------------------------------
Eval num_timesteps=50000, episode_reward=0.15 +/- 0.55
Episode length: 376.40 +/- 227.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 376      |
|    mean_reward      | 0.149    |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 3.41e-05 |
|    n_updates        | 9999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.073   |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 786      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50110    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 7.57e-05 |
|    n_updates        | 10027    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0745  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 794      |
|    time_elapsed     | 65       |
|    total_timesteps  | 51776    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 0.000329 |
|    n_updates        | 10443    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0745  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 803      |
|    time_elapsed     | 67       |
|    total_timesteps  | 53876    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 0.000101 |
|    n_updates        | 10968    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 2.76e-05 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.086   |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 781      |
|    time_elapsed     | 71       |
|    total_timesteps  | 55976    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 0.000259 |
|    n_updates        | 11493    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.0962  |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 791      |
|    time_elapsed     | 73       |
|    total_timesteps  | 58076    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 3.85e-05 |
|    n_updates        | 12018    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000146 |
|    loss             | 6.21e-05 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:20:03,000] Trial 46 finished with value: -0.21000000000000002 and parameters: {'batch_size': 64, 'learning_rate': 0.00014602152223755014, 'buffer_size': 26827, 'gamma': 0.9657316605611812, 'exploration_fraction': 0.10848036412523655, 'exploration_final_eps': 0.08081485809965651}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | 0.0568   |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4216     |
|    time_elapsed     | 0        |
|    total_timesteps  | 1932     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.0766  |
|    exploration_rate | 0.873    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4244     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4032     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.843    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 469      |
|    ep_rew_mean      | 0.0623   |
|    exploration_rate | 0.823    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1389     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5633     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.00578 |
|    exploration_rate | 0.757    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1699     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7733     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0466  |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1948     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9833     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.686    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0739  |
|    exploration_rate | 0.625    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1285     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11933    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.75e-06 |
|    n_updates        | 483      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.0933  |
|    exploration_rate | 0.56     |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1287     |
|    time_elapsed     | 10       |
|    total_timesteps  | 14033    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.25e-06 |
|    n_updates        | 1008     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.529    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 1.64e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 504      |
|    ep_rew_mean      | -0.108   |
|    exploration_rate | 0.494    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1054     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16133    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 1.03e-06 |
|    n_updates        | 1533     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 506      |
|    ep_rew_mean      | -0.119   |
|    exploration_rate | 0.428    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1073     |
|    time_elapsed     | 16       |
|    total_timesteps  | 18233    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 1.08e-06 |
|    n_updates        | 2058     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 496      |
|    ep_rew_mean      | -0.0982  |
|    exploration_rate | 0.378    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1083     |
|    time_elapsed     | 18       |
|    total_timesteps  | 19823    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.91e-06 |
|    n_updates        | 2455     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.372    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.41e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0833  |
|    exploration_rate | 0.32     |
| time/               |          |
|    episodes         | 44       |
|    fps              | 960      |
|    time_elapsed     | 22       |
|    total_timesteps  | 21669    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.12e-06 |
|    n_updates        | 2917     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0939  |
|    exploration_rate | 0.254    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 975      |
|    time_elapsed     | 24       |
|    total_timesteps  | 23769    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.34e-06 |
|    n_updates        | 3442     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.215    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 8.14e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.103   |
|    exploration_rate | 0.188    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 893      |
|    time_elapsed     | 28       |
|    total_timesteps  | 25869    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.54e-06 |
|    n_updates        | 3967     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.122    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 906      |
|    time_elapsed     | 30       |
|    total_timesteps  | 27969    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.89e-06 |
|    n_updates        | 4492     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0972  |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 914      |
|    time_elapsed     | 32       |
|    total_timesteps  | 29578    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 5.27e-06 |
|    n_updates        | 4894     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.36
Episode length: 476.40 +/- 145.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 476      |
|    mean_reward      | -0.0905  |
| rollout/            |          |
|    exploration_rate | 0.0937   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.36e-06 |
|    n_updates        | 4999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.104   |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 861      |
|    time_elapsed     | 36       |
|    total_timesteps  | 31678    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.52e-06 |
|    n_updates        | 5419     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0932  |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 870      |
|    time_elapsed     | 38       |
|    total_timesteps  | 33344    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 4.12e-06 |
|    n_updates        | 5835     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0937   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 1.96e-06 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0844  |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 823      |
|    time_elapsed     | 42       |
|    total_timesteps  | 35195    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 8.69e-06 |
|    n_updates        | 6298     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 491      |
|    ep_rew_mean      | -0.091   |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 833      |
|    time_elapsed     | 44       |
|    total_timesteps  | 37295    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.24e-06 |
|    n_updates        | 6823     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.097   |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 844      |
|    time_elapsed     | 46       |
|    total_timesteps  | 39395    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.98e-06 |
|    n_updates        | 7348     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0937   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 3.3e-06  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 808      |
|    time_elapsed     | 51       |
|    total_timesteps  | 41495    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 3.26e-06 |
|    n_updates        | 7873     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 818      |
|    time_elapsed     | 53       |
|    total_timesteps  | 43595    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.64e-06 |
|    n_updates        | 8398     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0937   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 4.9e-06  |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.112   |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 788      |
|    time_elapsed     | 57       |
|    total_timesteps  | 45695    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 3.27e-06 |
|    n_updates        | 8923     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.116   |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 798      |
|    time_elapsed     | 59       |
|    total_timesteps  | 47795    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 1.17e-05 |
|    n_updates        | 9448     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 499      |
|    ep_rew_mean      | -0.12    |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 807      |
|    time_elapsed     | 61       |
|    total_timesteps  | 49895    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 9.21e-06 |
|    n_updates        | 9973     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.10 +/- 0.34
Episode length: 492.10 +/- 98.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 492      |
|    mean_reward      | -0.0968  |
| rollout/            |          |
|    exploration_rate | 0.0937   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.15e-05 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 785      |
|    time_elapsed     | 66       |
|    total_timesteps  | 51995    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 1.45e-05 |
|    n_updates        | 10498    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.13    |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 793      |
|    time_elapsed     | 68       |
|    total_timesteps  | 54095    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 1.36e-05 |
|    n_updates        | 11023    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.09 +/- 0.35
Episode length: 482.70 +/- 126.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 483      |
|    mean_reward      | -0.093   |
| rollout/            |          |
|    exploration_rate | 0.0937   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 3.96e-06 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.14    |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 769      |
|    time_elapsed     | 72       |
|    total_timesteps  | 55688    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 8.1e-06  |
|    n_updates        | 11421    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 494      |
|    ep_rew_mean      | -0.118   |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 774      |
|    time_elapsed     | 73       |
|    total_timesteps  | 57154    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 9.25e-06 |
|    n_updates        | 11788    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.107   |
|    exploration_rate | 0.0937   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 782      |
|    time_elapsed     | 75       |
|    total_timesteps  | 59129    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 2.16e-05 |
|    n_updates        | 12282    |
----------------------------------
Eval num_timesteps=60000, episode_reward=0.15 +/- 0.54
Episode length: 383.90 +/- 216.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 384      |
|    mean_reward      | 0.146    |
| rollout/            |          |
|    exploration_rate | 0.0937   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 4.3e-05  |
|    loss             | 7.64e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:21:24,501] Trial 47 finished with value: -0.21000000000000002 and parameters: {'batch_size': 128, 'learning_rate': 4.300251976342624e-05, 'buffer_size': 51340, 'gamma': 0.9950601441782183, 'exploration_fraction': 0.4812810425332621, 'exploration_final_eps': 0.09370612443299345}. Best is trial 23 with value: 0.029390000000000017.
New best mean reward!
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4179     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 465      |
|    ep_rew_mean      | -0.0611  |
|    exploration_rate | 0.873    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4194     |
|    time_elapsed     | 0        |
|    total_timesteps  | 3722     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.829    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 473      |
|    ep_rew_mean      | -0.0226  |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1384     |
|    time_elapsed     | 4        |
|    total_timesteps  | 5680     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 468      |
|    ep_rew_mean      | 0.000419 |
|    exploration_rate | 0.745    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1651     |
|    time_elapsed     | 4        |
|    total_timesteps  | 7484     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.0417  |
|    exploration_rate | 0.673    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1907     |
|    time_elapsed     | 5        |
|    total_timesteps  | 9584     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.659    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0697  |
|    exploration_rate | 0.601    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1286     |
|    time_elapsed     | 9        |
|    total_timesteps  | 11684    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.84e-06 |
|    n_updates        | 420      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 477      |
|    ep_rew_mean      | -0.0481  |
|    exploration_rate | 0.544    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1292     |
|    time_elapsed     | 10       |
|    total_timesteps  | 13366    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.37e-06 |
|    n_updates        | 841      |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.09 +/- 0.36
Episode length: 473.40 +/- 154.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.488    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.3e-06  |
|    n_updates        | 1249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0683  |
|    exploration_rate | 0.472    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1071     |
|    time_elapsed     | 14       |
|    total_timesteps  | 15466    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.35e-05 |
|    n_updates        | 1366     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 479      |
|    ep_rew_mean      | -0.0528  |
|    exploration_rate | 0.411    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1093     |
|    time_elapsed     | 15       |
|    total_timesteps  | 17252    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 2.28e-06 |
|    n_updates        | 1812     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0685  |
|    exploration_rate | 0.34     |
| time/               |          |
|    episodes         | 40       |
|    fps              | 1113     |
|    time_elapsed     | 17       |
|    total_timesteps  | 19352    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.19e-06 |
|    n_updates        | 2337     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.318    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.63e-06 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 483      |
|    ep_rew_mean      | -0.0569  |
|    exploration_rate | 0.274    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 982      |
|    time_elapsed     | 21       |
|    total_timesteps  | 21264    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 7.86e-07 |
|    n_updates        | 2815     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 485      |
|    ep_rew_mean      | -0.0481  |
|    exploration_rate | 0.206    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 997      |
|    time_elapsed     | 23       |
|    total_timesteps  | 23273    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 5.01e-07 |
|    n_updates        | 3318     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.147    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 5.84e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0396  |
|    exploration_rate | 0.142    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 910      |
|    time_elapsed     | 27       |
|    total_timesteps  | 25144    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 6.58e-07 |
|    n_updates        | 3785     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 478      |
|    ep_rew_mean      | -0.0303  |
|    exploration_rate | 0.0872   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 922      |
|    time_elapsed     | 28       |
|    total_timesteps  | 26749    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 9.31e-07 |
|    n_updates        | 4187     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0423  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 937      |
|    time_elapsed     | 30       |
|    total_timesteps  | 28849    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.25e-06 |
|    n_updates        | 4712     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0851   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 4.86e-07 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 484      |
|    ep_rew_mean      | -0.0528  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 877      |
|    time_elapsed     | 35       |
|    total_timesteps  | 30949    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 2.49e-06 |
|    n_updates        | 5237     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.062   |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 891      |
|    time_elapsed     | 37       |
|    total_timesteps  | 33049    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 4.57e-07 |
|    n_updates        | 5762     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.36
Episode length: 474.20 +/- 152.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0897  |
| rollout/            |          |
|    exploration_rate | 0.0851   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 2.7e-07  |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 488      |
|    ep_rew_mean      | -0.0703  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 851      |
|    time_elapsed     | 41       |
|    total_timesteps  | 35149    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.63e-07 |
|    n_updates        | 6287     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 490      |
|    ep_rew_mean      | -0.0776  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 864      |
|    time_elapsed     | 43       |
|    total_timesteps  | 37249    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 6.39e-07 |
|    n_updates        | 6812     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 492      |
|    ep_rew_mean      | -0.0842  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 876      |
|    time_elapsed     | 44       |
|    total_timesteps  | 39349    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 0.0143   |
|    n_updates        | 7337     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.35
Episode length: 483.90 +/- 123.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 484      |
|    mean_reward      | -0.0935  |
| rollout/            |          |
|    exploration_rate | 0.0851   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 4.21e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 493      |
|    ep_rew_mean      | -0.0902  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 842      |
|    time_elapsed     | 49       |
|    total_timesteps  | 41449    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 3.51e-06 |
|    n_updates        | 7862     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0957  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 852      |
|    time_elapsed     | 51       |
|    total_timesteps  | 43549    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 4.42e-06 |
|    n_updates        | 8387     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 486      |
|    ep_rew_mean      | -0.0747  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 858      |
|    time_elapsed     | 52       |
|    total_timesteps  | 44688    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.16e-06 |
|    n_updates        | 8671     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0851   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.66e-06 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 487      |
|    ep_rew_mean      | -0.0804  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 827      |
|    time_elapsed     | 56       |
|    total_timesteps  | 46788    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 6.45e-07 |
|    n_updates        | 9196     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0855  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 837      |
|    time_elapsed     | 58       |
|    total_timesteps  | 48888    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 7.49e-07 |
|    n_updates        | 9721     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 477.30 +/- 143.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 477      |
|    mean_reward      | -0.0909  |
| rollout/            |          |
|    exploration_rate | 0.0851   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 4.74e-07 |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0855  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 814      |
|    time_elapsed     | 62       |
|    total_timesteps  | 50988    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 4.34e-07 |
|    n_updates        | 10246    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 489      |
|    ep_rew_mean      | -0.0855  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 821      |
|    time_elapsed     | 64       |
|    total_timesteps  | 52608    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 4.15e-07 |
|    n_updates        | 10651    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 482      |
|    ep_rew_mean      | -0.0729  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 827      |
|    time_elapsed     | 65       |
|    total_timesteps  | 53901    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 6.92e-07 |
|    n_updates        | 10975    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0851   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 4.8e-07  |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0724  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 802      |
|    time_elapsed     | 69       |
|    total_timesteps  | 55599    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 6.29e-07 |
|    n_updates        | 11399    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0724  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 120      |
|    fps              | 811      |
|    time_elapsed     | 71       |
|    total_timesteps  | 57699    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 7.46e-07 |
|    n_updates        | 11924    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 481      |
|    ep_rew_mean      | -0.0724  |
|    exploration_rate | 0.0851   |
| time/               |          |
|    episodes         | 124      |
|    fps              | 820      |
|    time_elapsed     | 72       |
|    total_timesteps  | 59799    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 2.44e-06 |
|    n_updates        | 12449    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0851   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 3.01e-05 |
|    loss             | 1.39e-07 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:22:43,474] Trial 48 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 3.0146795307491216e-05, 'buffer_size': 41286, 'gamma': 0.9252744360603119, 'exploration_fraction': 0.44685528387204176, 'exploration_final_eps': 0.08511257614852549}. Best is trial 23 with value: 0.029390000000000017.
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.905    |
| time/               |          |
|    episodes         | 4        |
|    fps              | 4227     |
|    time_elapsed     | 0        |
|    total_timesteps  | 2100     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.81     |
| time/               |          |
|    episodes         | 8        |
|    fps              | 4250     |
|    time_elapsed     | 0        |
|    total_timesteps  | 4200     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.774    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.715    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 1495     |
|    time_elapsed     | 4        |
|    total_timesteps  | 6300     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.62     |
| time/               |          |
|    episodes         | 16       |
|    fps              | 1782     |
|    time_elapsed     | 4        |
|    total_timesteps  | 8400     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.548    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.525    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 1283     |
|    time_elapsed     | 8        |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 9.21e-07 |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.43     |
| time/               |          |
|    episodes         | 24       |
|    fps              | 1294     |
|    time_elapsed     | 9        |
|    total_timesteps  | 12600    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.48e-06 |
|    n_updates        | 649      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.335    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 1295     |
|    time_elapsed     | 11       |
|    total_timesteps  | 14700    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 3.03e-06 |
|    n_updates        | 1174     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.321    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.31e-06 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 525      |
|    ep_rew_mean      | -0.21    |
|    exploration_rate | 0.24     |
| time/               |          |
|    episodes         | 32       |
|    fps              | 1068     |
|    time_elapsed     | 15       |
|    total_timesteps  | 16800    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.38e-06 |
|    n_updates        | 1699     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 524      |
|    ep_rew_mean      | -0.182   |
|    exploration_rate | 0.146    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 1082     |
|    time_elapsed     | 17       |
|    total_timesteps  | 18882    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 2.37e-06 |
|    n_updates        | 2220     |
----------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 473      |
|    mean_reward      | -0.0893  |
| rollout/            |          |
|    exploration_rate | 0.0952   |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 3.65e-06 |
|    n_updates        | 2499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 521      |
|    ep_rew_mean      | -0.158   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 40       |
|    fps              | 965      |
|    time_elapsed     | 21       |
|    total_timesteps  | 20848    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 2.43e-07 |
|    n_updates        | 2711     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 522      |
|    ep_rew_mean      | -0.163   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 44       |
|    fps              | 980      |
|    time_elapsed     | 23       |
|    total_timesteps  | 22948    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.08e-06 |
|    n_updates        | 3236     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0701   |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 2.79e-07 |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 522      |
|    ep_rew_mean      | -0.167   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 48       |
|    fps              | 897      |
|    time_elapsed     | 27       |
|    total_timesteps  | 25048    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 7.37e-07 |
|    n_updates        | 3761     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 522      |
|    ep_rew_mean      | -0.17    |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 52       |
|    fps              | 912      |
|    time_elapsed     | 29       |
|    total_timesteps  | 27148    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 3.58e-07 |
|    n_updates        | 4286     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.154   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 56       |
|    fps              | 924      |
|    time_elapsed     | 31       |
|    total_timesteps  | 28993    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 3.06e-07 |
|    n_updates        | 4748     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0701   |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 4.51e-06 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 518      |
|    ep_rew_mean      | -0.157   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 60       |
|    fps              | 866      |
|    time_elapsed     | 35       |
|    total_timesteps  | 31093    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 2.03e-07 |
|    n_updates        | 5273     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 511      |
|    ep_rew_mean      | -0.142   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 64       |
|    fps              | 876      |
|    time_elapsed     | 37       |
|    total_timesteps  | 32707    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.01e-07 |
|    n_updates        | 5676     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 512      |
|    ep_rew_mean      | -0.146   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 68       |
|    fps              | 889      |
|    time_elapsed     | 39       |
|    total_timesteps  | 34807    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.18e-07 |
|    n_updates        | 6201     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0701   |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.87e-07 |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 508      |
|    ep_rew_mean      | -0.134   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 72       |
|    fps              | 840      |
|    time_elapsed     | 43       |
|    total_timesteps  | 36544    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 3.62e-07 |
|    n_updates        | 6635     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.122   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 76       |
|    fps              | 849      |
|    time_elapsed     | 44       |
|    total_timesteps  | 38126    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 4.63e-06 |
|    n_updates        | 7031     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 473.90 +/- 153.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 474      |
|    mean_reward      | -0.0895  |
| rollout/            |          |
|    exploration_rate | 0.0701   |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 2.96e-07 |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.113   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 80       |
|    fps              | 817      |
|    time_elapsed     | 49       |
|    total_timesteps  | 40106    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.26e-07 |
|    n_updates        | 7526     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 502      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 84       |
|    fps              | 829      |
|    time_elapsed     | 50       |
|    total_timesteps  | 42179    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 2.19e-05 |
|    n_updates        | 8044     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 497      |
|    ep_rew_mean      | -0.0966  |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 88       |
|    fps              | 837      |
|    time_elapsed     | 52       |
|    total_timesteps  | 43762    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 6.52e-07 |
|    n_updates        | 8440     |
----------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0701   |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 4.31e-08 |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 498      |
|    ep_rew_mean      | -0.102   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 92       |
|    fps              | 806      |
|    time_elapsed     | 56       |
|    total_timesteps  | 45862    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 4.9e-06  |
|    n_updates        | 8965     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | -0.106   |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 96       |
|    fps              | 818      |
|    time_elapsed     | 58       |
|    total_timesteps  | 47962    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 8e-07    |
|    n_updates        | 9490     |
----------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0701   |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.8e-06  |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 100      |
|    fps              | 792      |
|    time_elapsed     | 63       |
|    total_timesteps  | 50062    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 8.88e-08 |
|    n_updates        | 10015    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 104      |
|    fps              | 802      |
|    time_elapsed     | 65       |
|    total_timesteps  | 52162    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.38e-05 |
|    n_updates        | 10540    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 501      |
|    ep_rew_mean      | -0.11    |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 108      |
|    fps              | 811      |
|    time_elapsed     | 66       |
|    total_timesteps  | 54262    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 6.26e-11 |
|    n_updates        | 11065    |
----------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0701   |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 1.88e-07 |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0982  |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 112      |
|    fps              | 786      |
|    time_elapsed     | 70       |
|    total_timesteps  | 55845    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 3.06e-08 |
|    n_updates        | 11461    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 495      |
|    ep_rew_mean      | -0.0982  |
|    exploration_rate | 0.0701   |
| time/               |          |
|    episodes         | 116      |
|    fps              | 796      |
|    time_elapsed     | 72       |
|    total_timesteps  | 57945    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 5.33e-07 |
|    n_updates        | 11986    |
----------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 525      |
|    mean_reward      | -0.21    |
| rollout/            |          |
|    exploration_rate | 0.0701   |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.000436 |
|    loss             | 3.63e-06 |
|    n_updates        | 12499    |
----------------------------------
[I 2024-07-21 05:24:03,917] Trial 49 finished with value: -0.21000000000000002 and parameters: {'batch_size': 32, 'learning_rate': 0.00043550497868573214, 'buffer_size': 35628, 'gamma': 0.9737062615647186, 'exploration_fraction': 0.3425743884079979, 'exploration_final_eps': 0.07011794436164305}. Best is trial 23 with value: 0.029390000000000017.
DQN Best trial: 0.029390000000000017
DQN Best hyperparameters: {'batch_size': 64, 'learning_rate': 4.093510223780286e-05, 'buffer_size': 50519, 'gamma': 0.9645830155352708, 'exploration_fraction': 0.4376412612352858, 'exploration_final_eps': 0.09084847874639472}
