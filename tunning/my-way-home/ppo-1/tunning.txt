[I 2024-07-21 02:31:25,142] A new study created in memory with name: no-name-0acbbb28-32f5-4dbc-b46d-3036453b3e39
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 976      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 525         |
|    ep_rew_mean          | -0.21       |
| time/                   |             |
|    fps                  | 909         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.029749919 |
|    clip_fraction        | 0.0739      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -3.09       |
|    learning_rate        | 0.00195     |
|    loss                 | -0.0205     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00954    |
|    value_loss           | 0.0164      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 525      |
|    mean_reward          | -0.21    |
| time/                   |          |
|    total_timesteps      | 5000     |
| train/                  |          |
|    approx_kl            | 0.078752 |
|    clip_fraction        | 0.247    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.74    |
|    explained_variance   | -5.78    |
|    learning_rate        | 0.00195  |
|    loss                 | 0.0136   |
|    n_updates            | 20       |
|    policy_gradient_loss | 0.00399  |
|    value_loss           | 0.000367 |
--------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 611      |
|    iterations      | 3        |
|    time_elapsed    | 10       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 525         |
|    ep_rew_mean          | -0.21       |
| time/                   |             |
|    fps                  | 667         |
|    iterations           | 4           |
|    time_elapsed         | 12          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.038480747 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -5.75       |
|    learning_rate        | 0.00195     |
|    loss                 | -0.0405     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.000111    |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.076434284 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -26.4       |
|    learning_rate        | 0.00195     |
|    loss                 | -0.0876     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 0.000533    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.15    |
| time/              |          |
|    fps             | 576      |
|    iterations      | 5        |
|    time_elapsed    | 17       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 488        |
|    ep_rew_mean          | -0.115     |
| time/                   |            |
|    fps                  | 615        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.08187698 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | -0.073     |
|    learning_rate        | 0.00195    |
|    loss                 | -0.0212    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0252    |
|    value_loss           | 0.00237    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | -0.128     |
| time/                   |            |
|    fps                  | 646        |
|    iterations           | 7          |
|    time_elapsed         | 22         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.20703533 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.43      |
|    explained_variance   | -0.118     |
|    learning_rate        | 0.00195    |
|    loss                 | -0.0537    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.00317    |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.13062134 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | -8.19      |
|    learning_rate        | 0.00195    |
|    loss                 | -0.122     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.068     |
|    value_loss           | 0.000121   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.136   |
| time/              |          |
|    fps             | 592      |
|    iterations      | 8        |
|    time_elapsed    | 27       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 499        |
|    ep_rew_mean          | -0.144     |
| time/                   |            |
|    fps                  | 617        |
|    iterations           | 9          |
|    time_elapsed         | 29         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.13414323 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.59      |
|    explained_variance   | -6.81      |
|    learning_rate        | 0.00195    |
|    loss                 | -0.111     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 4.48e-05   |
----------------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.2797567 |
|    clip_fraction        | 0.574     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.34     |
|    explained_variance   | -12.2     |
|    learning_rate        | 0.00195   |
|    loss                 | -0.107    |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.0515   |
|    value_loss           | 0.000883  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.125   |
| time/              |          |
|    fps             | 579      |
|    iterations      | 10       |
|    time_elapsed    | 35       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 495        |
|    ep_rew_mean          | -0.109     |
| time/                   |            |
|    fps                  | 599        |
|    iterations           | 11         |
|    time_elapsed         | 37         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.21595725 |
|    clip_fraction        | 0.542      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | -0.0843    |
|    learning_rate        | 0.00195    |
|    loss                 | -0.023     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0483    |
|    value_loss           | 0.00186    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 498       |
|    ep_rew_mean          | -0.117    |
| time/                   |           |
|    fps                  | 617       |
|    iterations           | 12        |
|    time_elapsed         | 39        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.2862742 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.33     |
|    explained_variance   | -0.463    |
|    learning_rate        | 0.00195   |
|    loss                 | -0.111    |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0639   |
|    value_loss           | 0.00278   |
---------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.5302068 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.33     |
|    explained_variance   | -9.03     |
|    learning_rate        | 0.00195   |
|    loss                 | -0.112    |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.0646   |
|    value_loss           | 0.000512  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.105   |
| time/              |          |
|    fps             | 587      |
|    iterations      | 13       |
|    time_elapsed    | 45       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 489        |
|    ep_rew_mean          | -0.0921    |
| time/                   |            |
|    fps                  | 603        |
|    iterations           | 14         |
|    time_elapsed         | 47         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.26748377 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | -0.0823    |
|    learning_rate        | 0.00195    |
|    loss                 | -0.0851    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0416    |
|    value_loss           | 0.00243    |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.7257085 |
|    clip_fraction        | 0.604     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.33     |
|    explained_variance   | -0.0781   |
|    learning_rate        | 0.00195   |
|    loss                 | -0.125    |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0332   |
|    value_loss           | 0.00207   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0997  |
| time/              |          |
|    fps             | 579      |
|    iterations      | 15       |
|    time_elapsed    | 53       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 493        |
|    ep_rew_mean          | -0.106     |
| time/                   |            |
|    fps                  | 592        |
|    iterations           | 16         |
|    time_elapsed         | 55         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.24786812 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | -1.77      |
|    learning_rate        | 0.00195    |
|    loss                 | -0.0691    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0567    |
|    value_loss           | 0.000641   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 489       |
|    ep_rew_mean          | -0.0971   |
| time/                   |           |
|    fps                  | 605       |
|    iterations           | 17        |
|    time_elapsed         | 57        |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.3486449 |
|    clip_fraction        | 0.608     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.2      |
|    explained_variance   | -3.56     |
|    learning_rate        | 0.00195   |
|    loss                 | -0.0462   |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0393   |
|    value_loss           | 0.00182   |
---------------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.19668782 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.089      |
|    learning_rate        | 0.00195    |
|    loss                 | -0.0899    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0423    |
|    value_loss           | 0.000979   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.103   |
| time/              |          |
|    fps             | 585      |
|    iterations      | 18       |
|    time_elapsed    | 63       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 492        |
|    ep_rew_mean          | -0.107     |
| time/                   |            |
|    fps                  | 597        |
|    iterations           | 19         |
|    time_elapsed         | 65         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.49666348 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | -10.6      |
|    learning_rate        | 0.00195    |
|    loss                 | -0.108     |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0622    |
|    value_loss           | 0.000739   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.4091345 |
|    clip_fraction        | 0.685     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.36     |
|    explained_variance   | -7.19     |
|    learning_rate        | 0.00195   |
|    loss                 | -0.0909   |
|    n_updates            | 190       |
|    policy_gradient_loss | -0.0548   |
|    value_loss           | 0.000397  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | -0.112   |
| time/              |          |
|    fps             | 579      |
|    iterations      | 20       |
|    time_elapsed    | 70       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 495        |
|    ep_rew_mean          | -0.117     |
| time/                   |            |
|    fps                  | 590        |
|    iterations           | 21         |
|    time_elapsed         | 72         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.43986267 |
|    clip_fraction        | 0.591      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | -12.9      |
|    learning_rate        | 0.00195    |
|    loss                 | -0.075     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.064     |
|    value_loss           | 0.000704   |
----------------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.4708495 |
|    clip_fraction        | 0.657     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.36     |
|    explained_variance   | -17.7     |
|    learning_rate        | 0.00195   |
|    loss                 | -0.107    |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.0454   |
|    value_loss           | 0.000345  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0977  |
| time/              |          |
|    fps             | 575      |
|    iterations      | 22       |
|    time_elapsed    | 78       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 490       |
|    ep_rew_mean          | -0.0909   |
| time/                   |           |
|    fps                  | 585       |
|    iterations           | 23        |
|    time_elapsed         | 80        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.3462903 |
|    clip_fraction        | 0.624     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.34     |
|    explained_variance   | -0.0939   |
|    learning_rate        | 0.00195   |
|    loss                 | -0.0694   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0336   |
|    value_loss           | 0.00308   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 487       |
|    ep_rew_mean          | -0.0849   |
| time/                   |           |
|    fps                  | 593       |
|    iterations           | 24        |
|    time_elapsed         | 82        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.5603703 |
|    clip_fraction        | 0.599     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.95     |
|    explained_variance   | -0.24     |
|    learning_rate        | 0.00195   |
|    loss                 | -0.0853   |
|    n_updates            | 230       |
|    policy_gradient_loss | -0.0471   |
|    value_loss           | 0.00199   |
---------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.3967285 |
|    clip_fraction        | 0.651     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.29     |
|    explained_variance   | -0.982    |
|    learning_rate        | 0.00195   |
|    loss                 | -0.0616   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0571   |
|    value_loss           | 0.00249   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0732  |
| time/              |          |
|    fps             | 579      |
|    iterations      | 25       |
|    time_elapsed    | 88       |
|    total_timesteps | 51200    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 482       |
|    ep_rew_mean          | -0.0628   |
| time/                   |           |
|    fps                  | 588       |
|    iterations           | 26        |
|    time_elapsed         | 90        |
|    total_timesteps      | 53248     |
| train/                  |           |
|    approx_kl            | 0.7577515 |
|    clip_fraction        | 0.641     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.07     |
|    explained_variance   | -0.272    |
|    learning_rate        | 0.00195   |
|    loss                 | -0.099    |
|    n_updates            | 250       |
|    policy_gradient_loss | -0.0599   |
|    value_loss           | 0.00294   |
---------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.33851632 |
|    clip_fraction        | 0.654      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | -0.452     |
|    learning_rate        | 0.00195    |
|    loss                 | -0.0977    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.06      |
|    value_loss           | 0.00304    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0628  |
| time/              |          |
|    fps             | 575      |
|    iterations      | 27       |
|    time_elapsed    | 96       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 477       |
|    ep_rew_mean          | -0.0509   |
| time/                   |           |
|    fps                  | 583       |
|    iterations           | 28        |
|    time_elapsed         | 98        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.5431572 |
|    clip_fraction        | 0.557     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.989    |
|    explained_variance   | -6.43     |
|    learning_rate        | 0.00195   |
|    loss                 | -0.089    |
|    n_updates            | 270       |
|    policy_gradient_loss | -0.00754  |
|    value_loss           | 0.000868  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 482       |
|    ep_rew_mean          | -0.0628   |
| time/                   |           |
|    fps                  | 591       |
|    iterations           | 29        |
|    time_elapsed         | 100       |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 0.3907976 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.22     |
|    explained_variance   | -0.0179   |
|    learning_rate        | 0.00195   |
|    loss                 | -0.1      |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0382   |
|    value_loss           | 0.00204   |
---------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.8185276 |
|    clip_fraction        | 0.567     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.802    |
|    explained_variance   | -1.25     |
|    learning_rate        | 0.00195   |
|    loss                 | -0.112    |
|    n_updates            | 290       |
|    policy_gradient_loss | 0.0259    |
|    value_loss           | 0.000418  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0746  |
| time/              |          |
|    fps             | 580      |
|    iterations      | 30       |
|    time_elapsed    | 105      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:33:15,435] Trial 0 finished with value: -0.21000000000000002 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0019530125233979552, 'gamma': 0.9369061511476863, 'gae_lambda': 0.9443505870492341}. Best is trial 0 with value: -0.21000000000000002.
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 519      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 860      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01697971 |
|    clip_fraction        | 0.0489     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | -1.18      |
|    learning_rate        | 0.00704    |
|    loss                 | -0.0231    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0023    |
|    value_loss           | 0.0365     |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 522      |
|    ep_rew_mean     | -0.177   |
| time/              |          |
|    fps             | 690      |
|    iterations      | 2        |
|    time_elapsed    | 23       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011600427 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00704     |
|    loss                 | -0.00819    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 5.16e-06    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | -0.187   |
| time/              |          |
|    fps             | 707      |
|    iterations      | 3        |
|    time_elapsed    | 34       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011088507 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.00704     |
|    loss                 | 0.0117      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00113    |
|    value_loss           | 3.52e-06    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 518      |
|    ep_rew_mean     | -0.176   |
| time/              |          |
|    fps             | 667      |
|    iterations      | 4        |
|    time_elapsed    | 49       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.009093765 |
|    clip_fraction        | 0.0823      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00704     |
|    loss                 | 0.0192      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 0.000475    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 518      |
|    ep_rew_mean     | -0.169   |
| time/              |          |
|    fps             | 646      |
|    iterations      | 5        |
|    time_elapsed    | 63       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0060074567 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | 0            |
|    learning_rate        | 0.00704      |
|    loss                 | 0.0113       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.000208    |
|    value_loss           | 0.00047      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | -0.164   |
| time/              |          |
|    fps             | 660      |
|    iterations      | 6        |
|    time_elapsed    | 74       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009899851 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00704     |
|    loss                 | -0.0169     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00194    |
|    value_loss           | 0.000468    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.177   |
| time/              |          |
|    fps             | 646      |
|    iterations      | 7        |
|    time_elapsed    | 88       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.014915498 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.00704     |
|    loss                 | -0.0278     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00272    |
|    value_loss           | 1.19e-06    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | -0.165   |
| time/              |          |
|    fps             | 635      |
|    iterations      | 8        |
|    time_elapsed    | 103      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 02:35:03,501] Trial 1 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 0.007039689204177109, 'gamma': 0.9665140852030887, 'gae_lambda': 0.8836243992098828}. Best is trial 0 with value: -0.21000000000000002.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1299     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 525        |
|    ep_rew_mean          | -0.21      |
| time/                   |            |
|    fps                  | 957        |
|    iterations           | 2          |
|    time_elapsed         | 4          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.04151863 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | -5.21      |
|    learning_rate        | 0.000946   |
|    loss                 | -0.104     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0282    |
|    value_loss           | 0.00202    |
----------------------------------------
Eval num_timesteps=5000, episode_reward=-0.11 +/- 0.31
Episode length: 519.70 +/- 15.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 520       |
|    mean_reward          | -0.108    |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 0.0954957 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.72     |
|    explained_variance   | -0.996    |
|    learning_rate        | 0.000946  |
|    loss                 | -0.12     |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.0649   |
|    value_loss           | 0.000835  |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | -0.114   |
| time/              |          |
|    fps             | 600      |
|    iterations      | 3        |
|    time_elapsed    | 10       |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 516        |
|    ep_rew_mean          | -0.14      |
| time/                   |            |
|    fps                  | 633        |
|    iterations           | 4          |
|    time_elapsed         | 12         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.07355337 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.00726    |
|    learning_rate        | 0.000946   |
|    loss                 | -0.0395    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0429    |
|    value_loss           | 0.00409    |
----------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.14142996 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | -0.768     |
|    learning_rate        | 0.000946   |
|    loss                 | -0.0865    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0641    |
|    value_loss           | 0.000578   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 518      |
|    ep_rew_mean     | -0.155   |
| time/              |          |
|    fps             | 542      |
|    iterations      | 5        |
|    time_elapsed    | 18       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 503        |
|    ep_rew_mean          | -0.118     |
| time/                   |            |
|    fps                  | 571        |
|    iterations           | 6          |
|    time_elapsed         | 21         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.15165563 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | -1.44      |
|    learning_rate        | 0.000946   |
|    loss                 | -0.11      |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0735    |
|    value_loss           | 0.000303   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 501        |
|    ep_rew_mean          | -0.0933    |
| time/                   |            |
|    fps                  | 590        |
|    iterations           | 7          |
|    time_elapsed         | 24         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.15495403 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.59      |
|    explained_variance   | 0.00439    |
|    learning_rate        | 0.000946   |
|    loss                 | -0.0769    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0523    |
|    value_loss           | 0.00361    |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.13054189 |
|    clip_fraction        | 0.512      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.59      |
|    explained_variance   | -0.0209    |
|    learning_rate        | 0.000946   |
|    loss                 | -0.123     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0631    |
|    value_loss           | 0.00362    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.0754  |
| time/              |          |
|    fps             | 540      |
|    iterations      | 8        |
|    time_elapsed    | 30       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 489        |
|    ep_rew_mean          | -0.0606    |
| time/                   |            |
|    fps                  | 558        |
|    iterations           | 9          |
|    time_elapsed         | 33         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.25768888 |
|    clip_fraction        | 0.66       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.4       |
|    explained_variance   | -0.139     |
|    learning_rate        | 0.000946   |
|    loss                 | -0.0947    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0709    |
|    value_loss           | 0.00339    |
----------------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.24007061 |
|    clip_fraction        | 0.647      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.44      |
|    explained_variance   | -0.00286   |
|    learning_rate        | 0.000946   |
|    loss                 | -0.0673    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.074     |
|    value_loss           | 0.00376    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.0752  |
| time/              |          |
|    fps             | 524      |
|    iterations      | 10       |
|    time_elapsed    | 39       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 496       |
|    ep_rew_mean          | -0.0871   |
| time/                   |           |
|    fps                  | 539       |
|    iterations           | 11        |
|    time_elapsed         | 41        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.2407672 |
|    clip_fraction        | 0.637     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.5      |
|    explained_variance   | -0.319    |
|    learning_rate        | 0.000946  |
|    loss                 | -0.138    |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0881   |
|    value_loss           | 0.000629  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 498       |
|    ep_rew_mean          | -0.0972   |
| time/                   |           |
|    fps                  | 552       |
|    iterations           | 12        |
|    time_elapsed         | 44        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.2919248 |
|    clip_fraction        | 0.656     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.38     |
|    explained_variance   | -2.55     |
|    learning_rate        | 0.000946  |
|    loss                 | -0.116    |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0777   |
|    value_loss           | 0.000235  |
---------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.37147143 |
|    clip_fraction        | 0.693      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | -3.4       |
|    learning_rate        | 0.000946   |
|    loss                 | -0.121     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0749    |
|    value_loss           | 0.000142   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 527      |
|    iterations      | 13       |
|    time_elapsed    | 50       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 502        |
|    ep_rew_mean          | -0.113     |
| time/                   |            |
|    fps                  | 538        |
|    iterations           | 14         |
|    time_elapsed         | 53         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.29311174 |
|    clip_fraction        | 0.648      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.4       |
|    explained_variance   | -2.92      |
|    learning_rate        | 0.000946   |
|    loss                 | -0.111     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0719    |
|    value_loss           | 5.68e-05   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.35
Episode length: 480.00 +/- 135.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 480       |
|    mean_reward          | -0.092    |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.3857004 |
|    clip_fraction        | 0.693     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.35     |
|    explained_variance   | -6.36     |
|    learning_rate        | 0.000946  |
|    loss                 | -0.133    |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0785   |
|    value_loss           | 0.000358  |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 521      |
|    iterations      | 15       |
|    time_elapsed    | 58       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 504        |
|    ep_rew_mean          | -0.109     |
| time/                   |            |
|    fps                  | 531        |
|    iterations           | 16         |
|    time_elapsed         | 61         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.42712918 |
|    clip_fraction        | 0.697      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | -3.94      |
|    learning_rate        | 0.000946   |
|    loss                 | -0.148     |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0788    |
|    value_loss           | 0.000124   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 503        |
|    ep_rew_mean          | -0.0999    |
| time/                   |            |
|    fps                  | 541        |
|    iterations           | 17         |
|    time_elapsed         | 64         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.30893797 |
|    clip_fraction        | 0.651      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -0.0227    |
|    learning_rate        | 0.000946   |
|    loss                 | -0.0968    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0495    |
|    value_loss           | 0.0028     |
----------------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.2908639 |
|    clip_fraction        | 0.673     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.49     |
|    explained_variance   | 0.0377    |
|    learning_rate        | 0.000946  |
|    loss                 | -0.0982   |
|    n_updates            | 170       |
|    policy_gradient_loss | -0.0754   |
|    value_loss           | 0.00284   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.0913  |
| time/              |          |
|    fps             | 524      |
|    iterations      | 18       |
|    time_elapsed    | 70       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 503        |
|    ep_rew_mean          | -0.0974    |
| time/                   |            |
|    fps                  | 532        |
|    iterations           | 19         |
|    time_elapsed         | 73         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.47620225 |
|    clip_fraction        | 0.706      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.169      |
|    learning_rate        | 0.000946   |
|    loss                 | -0.0687    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.07      |
|    value_loss           | 0.00232    |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.2616092 |
|    clip_fraction        | 0.678     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.37     |
|    explained_variance   | -1.88     |
|    learning_rate        | 0.000946  |
|    loss                 | -0.134    |
|    n_updates            | 190       |
|    policy_gradient_loss | -0.0828   |
|    value_loss           | 0.000517  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.103   |
| time/              |          |
|    fps             | 518      |
|    iterations      | 20       |
|    time_elapsed    | 78       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 505        |
|    ep_rew_mean          | -0.108     |
| time/                   |            |
|    fps                  | 527        |
|    iterations           | 21         |
|    time_elapsed         | 81         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.26637894 |
|    clip_fraction        | 0.663      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.44      |
|    explained_variance   | -0.884     |
|    learning_rate        | 0.000946   |
|    loss                 | -0.123     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.089     |
|    value_loss           | 9.65e-05   |
----------------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.31941205 |
|    clip_fraction        | 0.629      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | -0.267     |
|    learning_rate        | 0.000946   |
|    loss                 | -0.119     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0776    |
|    value_loss           | 0.000512   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.112   |
| time/              |          |
|    fps             | 514      |
|    iterations      | 22       |
|    time_elapsed    | 87       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 507        |
|    ep_rew_mean          | -0.116     |
| time/                   |            |
|    fps                  | 522        |
|    iterations           | 23         |
|    time_elapsed         | 90         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.40619367 |
|    clip_fraction        | 0.699      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | -2.64      |
|    learning_rate        | 0.000946   |
|    loss                 | -0.144     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0842    |
|    value_loss           | 0.000142   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 508        |
|    ep_rew_mean          | -0.12      |
| time/                   |            |
|    fps                  | 529        |
|    iterations           | 24         |
|    time_elapsed         | 92         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.38789862 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.889     |
|    explained_variance   | -3.2       |
|    learning_rate        | 0.000946   |
|    loss                 | -0.0581    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0459    |
|    value_loss           | 0.00122    |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.3701694 |
|    clip_fraction        | 0.742     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.15     |
|    explained_variance   | 0.277     |
|    learning_rate        | 0.000946  |
|    loss                 | -0.092    |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0846   |
|    value_loss           | 8.03e-05  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.112   |
| time/              |          |
|    fps             | 518      |
|    iterations      | 25       |
|    time_elapsed    | 98       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 502        |
|    ep_rew_mean          | -0.101     |
| time/                   |            |
|    fps                  | 524        |
|    iterations           | 26         |
|    time_elapsed         | 101        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.19015907 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | -0.00204   |
|    learning_rate        | 0.000946   |
|    loss                 | -0.0847    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.00302    |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.37879264 |
|    clip_fraction        | 0.68       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | -0.404     |
|    learning_rate        | 0.000946   |
|    loss                 | -0.0923    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0786    |
|    value_loss           | 0.00323    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.111   |
| time/              |          |
|    fps             | 514      |
|    iterations      | 27       |
|    time_elapsed    | 107      |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 503        |
|    ep_rew_mean          | -0.111     |
| time/                   |            |
|    fps                  | 519        |
|    iterations           | 28         |
|    time_elapsed         | 110        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.41050422 |
|    clip_fraction        | 0.705      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | -5.71      |
|    learning_rate        | 0.000946   |
|    loss                 | -0.106     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0819    |
|    value_loss           | 0.000467   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 503        |
|    ep_rew_mean          | -0.111     |
| time/                   |            |
|    fps                  | 525        |
|    iterations           | 29         |
|    time_elapsed         | 113        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.42766154 |
|    clip_fraction        | 0.715      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | -7.16      |
|    learning_rate        | 0.000946   |
|    loss                 | -0.142     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0903    |
|    value_loss           | 0.000297   |
----------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.23317689 |
|    clip_fraction        | 0.632      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.46      |
|    explained_variance   | -3.91      |
|    learning_rate        | 0.000946   |
|    loss                 | -0.13      |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0923    |
|    value_loss           | 1.5e-05    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | -0.123   |
| time/              |          |
|    fps             | 516      |
|    iterations      | 30       |
|    time_elapsed    | 118      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:37:07,184] Trial 2 finished with value: -0.21000000000000002 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 0.0009457851692309154, 'gamma': 0.9900917798512573, 'gae_lambda': 0.9299464958451094}. Best is trial 0 with value: -0.21000000000000002.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1292     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011189207 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.181       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0362     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.00873     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 796      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009317712 |
|    clip_fraction        | 0.097       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0145     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.0041      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 519      |
|    ep_rew_mean     | -0.164   |
| time/              |          |
|    fps             | 707      |
|    iterations      | 3        |
|    time_elapsed    | 17       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.09 +/- 0.36
Episode length: 473.60 +/- 154.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0894     |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.015200016 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.000155    |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0324     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.00299     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -0.139   |
| time/              |          |
|    fps             | 676      |
|    iterations      | 4        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011818193 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.213       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0367     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 0.00227     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.154   |
| time/              |          |
|    fps             | 653      |
|    iterations      | 5        |
|    time_elapsed    | 31       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 495         |
|    ep_rew_mean          | -0.116      |
| time/                   |             |
|    fps                  | 698         |
|    iterations           | 6           |
|    time_elapsed         | 35          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.016925031 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -0.502      |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0405     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.00157     |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015791772 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.793      |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0443     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.00254     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | -0.111   |
| time/              |          |
|    fps             | 676      |
|    iterations      | 7        |
|    time_elapsed    | 42       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.016796663 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -1.29       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0644     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0398     |
|    value_loss           | 0.00165     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0747  |
| time/              |          |
|    fps             | 661      |
|    iterations      | 8        |
|    time_elapsed    | 49       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.36
Episode length: 475.00 +/- 150.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 475         |
|    mean_reward          | -0.09       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.022122312 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | -0.664      |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0457     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.00282     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0755  |
| time/              |          |
|    fps             | 654      |
|    iterations      | 9        |
|    time_elapsed    | 56       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.020807697 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | -1.61       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0517     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0419     |
|    value_loss           | 0.00187     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.0869  |
| time/              |          |
|    fps             | 645      |
|    iterations      | 10       |
|    time_elapsed    | 63       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=0.03 +/- 0.48
Episode length: 423.00 +/- 204.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 423         |
|    mean_reward          | 0.0308      |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.020444192 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | -7.58       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0542     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.041      |
|    value_loss           | 0.001       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.0866  |
| time/              |          |
|    fps             | 643      |
|    iterations      | 11       |
|    time_elapsed    | 69       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 487        |
|    ep_rew_mean          | -0.0846    |
| time/                   |            |
|    fps                  | 665        |
|    iterations           | 12         |
|    time_elapsed         | 73         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.02017102 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | -1.54      |
|    learning_rate        | 0.000183   |
|    loss                 | -0.0356    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.00153    |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.021605134 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | -1.57       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0821     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0426     |
|    value_loss           | 0.00158     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.0846  |
| time/              |          |
|    fps             | 657      |
|    iterations      | 13       |
|    time_elapsed    | 80       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.026292438 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | -8.02       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0729     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0458     |
|    value_loss           | 0.0012      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0745  |
| time/              |          |
|    fps             | 651      |
|    iterations      | 14       |
|    time_elapsed    | 88       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.02408291 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | -1.02      |
|    learning_rate        | 0.000183   |
|    loss                 | -0.063     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0429    |
|    value_loss           | 0.00124    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0732  |
| time/              |          |
|    fps             | 645      |
|    iterations      | 15       |
|    time_elapsed    | 95       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:38:46,523] Trial 3 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.00018339123262596456, 'gamma': 0.9137676386278932, 'gae_lambda': 0.8333735532509762}. Best is trial 0 with value: -0.21000000000000002.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 454      |
|    ep_rew_mean     | 0.0405   |
| time/              |          |
|    fps             | 1285     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.00375768 |
|    clip_fraction        | 0.0155     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.79      |
|    explained_variance   | -1.23      |
|    learning_rate        | 2.69e-05   |
|    loss                 | -0.00119   |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00403   |
|    value_loss           | 0.00535    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0691  |
| time/              |          |
|    fps             | 754      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.007241021 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.461       |
|    learning_rate        | 2.69e-05    |
|    loss                 | 0.0164      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 0.00171     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 466      |
|    ep_rew_mean     | 0.0445   |
| time/              |          |
|    fps             | 663      |
|    iterations      | 3        |
|    time_elapsed    | 18       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.006060854 |
|    clip_fraction        | 0.0623      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.386      |
|    learning_rate        | 2.69e-05    |
|    loss                 | -0.027      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0067     |
|    value_loss           | 0.00432     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | -0.0154  |
| time/              |          |
|    fps             | 626      |
|    iterations      | 4        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0047077937 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.115        |
|    learning_rate        | 2.69e-05     |
|    loss                 | -0.00843     |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0061      |
|    value_loss           | 0.00173      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 453      |
|    ep_rew_mean     | 0.0633   |
| time/              |          |
|    fps             | 605      |
|    iterations      | 5        |
|    time_elapsed    | 33       |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 449          |
|    ep_rew_mean          | 0.0612       |
| time/                   |              |
|    fps                  | 642          |
|    iterations           | 6            |
|    time_elapsed         | 38           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0071844887 |
|    clip_fraction        | 0.0667       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.13         |
|    learning_rate        | 2.69e-05     |
|    loss                 | -0.00322     |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.005       |
|    value_loss           | 0.0043       |
------------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0073470687 |
|    clip_fraction        | 0.0517       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.0405       |
|    learning_rate        | 2.69e-05     |
|    loss                 | -0.014       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00629     |
|    value_loss           | 0.00287      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 450      |
|    ep_rew_mean     | 0.0741   |
| time/              |          |
|    fps             | 624      |
|    iterations      | 7        |
|    time_elapsed    | 45       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0056509767 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | -0.511       |
|    learning_rate        | 2.69e-05     |
|    loss                 | -0.0041      |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.0059      |
|    value_loss           | 0.00463      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 455      |
|    ep_rew_mean     | 0.0543   |
| time/              |          |
|    fps             | 610      |
|    iterations      | 8        |
|    time_elapsed    | 53       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.006095327 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -0.427      |
|    learning_rate        | 2.69e-05    |
|    loss                 | -0.0158     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00833    |
|    value_loss           | 0.00238     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 454      |
|    ep_rew_mean     | 0.0532   |
| time/              |          |
|    fps             | 601      |
|    iterations      | 9        |
|    time_elapsed    | 61       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0059426045 |
|    clip_fraction        | 0.0452       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | -0.159       |
|    learning_rate        | 2.69e-05     |
|    loss                 | 0.00204      |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00688     |
|    value_loss           | 0.00304      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 459      |
|    ep_rew_mean     | 0.0411   |
| time/              |          |
|    fps             | 592      |
|    iterations      | 10       |
|    time_elapsed    | 69       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.007445113 |
|    clip_fraction        | 0.057       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -0.613      |
|    learning_rate        | 2.69e-05    |
|    loss                 | -0.0404     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00788    |
|    value_loss           | 0.00256     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 463      |
|    ep_rew_mean     | 0.0418   |
| time/              |          |
|    fps             | 586      |
|    iterations      | 11       |
|    time_elapsed    | 76       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 464          |
|    ep_rew_mean          | 0.0343       |
| time/                   |              |
|    fps                  | 605          |
|    iterations           | 12           |
|    time_elapsed         | 81           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0053979717 |
|    clip_fraction        | 0.0577       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.036        |
|    learning_rate        | 2.69e-05     |
|    loss                 | -0.00844     |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00856     |
|    value_loss           | 0.00278      |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008308095 |
|    clip_fraction        | 0.0795      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -0.334      |
|    learning_rate        | 2.69e-05    |
|    loss                 | -0.00474    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00998    |
|    value_loss           | 0.00216     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 462      |
|    ep_rew_mean     | 0.0354   |
| time/              |          |
|    fps             | 599      |
|    iterations      | 13       |
|    time_elapsed    | 88       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.004033397 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -0.8        |
|    learning_rate        | 2.69e-05    |
|    loss                 | 0.00745     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00859    |
|    value_loss           | 0.0028      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 466      |
|    ep_rew_mean     | 0.0136   |
| time/              |          |
|    fps             | 594      |
|    iterations      | 14       |
|    time_elapsed    | 96       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0064150332 |
|    clip_fraction        | 0.0455       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | -0.197       |
|    learning_rate        | 2.69e-05     |
|    loss                 | -0.0465      |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00951     |
|    value_loss           | 0.00293      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 457      |
|    ep_rew_mean     | 0.047    |
| time/              |          |
|    fps             | 589      |
|    iterations      | 15       |
|    time_elapsed    | 104      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:40:35,518] Trial 4 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 2.6904982254406317e-05, 'gamma': 0.9877380570444704, 'gae_lambda': 0.8860278026952497}. Best is trial 0 with value: -0.21000000000000002.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.0781  |
| time/              |          |
|    fps             | 1283     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012993689 |
|    clip_fraction        | 0.0814      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.331      |
|    learning_rate        | 0.00855     |
|    loss                 | -0.0176     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00484    |
|    value_loss           | 0.0625      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 457      |
|    ep_rew_mean     | 0.0527   |
| time/              |          |
|    fps             | 759      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.017332759 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00855     |
|    loss                 | -0.0188     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00781    |
|    value_loss           | 0.00151     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 454      |
|    ep_rew_mean     | 0.0408   |
| time/              |          |
|    fps             | 664      |
|    iterations      | 3        |
|    time_elapsed    | 18       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.010031574 |
|    clip_fraction        | 0.074       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00855     |
|    loss                 | 0.015       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 0.00103     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | -0.0108  |
| time/              |          |
|    fps             | 626      |
|    iterations      | 4        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012397118 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00855     |
|    loss                 | 0.0169      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 3.18e-06    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | -0.0488  |
| time/              |          |
|    fps             | 606      |
|    iterations      | 5        |
|    time_elapsed    | 33       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 478         |
|    ep_rew_mean          | -0.0537     |
| time/                   |             |
|    fps                  | 643         |
|    iterations           | 6           |
|    time_elapsed         | 38          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.013485275 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00855     |
|    loss                 | 0.00993     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0034     |
|    value_loss           | 1.25e-06    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.36
Episode length: 473.60 +/- 154.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0894     |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.016784746 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00855     |
|    loss                 | -0.0024     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 0.000545    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0749  |
| time/              |          |
|    fps             | 630      |
|    iterations      | 7        |
|    time_elapsed    | 45       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.018435787 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00855     |
|    loss                 | 6.94e-05    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00353    |
|    value_loss           | 1.06e-07    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.091   |
| time/              |          |
|    fps             | 615      |
|    iterations      | 8        |
|    time_elapsed    | 53       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0079448335 |
|    clip_fraction        | 0.0804       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.00855      |
|    loss                 | 0.00743      |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00294     |
|    value_loss           | 3.78e-08     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0744  |
| time/              |          |
|    fps             | 605      |
|    iterations      | 9        |
|    time_elapsed    | 60       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.016159974 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.00855     |
|    loss                 | -0.0217     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00635    |
|    value_loss           | 0.00111     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | -0.0609  |
| time/              |          |
|    fps             | 597      |
|    iterations      | 10       |
|    time_elapsed    | 68       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.013342037 |
|    clip_fraction        | 0.0696      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00855     |
|    loss                 | -0.0137     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 0.00109     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | -0.0633  |
| time/              |          |
|    fps             | 591      |
|    iterations      | 11       |
|    time_elapsed    | 76       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 477          |
|    ep_rew_mean          | -0.0707      |
| time/                   |              |
|    fps                  | 609          |
|    iterations           | 12           |
|    time_elapsed         | 80           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0069702053 |
|    clip_fraction        | 0.0779       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.44        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.00855      |
|    loss                 | -0.0242      |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00227     |
|    value_loss           | 0.000551     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.00892007 |
|    clip_fraction        | 0.055      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0          |
|    learning_rate        | 0.00855    |
|    loss                 | 0.0145     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.00127   |
|    value_loss           | 0.000552   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | -0.081   |
| time/              |          |
|    fps             | 602      |
|    iterations      | 13       |
|    time_elapsed    | 88       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.010761162 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00855     |
|    loss                 | 0.0187      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 0.000554    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | -0.0922  |
| time/              |          |
|    fps             | 596      |
|    iterations      | 14       |
|    time_elapsed    | 96       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.018991884 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00855     |
|    loss                 | -0.0146     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 0.000555    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.104   |
| time/              |          |
|    fps             | 592      |
|    iterations      | 15       |
|    time_elapsed    | 103      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:42:23,708] Trial 5 finished with value: -0.08930999999999999 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.008550634509002368, 'gamma': 0.9104259032323191, 'gae_lambda': 0.8143358107423844}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 471      |
|    ep_rew_mean     | -0.0632  |
| time/              |          |
|    fps             | 1288     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012630375 |
|    clip_fraction        | 0.0379      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -1.71       |
|    learning_rate        | 0.00565     |
|    loss                 | 0.00979     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 0.172       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.137   |
| time/              |          |
|    fps             | 689      |
|    iterations      | 2        |
|    time_elapsed    | 11       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.006996497 |
|    clip_fraction        | 0.0358      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00565     |
|    loss                 | 0.0276      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000844   |
|    value_loss           | 2.37e-05    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.071   |
| time/              |          |
|    fps             | 594      |
|    iterations      | 3        |
|    time_elapsed    | 20       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.010001813 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.00565     |
|    loss                 | -0.0291     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 0.00216     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.0742  |
| time/              |          |
|    fps             | 556      |
|    iterations      | 4        |
|    time_elapsed    | 29       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.01711876 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.00565    |
|    loss                 | 0.0335     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00449   |
|    value_loss           | 0.0011     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0749  |
| time/              |          |
|    fps             | 536      |
|    iterations      | 5        |
|    time_elapsed    | 38       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 491         |
|    ep_rew_mean          | -0.0765     |
| time/                   |             |
|    fps                  | 562         |
|    iterations           | 6           |
|    time_elapsed         | 43          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.008355755 |
|    clip_fraction        | 0.0497      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.00565     |
|    loss                 | -0.00999    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00152    |
|    value_loss           | 0.00111     |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010284488 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00565     |
|    loss                 | -0.0149     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00398    |
|    value_loss           | 0.00111     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.0752  |
| time/              |          |
|    fps             | 546      |
|    iterations      | 7        |
|    time_elapsed    | 52       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.011424297 |
|    clip_fraction        | 0.0879      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00565     |
|    loss                 | 0.00393     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 0.00111     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | -0.0915  |
| time/              |          |
|    fps             | 534      |
|    iterations      | 8        |
|    time_elapsed    | 61       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014791307 |
|    clip_fraction        | 0.0489      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00565     |
|    loss                 | 0.00451     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00191    |
|    value_loss           | 2.16e-07    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.0885  |
| time/              |          |
|    fps             | 525      |
|    iterations      | 9        |
|    time_elapsed    | 70       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.00625434 |
|    clip_fraction        | 0.0232     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | 0          |
|    learning_rate        | 0.00565    |
|    loss                 | 0.0276     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.000101  |
|    value_loss           | 0.00112    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.0874  |
| time/              |          |
|    fps             | 518      |
|    iterations      | 10       |
|    time_elapsed    | 78       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.011720686 |
|    clip_fraction        | 0.063       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00565     |
|    loss                 | 0.0218      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 0.00112     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.0982  |
| time/              |          |
|    fps             | 514      |
|    iterations      | 11       |
|    time_elapsed    | 87       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 495        |
|    ep_rew_mean          | -0.107     |
| time/                   |            |
|    fps                  | 527        |
|    iterations           | 12         |
|    time_elapsed         | 93         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.01691844 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.66      |
|    explained_variance   | 0          |
|    learning_rate        | 0.00565    |
|    loss                 | -0.0259    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00513   |
|    value_loss           | 5.43e-07   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010633184 |
|    clip_fraction        | 0.0453      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.00565     |
|    loss                 | 0.0168      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0017     |
|    value_loss           | 3.02e-07    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.0855  |
| time/              |          |
|    fps             | 522      |
|    iterations      | 13       |
|    time_elapsed    | 101      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.017614331 |
|    clip_fraction        | 0.0675      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0           |
|    learning_rate        | 0.00565     |
|    loss                 | 0.0286      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 0.00333     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0743  |
| time/              |          |
|    fps             | 517      |
|    iterations      | 14       |
|    time_elapsed    | 110      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.010640806 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00565     |
|    loss                 | -0.0191     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.000352   |
|    value_loss           | 0.00112     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0743  |
| time/              |          |
|    fps             | 514      |
|    iterations      | 15       |
|    time_elapsed    | 119      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:44:29,023] Trial 6 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.005646327706485301, 'gamma': 0.9321699219071415, 'gae_lambda': 0.9459707073886814}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.069   |
| time/              |          |
|    fps             | 1277     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.09 +/- 0.36
Episode length: 474.70 +/- 150.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 475         |
|    mean_reward          | -0.0898     |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.027336469 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -1.54       |
|    learning_rate        | 0.000633    |
|    loss                 | -0.0497     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.00357     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 481      |
|    ep_rew_mean     | -0.0747  |
| time/              |          |
|    fps             | 777      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.05623127 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | -0.811     |
|    learning_rate        | 0.000633   |
|    loss                 | -0.102     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.00213    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.0718  |
| time/              |          |
|    fps             | 675      |
|    iterations      | 3        |
|    time_elapsed    | 18       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.057993468 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -0.228      |
|    learning_rate        | 0.000633    |
|    loss                 | -0.0935     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.048      |
|    value_loss           | 0.00173     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | -0.0437  |
| time/              |          |
|    fps             | 634      |
|    iterations      | 4        |
|    time_elapsed    | 25       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.06040734 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | 0.017      |
|    learning_rate        | 0.000633   |
|    loss                 | -0.087     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0486    |
|    value_loss           | 0.00301    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0754  |
| time/              |          |
|    fps             | 611      |
|    iterations      | 5        |
|    time_elapsed    | 33       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 492       |
|    ep_rew_mean          | -0.0946   |
| time/                   |           |
|    fps                  | 649       |
|    iterations           | 6         |
|    time_elapsed         | 37        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.0783062 |
|    clip_fraction        | 0.428     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.69     |
|    explained_variance   | -4.47     |
|    learning_rate        | 0.000633  |
|    loss                 | -0.125    |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0728   |
|    value_loss           | 0.000216  |
---------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.109907836 |
|    clip_fraction        | 0.502       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | -9.19       |
|    learning_rate        | 0.000633    |
|    loss                 | -0.0997     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0737     |
|    value_loss           | 7.69e-05    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | -0.0748  |
| time/              |          |
|    fps             | 629      |
|    iterations      | 7        |
|    time_elapsed    | 45       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.11812587 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.53      |
|    explained_variance   | 0.0152     |
|    learning_rate        | 0.000633   |
|    loss                 | -0.0477    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0443    |
|    value_loss           | 0.00278    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.0603  |
| time/              |          |
|    fps             | 615      |
|    iterations      | 8        |
|    time_elapsed    | 53       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.102004185 |
|    clip_fraction        | 0.534       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.122       |
|    learning_rate        | 0.000633    |
|    loss                 | -0.0907     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0628     |
|    value_loss           | 0.0025      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0762  |
| time/              |          |
|    fps             | 605      |
|    iterations      | 9        |
|    time_elapsed    | 60       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.15924157 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.49      |
|    explained_variance   | -21.6      |
|    learning_rate        | 0.000633   |
|    loss                 | -0.0825    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0687    |
|    value_loss           | 0.000188   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.0767  |
| time/              |          |
|    fps             | 596      |
|    iterations      | 10       |
|    time_elapsed    | 68       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.11196752 |
|    clip_fraction        | 0.533      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.52      |
|    explained_variance   | 0.271      |
|    learning_rate        | 0.000633   |
|    loss                 | -0.102     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.000923   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.0757  |
| time/              |          |
|    fps             | 591      |
|    iterations      | 11       |
|    time_elapsed    | 76       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 489        |
|    ep_rew_mean          | -0.0657    |
| time/                   |            |
|    fps                  | 610        |
|    iterations           | 12         |
|    time_elapsed         | 80         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.11492153 |
|    clip_fraction        | 0.562      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.000633   |
|    loss                 | -0.115     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.063     |
|    value_loss           | 0.00067    |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.09606299 |
|    clip_fraction        | 0.508      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | 0.216      |
|    learning_rate        | 0.000633   |
|    loss                 | -0.0977    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.00205    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.0769  |
| time/              |          |
|    fps             | 604      |
|    iterations      | 13       |
|    time_elapsed    | 88       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.09 +/- 0.36
Episode length: 478.70 +/- 138.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 479        |
|    mean_reward          | -0.0914    |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.16979602 |
|    clip_fraction        | 0.617      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.51      |
|    explained_variance   | -47.1      |
|    learning_rate        | 0.000633   |
|    loss                 | -0.102     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0859    |
|    value_loss           | 0.000108   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.0785  |
| time/              |          |
|    fps             | 600      |
|    iterations      | 14       |
|    time_elapsed    | 95       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.17084235 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.47      |
|    explained_variance   | 0.0413     |
|    learning_rate        | 0.000633   |
|    loss                 | -0.0399    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.00108    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.0886  |
| time/              |          |
|    fps             | 595      |
|    iterations      | 15       |
|    time_elapsed    | 103      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:46:17,001] Trial 7 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.0006333826690538538, 'gamma': 0.9612259588237023, 'gae_lambda': 0.965041152245923}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0713  |
| time/              |          |
|    fps             | 856      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.004688913 |
|    clip_fraction        | 0.0152      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.352       |
|    learning_rate        | 1.39e-05    |
|    loss                 | -0.00328    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 0.00456     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.0747  |
| time/              |          |
|    fps             | 659      |
|    iterations      | 2        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.006240597 |
|    clip_fraction        | 0.0466      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.182       |
|    learning_rate        | 1.39e-05    |
|    loss                 | -0.0166     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 0.00274     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.0765  |
| time/              |          |
|    fps             | 666      |
|    iterations      | 3        |
|    time_elapsed    | 36       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.00872277 |
|    clip_fraction        | 0.0471     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0.332      |
|    learning_rate        | 1.39e-05   |
|    loss                 | 0.00736    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.00242   |
|    value_loss           | 0.0019     |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.0757  |
| time/              |          |
|    fps             | 628      |
|    iterations      | 4        |
|    time_elapsed    | 52       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0049267374 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | -0.0535      |
|    learning_rate        | 1.39e-05     |
|    loss                 | -0.0212      |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00353     |
|    value_loss           | 0.00162      |
------------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.0644  |
| time/              |          |
|    fps             | 606      |
|    iterations      | 5        |
|    time_elapsed    | 67       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0046740198 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | -0.413       |
|    learning_rate        | 1.39e-05     |
|    loss                 | -0.00754     |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00308     |
|    value_loss           | 0.0019       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.0552  |
| time/              |          |
|    fps             | 617      |
|    iterations      | 6        |
|    time_elapsed    | 79       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.005536665 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.42       |
|    learning_rate        | 1.39e-05    |
|    loss                 | 0.0126      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 0.00171     |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0443  |
| time/              |          |
|    fps             | 603      |
|    iterations      | 7        |
|    time_elapsed    | 94       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0034644199 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | -0.474       |
|    learning_rate        | 1.39e-05     |
|    loss                 | -0.00506     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00294     |
|    value_loss           | 0.00169      |
------------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.044   |
| time/              |          |
|    fps             | 594      |
|    iterations      | 8        |
|    time_elapsed    | 110      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 02:48:13,216] Trial 8 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 1.3881528961986286e-05, 'gamma': 0.9417622539139561, 'gae_lambda': 0.861570271618159}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | 0.0902   |
| time/              |          |
|    fps             | 1290     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 428         |
|    ep_rew_mean          | 0.0512      |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.006524099 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.00563    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.00583     |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011537947 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.407       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0229     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.00745     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 453      |
|    ep_rew_mean     | 0.0494   |
| time/              |          |
|    fps             | 595      |
|    iterations      | 3        |
|    time_elapsed    | 10       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 470         |
|    ep_rew_mean          | -0.0116     |
| time/                   |             |
|    fps                  | 626         |
|    iterations           | 4           |
|    time_elapsed         | 13          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.016614398 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0567     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0332     |
|    value_loss           | 0.00544     |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.015431313 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.148       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.056      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 0.00263     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 481      |
|    ep_rew_mean     | -0.0494  |
| time/              |          |
|    fps             | 533      |
|    iterations      | 5        |
|    time_elapsed    | 19       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 483        |
|    ep_rew_mean          | -0.0333    |
| time/                   |            |
|    fps                  | 559        |
|    iterations           | 6          |
|    time_elapsed         | 21         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01803597 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | 0.181      |
|    learning_rate        | 0.000134   |
|    loss                 | -0.0687    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0406    |
|    value_loss           | 0.00232    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 489         |
|    ep_rew_mean          | -0.0577     |
| time/                   |             |
|    fps                  | 581         |
|    iterations           | 7           |
|    time_elapsed         | 24          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.021876564 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.225       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0897     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0398     |
|    value_loss           | 0.00548     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.023322139 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.391      |
|    learning_rate        | 0.000134    |
|    loss                 | -0.075      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0494     |
|    value_loss           | 0.00252     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.0454  |
| time/              |          |
|    fps             | 532      |
|    iterations      | 8        |
|    time_elapsed    | 30       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 494         |
|    ep_rew_mean          | -0.0356     |
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 9           |
|    time_elapsed         | 33          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.028561406 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.0757      |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0889     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0501     |
|    value_loss           | 0.00549     |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.03381283 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.6       |
|    explained_variance   | -0.426     |
|    learning_rate        | 0.000134   |
|    loss                 | -0.0903    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0504    |
|    value_loss           | 0.00679    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.0526  |
| time/              |          |
|    fps             | 518      |
|    iterations      | 10       |
|    time_elapsed    | 39       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | -0.0666     |
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 11          |
|    time_elapsed         | 42          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.024471536 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | -0.575      |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0835     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0456     |
|    value_loss           | 0.0017      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 493         |
|    ep_rew_mean          | -0.0338     |
| time/                   |             |
|    fps                  | 548         |
|    iterations           | 12          |
|    time_elapsed         | 44          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.034700945 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | -0.975      |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0793     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0511     |
|    value_loss           | 0.00179     |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.028917437 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.069       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0743     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0488     |
|    value_loss           | 0.00685     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.0471  |
| time/              |          |
|    fps             | 525      |
|    iterations      | 13       |
|    time_elapsed    | 50       |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | -0.0586   |
| time/                   |           |
|    fps                  | 537       |
|    iterations           | 14        |
|    time_elapsed         | 53        |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.0307229 |
|    clip_fraction        | 0.289     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.65     |
|    explained_variance   | -3.1      |
|    learning_rate        | 0.000134  |
|    loss                 | -0.108    |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.0592   |
|    value_loss           | 0.00204   |
---------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.037015624 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | -3.43       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.118      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.062      |
|    value_loss           | 0.00142     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.0526  |
| time/              |          |
|    fps             | 518      |
|    iterations      | 15       |
|    time_elapsed    | 59       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 496         |
|    ep_rew_mean          | -0.0622     |
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 16          |
|    time_elapsed         | 61          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.034941107 |
|    clip_fraction        | 0.293       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | -0.187      |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0619     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0469     |
|    value_loss           | 0.00499     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 493         |
|    ep_rew_mean          | -0.0542     |
| time/                   |             |
|    fps                  | 538         |
|    iterations           | 17          |
|    time_elapsed         | 64          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.029274758 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | -1.33       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0386     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0405     |
|    value_loss           | 0.000658    |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.36
Episode length: 475.80 +/- 147.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | -0.0903     |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.046911016 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | -0.387      |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0979     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0474     |
|    value_loss           | 0.00486     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.0489  |
| time/              |          |
|    fps             | 524      |
|    iterations      | 18       |
|    time_elapsed    | 70       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 491        |
|    ep_rew_mean          | -0.0571    |
| time/                   |            |
|    fps                  | 532        |
|    iterations           | 19         |
|    time_elapsed         | 73         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.04907828 |
|    clip_fraction        | 0.36       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.44      |
|    explained_variance   | -0.329     |
|    learning_rate        | 0.000134   |
|    loss                 | -0.0836    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0593    |
|    value_loss           | 0.00338    |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.042942546 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | -4.07       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0975     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0602     |
|    value_loss           | 0.00119     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.0644  |
| time/              |          |
|    fps             | 518      |
|    iterations      | 20       |
|    time_elapsed    | 78       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 494        |
|    ep_rew_mean          | -0.0711    |
| time/                   |            |
|    fps                  | 526        |
|    iterations           | 21         |
|    time_elapsed         | 81         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.04049577 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.53      |
|    explained_variance   | -1.9       |
|    learning_rate        | 0.000134   |
|    loss                 | -0.109     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0601    |
|    value_loss           | 0.000678   |
----------------------------------------
Eval num_timesteps=45000, episode_reward=-0.10 +/- 0.33
Episode length: 496.40 +/- 85.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 496         |
|    mean_reward          | -0.0985     |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.048422366 |
|    clip_fraction        | 0.376       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | -1.18       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0984     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0662     |
|    value_loss           | 0.000531    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.0642  |
| time/              |          |
|    fps             | 514      |
|    iterations      | 22       |
|    time_elapsed    | 87       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 492         |
|    ep_rew_mean          | -0.0703     |
| time/                   |             |
|    fps                  | 521         |
|    iterations           | 23          |
|    time_elapsed         | 90          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.046767164 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | -0.121      |
|    learning_rate        | 0.000134    |
|    loss                 | -0.097      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0549     |
|    value_loss           | 0.00388     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 485         |
|    ep_rew_mean          | -0.054      |
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 24          |
|    time_elapsed         | 92          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.052295122 |
|    clip_fraction        | 0.392       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | -2.81       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.1        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.066      |
|    value_loss           | 0.001       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.07238673 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.129      |
|    learning_rate        | 0.000134   |
|    loss                 | -0.0595    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0575    |
|    value_loss           | 0.00612    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.066   |
| time/              |          |
|    fps             | 517      |
|    iterations      | 25       |
|    time_elapsed    | 98       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 494         |
|    ep_rew_mean          | -0.0775     |
| time/                   |             |
|    fps                  | 523         |
|    iterations           | 26          |
|    time_elapsed         | 101         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.053980537 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | -2          |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0949     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0598     |
|    value_loss           | 0.000968    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.07167995 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | -1.97      |
|    learning_rate        | 0.000134   |
|    loss                 | -0.118     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0605    |
|    value_loss           | 0.000951   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0763  |
| time/              |          |
|    fps             | 513      |
|    iterations      | 27       |
|    time_elapsed    | 107      |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 491         |
|    ep_rew_mean          | -0.0763     |
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 28          |
|    time_elapsed         | 110         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.085049324 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.114       |
|    learning_rate        | 0.000134    |
|    loss                 | -0.077      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0471     |
|    value_loss           | 0.00372     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 489         |
|    ep_rew_mean          | -0.0557     |
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 29          |
|    time_elapsed         | 113         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.053966854 |
|    clip_fraction        | 0.407       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | -0.344      |
|    learning_rate        | 0.000134    |
|    loss                 | -0.0821     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0744     |
|    value_loss           | 0.000961    |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.07846845 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.0109     |
|    learning_rate        | 0.000134   |
|    loss                 | -0.0932    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.046     |
|    value_loss           | 0.00756    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.0553  |
| time/              |          |
|    fps             | 515      |
|    iterations      | 30       |
|    time_elapsed    | 119      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:50:17,038] Trial 9 finished with value: -0.21000000000000002 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 0.0001344192430166369, 'gamma': 0.9945061524772627, 'gae_lambda': 0.9413878451447913}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | -0.0652  |
| time/              |          |
|    fps             | 1281     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010203321 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.145      |
|    learning_rate        | 0.00982     |
|    loss                 | 0.000756    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.000482   |
|    value_loss           | 0.334       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 448      |
|    ep_rew_mean     | 0.0432   |
| time/              |          |
|    fps             | 749      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.09 +/- 0.36
Episode length: 473.20 +/- 155.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 473         |
|    mean_reward          | -0.0893     |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010422373 |
|    clip_fraction        | 0.0632      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00982     |
|    loss                 | 0.0167      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 0.00144     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 455      |
|    ep_rew_mean     | 0.00334  |
| time/              |          |
|    fps             | 672      |
|    iterations      | 3        |
|    time_elapsed    | 18       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.010158731 |
|    clip_fraction        | 0.0786      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00982     |
|    loss                 | -0.0174     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00144    |
|    value_loss           | 0.000493    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 464      |
|    ep_rew_mean     | -0.0143  |
| time/              |          |
|    fps             | 631      |
|    iterations      | 4        |
|    time_elapsed    | 25       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.015839517 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00982     |
|    loss                 | 0.0148      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00421    |
|    value_loss           | 0.000503    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 465      |
|    ep_rew_mean     | -0.023   |
| time/              |          |
|    fps             | 611      |
|    iterations      | 5        |
|    time_elapsed    | 33       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 458         |
|    ep_rew_mean          | 0.00544     |
| time/                   |             |
|    fps                  | 648         |
|    iterations           | 6           |
|    time_elapsed         | 37          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.016161365 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.00982     |
|    loss                 | 0.0264      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 0.000508    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015289253 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00982     |
|    loss                 | 0.0179      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 0.00154     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | -0.0228  |
| time/              |          |
|    fps             | 629      |
|    iterations      | 7        |
|    time_elapsed    | 45       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.01272405 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.00982    |
|    loss                 | 0.0226     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.00382   |
|    value_loss           | 1.25e-07   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | -0.0445  |
| time/              |          |
|    fps             | 616      |
|    iterations      | 8        |
|    time_elapsed    | 53       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.015078947 |
|    clip_fraction        | 0.0822      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00982     |
|    loss                 | 0.0314      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00178    |
|    value_loss           | 4.05e-08    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0598  |
| time/              |          |
|    fps             | 605      |
|    iterations      | 9        |
|    time_elapsed    | 60       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.014368245 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00982     |
|    loss                 | -0.0174     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 2.17e-08    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | -0.0625  |
| time/              |          |
|    fps             | 598      |
|    iterations      | 10       |
|    time_elapsed    | 68       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.011340711 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00982     |
|    loss                 | 0.0232      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 0.000527    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0527  |
| time/              |          |
|    fps             | 592      |
|    iterations      | 11       |
|    time_elapsed    | 76       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 476         |
|    ep_rew_mean          | -0.0504     |
| time/                   |             |
|    fps                  | 611         |
|    iterations           | 12          |
|    time_elapsed         | 80          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.015287567 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00982     |
|    loss                 | 0.0261      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 0.00105     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009917999 |
|    clip_fraction        | 0.0792      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.00982     |
|    loss                 | 0.000611    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00236    |
|    value_loss           | 0.000526    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | -0.062   |
| time/              |          |
|    fps             | 604      |
|    iterations      | 13       |
|    time_elapsed    | 88       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.011212584 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.00982     |
|    loss                 | -0.0283     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 4.8e-08     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.096   |
| time/              |          |
|    fps             | 599      |
|    iterations      | 14       |
|    time_elapsed    | 95       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0111248 |
|    clip_fraction        | 0.0915    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.63     |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.00982   |
|    loss                 | -0.00447  |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.00241  |
|    value_loss           | 2.42e-08  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.108   |
| time/              |          |
|    fps             | 594      |
|    iterations      | 15       |
|    time_elapsed    | 103      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:52:05,148] Trial 10 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.009815186587440126, 'gamma': 0.9004970821539003, 'gae_lambda': 0.804955383368751}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1274     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 494        |
|    ep_rew_mean          | -0.0727    |
| time/                   |            |
|    fps                  | 1073       |
|    iterations           | 2          |
|    time_elapsed         | 3          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.02916246 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | -9.4       |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0591    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0276    |
|    value_loss           | 0.00336    |
----------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.038268935 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.265      |
|    learning_rate        | 0.00194     |
|    loss                 | -0.0241     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00447    |
|    value_loss           | 0.00315     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 470      |
|    ep_rew_mean     | 0.0429   |
| time/              |          |
|    fps             | 660      |
|    iterations      | 3        |
|    time_elapsed    | 9        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 464         |
|    ep_rew_mean          | 0.0499      |
| time/                   |             |
|    fps                  | 714         |
|    iterations           | 4           |
|    time_elapsed         | 11          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.062120847 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -0.0786     |
|    learning_rate        | 0.00194     |
|    loss                 | -0.0417     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.00528     |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.15295282 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | -0.301     |
|    learning_rate        | 0.00194    |
|    loss                 | -0.093     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0436    |
|    value_loss           | 0.00317    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 459      |
|    ep_rew_mean     | 0.0893   |
| time/              |          |
|    fps             | 604      |
|    iterations      | 5        |
|    time_elapsed    | 16       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 469        |
|    ep_rew_mean          | 0.0433     |
| time/                   |            |
|    fps                  | 641        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.16654399 |
|    clip_fraction        | 0.518      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.53      |
|    explained_variance   | 0.0146     |
|    learning_rate        | 0.00194    |
|    loss                 | -0.065     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 0.0032     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 476        |
|    ep_rew_mean          | 0.00951    |
| time/                   |            |
|    fps                  | 672        |
|    iterations           | 7          |
|    time_elapsed         | 21         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.08598034 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.67      |
|    explained_variance   | -170       |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0961    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0603    |
|    value_loss           | 0.000143   |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.19460893 |
|    clip_fraction        | 0.541      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | -249       |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0893    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.048     |
|    value_loss           | 0.00098    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 458      |
|    ep_rew_mean     | 0.0455   |
| time/              |          |
|    fps             | 610      |
|    iterations      | 8        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 455        |
|    ep_rew_mean          | 0.0682     |
| time/                   |            |
|    fps                  | 635        |
|    iterations           | 9          |
|    time_elapsed         | 28         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.20962642 |
|    clip_fraction        | 0.569      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | 0.0539     |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0782    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0357    |
|    value_loss           | 0.00376    |
----------------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.15883876 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.52      |
|    explained_variance   | -0.12      |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0576    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0613    |
|    value_loss           | 0.00349    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | 0.0429   |
| time/              |          |
|    fps             | 594      |
|    iterations      | 10       |
|    time_elapsed    | 34       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 466        |
|    ep_rew_mean          | 0.0218     |
| time/                   |            |
|    fps                  | 614        |
|    iterations           | 11         |
|    time_elapsed         | 36         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.17358948 |
|    clip_fraction        | 0.528      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.43      |
|    explained_variance   | -385       |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0935    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.000312   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 471        |
|    ep_rew_mean          | 0.00401    |
| time/                   |            |
|    fps                  | 632        |
|    iterations           | 12         |
|    time_elapsed         | 38         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.29531223 |
|    clip_fraction        | 0.654      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.44      |
|    explained_variance   | -254       |
|    learning_rate        | 0.00194    |
|    loss                 | -0.119     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0847    |
|    value_loss           | 9.07e-05   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.35
Episode length: 487.20 +/- 113.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 487        |
|    mean_reward          | -0.0949    |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.24030992 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | -60        |
|    learning_rate        | 0.00194    |
|    loss                 | -0.059     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0379    |
|    value_loss           | 0.000503   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | -0.0113  |
| time/              |          |
|    fps             | 603      |
|    iterations      | 13       |
|    time_elapsed    | 44       |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 477       |
|    ep_rew_mean          | -0.0214   |
| time/                   |           |
|    fps                  | 618       |
|    iterations           | 14        |
|    time_elapsed         | 46        |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.1291767 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.4      |
|    explained_variance   | -32.5     |
|    learning_rate        | 0.00194   |
|    loss                 | -0.0815   |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.0483   |
|    value_loss           | 8.59e-05  |
---------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.31614554 |
|    clip_fraction        | 0.532      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | -65.9      |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0588    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0292    |
|    value_loss           | 0.000276   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 462      |
|    ep_rew_mean     | 0.012    |
| time/              |          |
|    fps             | 592      |
|    iterations      | 15       |
|    time_elapsed    | 51       |
|    total_timesteps | 30720    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 466       |
|    ep_rew_mean          | -0.000686 |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 16        |
|    time_elapsed         | 54        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 1.3908592 |
|    clip_fraction        | 0.554     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.1      |
|    explained_variance   | -0.115    |
|    learning_rate        | 0.00194   |
|    loss                 | -0.0957   |
|    n_updates            | 150       |
|    policy_gradient_loss | -0.0443   |
|    value_loss           | 0.00787   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 469       |
|    ep_rew_mean          | -0.012    |
| time/                   |           |
|    fps                  | 619       |
|    iterations           | 17        |
|    time_elapsed         | 56        |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.2245264 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.35     |
|    explained_variance   | -340      |
|    learning_rate        | 0.00194   |
|    loss                 | -0.107    |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.058    |
|    value_loss           | 0.00091   |
---------------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.90615934 |
|    clip_fraction        | 0.607      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1         |
|    explained_variance   | -170       |
|    learning_rate        | 0.00194    |
|    loss                 | -0.105     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0614    |
|    value_loss           | 0.00068    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 462      |
|    ep_rew_mean     | 0.00525  |
| time/              |          |
|    fps             | 597      |
|    iterations      | 18       |
|    time_elapsed    | 61       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 465        |
|    ep_rew_mean          | -0.00513   |
| time/                   |            |
|    fps                  | 608        |
|    iterations           | 19         |
|    time_elapsed         | 63         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.39321023 |
|    clip_fraction        | 0.641      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | -0.0631    |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0624    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0329    |
|    value_loss           | 0.00325    |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.32519272 |
|    clip_fraction        | 0.654      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | -129       |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0931    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0639    |
|    value_loss           | 0.000846   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | -0.0145  |
| time/              |          |
|    fps             | 590      |
|    iterations      | 20       |
|    time_elapsed    | 69       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 470        |
|    ep_rew_mean          | -0.0231    |
| time/                   |            |
|    fps                  | 601        |
|    iterations           | 21         |
|    time_elapsed         | 71         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.56476164 |
|    clip_fraction        | 0.624      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | -117       |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0671    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.000184   |
----------------------------------------
Eval num_timesteps=45000, episode_reward=-0.10 +/- 0.34
Episode length: 490.50 +/- 103.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 490        |
|    mean_reward          | -0.0962    |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.47605556 |
|    clip_fraction        | 0.588      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1         |
|    explained_variance   | -159       |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0428    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0336    |
|    value_loss           | 0.00112    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | -0.0208  |
| time/              |          |
|    fps             | 586      |
|    iterations      | 22       |
|    time_elapsed    | 76       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 471        |
|    ep_rew_mean          | -0.0284    |
| time/                   |            |
|    fps                  | 596        |
|    iterations           | 23         |
|    time_elapsed         | 79         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.39062977 |
|    clip_fraction        | 0.657      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | -0.0132    |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0635    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0462    |
|    value_loss           | 0.00225    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 471       |
|    ep_rew_mean          | -0.0284   |
| time/                   |           |
|    fps                  | 605       |
|    iterations           | 24        |
|    time_elapsed         | 81        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.4870435 |
|    clip_fraction        | 0.687     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.36     |
|    explained_variance   | -288      |
|    learning_rate        | 0.00194   |
|    loss                 | -0.0978   |
|    n_updates            | 230       |
|    policy_gradient_loss | -0.0783   |
|    value_loss           | 0.000136  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=-0.10 +/- 0.33
Episode length: 499.10 +/- 77.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 499        |
|    mean_reward          | -0.0996    |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.33731812 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.43      |
|    explained_variance   | -122       |
|    learning_rate        | 0.00194    |
|    loss                 | -0.102     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0713    |
|    value_loss           | 1.91e-05   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 473      |
|    ep_rew_mean     | -0.0394  |
| time/              |          |
|    fps             | 591      |
|    iterations      | 25       |
|    time_elapsed    | 86       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 477        |
|    ep_rew_mean          | -0.0509    |
| time/                   |            |
|    fps                  | 600        |
|    iterations           | 26         |
|    time_elapsed         | 88         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.45996475 |
|    clip_fraction        | 0.631      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | -87.3      |
|    learning_rate        | 0.00194    |
|    loss                 | -0.104     |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.063     |
|    value_loss           | 0.00066    |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.09 +/- 0.36
Episode length: 474.60 +/- 151.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 475       |
|    mean_reward          | -0.0898   |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 1.5628811 |
|    clip_fraction        | 0.597     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.06     |
|    explained_variance   | -50.5     |
|    learning_rate        | 0.00194   |
|    loss                 | -0.0963   |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.0877   |
|    value_loss           | 0.000766  |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | -0.0508  |
| time/              |          |
|    fps             | 588      |
|    iterations      | 27       |
|    time_elapsed    | 93       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 478        |
|    ep_rew_mean          | -0.0613    |
| time/                   |            |
|    fps                  | 596        |
|    iterations           | 28         |
|    time_elapsed         | 96         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.34053832 |
|    clip_fraction        | 0.596      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | -0.273     |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0621    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0359    |
|    value_loss           | 0.002      |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 474      |
|    ep_rew_mean          | -0.0496  |
| time/                   |          |
|    fps                  | 603      |
|    iterations           | 29       |
|    time_elapsed         | 98       |
|    total_timesteps      | 59392    |
| train/                  |          |
|    approx_kl            | 0.41714  |
|    clip_fraction        | 0.591    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.28    |
|    explained_variance   | -0.322   |
|    learning_rate        | 0.00194  |
|    loss                 | -0.0674  |
|    n_updates            | 280      |
|    policy_gradient_loss | -0.044   |
|    value_loss           | 0.00475  |
--------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.71520126 |
|    clip_fraction        | 0.543      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.888     |
|    explained_variance   | -0.00628   |
|    learning_rate        | 0.00194    |
|    loss                 | -0.0887    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0645    |
|    value_loss           | 0.00476    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | -0.0496  |
| time/              |          |
|    fps             | 591      |
|    iterations      | 30       |
|    time_elapsed    | 103      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:53:53,174] Trial 11 finished with value: -0.21000000000000002 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.001936631741258792, 'gamma': 0.9235137497624883, 'gae_lambda': 0.9887079770381273}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1287     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 525         |
|    ep_rew_mean          | -0.21       |
| time/                   |             |
|    fps                  | 1079        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.015881203 |
|    clip_fraction        | 0.0573      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -2.17       |
|    learning_rate        | 0.00255     |
|    loss                 | -0.0402     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 0.0234      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.15955211 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | -5.78      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0618    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0114    |
|    value_loss           | 0.00122    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 658      |
|    iterations      | 3        |
|    time_elapsed    | 9        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 525        |
|    ep_rew_mean          | -0.21      |
| time/                   |            |
|    fps                  | 708        |
|    iterations           | 4          |
|    time_elapsed         | 11         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.14770156 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | -5.28      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0735    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.036     |
|    value_loss           | 0.000696   |
----------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.1321889 |
|    clip_fraction        | 0.341     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.65     |
|    explained_variance   | -20.8     |
|    learning_rate        | 0.00255   |
|    loss                 | -0.064    |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0224   |
|    value_loss           | 9.64e-05  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 600      |
|    iterations      | 5        |
|    time_elapsed    | 17       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 516        |
|    ep_rew_mean          | -0.163     |
| time/                   |            |
|    fps                  | 637        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.11726467 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | -10.1      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0418    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0281    |
|    value_loss           | 0.00103    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 501        |
|    ep_rew_mean          | -0.129     |
| time/                   |            |
|    fps                  | 666        |
|    iterations           | 7          |
|    time_elapsed         | 21         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.21205813 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.53      |
|    explained_variance   | -0.86      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0424    |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.183      |
|    value_loss           | 0.00363    |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.14679158 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | -0.0461    |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0205    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.00159    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.139   |
| time/              |          |
|    fps             | 605      |
|    iterations      | 8        |
|    time_elapsed    | 27       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 506        |
|    ep_rew_mean          | -0.147     |
| time/                   |            |
|    fps                  | 630        |
|    iterations           | 9          |
|    time_elapsed         | 29         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.19132006 |
|    clip_fraction        | 0.498      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.52      |
|    explained_variance   | -11.1      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0453    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0225    |
|    value_loss           | 0.000866   |
----------------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.24760184 |
|    clip_fraction        | 0.525      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.4       |
|    explained_variance   | -10.6      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.109     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0417    |
|    value_loss           | 0.000757   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.153   |
| time/              |          |
|    fps             | 589      |
|    iterations      | 10       |
|    time_elapsed    | 34       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 510        |
|    ep_rew_mean          | -0.158     |
| time/                   |            |
|    fps                  | 609        |
|    iterations           | 11         |
|    time_elapsed         | 36         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.31042328 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | -19.3      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0436    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 0.000796   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 511       |
|    ep_rew_mean          | -0.163    |
| time/                   |           |
|    fps                  | 627       |
|    iterations           | 12        |
|    time_elapsed         | 39        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.4531356 |
|    clip_fraction        | 0.488     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.23     |
|    explained_variance   | -26.3     |
|    learning_rate        | 0.00255   |
|    loss                 | -0.0392   |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0304   |
|    value_loss           | 0.00223   |
---------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.22663902 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | -5.8       |
|    learning_rate        | 0.00255    |
|    loss                 | -0.08      |
|    n_updates            | 120        |
|    policy_gradient_loss | 0.00716    |
|    value_loss           | 0.000292   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.166   |
| time/              |          |
|    fps             | 595      |
|    iterations      | 13       |
|    time_elapsed    | 44       |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 513       |
|    ep_rew_mean          | -0.169    |
| time/                   |           |
|    fps                  | 611       |
|    iterations           | 14        |
|    time_elapsed         | 46        |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.2634465 |
|    clip_fraction        | 0.594     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.31     |
|    explained_variance   | -3.53     |
|    learning_rate        | 0.00255   |
|    loss                 | -0.122    |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.0693   |
|    value_loss           | 0.00138   |
---------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.45860523 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.72      |
|    explained_variance   | -4.34      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0602    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0269    |
|    value_loss           | 0.00125    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | -0.171   |
| time/              |          |
|    fps             | 586      |
|    iterations      | 15       |
|    time_elapsed    | 52       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 514        |
|    ep_rew_mean          | -0.174     |
| time/                   |            |
|    fps                  | 599        |
|    iterations           | 16         |
|    time_elapsed         | 54         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.33618957 |
|    clip_fraction        | 0.582      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.41      |
|    explained_variance   | -18.5      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0803    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0413    |
|    value_loss           | 0.000553   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 514       |
|    ep_rew_mean          | -0.161    |
| time/                   |           |
|    fps                  | 612       |
|    iterations           | 17        |
|    time_elapsed         | 56        |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.3336697 |
|    clip_fraction        | 0.58      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.4      |
|    explained_variance   | -23.7     |
|    learning_rate        | 0.00255   |
|    loss                 | -0.0408   |
|    n_updates            | 160       |
|    policy_gradient_loss | -0.0498   |
|    value_loss           | 0.00105   |
---------------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.36039165 |
|    clip_fraction        | 0.537      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | -0.17      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0844    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0334    |
|    value_loss           | 0.00179    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | -0.164   |
| time/              |          |
|    fps             | 591      |
|    iterations      | 18       |
|    time_elapsed    | 62       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 515        |
|    ep_rew_mean          | -0.166     |
| time/                   |            |
|    fps                  | 603        |
|    iterations           | 19         |
|    time_elapsed         | 64         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.81722593 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.878     |
|    explained_variance   | -1.61      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0904    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0464    |
|    value_loss           | 0.00262    |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.44740447 |
|    clip_fraction        | 0.58       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | -10.3      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0851    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.057     |
|    value_loss           | 0.000872   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | -0.168   |
| time/              |          |
|    fps             | 585      |
|    iterations      | 20       |
|    time_elapsed    | 69       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 516        |
|    ep_rew_mean          | -0.17      |
| time/                   |            |
|    fps                  | 596        |
|    iterations           | 21         |
|    time_elapsed         | 72         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.32788438 |
|    clip_fraction        | 0.614      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -22.5      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0864    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.037     |
|    value_loss           | 0.00102    |
----------------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.42712325 |
|    clip_fraction        | 0.541      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.42      |
|    explained_variance   | -10.9      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.109     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.064     |
|    value_loss           | 0.000288   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 580      |
|    iterations      | 22       |
|    time_elapsed    | 77       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 516       |
|    ep_rew_mean          | -0.162    |
| time/                   |           |
|    fps                  | 589       |
|    iterations           | 23        |
|    time_elapsed         | 79        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.7483196 |
|    clip_fraction        | 0.57      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.2      |
|    explained_variance   | -0.105    |
|    learning_rate        | 0.00255   |
|    loss                 | -0.0351   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0455   |
|    value_loss           | 0.00166   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 516        |
|    ep_rew_mean          | -0.164     |
| time/                   |            |
|    fps                  | 598        |
|    iterations           | 24         |
|    time_elapsed         | 82         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.42933208 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | -29.1      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0977    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0588    |
|    value_loss           | 0.000797   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.39574975 |
|    clip_fraction        | 0.617      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | -3.23      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0817    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0461    |
|    value_loss           | 0.001      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.166   |
| time/              |          |
|    fps             | 584      |
|    iterations      | 25       |
|    time_elapsed    | 87       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 517        |
|    ep_rew_mean          | -0.167     |
| time/                   |            |
|    fps                  | 593        |
|    iterations           | 26         |
|    time_elapsed         | 89         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.30599928 |
|    clip_fraction        | 0.619      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | -18.7      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0974    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0626    |
|    value_loss           | 0.000203   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.4583077 |
|    clip_fraction        | 0.668     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.39     |
|    explained_variance   | -5.6      |
|    learning_rate        | 0.00255   |
|    loss                 | -0.129    |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.0793   |
|    value_loss           | 6.02e-05  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.167   |
| time/              |          |
|    fps             | 580      |
|    iterations      | 27       |
|    time_elapsed    | 95       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 515        |
|    ep_rew_mean          | -0.156     |
| time/                   |            |
|    fps                  | 588        |
|    iterations           | 28         |
|    time_elapsed         | 97         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.49658617 |
|    clip_fraction        | 0.675      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | -8.24      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0899    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0563    |
|    value_loss           | 0.0046     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 515       |
|    ep_rew_mean          | -0.156    |
| time/                   |           |
|    fps                  | 595       |
|    iterations           | 29        |
|    time_elapsed         | 99        |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 0.4929783 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.22     |
|    explained_variance   | -0.529    |
|    learning_rate        | 0.00255   |
|    loss                 | -0.0683   |
|    n_updates            | 280       |
|    policy_gradient_loss | -0.0464   |
|    value_loss           | 0.00341   |
---------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.63111866 |
|    clip_fraction        | 0.673      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | -40.6      |
|    learning_rate        | 0.00255    |
|    loss                 | -0.0416    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0534    |
|    value_loss           | 0.00258    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.145   |
| time/              |          |
|    fps             | 584      |
|    iterations      | 30       |
|    time_elapsed    | 105      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:55:42,440] Trial 12 finished with value: -0.21000000000000002 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0025522643044657654, 'gamma': 0.902249861935651, 'gae_lambda': 0.9132840619074419}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | 0.0627   |
| time/              |          |
|    fps             | 1284     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 497         |
|    ep_rew_mean          | -0.0737     |
| time/                   |             |
|    fps                  | 1076        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.051246658 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -1.62       |
|    learning_rate        | 0.00263     |
|    loss                 | -0.0616     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.0268      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.04512763 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.75      |
|    explained_variance   | -0.895     |
|    learning_rate        | 0.00263    |
|    loss                 | -0.065     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.000336   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 663      |
|    iterations      | 3        |
|    time_elapsed    | 9        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 511        |
|    ep_rew_mean          | -0.142     |
| time/                   |            |
|    fps                  | 714        |
|    iterations           | 4          |
|    time_elapsed         | 11         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.07984857 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.73      |
|    explained_variance   | -1.49      |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0658    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 0.000525   |
----------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.1271427 |
|    clip_fraction        | 0.327     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.66     |
|    explained_variance   | -5.49     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.0689   |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0281   |
|    value_loss           | 0.000364  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.0968  |
| time/              |          |
|    fps             | 604      |
|    iterations      | 5        |
|    time_elapsed    | 16       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 483        |
|    ep_rew_mean          | -0.073     |
| time/                   |            |
|    fps                  | 641        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.06484253 |
|    clip_fraction        | 0.365      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | -0.0241    |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0235    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0322    |
|    value_loss           | 0.00203    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 488        |
|    ep_rew_mean          | -0.0919    |
| time/                   |            |
|    fps                  | 671        |
|    iterations           | 7          |
|    time_elapsed         | 21         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.14771417 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.61      |
|    explained_variance   | -0.173     |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0689    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0272    |
|    value_loss           | 0.00304    |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.103992596 |
|    clip_fraction        | 0.378       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | -4          |
|    learning_rate        | 0.00263     |
|    loss                 | -0.093      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.00122     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 610      |
|    iterations      | 8        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 496        |
|    ep_rew_mean          | -0.117     |
| time/                   |            |
|    fps                  | 634        |
|    iterations           | 9          |
|    time_elapsed         | 29         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.14855677 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | -4.77      |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0395    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0304    |
|    value_loss           | 0.00195    |
----------------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.18105671 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.39      |
|    explained_variance   | -5.1       |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0367    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0471    |
|    value_loss           | 0.000123   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.126   |
| time/              |          |
|    fps             | 592      |
|    iterations      | 10       |
|    time_elapsed    | 34       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 501        |
|    ep_rew_mean          | -0.132     |
| time/                   |            |
|    fps                  | 612        |
|    iterations           | 11         |
|    time_elapsed         | 36         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.21075222 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | -7.87      |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0932    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0394    |
|    value_loss           | 4.52e-05   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 497       |
|    ep_rew_mean          | -0.117    |
| time/                   |           |
|    fps                  | 630       |
|    iterations           | 12        |
|    time_elapsed         | 39        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.3255998 |
|    clip_fraction        | 0.479     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.22     |
|    explained_variance   | -4.38     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.0751   |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0267   |
|    value_loss           | 0.00428   |
---------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.49788618 |
|    clip_fraction        | 0.624      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | -0.392     |
|    learning_rate        | 0.00263    |
|    loss                 | -0.098     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0575    |
|    value_loss           | 0.00206    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.124   |
| time/              |          |
|    fps             | 598      |
|    iterations      | 13       |
|    time_elapsed    | 44       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 501        |
|    ep_rew_mean          | -0.13      |
| time/                   |            |
|    fps                  | 613        |
|    iterations           | 14         |
|    time_elapsed         | 46         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.43522376 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | -3.03      |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0954    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0483    |
|    value_loss           | 0.00135    |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.5503372 |
|    clip_fraction        | 0.545     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.23     |
|    explained_variance   | 0.097     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.0853   |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.044    |
|    value_loss           | 0.00141   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.136   |
| time/              |          |
|    fps             | 587      |
|    iterations      | 15       |
|    time_elapsed    | 52       |
|    total_timesteps | 30720    |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 504      |
|    ep_rew_mean          | -0.14    |
| time/                   |          |
|    fps                  | 601      |
|    iterations           | 16       |
|    time_elapsed         | 54       |
|    total_timesteps      | 32768    |
| train/                  |          |
|    approx_kl            | 1.206696 |
|    clip_fraction        | 0.739    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.901   |
|    explained_variance   | 0.54     |
|    learning_rate        | 0.00263  |
|    loss                 | -0.118   |
|    n_updates            | 150      |
|    policy_gradient_loss | -0.0787  |
|    value_loss           | 0.000597 |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 505        |
|    ep_rew_mean          | -0.143     |
| time/                   |            |
|    fps                  | 614        |
|    iterations           | 17         |
|    time_elapsed         | 56         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.38112938 |
|    clip_fraction        | 0.661      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | -0.941     |
|    learning_rate        | 0.00263    |
|    loss                 | -0.105     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0679    |
|    value_loss           | 0.00015    |
----------------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.5820682 |
|    clip_fraction        | 0.622     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.25     |
|    explained_variance   | -4.29     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.0808   |
|    n_updates            | 170       |
|    policy_gradient_loss | -0.0506   |
|    value_loss           | 0.00053   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.147   |
| time/              |          |
|    fps             | 593      |
|    iterations      | 18       |
|    time_elapsed    | 62       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 507        |
|    ep_rew_mean          | -0.15      |
| time/                   |            |
|    fps                  | 605        |
|    iterations           | 19         |
|    time_elapsed         | 64         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.50801766 |
|    clip_fraction        | 0.66       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | -10        |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0979    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0695    |
|    value_loss           | 0.000798   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 1.0321709 |
|    clip_fraction        | 0.627     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.979    |
|    explained_variance   | -3.58     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.0978   |
|    n_updates            | 190       |
|    policy_gradient_loss | -0.0206   |
|    value_loss           | 0.00696   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.153   |
| time/              |          |
|    fps             | 587      |
|    iterations      | 20       |
|    time_elapsed    | 69       |
|    total_timesteps | 40960    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 506       |
|    ep_rew_mean          | -0.131    |
| time/                   |           |
|    fps                  | 597       |
|    iterations           | 21        |
|    time_elapsed         | 71        |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.7368116 |
|    clip_fraction        | 0.568     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.06     |
|    explained_variance   | 0.0905    |
|    learning_rate        | 0.00263   |
|    loss                 | -0.0738   |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0483   |
|    value_loss           | 0.00198   |
---------------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.5356049 |
|    clip_fraction        | 0.642     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.16     |
|    explained_variance   | -1.22     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.0606   |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.0423   |
|    value_loss           | 0.00473   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | -0.135   |
| time/              |          |
|    fps             | 582      |
|    iterations      | 22       |
|    time_elapsed    | 77       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 508       |
|    ep_rew_mean          | -0.138    |
| time/                   |           |
|    fps                  | 591       |
|    iterations           | 23        |
|    time_elapsed         | 79        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 1.7521636 |
|    clip_fraction        | 0.667     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.07     |
|    explained_variance   | -3.32     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.0567   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0542   |
|    value_loss           | 0.00209   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 509       |
|    ep_rew_mean          | -0.141    |
| time/                   |           |
|    fps                  | 601       |
|    iterations           | 24        |
|    time_elapsed         | 81        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.6814997 |
|    clip_fraction        | 0.525     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.1      |
|    explained_variance   | 0.759     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.095    |
|    n_updates            | 230       |
|    policy_gradient_loss | -0.0675   |
|    value_loss           | 0.000187  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.7857959 |
|    clip_fraction        | 0.672     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.03     |
|    explained_variance   | -0.48     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.081    |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0497   |
|    value_loss           | 0.000335  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.144   |
| time/              |          |
|    fps             | 586      |
|    iterations      | 25       |
|    time_elapsed    | 87       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 512        |
|    ep_rew_mean          | -0.155     |
| time/                   |            |
|    fps                  | 595        |
|    iterations           | 26         |
|    time_elapsed         | 89         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.62203866 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.933     |
|    explained_variance   | -2.29      |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0681    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0274    |
|    value_loss           | 0.00329    |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.54336053 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | -3.45      |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0812    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0435    |
|    value_loss           | 0.000575   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.155   |
| time/              |          |
|    fps             | 582      |
|    iterations      | 27       |
|    time_elapsed    | 94       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 512       |
|    ep_rew_mean          | -0.155    |
| time/                   |           |
|    fps                  | 590       |
|    iterations           | 28        |
|    time_elapsed         | 97        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.5839735 |
|    clip_fraction        | 0.539     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.842    |
|    explained_variance   | -7.09     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.0968   |
|    n_updates            | 270       |
|    policy_gradient_loss | -0.0667   |
|    value_loss           | 0.000354  |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 512        |
|    ep_rew_mean          | -0.155     |
| time/                   |            |
|    fps                  | 598        |
|    iterations           | 29         |
|    time_elapsed         | 99         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.76000094 |
|    clip_fraction        | 0.632      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | -2.18      |
|    learning_rate        | 0.00263    |
|    loss                 | -0.0745    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0493    |
|    value_loss           | 0.000846   |
----------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.5266229 |
|    clip_fraction        | 0.505     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1        |
|    explained_variance   | -1.68     |
|    learning_rate        | 0.00263   |
|    loss                 | -0.099    |
|    n_updates            | 290       |
|    policy_gradient_loss | -0.0531   |
|    value_loss           | 0.00123   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | -0.156   |
| time/              |          |
|    fps             | 586      |
|    iterations      | 30       |
|    time_elapsed    | 104      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 02:57:31,331] Trial 13 finished with value: -0.21000000000000002 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0026293627625244118, 'gamma': 0.9485661418549349, 'gae_lambda': 0.8495183655906412}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.09 +/- 0.36
Episode length: 474.40 +/- 151.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | -0.0897  |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 880      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.019554745 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -1.1        |
|    learning_rate        | 0.000563    |
|    loss                 | -0.0306     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.0018      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.11 +/- 0.31
Episode length: 520.90 +/- 12.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 521      |
|    mean_reward     | -0.108   |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.103   |
| time/              |          |
|    fps             | 662      |
|    iterations      | 2        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.038380977 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.0339     |
|    learning_rate        | 0.000563    |
|    loss                 | -0.112      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0365     |
|    value_loss           | 0.00118     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | -0.094   |
| time/              |          |
|    fps             | 670      |
|    iterations      | 3        |
|    time_elapsed    | 36       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.072376646 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.161       |
|    learning_rate        | 0.000563    |
|    loss                 | -0.0839     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0498     |
|    value_loss           | 0.000726    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0735  |
| time/              |          |
|    fps             | 630      |
|    iterations      | 4        |
|    time_elapsed    | 51       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.07804786 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | -0.0274    |
|    learning_rate        | 0.000563   |
|    loss                 | -0.064     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.000823   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | -0.0625  |
| time/              |          |
|    fps             | 608      |
|    iterations      | 5        |
|    time_elapsed    | 67       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.11378889 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.61      |
|    explained_variance   | 0.00727    |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0692    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.00086    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | -0.0498  |
| time/              |          |
|    fps             | 619      |
|    iterations      | 6        |
|    time_elapsed    | 79       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.1288296 |
|    clip_fraction        | 0.517     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.58     |
|    explained_variance   | -0.00685  |
|    learning_rate        | 0.000563  |
|    loss                 | -0.0699   |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0623   |
|    value_loss           | 0.000856  |
---------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 466      |
|    ep_rew_mean     | -0.0264  |
| time/              |          |
|    fps             | 605      |
|    iterations      | 7        |
|    time_elapsed    | 94       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.12656261 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | 0.00227    |
|    learning_rate        | 0.000563   |
|    loss                 | -0.0894    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0615    |
|    value_loss           | 0.000843   |
----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | -0.00444 |
| time/              |          |
|    fps             | 596      |
|    iterations      | 8        |
|    time_elapsed    | 109      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 02:59:26,868] Trial 14 finished with value: -0.09019999999999999 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.0005632952150057929, 'gamma': 0.9214655365414866, 'gae_lambda': 0.8067459065091451}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 453      |
|    ep_rew_mean     | 0.0542   |
| time/              |          |
|    fps             | 857      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0065088393 |
|    clip_fraction        | 0.0666       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.474        |
|    learning_rate        | 7.15e-05     |
|    loss                 | -0.00681     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00859     |
|    value_loss           | 0.0144       |
------------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | -0.0427  |
| time/              |          |
|    fps             | 688      |
|    iterations      | 2        |
|    time_elapsed    | 23       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 473.50 +/- 154.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0894     |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.008851925 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.55        |
|    learning_rate        | 7.15e-05    |
|    loss                 | -0.0239     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.00489     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0534  |
| time/              |          |
|    fps             | 710      |
|    iterations      | 3        |
|    time_elapsed    | 34       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011081109 |
|    clip_fraction        | 0.0953      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.551       |
|    learning_rate        | 7.15e-05    |
|    loss                 | -0.0109     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.00318     |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.0591  |
| time/              |          |
|    fps             | 670      |
|    iterations      | 4        |
|    time_elapsed    | 48       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.008271698 |
|    clip_fraction        | 0.0704      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.369       |
|    learning_rate        | 7.15e-05    |
|    loss                 | -0.00374    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.00208     |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 473.60 +/- 154.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | -0.0894  |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.0766  |
| time/              |          |
|    fps             | 651      |
|    iterations      | 5        |
|    time_elapsed    | 62       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.008926263 |
|    clip_fraction        | 0.0829      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -0.216      |
|    learning_rate        | 7.15e-05    |
|    loss                 | -0.0255     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.00216     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.086   |
| time/              |          |
|    fps             | 664      |
|    iterations      | 6        |
|    time_elapsed    | 74       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008057615 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.719      |
|    learning_rate        | 7.15e-05    |
|    loss                 | -0.0223     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.00183     |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0633  |
| time/              |          |
|    fps             | 649      |
|    iterations      | 7        |
|    time_elapsed    | 88       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.007949047 |
|    clip_fraction        | 0.0825      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.292      |
|    learning_rate        | 7.15e-05    |
|    loss                 | 0.00652     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.00281     |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | -0.0408  |
| time/              |          |
|    fps             | 638      |
|    iterations      | 8        |
|    time_elapsed    | 102      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:01:14,142] Trial 15 finished with value: -0.08957 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 7.150772918899334e-05, 'gamma': 0.9215799670608408, 'gae_lambda': 0.8026885278813176}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 430      |
|    ep_rew_mean     | 0.106    |
| time/              |          |
|    fps             | 849      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0063621886 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -0.617       |
|    learning_rate        | 6.18e-05     |
|    loss                 | -0.0107      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00542     |
|    value_loss           | 0.00209      |
------------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 462      |
|    ep_rew_mean     | -0.0134  |
| time/              |          |
|    fps             | 683      |
|    iterations      | 2        |
|    time_elapsed    | 23       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 473.50 +/- 154.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0894     |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.005637575 |
|    clip_fraction        | 0.0467      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.409      |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.0105     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00722    |
|    value_loss           | 0.000967    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 460      |
|    ep_rew_mean     | -0.0143  |
| time/              |          |
|    fps             | 710      |
|    iterations      | 3        |
|    time_elapsed    | 34       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0082037635 |
|    clip_fraction        | 0.0478       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -0.368       |
|    learning_rate        | 6.18e-05     |
|    loss                 | 0.000296     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00595     |
|    value_loss           | 0.00139      |
------------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | -0.0424  |
| time/              |          |
|    fps             | 670      |
|    iterations      | 4        |
|    time_elapsed    | 48       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.006258327 |
|    clip_fraction        | 0.0444      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -3.44       |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.0295     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.00201     |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | -0.0616  |
| time/              |          |
|    fps             | 646      |
|    iterations      | 5        |
|    time_elapsed    | 63       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0067473007 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | -2.84        |
|    learning_rate        | 6.18e-05     |
|    loss                 | -0.0131      |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.00198      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0932  |
| time/              |          |
|    fps             | 660      |
|    iterations      | 6        |
|    time_elapsed    | 74       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0071649775 |
|    clip_fraction        | 0.0607       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | -9.64        |
|    learning_rate        | 6.18e-05     |
|    loss                 | -0.0305      |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0161      |
|    value_loss           | 0.00164      |
------------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.116   |
| time/              |          |
|    fps             | 645      |
|    iterations      | 7        |
|    time_elapsed    | 88       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.006735445 |
|    clip_fraction        | 0.0633      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -1.7        |
|    learning_rate        | 6.18e-05    |
|    loss                 | -0.0305     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.0017      |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.09 +/- 0.36
Episode length: 473.60 +/- 154.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | -0.0894  |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.116   |
| time/              |          |
|    fps             | 637      |
|    iterations      | 8        |
|    time_elapsed    | 102      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:03:01,900] Trial 16 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 6.176238296103689e-05, 'gamma': 0.9148330128326162, 'gae_lambda': 0.8288826822604435}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 460      |
|    ep_rew_mean     | -0.00759 |
| time/              |          |
|    fps             | 855      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0108120795 |
|    clip_fraction        | 0.0907       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.463        |
|    learning_rate        | 5.67e-05     |
|    loss                 | 0.00633      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00661     |
|    value_loss           | 0.00511      |
------------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 473      |
|    ep_rew_mean     | -0.0128  |
| time/              |          |
|    fps             | 684      |
|    iterations      | 2        |
|    time_elapsed    | 23       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.009162119 |
|    clip_fraction        | 0.053       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.366       |
|    learning_rate        | 5.67e-05    |
|    loss                 | -0.0243     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00857    |
|    value_loss           | 0.00324     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 465      |
|    ep_rew_mean     | 0.0447   |
| time/              |          |
|    fps             | 704      |
|    iterations      | 3        |
|    time_elapsed    | 34       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.006119781 |
|    clip_fraction        | 0.0534      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.158       |
|    learning_rate        | 5.67e-05    |
|    loss                 | -0.0067     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00916    |
|    value_loss           | 0.00366     |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | 0.0443   |
| time/              |          |
|    fps             | 666      |
|    iterations      | 4        |
|    time_elapsed    | 49       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.008459739 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -0.0991     |
|    learning_rate        | 5.67e-05    |
|    loss                 | -0.00313    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.00254     |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 470      |
|    ep_rew_mean     | 0.00961  |
| time/              |          |
|    fps             | 645      |
|    iterations      | 5        |
|    time_elapsed    | 63       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.007346242 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -1.05       |
|    learning_rate        | 5.67e-05    |
|    loss                 | -0.0338     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.00128     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | 0.0124   |
| time/              |          |
|    fps             | 660      |
|    iterations      | 6        |
|    time_elapsed    | 74       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 474.80 +/- 150.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 475          |
|    mean_reward          | -0.0899      |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0070747114 |
|    clip_fraction        | 0.0667       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | -0.749       |
|    learning_rate        | 5.67e-05     |
|    loss                 | -0.0224      |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0128      |
|    value_loss           | 0.00244      |
------------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 473      |
|    ep_rew_mean     | 0.0207   |
| time/              |          |
|    fps             | 647      |
|    iterations      | 7        |
|    time_elapsed    | 88       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=0.01 +/- 0.45
Episode length: 465.20 +/- 145.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 465        |
|    mean_reward          | 0.014      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.00785967 |
|    clip_fraction        | 0.0604     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | -0.755     |
|    learning_rate        | 5.67e-05   |
|    loss                 | -0.0222    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 0.00189    |
----------------------------------------
New best mean reward!
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.011   |
| time/              |          |
|    fps             | 639      |
|    iterations      | 8        |
|    time_elapsed    | 102      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:04:49,394] Trial 17 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 5.6665342031927324e-05, 'gamma': 0.9101996872949238, 'gae_lambda': 0.8250269641468766}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1297     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.016930286 |
|    clip_fraction        | 0.0937      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.588      |
|    learning_rate        | 0.00032     |
|    loss                 | -0.0714     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.0028      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 793      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.024211891 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -0.232      |
|    learning_rate        | 0.00032     |
|    loss                 | -0.0627     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0362     |
|    value_loss           | 0.00173     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | -0.164   |
| time/              |          |
|    fps             | 700      |
|    iterations      | 3        |
|    time_elapsed    | 17       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.028672457 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -2.57       |
|    learning_rate        | 0.00032     |
|    loss                 | -0.0799     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0516     |
|    value_loss           | 0.00245     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 663      |
|    iterations      | 4        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.023452839 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | -0.604      |
|    learning_rate        | 0.00032     |
|    loss                 | -0.0371     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0437     |
|    value_loss           | 0.00139     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | -0.154   |
| time/              |          |
|    fps             | 643      |
|    iterations      | 5        |
|    time_elapsed    | 31       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 513         |
|    ep_rew_mean          | -0.163      |
| time/                   |             |
|    fps                  | 687         |
|    iterations           | 6           |
|    time_elapsed         | 35          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.034902364 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | -7.97       |
|    learning_rate        | 0.00032     |
|    loss                 | -0.0808     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0549     |
|    value_loss           | 0.00105     |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.03701933 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | -6.29      |
|    learning_rate        | 0.00032    |
|    loss                 | -0.097     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0595    |
|    value_loss           | 0.000859   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | -0.169   |
| time/              |          |
|    fps             | 667      |
|    iterations      | 7        |
|    time_elapsed    | 42       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.037123874 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | -11.5       |
|    learning_rate        | 0.00032     |
|    loss                 | -0.093      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0664     |
|    value_loss           | 0.000591    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 653      |
|    iterations      | 8        |
|    time_elapsed    | 50       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.03713963 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.66      |
|    explained_variance   | -0.371     |
|    learning_rate        | 0.00032    |
|    loss                 | -0.0374    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0488    |
|    value_loss           | 0.00174    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.148   |
| time/              |          |
|    fps             | 643      |
|    iterations      | 9        |
|    time_elapsed    | 57       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.051248167 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | -8.6        |
|    learning_rate        | 0.00032     |
|    loss                 | -0.0848     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0585     |
|    value_loss           | 0.000779    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.126   |
| time/              |          |
|    fps             | 636      |
|    iterations      | 10       |
|    time_elapsed    | 64       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.037250597 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | -0.181      |
|    learning_rate        | 0.00032     |
|    loss                 | -0.0854     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0463     |
|    value_loss           | 0.00147     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.122   |
| time/              |          |
|    fps             | 629      |
|    iterations      | 11       |
|    time_elapsed    | 71       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 504        |
|    ep_rew_mean          | -0.129     |
| time/                   |            |
|    fps                  | 651        |
|    iterations           | 12         |
|    time_elapsed         | 75         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.06466621 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.49      |
|    explained_variance   | -0.629     |
|    learning_rate        | 0.00032    |
|    loss                 | -0.0876    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0615    |
|    value_loss           | 0.00117    |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0460231 |
|    clip_fraction        | 0.353     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.59     |
|    explained_variance   | -4.54     |
|    learning_rate        | 0.00032   |
|    loss                 | -0.109    |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.0661   |
|    value_loss           | 0.000245  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.121   |
| time/              |          |
|    fps             | 644      |
|    iterations      | 13       |
|    time_elapsed    | 82       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.060615137 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | -0.317      |
|    learning_rate        | 0.00032     |
|    loss                 | -0.0982     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0534     |
|    value_loss           | 0.000936    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.109   |
| time/              |          |
|    fps             | 639      |
|    iterations      | 14       |
|    time_elapsed    | 89       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.06579285 |
|    clip_fraction        | 0.369      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -0.378     |
|    learning_rate        | 0.00032    |
|    loss                 | -0.063     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.000941   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 634      |
|    iterations      | 15       |
|    time_elapsed    | 96       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 03:06:30,462] Trial 18 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.00032033163305619246, 'gamma': 0.9273291642727283, 'gae_lambda': 0.8672896253904548}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | -0.0143  |
| time/              |          |
|    fps             | 848      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.00733895 |
|    clip_fraction        | 0.0623     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.79      |
|    explained_variance   | -0.191     |
|    learning_rate        | 6.71e-05   |
|    loss                 | 0.00435    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00834   |
|    value_loss           | 0.00361    |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.09 +/- 0.36
Episode length: 473.20 +/- 155.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 473      |
|    mean_reward     | -0.0893  |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | 0.0191   |
| time/              |          |
|    fps             | 692      |
|    iterations      | 2        |
|    time_elapsed    | 23       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.00670484 |
|    clip_fraction        | 0.056      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0.486      |
|    learning_rate        | 6.71e-05   |
|    loss                 | -0.0217    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00666   |
|    value_loss           | 0.00236    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0147  |
| time/              |          |
|    fps             | 708      |
|    iterations      | 3        |
|    time_elapsed    | 34       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.007009808 |
|    clip_fraction        | 0.0793      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.47        |
|    learning_rate        | 6.71e-05    |
|    loss                 | -0.00352    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 0.00152     |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | -0.015   |
| time/              |          |
|    fps             | 668      |
|    iterations      | 4        |
|    time_elapsed    | 48       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.009575512 |
|    clip_fraction        | 0.0972      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -0.214      |
|    learning_rate        | 6.71e-05    |
|    loss                 | 0.00211     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.00219     |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0265  |
| time/              |          |
|    fps             | 647      |
|    iterations      | 5        |
|    time_elapsed    | 63       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0071203886 |
|    clip_fraction        | 0.073        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.73        |
|    explained_variance   | -0.538       |
|    learning_rate        | 6.71e-05     |
|    loss                 | -0.0278      |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0134      |
|    value_loss           | 0.00192      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | 0.00229  |
| time/              |          |
|    fps             | 662      |
|    iterations      | 6        |
|    time_elapsed    | 74       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.007840099 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | -0.425      |
|    learning_rate        | 6.71e-05    |
|    loss                 | 0.00524     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.00314     |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.09 +/- 0.36
Episode length: 475.50 +/- 148.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | -0.0902  |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | 0.00128  |
| time/              |          |
|    fps             | 650      |
|    iterations      | 7        |
|    time_elapsed    | 88       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.36
Episode length: 474.30 +/- 152.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0897     |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.008125229 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -0.549      |
|    learning_rate        | 6.71e-05    |
|    loss                 | -0.0337     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.00239     |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.10 +/- 0.34
Episode length: 487.80 +/- 111.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | -0.0951  |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | 0.0133   |
| time/              |          |
|    fps             | 638      |
|    iterations      | 8        |
|    time_elapsed    | 102      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:08:17,974] Trial 19 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 6.70664602239252e-05, 'gamma': 0.957216981087685, 'gae_lambda': 0.8470115631051853}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.143   |
| time/              |          |
|    fps             | 854      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.09 +/- 0.36
Episode length: 474.20 +/- 152.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 474          |
|    mean_reward          | -0.0897      |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0052244174 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.449        |
|    learning_rate        | 2.42e-05     |
|    loss                 | -0.00412     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0049      |
|    value_loss           | 0.00248      |
------------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | -0.0788  |
| time/              |          |
|    fps             | 695      |
|    iterations      | 2        |
|    time_elapsed    | 23       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0060291965 |
|    clip_fraction        | 0.0523       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.671        |
|    learning_rate        | 2.42e-05     |
|    loss                 | -0.00658     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00495     |
|    value_loss           | 0.00195      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.0358  |
| time/              |          |
|    fps             | 709      |
|    iterations      | 3        |
|    time_elapsed    | 34       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0065269005 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.658        |
|    learning_rate        | 2.42e-05     |
|    loss                 | -0.0185      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00459     |
|    value_loss           | 0.00229      |
------------------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 473      |
|    mean_reward     | -0.0893  |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.0604  |
| time/              |          |
|    fps             | 673      |
|    iterations      | 4        |
|    time_elapsed    | 48       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0049054334 |
|    clip_fraction        | 0.0382       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.715        |
|    learning_rate        | 2.42e-05     |
|    loss                 | -0.0217      |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00426     |
|    value_loss           | 0.00136      |
------------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.0522  |
| time/              |          |
|    fps             | 650      |
|    iterations      | 5        |
|    time_elapsed    | 62       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.006141612 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.229       |
|    learning_rate        | 2.42e-05    |
|    loss                 | -0.00372    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 0.00148     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0243  |
| time/              |          |
|    fps             | 664      |
|    iterations      | 6        |
|    time_elapsed    | 74       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0068244063 |
|    clip_fraction        | 0.0563       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.246        |
|    learning_rate        | 2.42e-05     |
|    loss                 | -0.000486    |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00582     |
|    value_loss           | 0.00241      |
------------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0127  |
| time/              |          |
|    fps             | 648      |
|    iterations      | 7        |
|    time_elapsed    | 88       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0047069886 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.73        |
|    explained_variance   | 0.213        |
|    learning_rate        | 2.42e-05     |
|    loss                 | -0.00946     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00452     |
|    value_loss           | 0.00132      |
------------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0131  |
| time/              |          |
|    fps             | 635      |
|    iterations      | 8        |
|    time_elapsed    | 103      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:10:05,983] Trial 20 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 2.4235770063364326e-05, 'gamma': 0.9757722138735123, 'gae_lambda': 0.8022388810153552}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 447      |
|    ep_rew_mean     | 0.0434   |
| time/              |          |
|    fps             | 850      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.09 +/- 0.36
Episode length: 477.80 +/- 141.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 478         |
|    mean_reward          | -0.0911     |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.020541772 |
|    clip_fraction        | 0.0918      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -1.33       |
|    learning_rate        | 0.000484    |
|    loss                 | -0.0487     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.00237     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | -0.0107  |
| time/              |          |
|    fps             | 662      |
|    iterations      | 2        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.048061438 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -0.82       |
|    learning_rate        | 0.000484    |
|    loss                 | -0.0773     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.052      |
|    value_loss           | 0.000882    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0744  |
| time/              |          |
|    fps             | 666      |
|    iterations      | 3        |
|    time_elapsed    | 36       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.05341102 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.73      |
|    explained_variance   | -0.758     |
|    learning_rate        | 0.000484   |
|    loss                 | -0.0783    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.000123   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0747  |
| time/              |          |
|    fps             | 627      |
|    iterations      | 4        |
|    time_elapsed    | 52       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 525      |
|    mean_reward          | -0.21    |
| time/                   |          |
|    total_timesteps      | 35000    |
| train/                  |          |
|    approx_kl            | 0.072853 |
|    clip_fraction        | 0.415    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.69    |
|    explained_variance   | -0.0657  |
|    learning_rate        | 0.000484 |
|    loss                 | -0.11    |
|    n_updates            | 40       |
|    policy_gradient_loss | -0.0596  |
|    value_loss           | 0.000592 |
--------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.101   |
| time/              |          |
|    fps             | 606      |
|    iterations      | 5        |
|    time_elapsed    | 67       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0776425 |
|    clip_fraction        | 0.449     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.69     |
|    explained_variance   | -3.95     |
|    learning_rate        | 0.000484  |
|    loss                 | -0.0977   |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0664   |
|    value_loss           | 1.51e-05  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 616      |
|    iterations      | 6        |
|    time_elapsed    | 79       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.11382161 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.6       |
|    explained_variance   | -0.0119    |
|    learning_rate        | 0.000484   |
|    loss                 | -0.0954    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0558    |
|    value_loss           | 0.000312   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 602      |
|    iterations      | 7        |
|    time_elapsed    | 95       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.36
Episode length: 477.40 +/- 142.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 477        |
|    mean_reward          | -0.091     |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.12033101 |
|    clip_fraction        | 0.514      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.56      |
|    explained_variance   | -0.0154    |
|    learning_rate        | 0.000484   |
|    loss                 | -0.0965    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0614    |
|    value_loss           | 0.000881   |
----------------------------------------
New best mean reward!
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.118   |
| time/              |          |
|    fps             | 593      |
|    iterations      | 8        |
|    time_elapsed    | 110      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:12:02,341] Trial 21 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.0004844782504442384, 'gamma': 0.9202884249495694, 'gae_lambda': 0.8139502787671283}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 455      |
|    ep_rew_mean     | 0.0534   |
| time/              |          |
|    fps             | 848      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.09 +/- 0.36
Episode length: 473.70 +/- 153.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0895     |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008396639 |
|    clip_fraction        | 0.0526      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -1.17       |
|    learning_rate        | 0.000132    |
|    loss                 | -0.0242     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.00265     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-0.09 +/- 0.36
Episode length: 473.80 +/- 153.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | -0.0895  |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 456      |
|    ep_rew_mean     | 0.0749   |
| time/              |          |
|    fps             | 671      |
|    iterations      | 2        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010507256 |
|    clip_fraction        | 0.084       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.352      |
|    learning_rate        | 0.000132    |
|    loss                 | -0.0408     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 0.00235     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 459      |
|    ep_rew_mean     | 0.0619   |
| time/              |          |
|    fps             | 671      |
|    iterations      | 3        |
|    time_elapsed    | 36       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.013638476 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.404      |
|    learning_rate        | 0.000132    |
|    loss                 | -0.0559     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0305     |
|    value_loss           | 0.00209     |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 449      |
|    ep_rew_mean     | 0.0671   |
| time/              |          |
|    fps             | 626      |
|    iterations      | 4        |
|    time_elapsed    | 52       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.36
Episode length: 475.90 +/- 147.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | -0.0903     |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.016628008 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -0.301      |
|    learning_rate        | 0.000132    |
|    loss                 | -0.0347     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.00207     |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 446      |
|    ep_rew_mean     | 0.0632   |
| time/              |          |
|    fps             | 607      |
|    iterations      | 5        |
|    time_elapsed    | 67       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0174466 |
|    clip_fraction        | 0.177     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.72     |
|    explained_variance   | -0.345    |
|    learning_rate        | 0.000132  |
|    loss                 | -0.0656   |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0375   |
|    value_loss           | 0.00165   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 452      |
|    ep_rew_mean     | 0.0492   |
| time/              |          |
|    fps             | 618      |
|    iterations      | 6        |
|    time_elapsed    | 79       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 473.50 +/- 154.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0894     |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.020357456 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.329      |
|    learning_rate        | 0.000132    |
|    loss                 | -0.0792     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0383     |
|    value_loss           | 0.00158     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=-0.10 +/- 0.32
Episode length: 507.20 +/- 53.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | -0.103   |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 452      |
|    ep_rew_mean     | 0.0391   |
| time/              |          |
|    fps             | 607      |
|    iterations      | 7        |
|    time_elapsed    | 94       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.36
Episode length: 475.40 +/- 148.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 475         |
|    mean_reward          | -0.0902     |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.021752598 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | -0.455      |
|    learning_rate        | 0.000132    |
|    loss                 | -0.0824     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0435     |
|    value_loss           | 0.000997    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.09 +/- 0.36
Episode length: 476.70 +/- 144.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | -0.0906  |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 450      |
|    ep_rew_mean     | 0.0202   |
| time/              |          |
|    fps             | 600      |
|    iterations      | 8        |
|    time_elapsed    | 109      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:13:57,532] Trial 22 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.00013227463995107313, 'gamma': 0.9090057269373583, 'gae_lambda': 0.8162742249237773}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 847      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.021818975 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.000897    |
|    loss                 | -0.0192     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.00579     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.09 +/- 0.35
Episode length: 481.90 +/- 129.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 482      |
|    mean_reward     | -0.0927  |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 609      |
|    iterations      | 2        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 475.20 +/- 149.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 475        |
|    mean_reward          | -0.09      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.06841294 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.354      |
|    learning_rate        | 0.000897   |
|    loss                 | -0.0771    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0385    |
|    value_loss           | 0.00221    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.116   |
| time/              |          |
|    fps             | 602      |
|    iterations      | 3        |
|    time_elapsed    | 40       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.098336585 |
|    clip_fraction        | 0.4         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.000897    |
|    loss                 | -0.112      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0486     |
|    value_loss           | 0.00108     |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.36
Episode length: 475.80 +/- 147.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | -0.0903  |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.0898  |
| time/              |          |
|    fps             | 563      |
|    iterations      | 4        |
|    time_elapsed    | 58       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.14720982 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | -0.0791    |
|    learning_rate        | 0.000897   |
|    loss                 | -0.111     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0578    |
|    value_loss           | 0.00121    |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 5        |
|    time_elapsed    | 75       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.17258722 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | -0.34      |
|    learning_rate        | 0.000897   |
|    loss                 | -0.0597    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.000503   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.0846  |
| time/              |          |
|    fps             | 547      |
|    iterations      | 6        |
|    time_elapsed    | 89       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.17616992 |
|    clip_fraction        | 0.522      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.49      |
|    explained_variance   | -0.228     |
|    learning_rate        | 0.000897   |
|    loss                 | -0.0687    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0498    |
|    value_loss           | 0.00113    |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0628  |
| time/              |          |
|    fps             | 534      |
|    iterations      | 7        |
|    time_elapsed    | 107      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.22979558 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | -0.106     |
|    learning_rate        | 0.000897   |
|    loss                 | -0.0845    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.00113    |
----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.09 +/- 0.36
Episode length: 474.60 +/- 151.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 475      |
|    mean_reward     | -0.0898  |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 464      |
|    ep_rew_mean     | -0.0155  |
| time/              |          |
|    fps             | 527      |
|    iterations      | 8        |
|    time_elapsed    | 124      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:16:09,602] Trial 23 finished with value: -0.09211999999999998 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0008969669432095237, 'gamma': 0.921252295768642, 'gae_lambda': 0.8401567427774798}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | -0.138   |
| time/              |          |
|    fps             | 847      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.016471103 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.917      |
|    learning_rate        | 0.000264    |
|    loss                 | 0.00169     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.00158     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 456      |
|    ep_rew_mean     | -0.0109  |
| time/              |          |
|    fps             | 652      |
|    iterations      | 2        |
|    time_elapsed    | 25       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.020750789 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.182      |
|    learning_rate        | 0.000264    |
|    loss                 | -0.0453     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.034      |
|    value_loss           | 0.00178     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 463      |
|    ep_rew_mean     | -0.0312  |
| time/              |          |
|    fps             | 660      |
|    iterations      | 3        |
|    time_elapsed    | 37       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.028910186 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.135       |
|    learning_rate        | 0.000264    |
|    loss                 | -0.077      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0407     |
|    value_loss           | 0.000888    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 473      |
|    ep_rew_mean     | -0.0588  |
| time/              |          |
|    fps             | 623      |
|    iterations      | 4        |
|    time_elapsed    | 52       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.03930553 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | -0.203     |
|    learning_rate        | 0.000264   |
|    loss                 | -0.093     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.000499   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 476.00 +/- 147.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 476      |
|    mean_reward     | -0.0904  |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | -0.074   |
| time/              |          |
|    fps             | 606      |
|    iterations      | 5        |
|    time_elapsed    | 67       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.11 +/- 0.31
Episode length: 513.60 +/- 34.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 514         |
|    mean_reward          | -0.105      |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.035878763 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.0332      |
|    learning_rate        | 0.000264    |
|    loss                 | -0.0881     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0479     |
|    value_loss           | 0.00035     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | -0.0717  |
| time/              |          |
|    fps             | 617      |
|    iterations      | 6        |
|    time_elapsed    | 79       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.042402085 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | -0.00422    |
|    learning_rate        | 0.000264    |
|    loss                 | -0.1        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0513     |
|    value_loss           | 0.000621    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.10 +/- 0.33
Episode length: 499.40 +/- 76.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 499      |
|    mean_reward     | -0.0997  |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 470      |
|    ep_rew_mean     | -0.038   |
| time/              |          |
|    fps             | 604      |
|    iterations      | 7        |
|    time_elapsed    | 94       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.055532232 |
|    clip_fraction        | 0.374       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.0139      |
|    learning_rate        | 0.000264    |
|    loss                 | -0.094      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0505     |
|    value_loss           | 0.00115     |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | -0.0403  |
| time/              |          |
|    fps             | 595      |
|    iterations      | 8        |
|    time_elapsed    | 109      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:18:05,666] Trial 24 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.0002635657733676794, 'gamma': 0.9431803552487974, 'gae_lambda': 0.8044578315198949}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1262     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.00961847 |
|    clip_fraction        | 0.0581     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.79      |
|    explained_variance   | -0.13      |
|    learning_rate        | 0.0041     |
|    loss                 | -0.0123    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00562   |
|    value_loss           | 0.0765     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 786      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.016603475 |
|    clip_fraction        | 0.0943      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.0041      |
|    loss                 | -0.00901    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00955    |
|    value_loss           | 0.00164     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 697      |
|    iterations      | 3        |
|    time_elapsed    | 17       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.048164442 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0041      |
|    loss                 | -0.0394     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.000788    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 481      |
|    ep_rew_mean     | -0.101   |
| time/              |          |
|    fps             | 660      |
|    iterations      | 4        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.09386873 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | 0.0293     |
|    learning_rate        | 0.0041     |
|    loss                 | -0.0578    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00101   |
|    value_loss           | 0.00255    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 481      |
|    ep_rew_mean     | -0.0972  |
| time/              |          |
|    fps             | 639      |
|    iterations      | 5        |
|    time_elapsed    | 32       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 488        |
|    ep_rew_mean          | -0.115     |
| time/                   |            |
|    fps                  | 684        |
|    iterations           | 6          |
|    time_elapsed         | 35         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.21997595 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.54      |
|    explained_variance   | -0.306     |
|    learning_rate        | 0.0041     |
|    loss                 | -0.0508    |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.0166     |
|    value_loss           | 0.00205    |
----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.27548188 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.232      |
|    learning_rate        | 0.0041     |
|    loss                 | -0.0326    |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.147      |
|    value_loss           | 0.000557   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.109   |
| time/              |          |
|    fps             | 665      |
|    iterations      | 7        |
|    time_elapsed    | 43       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.20522252 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | -0.187     |
|    learning_rate        | 0.0041     |
|    loss                 | 0.00102    |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.000145   |
|    value_loss           | 0.00102    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.121   |
| time/              |          |
|    fps             | 652      |
|    iterations      | 8        |
|    time_elapsed    | 50       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.14209525 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.51      |
|    explained_variance   | -4.79      |
|    learning_rate        | 0.0041     |
|    loss                 | -0.0685    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0314    |
|    value_loss           | 0.000437   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.102   |
| time/              |          |
|    fps             | 641      |
|    iterations      | 9        |
|    time_elapsed    | 57       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 474.50 +/- 151.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 474        |
|    mean_reward          | -0.0898    |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.09815931 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | -0.027     |
|    learning_rate        | 0.0041     |
|    loss                 | -0.0159    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0251    |
|    value_loss           | 0.00154    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.111   |
| time/              |          |
|    fps             | 636      |
|    iterations      | 10       |
|    time_elapsed    | 64       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.11 +/- 0.31
Episode length: 513.80 +/- 33.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 514        |
|    mean_reward          | -0.106     |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.29176074 |
|    clip_fraction        | 0.556      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | -0.859     |
|    learning_rate        | 0.0041     |
|    loss                 | -0.0741    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.00904   |
|    value_loss           | 0.00139    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.109   |
| time/              |          |
|    fps             | 631      |
|    iterations      | 11       |
|    time_elapsed    | 71       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 494        |
|    ep_rew_mean          | -0.107     |
| time/                   |            |
|    fps                  | 652        |
|    iterations           | 12         |
|    time_elapsed         | 75         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.21816301 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.42      |
|    explained_variance   | -0.655     |
|    learning_rate        | 0.0041     |
|    loss                 | -0.053     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0065    |
|    value_loss           | 0.00195    |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.16211861 |
|    clip_fraction        | 0.519      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | -0.354     |
|    learning_rate        | 0.0041     |
|    loss                 | -0.0533    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0397    |
|    value_loss           | 0.000789   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0965  |
| time/              |          |
|    fps             | 646      |
|    iterations      | 13       |
|    time_elapsed    | 82       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.32059085 |
|    clip_fraction        | 0.486      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | -0.143     |
|    learning_rate        | 0.0041     |
|    loss                 | -0.0634    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0306    |
|    value_loss           | 0.00112    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0733  |
| time/              |          |
|    fps             | 640      |
|    iterations      | 14       |
|    time_elapsed    | 89       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.39076012 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | -0.478     |
|    learning_rate        | 0.0041     |
|    loss                 | -0.0601    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0221    |
|    value_loss           | 0.00306    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | -0.0616  |
| time/              |          |
|    fps             | 635      |
|    iterations      | 15       |
|    time_elapsed    | 96       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 03:19:46,572] Trial 25 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.004096355915306943, 'gamma': 0.9326427942399673, 'gae_lambda': 0.8207117088450908}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.0699  |
| time/              |          |
|    fps             | 848      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.041766748 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.0339      |
|    learning_rate        | 0.00114     |
|    loss                 | -0.00893    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 0.00494     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 462      |
|    ep_rew_mean     | -0.0132  |
| time/              |          |
|    fps             | 651      |
|    iterations      | 2        |
|    time_elapsed    | 25       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 473.90 +/- 153.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 474        |
|    mean_reward          | -0.0895    |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.06990188 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | -0.182     |
|    learning_rate        | 0.00114    |
|    loss                 | -0.0557    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0231    |
|    value_loss           | 0.00225    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 481      |
|    ep_rew_mean     | -0.0749  |
| time/              |          |
|    fps             | 663      |
|    iterations      | 3        |
|    time_elapsed    | 37       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.07495053 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | -0.442     |
|    learning_rate        | 0.00114    |
|    loss                 | -0.0517    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.000263   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0733  |
| time/              |          |
|    fps             | 624      |
|    iterations      | 4        |
|    time_elapsed    | 52       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.11541274 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.61      |
|    explained_variance   | -0.27      |
|    learning_rate        | 0.00114    |
|    loss                 | -0.0647    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.000732   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.0997  |
| time/              |          |
|    fps             | 604      |
|    iterations      | 5        |
|    time_elapsed    | 67       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.16217062 |
|    clip_fraction        | 0.479      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.56      |
|    explained_variance   | -1.51      |
|    learning_rate        | 0.00114    |
|    loss                 | -0.0753    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0559    |
|    value_loss           | 0.000106   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.117   |
| time/              |          |
|    fps             | 614      |
|    iterations      | 6        |
|    time_elapsed    | 79       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.19197184 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | -3.64      |
|    learning_rate        | 0.00114    |
|    loss                 | -0.0964    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0543    |
|    value_loss           | 9.55e-05   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.129   |
| time/              |          |
|    fps             | 601      |
|    iterations      | 7        |
|    time_elapsed    | 95       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.19050951 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | -0.0496    |
|    learning_rate        | 0.00114    |
|    loss                 | -0.0853    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.056     |
|    value_loss           | 0.000454   |
----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | -0.176   |
| time/              |          |
|    fps             | 592      |
|    iterations      | 8        |
|    time_elapsed    | 110      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:21:43,211] Trial 26 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.001143561336287539, 'gamma': 0.9072839865390219, 'gae_lambda': 0.8597162076293259}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 405      |
|    ep_rew_mean     | 0.171    |
| time/              |          |
|    fps             | 1291     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0094464775 |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -0.323       |
|    learning_rate        | 0.000108     |
|    loss                 | -0.0175      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0122      |
|    value_loss           | 0.00353      |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 462      |
|    ep_rew_mean     | -0.00819 |
| time/              |          |
|    fps             | 747      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010592506 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -1.02       |
|    learning_rate        | 0.000108    |
|    loss                 | -0.0432     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 0.00268     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | 0.00791  |
| time/              |          |
|    fps             | 659      |
|    iterations      | 3        |
|    time_elapsed    | 18       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0097867865 |
|    clip_fraction        | 0.0796       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -0.301       |
|    learning_rate        | 0.000108     |
|    loss                 | -0.0493      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0205      |
|    value_loss           | 0.00224      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | -0.0102  |
| time/              |          |
|    fps             | 622      |
|    iterations      | 4        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012025305 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -0.815      |
|    learning_rate        | 0.000108    |
|    loss                 | -0.0418     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.00173     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0482  |
| time/              |          |
|    fps             | 604      |
|    iterations      | 5        |
|    time_elapsed    | 33       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 478         |
|    ep_rew_mean          | -0.0538     |
| time/                   |             |
|    fps                  | 642         |
|    iterations           | 6           |
|    time_elapsed         | 38          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.016689263 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -0.279      |
|    learning_rate        | 0.000108    |
|    loss                 | -0.0621     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0292     |
|    value_loss           | 0.000912    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012927289 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -0.952      |
|    learning_rate        | 0.000108    |
|    loss                 | -0.0242     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0312     |
|    value_loss           | 0.00152     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0556  |
| time/              |          |
|    fps             | 623      |
|    iterations      | 7        |
|    time_elapsed    | 45       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.01779836 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | -1.6       |
|    learning_rate        | 0.000108   |
|    loss                 | -0.0665    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0358    |
|    value_loss           | 0.00181    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.074   |
| time/              |          |
|    fps             | 610      |
|    iterations      | 8        |
|    time_elapsed    | 53       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.018081903 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -3.54       |
|    learning_rate        | 0.000108    |
|    loss                 | -0.0462     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0356     |
|    value_loss           | 0.000965    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.0885  |
| time/              |          |
|    fps             | 599      |
|    iterations      | 9        |
|    time_elapsed    | 61       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.016873086 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -4.87       |
|    learning_rate        | 0.000108    |
|    loss                 | -0.0289     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0376     |
|    value_loss           | 0.000882    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.0867  |
| time/              |          |
|    fps             | 590      |
|    iterations      | 10       |
|    time_elapsed    | 69       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.019294575 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | -1.38       |
|    learning_rate        | 0.000108    |
|    loss                 | -0.0696     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0382     |
|    value_loss           | 0.00149     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0748  |
| time/              |          |
|    fps             | 584      |
|    iterations      | 11       |
|    time_elapsed    | 77       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 489         |
|    ep_rew_mean          | -0.0856     |
| time/                   |             |
|    fps                  | 602         |
|    iterations           | 12          |
|    time_elapsed         | 81          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.020680778 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | -0.802      |
|    learning_rate        | 0.000108    |
|    loss                 | -0.0658     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0418     |
|    value_loss           | 0.00213     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.019718153 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | -4.88       |
|    learning_rate        | 0.000108    |
|    loss                 | -0.066      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.042      |
|    value_loss           | 0.000684    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 596      |
|    iterations      | 13       |
|    time_elapsed    | 89       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.021220343 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | -7.1        |
|    learning_rate        | 0.000108    |
|    loss                 | -0.0513     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0423     |
|    value_loss           | 0.000773    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.109   |
| time/              |          |
|    fps             | 591      |
|    iterations      | 14       |
|    time_elapsed    | 96       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.024548907 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | -1.63       |
|    learning_rate        | 0.000108    |
|    loss                 | -0.0732     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0449     |
|    value_loss           | 0.00152     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 587      |
|    iterations      | 15       |
|    time_elapsed    | 104      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 03:23:32,565] Trial 27 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.00010751538269618867, 'gamma': 0.9168951142510283, 'gae_lambda': 0.8025114309592167}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 849      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0046358043 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -4.27        |
|    learning_rate        | 3.06e-05     |
|    loss                 | -0.00489     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00415     |
|    value_loss           | 0.00155      |
------------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 463      |
|    ep_rew_mean     | -0.0136  |
| time/              |          |
|    fps             | 597      |
|    iterations      | 2        |
|    time_elapsed    | 27       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0066663004 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -0.351       |
|    learning_rate        | 3.06e-05     |
|    loss                 | 0.0192       |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00553     |
|    value_loss           | 0.00251      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | -0.0138  |
| time/              |          |
|    fps             | 588      |
|    iterations      | 3        |
|    time_elapsed    | 41       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0065733865 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -1.04        |
|    learning_rate        | 3.06e-05     |
|    loss                 | -0.0284      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00946     |
|    value_loss           | 0.00232      |
------------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 481      |
|    ep_rew_mean     | -0.0158  |
| time/              |          |
|    fps             | 555      |
|    iterations      | 4        |
|    time_elapsed    | 59       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0067466497 |
|    clip_fraction        | 0.0473       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | -0.59        |
|    learning_rate        | 3.06e-05     |
|    loss                 | -0.00731     |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00954     |
|    value_loss           | 0.00216      |
------------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 465      |
|    ep_rew_mean     | 0.00739  |
| time/              |          |
|    fps             | 534      |
|    iterations      | 5        |
|    time_elapsed    | 76       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0073033334 |
|    clip_fraction        | 0.0601       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | -0.403       |
|    learning_rate        | 3.06e-05     |
|    loss                 | -0.0324      |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 0.00282      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 471      |
|    ep_rew_mean     | -0.00849 |
| time/              |          |
|    fps             | 541      |
|    iterations      | 6        |
|    time_elapsed    | 90       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.007890927 |
|    clip_fraction        | 0.0723      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -1.3        |
|    learning_rate        | 3.06e-05    |
|    loss                 | -0.0272     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.00128     |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | -0.00888 |
| time/              |          |
|    fps             | 530      |
|    iterations      | 7        |
|    time_elapsed    | 108      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.007299916 |
|    clip_fraction        | 0.0643      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -0.926      |
|    learning_rate        | 3.06e-05    |
|    loss                 | -0.0274     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.00168     |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | -0.0315  |
| time/              |          |
|    fps             | 521      |
|    iterations      | 8        |
|    time_elapsed    | 125      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:25:46,183] Trial 28 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 3.0649871830825446e-05, 'gamma': 0.9288970186826656, 'gae_lambda': 0.9087399275301853}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 466      |
|    ep_rew_mean     | -0.0613  |
| time/              |          |
|    fps             | 1281     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 5000          |
| train/                  |               |
|    approx_kl            | 0.00024279226 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.79         |
|    explained_variance   | -1.6          |
|    learning_rate        | 1.06e-05      |
|    loss                 | 0.00211       |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.000887     |
|    value_loss           | 0.00418       |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.136   |
| time/              |          |
|    fps             | 790      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0005660421 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -0.371       |
|    learning_rate        | 1.06e-05     |
|    loss                 | 0.00252      |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000683    |
|    value_loss           | 0.000969     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.116   |
| time/              |          |
|    fps             | 702      |
|    iterations      | 3        |
|    time_elapsed    | 17       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0009985387 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -0.858       |
|    learning_rate        | 1.06e-05     |
|    loss                 | -0.002       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 0.00165      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | -0.044   |
| time/              |          |
|    fps             | 666      |
|    iterations      | 4        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0017244336 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -0.344       |
|    learning_rate        | 1.06e-05     |
|    loss                 | -0.00449     |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.000583    |
|    value_loss           | 0.0024       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | -0.0474  |
| time/              |          |
|    fps             | 646      |
|    iterations      | 5        |
|    time_elapsed    | 31       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 469         |
|    ep_rew_mean          | -0.0529     |
| time/                   |             |
|    fps                  | 690         |
|    iterations           | 6           |
|    time_elapsed         | 35          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.002747097 |
|    clip_fraction        | 0.00881     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.438      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 0.00372     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 0.000973    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.003951141 |
|    clip_fraction        | 0.00742     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -1.02       |
|    learning_rate        | 1.06e-05    |
|    loss                 | -0.0238     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00301    |
|    value_loss           | 0.00134     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | -0.0543  |
| time/              |          |
|    fps             | 671      |
|    iterations      | 7        |
|    time_elapsed    | 42       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0019498802 |
|    clip_fraction        | 0.00173      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -1.28        |
|    learning_rate        | 1.06e-05     |
|    loss                 | -0.00431     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 0.00152      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 464      |
|    ep_rew_mean     | -0.0286  |
| time/              |          |
|    fps             | 657      |
|    iterations      | 8        |
|    time_elapsed    | 49       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0030805976 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -0.396       |
|    learning_rate        | 1.06e-05     |
|    loss                 | -0.00458     |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 0.00264      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 465      |
|    ep_rew_mean     | -0.0342  |
| time/              |          |
|    fps             | 646      |
|    iterations      | 9        |
|    time_elapsed    | 57       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 474.80 +/- 150.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 475          |
|    mean_reward          | -0.0899      |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0036418266 |
|    clip_fraction        | 0.00959      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | -0.442       |
|    learning_rate        | 1.06e-05     |
|    loss                 | -0.0152      |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00301     |
|    value_loss           | 0.00101      |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | -0.0379  |
| time/              |          |
|    fps             | 641      |
|    iterations      | 10       |
|    time_elapsed    | 63       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.003931072 |
|    clip_fraction        | 0.00845     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -0.979      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 0.00533     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00189    |
|    value_loss           | 0.00131     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 473      |
|    ep_rew_mean     | -0.0524  |
| time/              |          |
|    fps             | 635      |
|    iterations      | 11       |
|    time_elapsed    | 70       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 480          |
|    ep_rew_mean          | -0.0621      |
| time/                   |              |
|    fps                  | 657          |
|    iterations           | 12           |
|    time_elapsed         | 74           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0041558947 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.73        |
|    explained_variance   | -5.15        |
|    learning_rate        | 1.06e-05     |
|    loss                 | -0.00922     |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.0023      |
|    value_loss           | 0.000536     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0061205886 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.73        |
|    explained_variance   | -0.653       |
|    learning_rate        | 1.06e-05     |
|    loss                 | -0.000609    |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00285     |
|    value_loss           | 0.00114      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | -0.0517  |
| time/              |          |
|    fps             | 649      |
|    iterations      | 13       |
|    time_elapsed    | 81       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0034723012 |
|    clip_fraction        | 0.00579      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | -0.69        |
|    learning_rate        | 1.06e-05     |
|    loss                 | -0.00905     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 0.00113      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | -0.0517  |
| time/              |          |
|    fps             | 643      |
|    iterations      | 14       |
|    time_elapsed    | 89       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0026505692 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.73        |
|    explained_variance   | -6.82        |
|    learning_rate        | 1.06e-05     |
|    loss                 | -0.00556     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 0.000378     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0745  |
| time/              |          |
|    fps             | 637      |
|    iterations      | 15       |
|    time_elapsed    | 96       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 03:27:26,798] Trial 29 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 1.0580276745743006e-05, 'gamma': 0.9383545988888553, 'gae_lambda': 0.8359408221520739}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0677  |
| time/              |          |
|    fps             | 846      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.00931591 |
|    clip_fraction        | 0.0811     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | -1.83      |
|    learning_rate        | 0.00161    |
|    loss                 | -0.0333    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00478   |
|    value_loss           | 0.00957    |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.105   |
| time/              |          |
|    fps             | 652      |
|    iterations      | 2        |
|    time_elapsed    | 25       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.039479252 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -0.472      |
|    learning_rate        | 0.00161     |
|    loss                 | -0.0388     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.000936    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | -0.0954  |
| time/              |          |
|    fps             | 658      |
|    iterations      | 3        |
|    time_elapsed    | 37       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.36
Episode length: 477.20 +/- 143.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 477        |
|    mean_reward          | -0.0908    |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.06986745 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | -0.193     |
|    learning_rate        | 0.00161    |
|    loss                 | -0.0693    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0294    |
|    value_loss           | 0.000864   |
----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | -0.0763  |
| time/              |          |
|    fps             | 624      |
|    iterations      | 4        |
|    time_elapsed    | 52       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.11946073 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | -0.145     |
|    learning_rate        | 0.00161    |
|    loss                 | -0.0791    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0324    |
|    value_loss           | 0.00108    |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.0767  |
| time/              |          |
|    fps             | 604      |
|    iterations      | 5        |
|    time_elapsed    | 67       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.10 +/- 0.33
Episode length: 502.30 +/- 68.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 502        |
|    mean_reward          | -0.101     |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.12638107 |
|    clip_fraction        | 0.417      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | -0.15      |
|    learning_rate        | 0.00161    |
|    loss                 | -0.0642    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0365    |
|    value_loss           | 0.000783   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.0871  |
| time/              |          |
|    fps             | 615      |
|    iterations      | 6        |
|    time_elapsed    | 79       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.15560368 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | -0.974     |
|    learning_rate        | 0.00161    |
|    loss                 | -0.0702    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.000794   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.0901  |
| time/              |          |
|    fps             | 601      |
|    iterations      | 7        |
|    time_elapsed    | 95       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.19251436 |
|    clip_fraction        | 0.507      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | -0.159     |
|    learning_rate        | 0.00161    |
|    loss                 | -0.094     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0505    |
|    value_loss           | 0.00083    |
----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.0782  |
| time/              |          |
|    fps             | 592      |
|    iterations      | 8        |
|    time_elapsed    | 110      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:29:23,452] Trial 30 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.0016086292317792565, 'gamma': 0.9057413918061503, 'gae_lambda': 0.8783639703928542}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 849      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.025840165 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.512      |
|    learning_rate        | 0.000427    |
|    loss                 | -0.0537     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.00238     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | -0.174   |
| time/              |          |
|    fps             | 607      |
|    iterations      | 2        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.050167568 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.176       |
|    learning_rate        | 0.000427    |
|    loss                 | -0.0523     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0462     |
|    value_loss           | 0.000502    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 519      |
|    ep_rew_mean     | -0.186   |
| time/              |          |
|    fps             | 597      |
|    iterations      | 3        |
|    time_elapsed    | 41       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.068015665 |
|    clip_fraction        | 0.395       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.19        |
|    learning_rate        | 0.000427    |
|    loss                 | -0.0988     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0622     |
|    value_loss           | 0.000153    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.175   |
| time/              |          |
|    fps             | 558      |
|    iterations      | 4        |
|    time_elapsed    | 58       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0880266 |
|    clip_fraction        | 0.488     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.67     |
|    explained_variance   | 0.0552    |
|    learning_rate        | 0.000427  |
|    loss                 | -0.0976   |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0693   |
|    value_loss           | 0.000413  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | -0.168   |
| time/              |          |
|    fps             | 538      |
|    iterations      | 5        |
|    time_elapsed    | 76       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.10 +/- 0.34
Episode length: 495.10 +/- 89.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 495         |
|    mean_reward          | -0.098      |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.112852454 |
|    clip_fraction        | 0.504       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | -0.114      |
|    learning_rate        | 0.000427    |
|    loss                 | -0.119      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0709     |
|    value_loss           | 0.000399    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | -0.153   |
| time/              |          |
|    fps             | 546      |
|    iterations      | 6        |
|    time_elapsed    | 89       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.11848714 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.6       |
|    explained_variance   | -0.018     |
|    learning_rate        | 0.000427   |
|    loss                 | -0.102     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0749    |
|    value_loss           | 0.000696   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.09 +/- 0.36
Episode length: 476.60 +/- 145.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 477      |
|    mean_reward     | -0.0906  |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | -0.145   |
| time/              |          |
|    fps             | 535      |
|    iterations      | 7        |
|    time_elapsed    | 107      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.35
Episode length: 480.50 +/- 133.50
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 480      |
|    mean_reward          | -0.0922  |
| time/                   |          |
|    total_timesteps      | 60000    |
| train/                  |          |
|    approx_kl            | 0.138075 |
|    clip_fraction        | 0.56     |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.56    |
|    explained_variance   | -0.0429  |
|    learning_rate        | 0.000427 |
|    loss                 | -0.127   |
|    n_updates            | 70       |
|    policy_gradient_loss | -0.0737  |
|    value_loss           | 0.000689 |
--------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | -0.145   |
| time/              |          |
|    fps             | 527      |
|    iterations      | 8        |
|    time_elapsed    | 124      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:31:35,735] Trial 31 finished with value: -0.09019999999999999 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.000426994415745603, 'gamma': 0.9223101723196171, 'gae_lambda': 0.8414852298565615}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.0739  |
| time/              |          |
|    fps             | 846      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.028898565 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.586      |
|    learning_rate        | 0.000406    |
|    loss                 | -0.0589     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.00209     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | -0.0157  |
| time/              |          |
|    fps             | 599      |
|    iterations      | 2        |
|    time_elapsed    | 27       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.0502418 |
|    clip_fraction        | 0.289     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.75     |
|    explained_variance   | 0.141     |
|    learning_rate        | 0.000406  |
|    loss                 | -0.1      |
|    n_updates            | 20        |
|    policy_gradient_loss | -0.0475   |
|    value_loss           | 0.00152   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | -0.0134  |
| time/              |          |
|    fps             | 595      |
|    iterations      | 3        |
|    time_elapsed    | 41       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.0753253 |
|    clip_fraction        | 0.434     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.69     |
|    explained_variance   | -0.0127   |
|    learning_rate        | 0.000406  |
|    loss                 | -0.108    |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.0613   |
|    value_loss           | 0.00106   |
---------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | -0.0132  |
| time/              |          |
|    fps             | 561      |
|    iterations      | 4        |
|    time_elapsed    | 58       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.09431266 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | 0.0247     |
|    learning_rate        | 0.000406   |
|    loss                 | -0.13      |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0666    |
|    value_loss           | 0.000987   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | -0.0159  |
| time/              |          |
|    fps             | 539      |
|    iterations      | 5        |
|    time_elapsed    | 75       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.108537406 |
|    clip_fraction        | 0.518       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.013       |
|    learning_rate        | 0.000406    |
|    loss                 | -0.132      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0711     |
|    value_loss           | 0.000951    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | -0.0109  |
| time/              |          |
|    fps             | 545      |
|    iterations      | 6        |
|    time_elapsed    | 90       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.13064285 |
|    clip_fraction        | 0.549      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | -0.0183    |
|    learning_rate        | 0.000406   |
|    loss                 | -0.121     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0724    |
|    value_loss           | 0.00098    |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.11 +/- 0.30
Episode length: 522.60 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 523      |
|    mean_reward     | -0.109   |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0112  |
| time/              |          |
|    fps             | 534      |
|    iterations      | 7        |
|    time_elapsed    | 107      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.13404699 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | -0.0413    |
|    learning_rate        | 0.000406   |
|    loss                 | -0.103     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0727    |
|    value_loss           | 0.00123    |
----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | -0.0122  |
| time/              |          |
|    fps             | 526      |
|    iterations      | 8        |
|    time_elapsed    | 124      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:33:48,403] Trial 32 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.00040600932887834885, 'gamma': 0.9258309533777324, 'gae_lambda': 0.8171778192805353}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.09 +/- 0.35
Episode length: 479.50 +/- 136.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 480      |
|    mean_reward     | -0.0918  |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 460      |
|    ep_rew_mean     | -0.00758 |
| time/              |          |
|    fps             | 881      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.021975644 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -1.46       |
|    learning_rate        | 0.000269    |
|    loss                 | -0.0544     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.00205     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | -0.0435  |
| time/              |          |
|    fps             | 614      |
|    iterations      | 2        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.02805049 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.76      |
|    explained_variance   | -0.375     |
|    learning_rate        | 0.000269   |
|    loss                 | -0.0281    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0383    |
|    value_loss           | 0.000906   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.0748  |
| time/              |          |
|    fps             | 601      |
|    iterations      | 3        |
|    time_elapsed    | 40       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.046919942 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | -0.682      |
|    learning_rate        | 0.000269    |
|    loss                 | -0.0912     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0568     |
|    value_loss           | 0.000472    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0747  |
| time/              |          |
|    fps             | 561      |
|    iterations      | 4        |
|    time_elapsed    | 58       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.054637536 |
|    clip_fraction        | 0.379       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.104      |
|    learning_rate        | 0.000269    |
|    loss                 | -0.107      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0604     |
|    value_loss           | 0.000654    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 474.00 +/- 153.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | -0.0896  |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.0878  |
| time/              |          |
|    fps             | 542      |
|    iterations      | 5        |
|    time_elapsed    | 75       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0658453 |
|    clip_fraction        | 0.41      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.68     |
|    explained_variance   | -0.169    |
|    learning_rate        | 0.000269  |
|    loss                 | -0.13     |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.067    |
|    value_loss           | 0.00035   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.086   |
| time/              |          |
|    fps             | 547      |
|    iterations      | 6        |
|    time_elapsed    | 89       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.07514212 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | 0.0144     |
|    learning_rate        | 0.000269   |
|    loss                 | -0.0969    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.062     |
|    value_loss           | 0.000663   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.109   |
| time/              |          |
|    fps             | 534      |
|    iterations      | 7        |
|    time_elapsed    | 107      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.08833933 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | -0.148     |
|    learning_rate        | 0.000269   |
|    loss                 | -0.118     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0707    |
|    value_loss           | 0.000368   |
----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 524      |
|    iterations      | 8        |
|    time_elapsed    | 125      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:36:01,657] Trial 33 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.00026861359812714277, 'gamma': 0.9161880819864877, 'gae_lambda': 0.8471475042106072}. Best is trial 5 with value: -0.08930999999999999.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 851      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.09 +/- 0.35
Episode length: 480.70 +/- 132.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 481         |
|    mean_reward          | -0.0922     |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.028385194 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.331      |
|    learning_rate        | 0.000652    |
|    loss                 | -0.00876    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.00188     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | -0.173   |
| time/              |          |
|    fps             | 606      |
|    iterations      | 2        |
|    time_elapsed    | 27       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.10 +/- 0.34
Episode length: 493.80 +/- 93.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 494        |
|    mean_reward          | -0.0975    |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.06083215 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.75      |
|    explained_variance   | 0.193      |
|    learning_rate        | 0.000652   |
|    loss                 | -0.051     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.000559   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 601      |
|    iterations      | 3        |
|    time_elapsed    | 40       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.1017633 |
|    clip_fraction        | 0.458     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.68     |
|    explained_variance   | 0.0597    |
|    learning_rate        | 0.000652  |
|    loss                 | -0.102    |
|    n_updates            | 30        |
|    policy_gradient_loss | -0.0604   |
|    value_loss           | 0.000779  |
---------------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.35
Episode length: 480.90 +/- 132.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 481      |
|    mean_reward     | -0.0923  |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.157   |
| time/              |          |
|    fps             | 564      |
|    iterations      | 4        |
|    time_elapsed    | 58       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.16379666 |
|    clip_fraction        | 0.539      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.6       |
|    explained_variance   | -0.268     |
|    learning_rate        | 0.000652   |
|    loss                 | -0.118     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0672    |
|    value_loss           | 7.12e-05   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.128   |
| time/              |          |
|    fps             | 541      |
|    iterations      | 5        |
|    time_elapsed    | 75       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.11349222 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.64      |
|    explained_variance   | 0.0192     |
|    learning_rate        | 0.000652   |
|    loss                 | -0.0918    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0598    |
|    value_loss           | 0.00101    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.0874  |
| time/              |          |
|    fps             | 548      |
|    iterations      | 6        |
|    time_elapsed    | 89       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 478.10 +/- 140.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 478       |
|    mean_reward          | -0.0912   |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.2023635 |
|    clip_fraction        | 0.59      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.52     |
|    explained_variance   | -0.0481   |
|    learning_rate        | 0.000652  |
|    loss                 | -0.119    |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0595   |
|    value_loss           | 0.00166   |
---------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.0789  |
| time/              |          |
|    fps             | 536      |
|    iterations      | 7        |
|    time_elapsed    | 106      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=0.01 +/- 0.45
Episode length: 468.60 +/- 140.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 469        |
|    mean_reward          | 0.0126     |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.19205269 |
|    clip_fraction        | 0.608      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | -0.356     |
|    learning_rate        | 0.000652   |
|    loss                 | -0.0749    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0716    |
|    value_loss           | 0.000404   |
----------------------------------------
New best mean reward!
Eval num_timesteps=65000, episode_reward=-0.10 +/- 0.34
Episode length: 494.00 +/- 93.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.0976  |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0211  |
| time/              |          |
|    fps             | 528      |
|    iterations      | 8        |
|    time_elapsed    | 123      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:38:12,778] Trial 34 finished with value: 0.026290000000000025 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0006516999665573909, 'gamma': 0.9345474231070908, 'gae_lambda': 0.8147459509364795}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 463      |
|    ep_rew_mean     | 0.0501   |
| time/              |          |
|    fps             | 849      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.019211777 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.127      |
|    learning_rate        | 0.000647    |
|    loss                 | -0.0311     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.00331     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 481      |
|    ep_rew_mean     | -0.041   |
| time/              |          |
|    fps             | 592      |
|    iterations      | 2        |
|    time_elapsed    | 27       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.054504424 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.000647    |
|    loss                 | -0.0556     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0442     |
|    value_loss           | 0.000695    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.0962  |
| time/              |          |
|    fps             | 585      |
|    iterations      | 3        |
|    time_elapsed    | 41       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.08058818 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.0998     |
|    learning_rate        | 0.000647   |
|    loss                 | -0.0557    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0525    |
|    value_loss           | 0.000126   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.108   |
| time/              |          |
|    fps             | 550      |
|    iterations      | 4        |
|    time_elapsed    | 59       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.11921336 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.66      |
|    explained_variance   | -0.0891    |
|    learning_rate        | 0.000647   |
|    loss                 | -0.0834    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0543    |
|    value_loss           | 0.000387   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.0998  |
| time/              |          |
|    fps             | 532      |
|    iterations      | 5        |
|    time_elapsed    | 76       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.12410903 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.64      |
|    explained_variance   | -0.101     |
|    learning_rate        | 0.000647   |
|    loss                 | -0.0752    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0581    |
|    value_loss           | 0.00064    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 539      |
|    iterations      | 6        |
|    time_elapsed    | 91       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.13921365 |
|    clip_fraction        | 0.518      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.6       |
|    explained_variance   | -0.187     |
|    learning_rate        | 0.000647   |
|    loss                 | -0.0971    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0666    |
|    value_loss           | 0.000385   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 528      |
|    iterations      | 7        |
|    time_elapsed    | 108      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.1759732 |
|    clip_fraction        | 0.518     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.57     |
|    explained_variance   | -0.321    |
|    learning_rate        | 0.000647  |
|    loss                 | -0.0169   |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.0651   |
|    value_loss           | 0.000399  |
---------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.151   |
| time/              |          |
|    fps             | 520      |
|    iterations      | 8        |
|    time_elapsed    | 125      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:40:27,049] Trial 35 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0006467918134786441, 'gamma': 0.9478045604066259, 'gae_lambda': 0.8138168225762331}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 438      |
|    ep_rew_mean     | 0.047    |
| time/              |          |
|    fps             | 1289     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0137603525 |
|    clip_fraction        | 0.0859       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.27         |
|    learning_rate        | 0.00445      |
|    loss                 | -0.00367     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00814     |
|    value_loss           | 0.0539       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 451      |
|    ep_rew_mean     | -0.0041  |
| time/              |          |
|    fps             | 793      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.02249971 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.00445    |
|    loss                 | -0.00563   |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.01      |
|    value_loss           | 0.00343    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | -0.07    |
| time/              |          |
|    fps             | 701      |
|    iterations      | 3        |
|    time_elapsed    | 17       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.044903927 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.00445     |
|    loss                 | -0.0476     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.00289     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.104   |
| time/              |          |
|    fps             | 663      |
|    iterations      | 4        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.058734234 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.00445     |
|    loss                 | -0.0469     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.000609    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0986  |
| time/              |          |
|    fps             | 642      |
|    iterations      | 5        |
|    time_elapsed    | 31       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 472        |
|    ep_rew_mean          | -0.0515    |
| time/                   |            |
|    fps                  | 686        |
|    iterations           | 6          |
|    time_elapsed         | 35         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.10615756 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.61      |
|    explained_variance   | -0.229     |
|    learning_rate        | 0.00445    |
|    loss                 | 0.0375     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0123    |
|    value_loss           | 0.00149    |
----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.22036578 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | -0.0749    |
|    learning_rate        | 0.00445    |
|    loss                 | -0.00935   |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.00228    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 471      |
|    ep_rew_mean     | -0.0384  |
| time/              |          |
|    fps             | 667      |
|    iterations      | 7        |
|    time_elapsed    | 42       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.09 +/- 0.36
Episode length: 479.10 +/- 137.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 479         |
|    mean_reward          | -0.0916     |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.120220385 |
|    clip_fraction        | 0.406       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | -0.0129     |
|    learning_rate        | 0.00445     |
|    loss                 | -0.0439     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.00191     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 466      |
|    ep_rew_mean     | -0.0148  |
| time/              |          |
|    fps             | 655      |
|    iterations      | 8        |
|    time_elapsed    | 49       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.24789974 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.47      |
|    explained_variance   | -0.0948    |
|    learning_rate        | 0.00445    |
|    loss                 | -0.0361    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0282    |
|    value_loss           | 0.00286    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | -0.0206  |
| time/              |          |
|    fps             | 645      |
|    iterations      | 9        |
|    time_elapsed    | 57       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.31785145 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | -1.56      |
|    learning_rate        | 0.00445    |
|    loss                 | -0.0888    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0291    |
|    value_loss           | 0.00169    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | -0.0382  |
| time/              |          |
|    fps             | 637      |
|    iterations      | 10       |
|    time_elapsed    | 64       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.27130628 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | -2.3       |
|    learning_rate        | 0.00445    |
|    loss                 | -0.0716    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0312    |
|    value_loss           | 0.000637   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | -0.0201  |
| time/              |          |
|    fps             | 630      |
|    iterations      | 11       |
|    time_elapsed    | 71       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 473        |
|    ep_rew_mean          | -0.039     |
| time/                   |            |
|    fps                  | 652        |
|    iterations           | 12         |
|    time_elapsed         | 75         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.31275874 |
|    clip_fraction        | 0.521      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | -0.126     |
|    learning_rate        | 0.00445    |
|    loss                 | -0.00586   |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.00342    |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.23074266 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | -0.302     |
|    learning_rate        | 0.00445    |
|    loss                 | -0.0936    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0452    |
|    value_loss           | 0.00251    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | -0.017   |
| time/              |          |
|    fps             | 645      |
|    iterations      | 13       |
|    time_elapsed    | 82       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.45633817 |
|    clip_fraction        | 0.581      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | -0.123     |
|    learning_rate        | 0.00445    |
|    loss                 | -0.0794    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0356    |
|    value_loss           | 0.00206    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | -0.0289  |
| time/              |          |
|    fps             | 639      |
|    iterations      | 14       |
|    time_elapsed    | 89       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.41009015 |
|    clip_fraction        | 0.574      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | -5.88      |
|    learning_rate        | 0.00445    |
|    loss                 | -0.0787    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0381    |
|    value_loss           | 0.000271   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | -0.0289  |
| time/              |          |
|    fps             | 634      |
|    iterations      | 15       |
|    time_elapsed    | 96       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 03:42:08,079] Trial 36 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 0.004454192309888641, 'gamma': 0.9345377078429786, 'gae_lambda': 0.8318595779451302}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 850      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.015470128 |
|    clip_fraction        | 0.095       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -1.2        |
|    learning_rate        | 0.0002      |
|    loss                 | -0.055      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.0019      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | -0.177   |
| time/              |          |
|    fps             | 607      |
|    iterations      | 2        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.028085079 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.0931     |
|    learning_rate        | 0.0002      |
|    loss                 | -0.0456     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0431     |
|    value_loss           | 0.00103     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 600      |
|    iterations      | 3        |
|    time_elapsed    | 40       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.36
Episode length: 474.10 +/- 152.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0896     |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.035911433 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | -0.0804     |
|    learning_rate        | 0.0002      |
|    loss                 | -0.0846     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.049      |
|    value_loss           | 0.00121     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.109   |
| time/              |          |
|    fps             | 564      |
|    iterations      | 4        |
|    time_elapsed    | 58       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.04113986 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | -0.251     |
|    learning_rate        | 0.0002     |
|    loss                 | -0.0718    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0588    |
|    value_loss           | 0.000752   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.103   |
| time/              |          |
|    fps             | 543      |
|    iterations      | 5        |
|    time_elapsed    | 75       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.050093815 |
|    clip_fraction        | 0.38        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | -0.167      |
|    learning_rate        | 0.0002      |
|    loss                 | -0.115      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0596     |
|    value_loss           | 0.000691    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.0872  |
| time/              |          |
|    fps             | 548      |
|    iterations      | 6        |
|    time_elapsed    | 89       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.048807796 |
|    clip_fraction        | 0.383       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | -0.00754    |
|    learning_rate        | 0.0002      |
|    loss                 | -0.0999     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0597     |
|    value_loss           | 0.000926    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.0555  |
| time/              |          |
|    fps             | 536      |
|    iterations      | 7        |
|    time_elapsed    | 106      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.06952803 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.59      |
|    explained_variance   | -0.161     |
|    learning_rate        | 0.0002     |
|    loss                 | -0.117     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0637    |
|    value_loss           | 0.00103    |
----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | -0.0421  |
| time/              |          |
|    fps             | 526      |
|    iterations      | 8        |
|    time_elapsed    | 124      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:44:20,654] Trial 37 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.00020022787142468472, 'gamma': 0.9378198566729955, 'gae_lambda': 0.8214343674194612}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1292     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.015964605 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.00128     |
|    loss                 | 0.00424     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.0141      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0716  |
| time/              |          |
|    fps             | 752      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.054833423 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.00128     |
|    loss                 | -0.105      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0384     |
|    value_loss           | 0.00562     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.0732  |
| time/              |          |
|    fps             | 658      |
|    iterations      | 3        |
|    time_elapsed    | 18       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.063099004 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.00128     |
|    loss                 | -0.0456     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.043      |
|    value_loss           | 0.00304     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 621      |
|    iterations      | 4        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.12796849 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | 0.519      |
|    learning_rate        | 0.00128    |
|    loss                 | -0.117     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0612    |
|    value_loss           | 0.00209    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.101   |
| time/              |          |
|    fps             | 601      |
|    iterations      | 5        |
|    time_elapsed    | 34       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 506        |
|    ep_rew_mean          | -0.119     |
| time/                   |            |
|    fps                  | 638        |
|    iterations           | 6          |
|    time_elapsed         | 38         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.17657441 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.6       |
|    explained_variance   | 0.099      |
|    learning_rate        | 0.00128    |
|    loss                 | -0.0795    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0542    |
|    value_loss           | 0.00131    |
----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.1574218 |
|    clip_fraction        | 0.487     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.61     |
|    explained_variance   | 0.545     |
|    learning_rate        | 0.00128   |
|    loss                 | -0.0773   |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0551   |
|    value_loss           | 0.000377  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.132   |
| time/              |          |
|    fps             | 620      |
|    iterations      | 7        |
|    time_elapsed    | 46       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.1596073 |
|    clip_fraction        | 0.505     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.55     |
|    explained_variance   | 0.0727    |
|    learning_rate        | 0.00128   |
|    loss                 | -0.0937   |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.0562   |
|    value_loss           | 0.000424  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | -0.142   |
| time/              |          |
|    fps             | 607      |
|    iterations      | 8        |
|    time_elapsed    | 53       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.24088016 |
|    clip_fraction        | 0.571      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.54      |
|    explained_variance   | -2.43      |
|    learning_rate        | 0.00128    |
|    loss                 | -0.103     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0655    |
|    value_loss           | 0.000228   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.149   |
| time/              |          |
|    fps             | 597      |
|    iterations      | 9        |
|    time_elapsed    | 61       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.26769465 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | -2.52      |
|    learning_rate        | 0.00128    |
|    loss                 | -0.0943    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.052     |
|    value_loss           | 0.000355   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | -0.155   |
| time/              |          |
|    fps             | 590      |
|    iterations      | 10       |
|    time_elapsed    | 69       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.33357406 |
|    clip_fraction        | 0.598      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | -3.48      |
|    learning_rate        | 0.00128    |
|    loss                 | -0.122     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.000542   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 584      |
|    iterations      | 11       |
|    time_elapsed    | 77       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 515        |
|    ep_rew_mean          | -0.164     |
| time/                   |            |
|    fps                  | 603        |
|    iterations           | 12         |
|    time_elapsed         | 81         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.27243066 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.0419     |
|    learning_rate        | 0.00128    |
|    loss                 | -0.126     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0678    |
|    value_loss           | 0.000141   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.2766257 |
|    clip_fraction        | 0.587     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.36     |
|    explained_variance   | -0.361    |
|    learning_rate        | 0.00128   |
|    loss                 | -0.107    |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.0712   |
|    value_loss           | 4.79e-05  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | -0.166   |
| time/              |          |
|    fps             | 596      |
|    iterations      | 13       |
|    time_elapsed    | 89       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.30732775 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.4       |
|    explained_variance   | -0.575     |
|    learning_rate        | 0.00128    |
|    loss                 | -0.0937    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0535    |
|    value_loss           | 0.000773   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.167   |
| time/              |          |
|    fps             | 590      |
|    iterations      | 14       |
|    time_elapsed    | 97       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.29909408 |
|    clip_fraction        | 0.614      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.39      |
|    explained_variance   | -0.181     |
|    learning_rate        | 0.00128    |
|    loss                 | -0.0271    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.063     |
|    value_loss           | 0.000713   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 519      |
|    ep_rew_mean     | -0.167   |
| time/              |          |
|    fps             | 586      |
|    iterations      | 15       |
|    time_elapsed    | 104      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 03:46:10,251] Trial 38 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.001275940455763534, 'gamma': 0.9558894062935283, 'gae_lambda': 0.8080657898285618}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1279     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 511         |
|    ep_rew_mean          | -0.0616     |
| time/                   |             |
|    fps                  | 1152        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.016228616 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.562      |
|    learning_rate        | 0.000731    |
|    loss                 | -0.0348     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.00548     |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.038431395 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.848      |
|    learning_rate        | 0.000731    |
|    loss                 | -0.0669     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0495     |
|    value_loss           | 0.00228     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | -0.116   |
| time/              |          |
|    fps             | 696      |
|    iterations      | 3        |
|    time_elapsed    | 8        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 519        |
|    ep_rew_mean          | -0.141     |
| time/                   |            |
|    fps                  | 761        |
|    iterations           | 4          |
|    time_elapsed         | 10         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.05899144 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | -1.93      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.101     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0561    |
|    value_loss           | 0.00131    |
----------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.059745353 |
|    clip_fraction        | 0.336       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | -2.49       |
|    learning_rate        | 0.000731    |
|    loss                 | -0.0836     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0469     |
|    value_loss           | 0.000858    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -0.102   |
| time/              |          |
|    fps             | 639      |
|    iterations      | 5        |
|    time_elapsed    | 16       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 508         |
|    ep_rew_mean          | -0.12       |
| time/                   |             |
|    fps                  | 684         |
|    iterations           | 6           |
|    time_elapsed         | 17          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.065312535 |
|    clip_fraction        | 0.42        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | -0.568      |
|    learning_rate        | 0.000731    |
|    loss                 | -0.0979     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0611     |
|    value_loss           | 0.00159     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 511        |
|    ep_rew_mean          | -0.133     |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 7          |
|    time_elapsed         | 19         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.08531021 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | -6.01      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.11      |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0745    |
|    value_loss           | 0.000678   |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.07647661 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.66      |
|    explained_variance   | -9.16      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.108     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0832    |
|    value_loss           | 0.000335   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 651      |
|    iterations      | 8        |
|    time_elapsed    | 25       |
|    total_timesteps | 16384    |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 514      |
|    ep_rew_mean          | -0.148   |
| time/                   |          |
|    fps                  | 679      |
|    iterations           | 9        |
|    time_elapsed         | 27       |
|    total_timesteps      | 18432    |
| train/                  |          |
|    approx_kl            | 0.140621 |
|    clip_fraction        | 0.5      |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.53    |
|    explained_variance   | -5.98    |
|    learning_rate        | 0.000731 |
|    loss                 | -0.106   |
|    n_updates            | 80       |
|    policy_gradient_loss | -0.065   |
|    value_loss           | 0.000558 |
--------------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.16653681 |
|    clip_fraction        | 0.539      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.46      |
|    explained_variance   | -8.34      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.115     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0757    |
|    value_loss           | 0.00051    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | -0.129   |
| time/              |          |
|    fps             | 633      |
|    iterations      | 10       |
|    time_elapsed    | 32       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 511        |
|    ep_rew_mean          | -0.136     |
| time/                   |            |
|    fps                  | 657        |
|    iterations           | 11         |
|    time_elapsed         | 34         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.14212936 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | -0.377     |
|    learning_rate        | 0.000731   |
|    loss                 | -0.119     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0659    |
|    value_loss           | 0.00146    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 512        |
|    ep_rew_mean          | -0.141     |
| time/                   |            |
|    fps                  | 679        |
|    iterations           | 12         |
|    time_elapsed         | 36         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.15160778 |
|    clip_fraction        | 0.548      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.42      |
|    explained_variance   | -6.22      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.108     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0704    |
|    value_loss           | 0.000304   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.19802508 |
|    clip_fraction        | 0.558      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | -10        |
|    learning_rate        | 0.000731   |
|    loss                 | -0.113     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0831    |
|    value_loss           | 0.000256   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | -0.146   |
| time/              |          |
|    fps             | 644      |
|    iterations      | 13       |
|    time_elapsed    | 41       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 514        |
|    ep_rew_mean          | -0.151     |
| time/                   |            |
|    fps                  | 662        |
|    iterations           | 14         |
|    time_elapsed         | 43         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.13202754 |
|    clip_fraction        | 0.56       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.49      |
|    explained_variance   | -4.53      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.127     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0753    |
|    value_loss           | 0.000103   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.18795666 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | -4.12      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.104     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0788    |
|    value_loss           | 0.000127   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | -0.136   |
| time/              |          |
|    fps             | 633      |
|    iterations      | 15       |
|    time_elapsed    | 48       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 508        |
|    ep_rew_mean          | -0.141     |
| time/                   |            |
|    fps                  | 650        |
|    iterations           | 16         |
|    time_elapsed         | 50         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.16044775 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -0.0829    |
|    learning_rate        | 0.000731   |
|    loss                 | -0.0987    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.058     |
|    value_loss           | 0.00116    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 503        |
|    ep_rew_mean          | -0.129     |
| time/                   |            |
|    fps                  | 665        |
|    iterations           | 17         |
|    time_elapsed         | 52         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.13804288 |
|    clip_fraction        | 0.539      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | -2.32      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.107     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0772    |
|    value_loss           | 5.91e-05   |
----------------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.36
Episode length: 474.70 +/- 150.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 475        |
|    mean_reward          | -0.0899    |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.21381018 |
|    clip_fraction        | 0.541      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | -0.0464    |
|    learning_rate        | 0.000731   |
|    loss                 | -0.0763    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0406    |
|    value_loss           | 0.00134    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.133   |
| time/              |          |
|    fps             | 644      |
|    iterations      | 18       |
|    time_elapsed    | 57       |
|    total_timesteps | 36864    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 505       |
|    ep_rew_mean          | -0.137    |
| time/                   |           |
|    fps                  | 658       |
|    iterations           | 19        |
|    time_elapsed         | 59        |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.2522034 |
|    clip_fraction        | 0.537     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.21     |
|    explained_variance   | -2.46     |
|    learning_rate        | 0.000731  |
|    loss                 | -0.104    |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0618   |
|    value_loss           | 0.000134  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.17000863 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | -1.99      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.0949    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0767    |
|    value_loss           | 2.58e-05   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 636      |
|    iterations      | 20       |
|    time_elapsed    | 64       |
|    total_timesteps | 40960    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 507       |
|    ep_rew_mean          | -0.143    |
| time/                   |           |
|    fps                  | 649       |
|    iterations           | 21        |
|    time_elapsed         | 66        |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.2505134 |
|    clip_fraction        | 0.513     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.23     |
|    explained_variance   | -6.07     |
|    learning_rate        | 0.000731  |
|    loss                 | -0.0724   |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0482   |
|    value_loss           | 3.3e-05   |
---------------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.25954473 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | -6.82      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.117     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0659    |
|    value_loss           | 1.61e-05   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.146   |
| time/              |          |
|    fps             | 629      |
|    iterations      | 22       |
|    time_elapsed    | 71       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 509        |
|    ep_rew_mean          | -0.149     |
| time/                   |            |
|    fps                  | 641        |
|    iterations           | 23         |
|    time_elapsed         | 73         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.24793087 |
|    clip_fraction        | 0.592      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | -8.7       |
|    learning_rate        | 0.000731   |
|    loss                 | -0.121     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0779    |
|    value_loss           | 1.24e-05   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 508       |
|    ep_rew_mean          | -0.141    |
| time/                   |           |
|    fps                  | 651       |
|    iterations           | 24        |
|    time_elapsed         | 75        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.2576379 |
|    clip_fraction        | 0.571     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.22     |
|    explained_variance   | -3.97     |
|    learning_rate        | 0.000731  |
|    loss                 | -0.0915   |
|    n_updates            | 230       |
|    policy_gradient_loss | -0.0702   |
|    value_loss           | 5.05e-06  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.22814666 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.00208    |
|    learning_rate        | 0.000731   |
|    loss                 | -0.0105    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0335    |
|    value_loss           | 0.0013     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.144   |
| time/              |          |
|    fps             | 635      |
|    iterations      | 25       |
|    time_elapsed    | 80       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 505        |
|    ep_rew_mean          | -0.132     |
| time/                   |            |
|    fps                  | 644        |
|    iterations           | 26         |
|    time_elapsed         | 82         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.21069008 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.839     |
|    explained_variance   | -1.43      |
|    learning_rate        | 0.000731   |
|    loss                 | -0.0798    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0626    |
|    value_loss           | 8.26e-05   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.21905255 |
|    clip_fraction        | 0.562      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.0123     |
|    learning_rate        | 0.000731   |
|    loss                 | -0.0349    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0608    |
|    value_loss           | 0.00102    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.142   |
| time/              |          |
|    fps             | 629      |
|    iterations      | 27       |
|    time_elapsed    | 87       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 503       |
|    ep_rew_mean          | -0.131    |
| time/                   |           |
|    fps                  | 638       |
|    iterations           | 28        |
|    time_elapsed         | 89        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.2053419 |
|    clip_fraction        | 0.506     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08     |
|    explained_variance   | -2.04     |
|    learning_rate        | 0.000731  |
|    loss                 | -0.0988   |
|    n_updates            | 270       |
|    policy_gradient_loss | -0.0765   |
|    value_loss           | 5.36e-05  |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 503      |
|    ep_rew_mean          | -0.121   |
| time/                   |          |
|    fps                  | 647      |
|    iterations           | 29       |
|    time_elapsed         | 91       |
|    total_timesteps      | 59392    |
| train/                  |          |
|    approx_kl            | 0.190238 |
|    clip_fraction        | 0.422    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.89    |
|    explained_variance   | 0.000392 |
|    learning_rate        | 0.000731 |
|    loss                 | -0.078   |
|    n_updates            | 280      |
|    policy_gradient_loss | -0.0386  |
|    value_loss           | 0.00131  |
--------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.32317424 |
|    clip_fraction        | 0.52       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.981     |
|    explained_variance   | -0.113     |
|    learning_rate        | 0.000731   |
|    loss                 | -0.0356    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0483    |
|    value_loss           | 0.00114    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.132   |
| time/              |          |
|    fps             | 633      |
|    iterations      | 30       |
|    time_elapsed    | 97       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 03:47:51,199] Trial 39 finished with value: -0.21000000000000002 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0007308376030896929, 'gamma': 0.9118608228127459, 'gae_lambda': 0.8003629339241142}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 851      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.09 +/- 0.36
Episode length: 474.00 +/- 153.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0896     |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014530794 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.163       |
|    learning_rate        | 3.73e-05    |
|    loss                 | -0.00681    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00444    |
|    value_loss           | 0.00599     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -0.108   |
| time/              |          |
|    fps             | 660      |
|    iterations      | 2        |
|    time_elapsed    | 24       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 473         |
|    mean_reward          | -0.0893     |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.006684362 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.0346      |
|    learning_rate        | 3.73e-05    |
|    loss                 | 0.0161      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 0.00284     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.0977  |
| time/              |          |
|    fps             | 672      |
|    iterations      | 3        |
|    time_elapsed    | 36       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.006486046 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -0.0416     |
|    learning_rate        | 3.73e-05    |
|    loss                 | -0.0229     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 0.00247     |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.0759  |
| time/              |          |
|    fps             | 631      |
|    iterations      | 4        |
|    time_elapsed    | 51       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0058590462 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | -0.368       |
|    learning_rate        | 3.73e-05     |
|    loss                 | -0.00301     |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00659     |
|    value_loss           | 0.00226      |
------------------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 473      |
|    mean_reward     | -0.0893  |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | -0.0877  |
| time/              |          |
|    fps             | 612      |
|    iterations      | 5        |
|    time_elapsed    | 66       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.008547554 |
|    clip_fraction        | 0.0646      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -1.58       |
|    learning_rate        | 3.73e-05    |
|    loss                 | -0.0153     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.00147     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0641  |
| time/              |          |
|    fps             | 623      |
|    iterations      | 6        |
|    time_elapsed    | 78       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.007253381 |
|    clip_fraction        | 0.0574      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.439      |
|    learning_rate        | 3.73e-05    |
|    loss                 | -0.005      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.00242     |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.09 +/- 0.36
Episode length: 473.30 +/- 155.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 473      |
|    mean_reward     | -0.0893  |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.0646  |
| time/              |          |
|    fps             | 609      |
|    iterations      | 7        |
|    time_elapsed    | 94       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0059576174 |
|    clip_fraction        | 0.0467       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | -3.15        |
|    learning_rate        | 3.73e-05     |
|    loss                 | -0.0288      |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.0125      |
|    value_loss           | 0.00197      |
------------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.086   |
| time/              |          |
|    fps             | 598      |
|    iterations      | 8        |
|    time_elapsed    | 109      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:49:46,715] Trial 40 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 3.7277202694803524e-05, 'gamma': 0.9302368122056943, 'gae_lambda': 0.8938014000189063}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 473      |
|    ep_rew_mean     | -0.0717  |
| time/              |          |
|    fps             | 846      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.023970425 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.398      |
|    learning_rate        | 0.00051     |
|    loss                 | -0.0477     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.00289     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | -0.0424  |
| time/              |          |
|    fps             | 604      |
|    iterations      | 2        |
|    time_elapsed    | 27       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.04699565 |
|    clip_fraction        | 0.238      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.76      |
|    explained_variance   | 0.11       |
|    learning_rate        | 0.00051    |
|    loss                 | -0.0376    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.036     |
|    value_loss           | 0.00149    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.0961  |
| time/              |          |
|    fps             | 595      |
|    iterations      | 3        |
|    time_elapsed    | 41       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.36
Episode length: 473.80 +/- 153.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 474        |
|    mean_reward          | -0.0895    |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.07446502 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | -0.997     |
|    learning_rate        | 0.00051    |
|    loss                 | -0.0903    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.000162   |
----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.108   |
| time/              |          |
|    fps             | 561      |
|    iterations      | 4        |
|    time_elapsed    | 58       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.097962536 |
|    clip_fraction        | 0.444       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | -0.129      |
|    learning_rate        | 0.00051     |
|    loss                 | -0.0956     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0594     |
|    value_loss           | 0.000355    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 539      |
|    iterations      | 5        |
|    time_elapsed    | 75       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.11599835 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.61      |
|    explained_variance   | -0.0511    |
|    learning_rate        | 0.00051    |
|    loss                 | -0.0753    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0552    |
|    value_loss           | 0.000658   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0866  |
| time/              |          |
|    fps             | 544      |
|    iterations      | 6        |
|    time_elapsed    | 90       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.12686649 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | -0.031     |
|    learning_rate        | 0.00051    |
|    loss                 | -0.0714    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.000957   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.0982  |
| time/              |          |
|    fps             | 531      |
|    iterations      | 7        |
|    time_elapsed    | 107      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.1503322 |
|    clip_fraction        | 0.466     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.55     |
|    explained_variance   | -0.319    |
|    learning_rate        | 0.00051   |
|    loss                 | -0.0968   |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.0626   |
|    value_loss           | 0.000386  |
---------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 522      |
|    iterations      | 8        |
|    time_elapsed    | 125      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:52:00,198] Trial 41 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0005102675149395219, 'gamma': 0.9201756319103359, 'gae_lambda': 0.8397666276783105}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 842      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012924664 |
|    clip_fraction        | 0.0818      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -2.24       |
|    learning_rate        | 0.000184    |
|    loss                 | -0.061      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.00127     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | -0.173   |
| time/              |          |
|    fps             | 603      |
|    iterations      | 2        |
|    time_elapsed    | 27       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.02298202 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | -0.0819    |
|    learning_rate        | 0.000184   |
|    loss                 | -0.0497    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0362    |
|    value_loss           | 0.000739   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 518      |
|    ep_rew_mean     | -0.186   |
| time/              |          |
|    fps             | 597      |
|    iterations      | 3        |
|    time_elapsed    | 41       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.36
Episode length: 473.40 +/- 154.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 473         |
|    mean_reward          | -0.0893     |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.027932651 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | -0.0558     |
|    learning_rate        | 0.000184    |
|    loss                 | -0.103      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0422     |
|    value_loss           | 0.000178    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | -0.192   |
| time/              |          |
|    fps             | 558      |
|    iterations      | 4        |
|    time_elapsed    | 58       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.03827016 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | -0.332     |
|    learning_rate        | 0.000184   |
|    loss                 | -0.103     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 7.46e-05   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | -0.181   |
| time/              |          |
|    fps             | 538      |
|    iterations      | 5        |
|    time_elapsed    | 76       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.04462126 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | 0.0375     |
|    learning_rate        | 0.000184   |
|    loss                 | -0.115     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0596    |
|    value_loss           | 0.000339   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | -0.163   |
| time/              |          |
|    fps             | 544      |
|    iterations      | 6        |
|    time_elapsed    | 90       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.04247036 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | -0.0483    |
|    learning_rate        | 0.000184   |
|    loss                 | -0.0809    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0589    |
|    value_loss           | 0.000627   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 531      |
|    iterations      | 7        |
|    time_elapsed    | 107      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.04603954 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | 0.00126    |
|    learning_rate        | 0.000184   |
|    loss                 | -0.122     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0589    |
|    value_loss           | 0.000958   |
----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.09 +/- 0.36
Episode length: 474.30 +/- 152.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 474      |
|    mean_reward     | -0.0897  |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.142   |
| time/              |          |
|    fps             | 524      |
|    iterations      | 8        |
|    time_elapsed    | 124      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:54:13,425] Trial 42 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.00018361147618607717, 'gamma': 0.9228407484751842, 'gae_lambda': 0.8253461568343793}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.0722  |
| time/              |          |
|    fps             | 848      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.017685443 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.0332     |
|    learning_rate        | 0.000354    |
|    loss                 | -0.0253     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.00314     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 464      |
|    ep_rew_mean     | -0.0141  |
| time/              |          |
|    fps             | 603      |
|    iterations      | 2        |
|    time_elapsed    | 27       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 474.30 +/- 152.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0897     |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.038569853 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.24        |
|    learning_rate        | 0.000354    |
|    loss                 | -0.0539     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0365     |
|    value_loss           | 0.00171     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0526  |
| time/              |          |
|    fps             | 599      |
|    iterations      | 3        |
|    time_elapsed    | 41       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.04535845 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.282      |
|    learning_rate        | 0.000354   |
|    loss                 | -0.0415    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.000544   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0443  |
| time/              |          |
|    fps             | 563      |
|    iterations      | 4        |
|    time_elapsed    | 58       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.06776521 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | -0.0913    |
|    learning_rate        | 0.000354   |
|    loss                 | -0.114     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0552    |
|    value_loss           | 0.000862   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0619  |
| time/              |          |
|    fps             | 541      |
|    iterations      | 5        |
|    time_elapsed    | 75       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.09 +/- 0.36
Episode length: 475.40 +/- 148.80
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 475      |
|    mean_reward          | -0.0901  |
| time/                   |          |
|    total_timesteps      | 45000    |
| train/                  |          |
|    approx_kl            | 0.078657 |
|    clip_fraction        | 0.44     |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.66    |
|    explained_variance   | -0.386   |
|    learning_rate        | 0.000354 |
|    loss                 | -0.1     |
|    n_updates            | 50       |
|    policy_gradient_loss | -0.0639  |
|    value_loss           | 0.000345 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 481      |
|    ep_rew_mean     | -0.0624  |
| time/              |          |
|    fps             | 548      |
|    iterations      | 6        |
|    time_elapsed    | 89       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 474.40 +/- 151.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0897     |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.104795426 |
|    clip_fraction        | 0.489       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | -0.00936    |
|    learning_rate        | 0.000354    |
|    loss                 | -0.137      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0696     |
|    value_loss           | 0.000591    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | -0.05    |
| time/              |          |
|    fps             | 536      |
|    iterations      | 7        |
|    time_elapsed    | 106      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.12941077 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | -0.0292    |
|    learning_rate        | 0.000354   |
|    loss                 | -0.119     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.07      |
|    value_loss           | 0.000853   |
----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0742  |
| time/              |          |
|    fps             | 526      |
|    iterations      | 8        |
|    time_elapsed    | 124      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:56:26,063] Trial 43 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.00035385276610919815, 'gamma': 0.917411970671112, 'gae_lambda': 0.8103877850461159}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 847      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009932303 |
|    clip_fraction        | 0.0546      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -1.35       |
|    learning_rate        | 0.00882     |
|    loss                 | -0.0499     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 0.167       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 602      |
|    iterations      | 2        |
|    time_elapsed    | 27       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.009858569 |
|    clip_fraction        | 0.0546      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00882     |
|    loss                 | -0.0058     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000406   |
|    value_loss           | 2.19e-06    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 518      |
|    ep_rew_mean     | -0.186   |
| time/              |          |
|    fps             | 590      |
|    iterations      | 3        |
|    time_elapsed    | 41       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.36
Episode length: 473.60 +/- 154.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0894     |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009976944 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00882     |
|    loss                 | 0.00521     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 0.000331    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | -0.174   |
| time/              |          |
|    fps             | 556      |
|    iterations      | 4        |
|    time_elapsed    | 58       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.09 +/- 0.36
Episode length: 473.60 +/- 154.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 474        |
|    mean_reward          | -0.0894    |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.00892527 |
|    clip_fraction        | 0.062      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0          |
|    learning_rate        | 0.00882    |
|    loss                 | -0.000598  |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00122   |
|    value_loss           | 0.000332   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | -0.154   |
| time/              |          |
|    fps             | 538      |
|    iterations      | 5        |
|    time_elapsed    | 76       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.007763768 |
|    clip_fraction        | 0.0452      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00882     |
|    loss                 | 0.0172      |
|    n_updates            | 50          |
|    policy_gradient_loss | 3.97e-05    |
|    value_loss           | 0.000665    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.151   |
| time/              |          |
|    fps             | 543      |
|    iterations      | 6        |
|    time_elapsed    | 90       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.012542927 |
|    clip_fraction        | 0.0816      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00882     |
|    loss                 | -0.00566    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 0.000333    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.131   |
| time/              |          |
|    fps             | 530      |
|    iterations      | 7        |
|    time_elapsed    | 107      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.36
Episode length: 473.90 +/- 153.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 474        |
|    mean_reward          | -0.0896    |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.01200722 |
|    clip_fraction        | 0.0667     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | 0          |
|    learning_rate        | 0.00882    |
|    loss                 | 0.0345     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.00103   |
|    value_loss           | 0.000668   |
----------------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.098   |
| time/              |          |
|    fps             | 522      |
|    iterations      | 8        |
|    time_elapsed    | 125      |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-21 03:58:39,412] Trial 44 finished with value: -0.21000000000000002 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.008821881571126252, 'gamma': 0.9249081551644116, 'gae_lambda': 0.8547297550081725}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 465      |
|    ep_rew_mean     | -0.0609  |
| time/              |          |
|    fps             | 1273     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.008112771 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -0.433      |
|    learning_rate        | 9.42e-05    |
|    loss                 | 0.00127     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.00308     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.135   |
| time/              |          |
|    fps             | 677      |
|    iterations      | 2        |
|    time_elapsed    | 12       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009219867 |
|    clip_fraction        | 0.0736      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -0.029      |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.0692     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.00154     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -0.16    |
| time/              |          |
|    fps             | 589      |
|    iterations      | 3        |
|    time_elapsed    | 20       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.009886228 |
|    clip_fraction        | 0.0816      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -1.26       |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.056      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.00124     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.138   |
| time/              |          |
|    fps             | 554      |
|    iterations      | 4        |
|    time_elapsed    | 29       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.014654084 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -0.928      |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.0705     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.00238     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.152   |
| time/              |          |
|    fps             | 533      |
|    iterations      | 5        |
|    time_elapsed    | 38       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 509         |
|    ep_rew_mean          | -0.162      |
| time/                   |             |
|    fps                  | 559         |
|    iterations           | 6           |
|    time_elapsed         | 43          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.016954128 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | -2.58       |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.0261     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0385     |
|    value_loss           | 0.00153     |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.017779669 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | -6.49       |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.0816     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0386     |
|    value_loss           | 0.00157     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -0.131   |
| time/              |          |
|    fps             | 546      |
|    iterations      | 7        |
|    time_elapsed    | 52       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.01879637 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | -0.714     |
|    learning_rate        | 9.42e-05   |
|    loss                 | -0.0733    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0381    |
|    value_loss           | 0.00258    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.0918  |
| time/              |          |
|    fps             | 536      |
|    iterations      | 8        |
|    time_elapsed    | 61       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.018451957 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | -0.376      |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.0545     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0356     |
|    value_loss           | 0.00308     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0741  |
| time/              |          |
|    fps             | 528      |
|    iterations      | 9        |
|    time_elapsed    | 69       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.09 +/- 0.36
Episode length: 473.50 +/- 154.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0894     |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.020118821 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -0.694      |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.0675     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0456     |
|    value_loss           | 0.00231     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0501  |
| time/              |          |
|    fps             | 523      |
|    iterations      | 10       |
|    time_elapsed    | 78       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.09 +/- 0.36
Episode length: 474.10 +/- 152.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0896     |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.021987326 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.258      |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.0275     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0397     |
|    value_loss           | 0.00289     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.053   |
| time/              |          |
|    fps             | 519      |
|    iterations      | 11       |
|    time_elapsed    | 86       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 480         |
|    ep_rew_mean          | -0.0418     |
| time/                   |             |
|    fps                  | 533         |
|    iterations           | 12          |
|    time_elapsed         | 92          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.019489491 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -0.372      |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.0476     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0417     |
|    value_loss           | 0.00124     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.023076946 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | -0.176      |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.0844     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0427     |
|    value_loss           | 0.0019      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0214  |
| time/              |          |
|    fps             | 528      |
|    iterations      | 13       |
|    time_elapsed    | 100      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0233921 |
|    clip_fraction        | 0.235     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.63     |
|    explained_variance   | -0.0358   |
|    learning_rate        | 9.42e-05  |
|    loss                 | -0.0504   |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.0406   |
|    value_loss           | 0.00262   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.011   |
| time/              |          |
|    fps             | 523      |
|    iterations      | 14       |
|    time_elapsed    | 109      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.028977938 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | -0.191      |
|    learning_rate        | 9.42e-05    |
|    loss                 | -0.104      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0482     |
|    value_loss           | 0.00148     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 470      |
|    ep_rew_mean     | 0.0121   |
| time/              |          |
|    fps             | 518      |
|    iterations      | 15       |
|    time_elapsed    | 118      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 04:00:43,545] Trial 45 finished with value: -0.21000000000000002 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 9.42469516589867e-05, 'gamma': 0.9420593159728287, 'gae_lambda': 0.8720732509596509}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | 0.0552   |
| time/              |          |
|    fps             | 1286     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 506        |
|    ep_rew_mean          | -0.0774    |
| time/                   |            |
|    fps                  | 1081       |
|    iterations           | 2          |
|    time_elapsed         | 3          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.02921316 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | -2.49      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.068     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.0376     |
----------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.047723606 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -2.33       |
|    learning_rate        | 0.00265     |
|    loss                 | -0.0626     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.000341    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | -0.114   |
| time/              |          |
|    fps             | 661      |
|    iterations      | 3        |
|    time_elapsed    | 9        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 515        |
|    ep_rew_mean          | -0.139     |
| time/                   |            |
|    fps                  | 712        |
|    iterations           | 4          |
|    time_elapsed         | 11         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.05974555 |
|    clip_fraction        | 0.193      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.73      |
|    explained_variance   | -4.39      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0573    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0255    |
|    value_loss           | 0.000817   |
----------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.11530825 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | -6.39      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0642    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0276    |
|    value_loss           | 0.00124    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.154   |
| time/              |          |
|    fps             | 600      |
|    iterations      | 5        |
|    time_elapsed    | 17       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 518         |
|    ep_rew_mean          | -0.164      |
| time/                   |             |
|    fps                  | 637         |
|    iterations           | 6           |
|    time_elapsed         | 19          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.124392785 |
|    clip_fraction        | 0.359       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | -7.72       |
|    learning_rate        | 0.00265     |
|    loss                 | -0.0609     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 0.00133     |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 495       |
|    ep_rew_mean          | -0.0908   |
| time/                   |           |
|    fps                  | 668       |
|    iterations           | 7         |
|    time_elapsed         | 21        |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 0.1023441 |
|    clip_fraction        | 0.318     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.7      |
|    explained_variance   | -10.8     |
|    learning_rate        | 0.00265   |
|    loss                 | -0.0629   |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0278   |
|    value_loss           | 0.000244  |
---------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.09661449 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.64      |
|    explained_variance   | -0.0556    |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0413    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0261    |
|    value_loss           | 0.00368    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 608      |
|    iterations      | 8        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 489        |
|    ep_rew_mean          | -0.0875    |
| time/                   |            |
|    fps                  | 632        |
|    iterations           | 9          |
|    time_elapsed         | 29         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.19448103 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.56      |
|    explained_variance   | -7.43      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.045     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.00285    |
----------------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.14220761 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | -0.206     |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0564    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0339    |
|    value_loss           | 0.00123    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0753  |
| time/              |          |
|    fps             | 590      |
|    iterations      | 10       |
|    time_elapsed    | 34       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 484        |
|    ep_rew_mean          | -0.0633    |
| time/                   |            |
|    fps                  | 611        |
|    iterations           | 11         |
|    time_elapsed         | 36         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.20242447 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.54      |
|    explained_variance   | -0.112     |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0575    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0311    |
|    value_loss           | 0.00403    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 485       |
|    ep_rew_mean          | -0.0539   |
| time/                   |           |
|    fps                  | 629       |
|    iterations           | 12        |
|    time_elapsed         | 39        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.2645254 |
|    clip_fraction        | 0.515     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.5      |
|    explained_variance   | -0.316    |
|    learning_rate        | 0.00265   |
|    loss                 | -0.0977   |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.0433   |
|    value_loss           | 0.00244   |
---------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.41772175 |
|    clip_fraction        | 0.613      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | -0.63      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0575    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0429    |
|    value_loss           | 0.00227    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.0655  |
| time/              |          |
|    fps             | 598      |
|    iterations      | 13       |
|    time_elapsed    | 44       |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 490       |
|    ep_rew_mean          | -0.0754   |
| time/                   |           |
|    fps                  | 613       |
|    iterations           | 14        |
|    time_elapsed         | 46        |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.6343537 |
|    clip_fraction        | 0.554     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.906    |
|    explained_variance   | -0.368    |
|    learning_rate        | 0.00265   |
|    loss                 | -0.1      |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.0607   |
|    value_loss           | 0.00109   |
---------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.27159327 |
|    clip_fraction        | 0.588      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | -8.61      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.113     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0661    |
|    value_loss           | 7.55e-05   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0673  |
| time/              |          |
|    fps             | 588      |
|    iterations      | 15       |
|    time_elapsed    | 52       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 486        |
|    ep_rew_mean          | -0.0602    |
| time/                   |            |
|    fps                  | 601        |
|    iterations           | 16         |
|    time_elapsed         | 54         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.31757694 |
|    clip_fraction        | 0.554      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | -0.0154    |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0156    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.041     |
|    value_loss           | 0.00123    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 489        |
|    ep_rew_mean          | -0.0686    |
| time/                   |            |
|    fps                  | 614        |
|    iterations           | 17         |
|    time_elapsed         | 56         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.34429783 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | -0.221     |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0784    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0425    |
|    value_loss           | 0.00209    |
----------------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 1.2495375 |
|    clip_fraction        | 0.713     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.04     |
|    explained_variance   | -2.44     |
|    learning_rate        | 0.00265   |
|    loss                 | -0.103    |
|    n_updates            | 170       |
|    policy_gradient_loss | -0.054    |
|    value_loss           | 0.00143   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0762  |
| time/              |          |
|    fps             | 594      |
|    iterations      | 18       |
|    time_elapsed    | 62       |
|    total_timesteps | 36864    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 492       |
|    ep_rew_mean          | -0.0702   |
| time/                   |           |
|    fps                  | 605       |
|    iterations           | 19        |
|    time_elapsed         | 64        |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 1.2388632 |
|    clip_fraction        | 0.706     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.892    |
|    explained_variance   | -0.594    |
|    learning_rate        | 0.00265   |
|    loss                 | -0.104    |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0905   |
|    value_loss           | 0.000534  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.43991506 |
|    clip_fraction        | 0.641      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | -0.529     |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0465    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.00169    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.0753  |
| time/              |          |
|    fps             | 587      |
|    iterations      | 20       |
|    time_elapsed    | 69       |
|    total_timesteps | 40960    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 495       |
|    ep_rew_mean          | -0.0816   |
| time/                   |           |
|    fps                  | 597       |
|    iterations           | 21        |
|    time_elapsed         | 71        |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.3414755 |
|    clip_fraction        | 0.587     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.2      |
|    explained_variance   | -4.13     |
|    learning_rate        | 0.00265   |
|    loss                 | -0.114    |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0425   |
|    value_loss           | 0.000347  |
---------------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.31809598 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | -8.46      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.119     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.062     |
|    value_loss           | 0.000656   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.0873  |
| time/              |          |
|    fps             | 582      |
|    iterations      | 22       |
|    time_elapsed    | 77       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | -0.0925    |
| time/                   |            |
|    fps                  | 591        |
|    iterations           | 23         |
|    time_elapsed         | 79         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.53404874 |
|    clip_fraction        | 0.622      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | -7.02      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0561    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0369    |
|    value_loss           | 0.0027     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 498        |
|    ep_rew_mean          | -0.0973    |
| time/                   |            |
|    fps                  | 600        |
|    iterations           | 24         |
|    time_elapsed         | 81         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.49356565 |
|    clip_fraction        | 0.587      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | -6.98      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0245    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0495    |
|    value_loss           | 0.000895   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 1.2338696 |
|    clip_fraction        | 0.585     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.839    |
|    explained_variance   | -2.22     |
|    learning_rate        | 0.00265   |
|    loss                 | -0.0848   |
|    n_updates            | 240       |
|    policy_gradient_loss | 0.195     |
|    value_loss           | 0.00106   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 586      |
|    iterations      | 25       |
|    time_elapsed    | 87       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 496        |
|    ep_rew_mean          | -0.0986    |
| time/                   |            |
|    fps                  | 594        |
|    iterations           | 26         |
|    time_elapsed         | 89         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.89210546 |
|    clip_fraction        | 0.622      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.777     |
|    explained_variance   | -1.83      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.119     |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0622    |
|    value_loss           | 0.000634   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.46284056 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.979     |
|    explained_variance   | -0.0307    |
|    learning_rate        | 0.00265    |
|    loss                 | -0.018     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.0011     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.0986  |
| time/              |          |
|    fps             | 581      |
|    iterations      | 27       |
|    time_elapsed    | 95       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 496        |
|    ep_rew_mean          | -0.0986    |
| time/                   |            |
|    fps                  | 589        |
|    iterations           | 28         |
|    time_elapsed         | 97         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.42321914 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | -5.18      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0757    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.000753   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 496        |
|    ep_rew_mean          | -0.0885    |
| time/                   |            |
|    fps                  | 597        |
|    iterations           | 29         |
|    time_elapsed         | 99         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.69056165 |
|    clip_fraction        | 0.589      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.78      |
|    explained_variance   | -3.14      |
|    learning_rate        | 0.00265    |
|    loss                 | -0.104     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0429    |
|    value_loss           | 0.000585   |
----------------------------------------
Eval num_timesteps=60000, episode_reward=-0.09 +/- 0.36
Episode length: 474.20 +/- 152.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 474        |
|    mean_reward          | -0.0897    |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.41234517 |
|    clip_fraction        | 0.657      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.93      |
|    explained_variance   | -0.181     |
|    learning_rate        | 0.00265    |
|    loss                 | -0.0907    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0433    |
|    value_loss           | 0.00129    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.0782  |
| time/              |          |
|    fps             | 586      |
|    iterations      | 30       |
|    time_elapsed    | 104      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 04:02:32,190] Trial 46 finished with value: -0.08967 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.002654254325977666, 'gamma': 0.9041387890675955, 'gae_lambda': 0.8335932067908962}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1279     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 525         |
|    ep_rew_mean          | -0.21       |
| time/                   |             |
|    fps                  | 1071        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.019131238 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -0.0409     |
|    learning_rate        | 0.0028      |
|    loss                 | 0.0213      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.0482      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.12476318 |
|    clip_fraction        | 0.234      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.73      |
|    explained_variance   | -0.234     |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0276    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0228    |
|    value_loss           | 0.00155    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 659      |
|    iterations      | 3        |
|    time_elapsed    | 9        |
|    total_timesteps | 6144     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 525       |
|    ep_rew_mean          | -0.21     |
| time/                   |           |
|    fps                  | 715       |
|    iterations           | 4         |
|    time_elapsed         | 11        |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.4484243 |
|    clip_fraction        | 0.651     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.17     |
|    explained_variance   | 0.538     |
|    learning_rate        | 0.0028    |
|    loss                 | -0.111    |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.346     |
|    value_loss           | 0.00214   |
---------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.33989498 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | 0.259      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.101     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.00175    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 605      |
|    iterations      | 5        |
|    time_elapsed    | 16       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 519         |
|    ep_rew_mean          | -0.164      |
| time/                   |             |
|    fps                  | 643         |
|    iterations           | 6           |
|    time_elapsed         | 19          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.112671696 |
|    clip_fraction        | 0.399       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | -1.51       |
|    learning_rate        | 0.0028      |
|    loss                 | -0.0272     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.000221    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 520        |
|    ep_rew_mean          | -0.171     |
| time/                   |            |
|    fps                  | 672        |
|    iterations           | 7          |
|    time_elapsed         | 21         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.15863642 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.51      |
|    explained_variance   | -0.0715    |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0331    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.00124    |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.13876271 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | -1.98      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0705    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0341    |
|    value_loss           | 0.00044    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | -0.176   |
| time/              |          |
|    fps             | 611      |
|    iterations      | 8        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 521        |
|    ep_rew_mean          | -0.18      |
| time/                   |            |
|    fps                  | 635        |
|    iterations           | 9          |
|    time_elapsed         | 28         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.16612679 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | -2.07      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0694    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0415    |
|    value_loss           | 0.000126   |
----------------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.22043054 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.54      |
|    explained_variance   | -6.74      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0777    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0257    |
|    value_loss           | 0.000412   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 522      |
|    ep_rew_mean     | -0.183   |
| time/              |          |
|    fps             | 594      |
|    iterations      | 10       |
|    time_elapsed    | 34       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 522        |
|    ep_rew_mean          | -0.185     |
| time/                   |            |
|    fps                  | 614        |
|    iterations           | 11         |
|    time_elapsed         | 36         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.31974232 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | -2.85      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0988    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0337    |
|    value_loss           | 0.000527   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 522        |
|    ep_rew_mean          | -0.188     |
| time/                   |            |
|    fps                  | 632        |
|    iterations           | 12         |
|    time_elapsed         | 38         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.25328487 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.51      |
|    explained_variance   | -5.41      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0769    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0391    |
|    value_loss           | 0.00122    |
----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.09 +/- 0.36
Episode length: 475.70 +/- 147.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 476        |
|    mean_reward          | -0.0903    |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.46025658 |
|    clip_fraction        | 0.569      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -6.99      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0914    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.05      |
|    value_loss           | 0.000772   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 522      |
|    ep_rew_mean     | -0.189   |
| time/              |          |
|    fps             | 603      |
|    iterations      | 13       |
|    time_elapsed    | 44       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 522        |
|    ep_rew_mean          | -0.19      |
| time/                   |            |
|    fps                  | 619        |
|    iterations           | 14         |
|    time_elapsed         | 46         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.32009017 |
|    clip_fraction        | 0.611      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | -0.57      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.096     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0239    |
|    value_loss           | 0.00014    |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.25967112 |
|    clip_fraction        | 0.531      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | -3.23      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0605    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0461    |
|    value_loss           | 0.00617    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | -0.192   |
| time/              |          |
|    fps             | 592      |
|    iterations      | 15       |
|    time_elapsed    | 51       |
|    total_timesteps | 30720    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 523       |
|    ep_rew_mean          | -0.193    |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 16        |
|    time_elapsed         | 53        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 0.3718089 |
|    clip_fraction        | 0.618     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.28     |
|    explained_variance   | -9.36     |
|    learning_rate        | 0.0028    |
|    loss                 | -0.0793   |
|    n_updates            | 150       |
|    policy_gradient_loss | -0.0322   |
|    value_loss           | 0.000867  |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 523        |
|    ep_rew_mean          | -0.194     |
| time/                   |            |
|    fps                  | 619        |
|    iterations           | 17         |
|    time_elapsed         | 56         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.28716737 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | -6.11      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0587    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0322    |
|    value_loss           | 0.000689   |
----------------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 1.7820802 |
|    clip_fraction        | 0.633     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.01     |
|    explained_variance   | 0.011     |
|    learning_rate        | 0.0028    |
|    loss                 | -0.0831   |
|    n_updates            | 170       |
|    policy_gradient_loss | -0.0842   |
|    value_loss           | 0.00312   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | -0.195   |
| time/              |          |
|    fps             | 598      |
|    iterations      | 18       |
|    time_elapsed    | 61       |
|    total_timesteps | 36864    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 523       |
|    ep_rew_mean          | -0.196    |
| time/                   |           |
|    fps                  | 609       |
|    iterations           | 19        |
|    time_elapsed         | 63        |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.3955351 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08     |
|    explained_variance   | 0.00743   |
|    learning_rate        | 0.0028    |
|    loss                 | -0.0672   |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.0467   |
|    value_loss           | 0.00121   |
---------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 1.0036497 |
|    clip_fraction        | 0.476     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.745    |
|    explained_variance   | -0.931    |
|    learning_rate        | 0.0028    |
|    loss                 | -0.044    |
|    n_updates            | 190       |
|    policy_gradient_loss | -0.0353   |
|    value_loss           | 0.00319   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | -0.196   |
| time/              |          |
|    fps             | 591      |
|    iterations      | 20       |
|    time_elapsed    | 69       |
|    total_timesteps | 40960    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 523       |
|    ep_rew_mean          | -0.197    |
| time/                   |           |
|    fps                  | 601       |
|    iterations           | 21        |
|    time_elapsed         | 71        |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.5025354 |
|    clip_fraction        | 0.551     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08     |
|    explained_variance   | -0.134    |
|    learning_rate        | 0.0028    |
|    loss                 | -0.0586   |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0277   |
|    value_loss           | 0.000914  |
---------------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.85553586 |
|    clip_fraction        | 0.641      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | -0.762     |
|    learning_rate        | 0.0028     |
|    loss                 | -0.124     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0686    |
|    value_loss           | 0.0019     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 523      |
|    ep_rew_mean     | -0.198   |
| time/              |          |
|    fps             | 585      |
|    iterations      | 22       |
|    time_elapsed    | 77       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 523       |
|    ep_rew_mean          | -0.198    |
| time/                   |           |
|    fps                  | 594       |
|    iterations           | 23        |
|    time_elapsed         | 79        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 1.6448869 |
|    clip_fraction        | 0.632     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.06     |
|    explained_variance   | 0.0652    |
|    learning_rate        | 0.0028    |
|    loss                 | -0.0905   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0549   |
|    value_loss           | 0.00246   |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 524      |
|    ep_rew_mean          | -0.199   |
| time/                   |          |
|    fps                  | 604      |
|    iterations           | 24       |
|    time_elapsed         | 81       |
|    total_timesteps      | 49152    |
| train/                  |          |
|    approx_kl            | 1.415139 |
|    clip_fraction        | 0.663    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.975   |
|    explained_variance   | -1.65    |
|    learning_rate        | 0.0028   |
|    loss                 | -0.0812  |
|    n_updates            | 230      |
|    policy_gradient_loss | -0.0459  |
|    value_loss           | 0.00224  |
--------------------------------------
Eval num_timesteps=50000, episode_reward=-0.09 +/- 0.36
Episode length: 474.70 +/- 150.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 475       |
|    mean_reward          | -0.0899   |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.7404084 |
|    clip_fraction        | 0.612     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.12     |
|    explained_variance   | -4.22     |
|    learning_rate        | 0.0028    |
|    loss                 | -0.0701   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0142   |
|    value_loss           | 0.00112   |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 519      |
|    ep_rew_mean     | -0.187   |
| time/              |          |
|    fps             | 592      |
|    iterations      | 25       |
|    time_elapsed    | 86       |
|    total_timesteps | 51200    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 519       |
|    ep_rew_mean          | -0.188    |
| time/                   |           |
|    fps                  | 600       |
|    iterations           | 26        |
|    time_elapsed         | 88        |
|    total_timesteps      | 53248     |
| train/                  |           |
|    approx_kl            | 0.7185838 |
|    clip_fraction        | 0.645     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.03     |
|    explained_variance   | -0.549    |
|    learning_rate        | 0.0028    |
|    loss                 | -0.0855   |
|    n_updates            | 250       |
|    policy_gradient_loss | -0.0442   |
|    value_loss           | 0.00206   |
---------------------------------------
Eval num_timesteps=55000, episode_reward=-0.11 +/- 0.30
Episode length: 523.10 +/- 5.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 523        |
|    mean_reward          | -0.109     |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.35559547 |
|    clip_fraction        | 0.582      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | -5.17      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.069     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0422    |
|    value_loss           | 0.000666   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | -0.176   |
| time/              |          |
|    fps             | 587      |
|    iterations      | 27       |
|    time_elapsed    | 94       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 514       |
|    ep_rew_mean          | -0.176    |
| time/                   |           |
|    fps                  | 595       |
|    iterations           | 28        |
|    time_elapsed         | 96        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 2.6165373 |
|    clip_fraction        | 0.55      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.812    |
|    explained_variance   | 0.0191    |
|    learning_rate        | 0.0028    |
|    loss                 | -0.105    |
|    n_updates            | 270       |
|    policy_gradient_loss | -0.048    |
|    value_loss           | 0.00152   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 514        |
|    ep_rew_mean          | -0.176     |
| time/                   |            |
|    fps                  | 602        |
|    iterations           | 29         |
|    time_elapsed         | 98         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.54965425 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.785     |
|    explained_variance   | -3.73      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0187    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0269    |
|    value_loss           | 0.00405    |
----------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.59609634 |
|    clip_fraction        | 0.643      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | -4.82      |
|    learning_rate        | 0.0028     |
|    loss                 | -0.0608    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0326    |
|    value_loss           | 0.00212    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | -0.176   |
| time/              |          |
|    fps             | 590      |
|    iterations      | 30       |
|    time_elapsed    | 104      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 04:04:20,432] Trial 47 finished with value: -0.21000000000000002 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0027977581516952605, 'gamma': 0.9031729081660609, 'gae_lambda': 0.8302535005367001}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1291     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 525        |
|    ep_rew_mean          | -0.21      |
| time/                   |            |
|    fps                  | 1075       |
|    iterations           | 2          |
|    time_elapsed         | 3          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.02542686 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0182    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00615   |
|    value_loss           | 0.0404     |
----------------------------------------
Eval num_timesteps=5000, episode_reward=-0.09 +/- 0.36
Episode length: 473.70 +/- 153.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 474         |
|    mean_reward          | -0.0894     |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011091974 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -0.17       |
|    learning_rate        | 0.00399     |
|    loss                 | 0.00413     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 0.00292     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 686      |
|    iterations      | 3        |
|    time_elapsed    | 8        |
|    total_timesteps | 6144     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 525       |
|    ep_rew_mean          | -0.21     |
| time/                   |           |
|    fps                  | 736       |
|    iterations           | 4         |
|    time_elapsed         | 11        |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.1270419 |
|    clip_fraction        | 0.499     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.62     |
|    explained_variance   | -1.59     |
|    learning_rate        | 0.00399   |
|    loss                 | -0.00376  |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.0188    |
|    value_loss           | 0.00416   |
---------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.016979815 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.0241      |
|    learning_rate        | 0.00399     |
|    loss                 | -0.0184     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 0.000244    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 615      |
|    iterations      | 5        |
|    time_elapsed    | 16       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 525         |
|    ep_rew_mean          | -0.21       |
| time/                   |             |
|    fps                  | 652         |
|    iterations           | 6           |
|    time_elapsed         | 18          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.013144544 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | -0.178      |
|    learning_rate        | 0.00399     |
|    loss                 | -0.0382     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00984    |
|    value_loss           | 6.31e-05    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 525        |
|    ep_rew_mean          | -0.21      |
| time/                   |            |
|    fps                  | 681        |
|    iterations           | 7          |
|    time_elapsed         | 21         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.05327152 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | -0.0455    |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0309    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00171   |
|    value_loss           | 0.00131    |
----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.04155363 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | -1.5       |
|    learning_rate        | 0.00399    |
|    loss                 | 0.00469    |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.00291    |
|    value_loss           | 0.00082    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 617      |
|    iterations      | 8        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 518         |
|    ep_rew_mean          | -0.179      |
| time/                   |             |
|    fps                  | 641         |
|    iterations           | 9           |
|    time_elapsed         | 28          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.021254733 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -2.42       |
|    learning_rate        | 0.00399     |
|    loss                 | -0.041      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0089     |
|    value_loss           | 2.16e-05    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=-0.09 +/- 0.36
Episode length: 473.50 +/- 154.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 474        |
|    mean_reward          | -0.0894    |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.05938662 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.45      |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0147    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.00894   |
|    value_loss           | 0.00229    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | -0.154   |
| time/              |          |
|    fps             | 604      |
|    iterations      | 10       |
|    time_elapsed    | 33       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 515        |
|    ep_rew_mean          | -0.16      |
| time/                   |            |
|    fps                  | 624        |
|    iterations           | 11         |
|    time_elapsed         | 36         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.18198308 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.61      |
|    explained_variance   | -0.306     |
|    learning_rate        | 0.00399    |
|    loss                 | -0.00516   |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.00552    |
|    value_loss           | 0.00146    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 508         |
|    ep_rew_mean          | -0.141      |
| time/                   |             |
|    fps                  | 641         |
|    iterations           | 12          |
|    time_elapsed         | 38          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.038963705 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | -6.56       |
|    learning_rate        | 0.00399     |
|    loss                 | 0.00388     |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.000178    |
|    value_loss           | 3.53e-05    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.053988848 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | -0.0645     |
|    learning_rate        | 0.00399     |
|    loss                 | -0.0317     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00953    |
|    value_loss           | 0.00238     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | -0.126   |
| time/              |          |
|    fps             | 608      |
|    iterations      | 13       |
|    time_elapsed    | 43       |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | -0.112    |
| time/                   |           |
|    fps                  | 623       |
|    iterations           | 14        |
|    time_elapsed         | 45        |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.0494098 |
|    clip_fraction        | 0.206     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.51     |
|    explained_variance   | -0.0674   |
|    learning_rate        | 0.00399   |
|    loss                 | -0.0287   |
|    n_updates            | 130       |
|    policy_gradient_loss | 0.00121   |
|    value_loss           | 0.00122   |
---------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.04784231 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.54      |
|    explained_variance   | -0.0644    |
|    learning_rate        | 0.00399    |
|    loss                 | 0.0176     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.00408   |
|    value_loss           | 0.00142    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 597      |
|    iterations      | 15       |
|    time_elapsed    | 51       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 503        |
|    ep_rew_mean          | -0.124     |
| time/                   |            |
|    fps                  | 610        |
|    iterations           | 16         |
|    time_elapsed         | 53         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.15291381 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.43      |
|    explained_variance   | -6.88      |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0226    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 0.00277    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 504        |
|    ep_rew_mean          | -0.129     |
| time/                   |            |
|    fps                  | 623        |
|    iterations           | 17         |
|    time_elapsed         | 55         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.09137305 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.59      |
|    explained_variance   | -5.18      |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0271    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 0.00111    |
----------------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 525       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.2925414 |
|    clip_fraction        | 0.268     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.28     |
|    explained_variance   | -8.47     |
|    learning_rate        | 0.00399   |
|    loss                 | -0.0506   |
|    n_updates            | 170       |
|    policy_gradient_loss | -0.0294   |
|    value_loss           | 0.000298  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -0.133   |
| time/              |          |
|    fps             | 601      |
|    iterations      | 18       |
|    time_elapsed    | 61       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 506        |
|    ep_rew_mean          | -0.137     |
| time/                   |            |
|    fps                  | 612        |
|    iterations           | 19         |
|    time_elapsed         | 63         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.14280325 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | -5.88      |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0176    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0232    |
|    value_loss           | 0.000938   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.16429818 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | -0.34      |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0311    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.00744   |
|    value_loss           | 0.00163    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 593      |
|    iterations      | 20       |
|    time_elapsed    | 68       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 508        |
|    ep_rew_mean          | -0.144     |
| time/                   |            |
|    fps                  | 604        |
|    iterations           | 21         |
|    time_elapsed         | 71         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.09576314 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | -2.14      |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0438    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0107    |
|    value_loss           | 0.00162    |
----------------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.118167266 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | -7.98       |
|    learning_rate        | 0.00399     |
|    loss                 | -0.0266     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.000895    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.134   |
| time/              |          |
|    fps             | 587      |
|    iterations      | 22       |
|    time_elapsed    | 76       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 505       |
|    ep_rew_mean          | -0.137    |
| time/                   |           |
|    fps                  | 597       |
|    iterations           | 23        |
|    time_elapsed         | 78        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.8030221 |
|    clip_fraction        | 0.204     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.24     |
|    explained_variance   | 0.0708    |
|    learning_rate        | 0.00399   |
|    loss                 | -0.0227   |
|    n_updates            | 220       |
|    policy_gradient_loss | 0.157     |
|    value_loss           | 0.00273   |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 503        |
|    ep_rew_mean          | -0.129     |
| time/                   |            |
|    fps                  | 606        |
|    iterations           | 24         |
|    time_elapsed         | 81         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.08177544 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.59      |
|    explained_variance   | -8.97      |
|    learning_rate        | 0.00399    |
|    loss                 | -0.00251   |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.000235  |
|    value_loss           | 5.89e-05   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.09864102 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.39      |
|    explained_variance   | 0.12       |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0246    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.00777   |
|    value_loss           | 0.00118    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.121   |
| time/              |          |
|    fps             | 591      |
|    iterations      | 25       |
|    time_elapsed    | 86       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 502        |
|    ep_rew_mean          | -0.121     |
| time/                   |            |
|    fps                  | 600        |
|    iterations           | 26         |
|    time_elapsed         | 88         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.19973958 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.41      |
|    explained_variance   | -0.038     |
|    learning_rate        | 0.00399    |
|    loss                 | 0.0127     |
|    n_updates            | 250        |
|    policy_gradient_loss | 0.00913    |
|    value_loss           | 0.00292    |
----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.04691681 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | -5.06      |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0346    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.00912   |
|    value_loss           | 0.00019    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.121   |
| time/              |          |
|    fps             | 587      |
|    iterations      | 27       |
|    time_elapsed    | 94       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 502        |
|    ep_rew_mean          | -0.121     |
| time/                   |            |
|    fps                  | 595        |
|    iterations           | 28         |
|    time_elapsed         | 96         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.11661732 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | -12.2      |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0227    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.00127   |
|    value_loss           | 0.000725   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 502         |
|    ep_rew_mean          | -0.121      |
| time/                   |             |
|    fps                  | 603         |
|    iterations           | 29          |
|    time_elapsed         | 98          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.032347977 |
|    clip_fraction        | 0.0897      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | -6.87       |
|    learning_rate        | 0.00399     |
|    loss                 | -0.0307     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0084     |
|    value_loss           | 0.000272    |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.03583728 |
|    clip_fraction        | 0.071      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.61      |
|    explained_variance   | -2.77      |
|    learning_rate        | 0.00399    |
|    loss                 | -0.0258    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 0.000563   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 591      |
|    iterations      | 30       |
|    time_elapsed    | 103      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 04:06:08,524] Trial 48 finished with value: -0.21000000000000002 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.003989612506648087, 'gamma': 0.9133332514317671, 'gae_lambda': 0.8111902955202253}. Best is trial 34 with value: 0.026290000000000025.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 1282     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 525          |
|    ep_rew_mean          | -0.21        |
| time/                   |              |
|    fps                  | 1080         |
|    iterations           | 2            |
|    time_elapsed         | 3            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0110908635 |
|    clip_fraction        | 0.0801       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -7.76        |
|    learning_rate        | 0.00579      |
|    loss                 | -0.0282      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00482     |
|    value_loss           | 0.181        |
------------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.014855256 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0217     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00743    |
|    value_loss           | 7.84e-06    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 518      |
|    ep_rew_mean     | -0.116   |
| time/              |          |
|    fps             | 660      |
|    iterations      | 3        |
|    time_elapsed    | 9        |
|    total_timesteps | 6144     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 520          |
|    ep_rew_mean          | -0.141       |
| time/                   |              |
|    fps                  | 712          |
|    iterations           | 4            |
|    time_elapsed         | 11           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0057018953 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.00579      |
|    loss                 | 0.00304      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00109     |
|    value_loss           | 0.00106      |
------------------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014467646 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | 0.000237    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00752    |
|    value_loss           | 6.65e-07    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.101   |
| time/              |          |
|    fps             | 603      |
|    iterations      | 5        |
|    time_elapsed    | 16       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 505        |
|    ep_rew_mean          | -0.119     |
| time/                   |            |
|    fps                  | 641        |
|    iterations           | 6          |
|    time_elapsed         | 19         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01322989 |
|    clip_fraction        | 0.0692     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | 0          |
|    learning_rate        | 0.00579    |
|    loss                 | -0.0252    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.00322   |
|    value_loss           | 0.00109    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 508         |
|    ep_rew_mean          | -0.132      |
| time/                   |             |
|    fps                  | 670         |
|    iterations           | 7           |
|    time_elapsed         | 21          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.012053098 |
|    clip_fraction        | 0.063       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0025     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00392    |
|    value_loss           | 4.45e-07    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.00910533 |
|    clip_fraction        | 0.0828     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.00579    |
|    loss                 | 0.00459    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0037    |
|    value_loss           | 7.46e-08   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | -0.142   |
| time/              |          |
|    fps             | 610      |
|    iterations      | 8        |
|    time_elapsed    | 26       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 512         |
|    ep_rew_mean          | -0.149      |
| time/                   |             |
|    fps                  | 634         |
|    iterations           | 9           |
|    time_elapsed         | 29          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.013859861 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | -4.77e-07   |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0063     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 3.77e-08    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010661237 |
|    clip_fraction        | 0.0523      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0261     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00293    |
|    value_loss           | 3.39e-08    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.126   |
| time/              |          |
|    fps             | 593      |
|    iterations      | 10       |
|    time_elapsed    | 34       |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 505          |
|    ep_rew_mean          | -0.134       |
| time/                   |              |
|    fps                  | 613          |
|    iterations           | 11           |
|    time_elapsed         | 36           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0030890596 |
|    clip_fraction        | 0.0323       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0            |
|    learning_rate        | 0.00579      |
|    loss                 | -0.0205      |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.000659    |
|    value_loss           | 0.00111      |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 507        |
|    ep_rew_mean          | -0.14      |
| time/                   |            |
|    fps                  | 632        |
|    iterations           | 12         |
|    time_elapsed         | 38         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.01323463 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.75      |
|    explained_variance   | 0          |
|    learning_rate        | 0.00579    |
|    loss                 | -0.0355    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00614   |
|    value_loss           | 4.57e-07   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012633012 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0228     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00538    |
|    value_loss           | 1.75e-07    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.146   |
| time/              |          |
|    fps             | 600      |
|    iterations      | 13       |
|    time_elapsed    | 44       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 510         |
|    ep_rew_mean          | -0.15       |
| time/                   |             |
|    fps                  | 615         |
|    iterations           | 14          |
|    time_elapsed         | 46          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.010589622 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | 0.0245      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00385    |
|    value_loss           | 1.18e-07    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.014667754 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0118     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 8.96e-08    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | -0.154   |
| time/              |          |
|    fps             | 589      |
|    iterations      | 15       |
|    time_elapsed    | 52       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 512         |
|    ep_rew_mean          | -0.158      |
| time/                   |             |
|    fps                  | 603         |
|    iterations           | 16          |
|    time_elapsed         | 54          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.009076428 |
|    clip_fraction        | 0.0274      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.00579     |
|    loss                 | 0.00925     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00186    |
|    value_loss           | 7.37e-08    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 512         |
|    ep_rew_mean          | -0.16       |
| time/                   |             |
|    fps                  | 616         |
|    iterations           | 17          |
|    time_elapsed         | 56          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.012465338 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0113     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 6.69e-08    |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.01647653 |
|    clip_fraction        | 0.0877     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.75      |
|    explained_variance   | 0          |
|    learning_rate        | 0.00579    |
|    loss                 | -0.0232    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.00423   |
|    value_loss           | 4.81e-08   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | -0.163   |
| time/              |          |
|    fps             | 594      |
|    iterations      | 18       |
|    time_elapsed    | 62       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 511         |
|    ep_rew_mean          | -0.152      |
| time/                   |             |
|    fps                  | 605         |
|    iterations           | 19          |
|    time_elapsed         | 64          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.007737449 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 1.79e-07    |
|    learning_rate        | 0.00579     |
|    loss                 | 0.00732     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00124    |
|    value_loss           | 6.15e-08    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0060106996 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.00579      |
|    loss                 | 0.00222      |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 0.00111      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | -0.155   |
| time/              |          |
|    fps             | 588      |
|    iterations      | 20       |
|    time_elapsed    | 69       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 512         |
|    ep_rew_mean          | -0.157      |
| time/                   |             |
|    fps                  | 598         |
|    iterations           | 21          |
|    time_elapsed         | 71          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.007574326 |
|    clip_fraction        | 0.0652      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00579     |
|    loss                 | 0.00688     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00235    |
|    value_loss           | 2.65e-07    |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.008959047 |
|    clip_fraction        | 0.0307      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | 0.0215      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00201    |
|    value_loss           | 1.21e-07    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.159   |
| time/              |          |
|    fps             | 583      |
|    iterations      | 22       |
|    time_elapsed    | 77       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 513         |
|    ep_rew_mean          | -0.161      |
| time/                   |             |
|    fps                  | 593         |
|    iterations           | 23          |
|    time_elapsed         | 79          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.013980653 |
|    clip_fraction        | 0.0633      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0193     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00323    |
|    value_loss           | 6.94e-08    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 513         |
|    ep_rew_mean          | -0.163      |
| time/                   |             |
|    fps                  | 602         |
|    iterations           | 24          |
|    time_elapsed         | 81          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.015258365 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0152     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 7.79e-08    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008358123 |
|    clip_fraction        | 0.0408      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0253     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 7.05e-08    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 510      |
|    ep_rew_mean     | -0.154   |
| time/              |          |
|    fps             | 588      |
|    iterations      | 25       |
|    time_elapsed    | 87       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 509         |
|    ep_rew_mean          | -0.143      |
| time/                   |             |
|    fps                  | 596         |
|    iterations           | 26          |
|    time_elapsed         | 89          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.013696587 |
|    clip_fraction        | 0.0238      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0196     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00179    |
|    value_loss           | 0.00111     |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.012949698 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | 0.00916     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 0.00109     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.143   |
| time/              |          |
|    fps             | 584      |
|    iterations      | 27       |
|    time_elapsed    | 94       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 509         |
|    ep_rew_mean          | -0.154      |
| time/                   |             |
|    fps                  | 591         |
|    iterations           | 28          |
|    time_elapsed         | 96          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.011647738 |
|    clip_fraction        | 0.0789      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | -3.58e-07   |
|    learning_rate        | 0.00579     |
|    loss                 | -0.000841   |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00416    |
|    value_loss           | 2.13e-07    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 509         |
|    ep_rew_mean          | -0.154      |
| time/                   |             |
|    fps                  | 599         |
|    iterations           | 29          |
|    time_elapsed         | 99          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.010533005 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | -0.0224     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00417    |
|    value_loss           | 3.63e-08    |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.009402031 |
|    clip_fraction        | 0.0761      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0           |
|    learning_rate        | 0.00579     |
|    loss                 | 0.00786     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 3.49e-08    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | -0.165   |
| time/              |          |
|    fps             | 587      |
|    iterations      | 30       |
|    time_elapsed    | 104      |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-21 04:07:57,369] Trial 49 finished with value: -0.21000000000000002 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.005787328103270582, 'gamma': 0.9006266202803507, 'gae_lambda': 0.8236486782618636}. Best is trial 34 with value: 0.026290000000000025.
PPO Best trial: 0.026290000000000025
PPO Best hyperparameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0006516999665573909, 'gamma': 0.9345474231070908, 'gae_lambda': 0.8147459509364795}
