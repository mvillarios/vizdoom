[I 2024-07-20 22:09:25,453] A new study created in memory with name: no-name-9a2b4859-f0f7-4358-ac28-2bef1545ef72
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.94 +/- 0.05
Episode length: 25.30 +/- 10.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.1     |
|    ep_rew_mean     | -87.2    |
| time/              |          |
|    fps             | 1135     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-5.71 +/- 48.55
Episode length: 13.70 +/- 2.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.7        |
|    mean_reward          | -5.71       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011424977 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -2.26e-06   |
|    learning_rate        | 7.86e-05    |
|    loss                 | 664         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0069     |
|    value_loss           | 1.53e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-28.40 +/- 39.26
Episode length: 16.10 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -28.4    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.7     |
|    ep_rew_mean     | -86.6    |
| time/              |          |
|    fps             | 1033     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=119.25 +/- 98.41
Episode length: 13.30 +/- 2.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.3        |
|    mean_reward          | 119         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011205789 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.0245      |
|    learning_rate        | 7.86e-05    |
|    loss                 | 537         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00976    |
|    value_loss           | 1.05e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | -73.5    |
| time/              |          |
|    fps             | 1012     |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=127.18 +/- 114.18
Episode length: 13.30 +/- 3.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.3        |
|    mean_reward          | 127         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012563216 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.114       |
|    learning_rate        | 7.86e-05    |
|    loss                 | 471         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 998         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=233.73 +/- 160.76
Episode length: 15.80 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -54.9    |
| time/              |          |
|    fps             | 998      |
|    iterations      | 4        |
|    time_elapsed    | 32       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=349.23 +/- 164.58
Episode length: 20.40 +/- 4.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 20.4         |
|    mean_reward          | 349          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0135055985 |
|    clip_fraction        | 0.248        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.152        |
|    learning_rate        | 7.86e-05     |
|    loss                 | 660          |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0191      |
|    value_loss           | 1.29e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=160.01 +/- 111.77
Episode length: 14.30 +/- 3.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.3     |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -40      |
| time/              |          |
|    fps             | 990      |
|    iterations      | 5        |
|    time_elapsed    | 41       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=348.82 +/- 215.20
Episode length: 18.70 +/- 5.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.7         |
|    mean_reward          | 349          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0138099305 |
|    clip_fraction        | 0.27         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.228        |
|    learning_rate        | 7.86e-05     |
|    loss                 | 770          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0211      |
|    value_loss           | 1.43e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.2     |
|    ep_rew_mean     | -12      |
| time/              |          |
|    fps             | 987      |
|    iterations      | 6        |
|    time_elapsed    | 49       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=211.32 +/- 165.98
Episode length: 15.60 +/- 4.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | 211         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.013670419 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.271       |
|    learning_rate        | 7.86e-05    |
|    loss                 | 732         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 1.83e+03    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=191.41 +/- 168.81
Episode length: 14.90 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 7.79     |
| time/              |          |
|    fps             | 982      |
|    iterations      | 7        |
|    time_elapsed    | 58       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=154.26 +/- 143.81
Episode length: 14.60 +/- 4.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.6       |
|    mean_reward          | 154        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.01415671 |
|    clip_fraction        | 0.271      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.69      |
|    explained_variance   | 0.268      |
|    learning_rate        | 7.86e-05   |
|    loss                 | 1.01e+03   |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 2.04e+03   |
----------------------------------------
Eval num_timesteps=65000, episode_reward=369.08 +/- 169.62
Episode length: 19.70 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.7     |
|    mean_reward     | 369      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 34.1     |
| time/              |          |
|    fps             | 978      |
|    iterations      | 8        |
|    time_elapsed    | 66       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:10:34,613] Trial 0 finished with value: 220.81066890000002 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 7.85681674987496e-05, 'gamma': 0.9727331463613923, 'gae_lambda': 0.9676250261181422}. Best is trial 0 with value: 220.81066890000002.
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | -80.4    |
| time/              |          |
|    fps             | 1184     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.25 +/- 63.01
Episode length: 14.90 +/- 5.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 14.9      |
|    mean_reward          | 0.254     |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 0.4896722 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.75     |
|    explained_variance   | -0.000126 |
|    learning_rate        | 0.00288   |
|    loss                 | 391       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.139     |
|    value_loss           | 672       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -30.7    |
| time/              |          |
|    fps             | 896      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=236.49 +/- 166.95
Episode length: 16.10 +/- 4.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.1      |
|    mean_reward          | 236       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 1.0633947 |
|    clip_fraction        | 0.667     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.57     |
|    explained_variance   | -0.0189   |
|    learning_rate        | 0.00288   |
|    loss                 | 145       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.0999    |
|    value_loss           | 565       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 41.1     |
| time/              |          |
|    fps             | 822      |
|    iterations      | 3        |
|    time_elapsed    | 14       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=245.16 +/- 159.24
Episode length: 16.30 +/- 4.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 245        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.35681707 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.958     |
|    explained_variance   | 0.0484     |
|    learning_rate        | 0.00288    |
|    loss                 | 372        |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0733     |
|    value_loss           | 1.14e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 782      |
|    iterations      | 4        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=302.85 +/- 188.14
Episode length: 18.50 +/- 5.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 18.5      |
|    mean_reward          | 303       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.9756936 |
|    clip_fraction        | 0.54      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.402    |
|    explained_variance   | 0.124     |
|    learning_rate        | 0.00288   |
|    loss                 | 789       |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.0482    |
|    value_loss           | 1.92e+03  |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 212      |
| time/              |          |
|    fps             | 766      |
|    iterations      | 5        |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15.8       |
|    ep_rew_mean          | 208        |
| time/                   |            |
|    fps                  | 759        |
|    iterations           | 6          |
|    time_elapsed         | 32         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.14236331 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0992    |
|    explained_variance   | 0.336      |
|    learning_rate        | 0.00288    |
|    loss                 | 1.65e+03   |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.00275    |
|    value_loss           | 2.64e+03   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=185.87 +/- 151.23
Episode length: 16.10 +/- 3.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.1       |
|    mean_reward          | 186        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.11761544 |
|    clip_fraction        | 0.0645     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0969    |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.00288    |
|    loss                 | 1.02e+03   |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.00269    |
|    value_loss           | 2.67e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 85.3     |
| time/              |          |
|    fps             | 753      |
|    iterations      | 7        |
|    time_elapsed    | 38       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=215.88 +/- 238.87
Episode length: 17.40 +/- 7.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17.4      |
|    mean_reward          | 216       |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.8887289 |
|    clip_fraction        | 0.348     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.33     |
|    explained_variance   | 0.504     |
|    learning_rate        | 0.00288   |
|    loss                 | 1.01e+03  |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.00677  |
|    value_loss           | 2.47e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 157      |
| time/              |          |
|    fps             | 743      |
|    iterations      | 8        |
|    time_elapsed    | 44       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=270.16 +/- 180.86
Episode length: 17.30 +/- 5.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17.3      |
|    mean_reward          | 270       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.4540494 |
|    clip_fraction        | 0.279     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.161    |
|    explained_variance   | 0.594     |
|    learning_rate        | 0.00288   |
|    loss                 | 866       |
|    n_updates            | 80        |
|    policy_gradient_loss | 0.0317    |
|    value_loss           | 2.15e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 235      |
| time/              |          |
|    fps             | 737      |
|    iterations      | 9        |
|    time_elapsed    | 49       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=235.70 +/- 199.76
Episode length: 17.90 +/- 6.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.9       |
|    mean_reward          | 236        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.14953695 |
|    clip_fraction        | 0.082      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0623    |
|    explained_variance   | 0.555      |
|    learning_rate        | 0.00288    |
|    loss                 | 768        |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.00808    |
|    value_loss           | 2.5e+03    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 733      |
|    iterations      | 10       |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=203.33 +/- 145.78
Episode length: 16.00 +/- 4.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16        |
|    mean_reward          | 203       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 1.4436065 |
|    clip_fraction        | 0.15      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0502   |
|    explained_variance   | 0.576     |
|    learning_rate        | 0.00288   |
|    loss                 | 768       |
|    n_updates            | 100       |
|    policy_gradient_loss | 0.0304    |
|    value_loss           | 2.34e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 731      |
|    iterations      | 11       |
|    time_elapsed    | 61       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.8        |
|    ep_rew_mean          | 219         |
| time/                   |             |
|    fps                  | 728         |
|    iterations           | 12          |
|    time_elapsed         | 67          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.093576744 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0246     |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.00288     |
|    loss                 | 773         |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00553     |
|    value_loss           | 2.46e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=221.18 +/- 201.64
Episode length: 16.10 +/- 6.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.1       |
|    mean_reward          | 221        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.37985587 |
|    clip_fraction        | 0.0476     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00845   |
|    explained_variance   | 0.525      |
|    learning_rate        | 0.00288    |
|    loss                 | 580        |
|    n_updates            | 120        |
|    policy_gradient_loss | 0.0153     |
|    value_loss           | 2.33e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 230      |
| time/              |          |
|    fps             | 725      |
|    iterations      | 13       |
|    time_elapsed    | 73       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=259.56 +/- 241.41
Episode length: 17.40 +/- 6.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.4       |
|    mean_reward          | 260        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.05790349 |
|    clip_fraction        | 0.0201     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00895   |
|    explained_variance   | 0.528      |
|    learning_rate        | 0.00288    |
|    loss                 | 764        |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.00362    |
|    value_loss           | 2.41e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 227      |
| time/              |          |
|    fps             | 722      |
|    iterations      | 14       |
|    time_elapsed    | 79       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=206.63 +/- 136.75
Episode length: 14.90 +/- 3.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | 207         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.054791473 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00896    |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.00288     |
|    loss                 | 936         |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.00202     |
|    value_loss           | 2.19e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | 208      |
| time/              |          |
|    fps             | 720      |
|    iterations      | 15       |
|    time_elapsed    | 85       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:12:02,617] Trial 1 finished with value: 266.347252 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.00287595185463439, 'gamma': 0.9166062397683631, 'gae_lambda': 0.8873892655923339}. Best is trial 1 with value: 266.347252.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.9     |
|    ep_rew_mean     | -81.1    |
| time/              |          |
|    fps             | 1190     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 32.4      |
|    ep_rew_mean          | -51.7     |
| time/                   |           |
|    fps                  | 1005      |
|    iterations           | 2         |
|    time_elapsed         | 4         |
|    total_timesteps      | 4096      |
| train/                  |           |
|    approx_kl            | 0.6782989 |
|    clip_fraction        | 0.739     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.78     |
|    explained_variance   | -0.000198 |
|    learning_rate        | 0.00191   |
|    loss                 | 233       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.122     |
|    value_loss           | 602       |
---------------------------------------
Eval num_timesteps=5000, episode_reward=-2.22 +/- 94.76
Episode length: 21.10 +/- 7.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.1       |
|    mean_reward          | -2.22      |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.12287955 |
|    clip_fraction        | 0.581      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.79      |
|    explained_variance   | 0.222      |
|    learning_rate        | 0.00191    |
|    loss                 | 191        |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.0457     |
|    value_loss           | 573        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.1     |
|    ep_rew_mean     | -37.9    |
| time/              |          |
|    fps             | 942      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.7       |
|    ep_rew_mean          | -32        |
| time/                   |            |
|    fps                  | 919        |
|    iterations           | 4          |
|    time_elapsed         | 8          |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.06424203 |
|    clip_fraction        | 0.506      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.84      |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.00191    |
|    loss                 | 244        |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0176     |
|    value_loss           | 586        |
----------------------------------------
Eval num_timesteps=10000, episode_reward=107.07 +/- 117.48
Episode length: 13.10 +/- 3.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.1        |
|    mean_reward          | 107         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.052758627 |
|    clip_fraction        | 0.473       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.00191     |
|    loss                 | 205         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00977     |
|    value_loss           | 637         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 19.5     |
| time/              |          |
|    fps             | 898      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 47.9        |
| time/                   |             |
|    fps                  | 892         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.048194982 |
|    clip_fraction        | 0.479       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.172       |
|    learning_rate        | 0.00191     |
|    loss                 | 402         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.0213      |
|    value_loss           | 800         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.1        |
|    ep_rew_mean          | 49.5        |
| time/                   |             |
|    fps                  | 890         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.059325743 |
|    clip_fraction        | 0.439       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.218       |
|    learning_rate        | 0.00191     |
|    loss                 | 254         |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00109     |
|    value_loss           | 908         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=184.62 +/- 124.32
Episode length: 15.50 +/- 4.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.5       |
|    mean_reward          | 185        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.04141676 |
|    clip_fraction        | 0.401      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.193      |
|    learning_rate        | 0.00191    |
|    loss                 | 295        |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.00929    |
|    value_loss           | 1.03e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | 63.7     |
| time/              |          |
|    fps             | 881      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.2        |
|    ep_rew_mean          | 99.1        |
| time/                   |             |
|    fps                  | 880         |
|    iterations           | 9           |
|    time_elapsed         | 20          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.062674254 |
|    clip_fraction        | 0.427       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.186       |
|    learning_rate        | 0.00191     |
|    loss                 | 378         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00894     |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=227.53 +/- 146.12
Episode length: 16.40 +/- 4.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.4       |
|    mean_reward          | 228        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.06392257 |
|    clip_fraction        | 0.379      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.258      |
|    learning_rate        | 0.00191    |
|    loss                 | 417        |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.00454    |
|    value_loss           | 1.22e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 96.3     |
| time/              |          |
|    fps             | 873      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.3        |
|    ep_rew_mean          | 152         |
| time/                   |             |
|    fps                  | 872         |
|    iterations           | 11          |
|    time_elapsed         | 25          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.114311844 |
|    clip_fraction        | 0.378       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.907      |
|    explained_variance   | 0.289       |
|    learning_rate        | 0.00191     |
|    loss                 | 358         |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.0215      |
|    value_loss           | 1.56e+03    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15.8       |
|    ep_rew_mean          | 138        |
| time/                   |            |
|    fps                  | 871        |
|    iterations           | 12         |
|    time_elapsed         | 28         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.13568084 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.628     |
|    explained_variance   | 0.323      |
|    learning_rate        | 0.00191    |
|    loss                 | 714        |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00525   |
|    value_loss           | 2.27e+03   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=233.52 +/- 152.68
Episode length: 17.70 +/- 5.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 234        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.25986278 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.552     |
|    explained_variance   | 0.433      |
|    learning_rate        | 0.00191    |
|    loss                 | 545        |
|    n_updates            | 120        |
|    policy_gradient_loss | 0.0192     |
|    value_loss           | 1.97e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 866      |
|    iterations      | 13       |
|    time_elapsed    | 30       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.2       |
|    ep_rew_mean          | 157        |
| time/                   |            |
|    fps                  | 866        |
|    iterations           | 14         |
|    time_elapsed         | 33         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.11862145 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.464     |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.00191    |
|    loss                 | 571        |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.0301     |
|    value_loss           | 1.96e+03   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=290.57 +/- 145.67
Episode length: 17.60 +/- 4.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.6       |
|    mean_reward          | 291        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.07470767 |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.439     |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.00191    |
|    loss                 | 510        |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.00237    |
|    value_loss           | 2.13e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | 186      |
| time/              |          |
|    fps             | 863      |
|    iterations      | 15       |
|    time_elapsed    | 35       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15.7       |
|    ep_rew_mean          | 180        |
| time/                   |            |
|    fps                  | 862        |
|    iterations           | 16         |
|    time_elapsed         | 37         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.05644382 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.333     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.00191    |
|    loss                 | 712        |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0033    |
|    value_loss           | 2.37e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.5        |
|    ep_rew_mean          | 207         |
| time/                   |             |
|    fps                  | 863         |
|    iterations           | 17          |
|    time_elapsed         | 40          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.055702046 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.307      |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.00191     |
|    loss                 | 673         |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 2.27e+03    |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=211.54 +/- 150.97
Episode length: 15.80 +/- 4.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | 212         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.060106024 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.254      |
|    explained_variance   | 0.542       |
|    learning_rate        | 0.00191     |
|    loss                 | 838         |
|    n_updates            | 170         |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 2.53e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 861      |
|    iterations      | 18       |
|    time_elapsed    | 42       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.8        |
|    ep_rew_mean          | 190         |
| time/                   |             |
|    fps                  | 861         |
|    iterations           | 19          |
|    time_elapsed         | 45          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.047126733 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.256      |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.00191     |
|    loss                 | 925         |
|    n_updates            | 180         |
|    policy_gradient_loss | 0.000247    |
|    value_loss           | 2.57e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=276.33 +/- 185.99
Episode length: 17.30 +/- 5.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 276        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.20240828 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.161     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.00191    |
|    loss                 | 374        |
|    n_updates            | 190        |
|    policy_gradient_loss | 0.0127     |
|    value_loss           | 2.66e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 858      |
|    iterations      | 20       |
|    time_elapsed    | 47       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.5        |
|    ep_rew_mean          | 232         |
| time/                   |             |
|    fps                  | 859         |
|    iterations           | 21          |
|    time_elapsed         | 50          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.015741551 |
|    clip_fraction        | 0.0423      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.544       |
|    learning_rate        | 0.00191     |
|    loss                 | 921         |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00494    |
|    value_loss           | 2.48e+03    |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=167.76 +/- 100.74
Episode length: 14.20 +/- 2.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 14.2      |
|    mean_reward          | 168       |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0844142 |
|    clip_fraction        | 0.0568    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.136    |
|    explained_variance   | 0.522     |
|    learning_rate        | 0.00191   |
|    loss                 | 1.06e+03  |
|    n_updates            | 210       |
|    policy_gradient_loss | -0.00418  |
|    value_loss           | 3.34e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 858      |
|    iterations      | 22       |
|    time_elapsed    | 52       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 14.9       |
|    ep_rew_mean          | 194        |
| time/                   |            |
|    fps                  | 858        |
|    iterations           | 23         |
|    time_elapsed         | 54         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.66666335 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0946    |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.00191    |
|    loss                 | 674        |
|    n_updates            | 220        |
|    policy_gradient_loss | 0.017      |
|    value_loss           | 2.76e+03   |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | 220          |
| time/                   |              |
|    fps                  | 858          |
|    iterations           | 24           |
|    time_elapsed         | 57           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0030175734 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0284      |
|    explained_variance   | 0.593        |
|    learning_rate        | 0.00191      |
|    loss                 | 873          |
|    n_updates            | 230          |
|    policy_gradient_loss | 0.000284     |
|    value_loss           | 2.89e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=246.87 +/- 167.11
Episode length: 17.30 +/- 4.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.004625383 |
|    clip_fraction        | 0.00508     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0286     |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.00191     |
|    loss                 | 1.06e+03    |
|    n_updates            | 240         |
|    policy_gradient_loss | 1.08e-05    |
|    value_loss           | 2.91e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 856      |
|    iterations      | 25       |
|    time_elapsed    | 59       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16           |
|    ep_rew_mean          | 227          |
| time/                   |              |
|    fps                  | 856          |
|    iterations           | 26           |
|    time_elapsed         | 62           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0013138694 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0358      |
|    explained_variance   | 0.541        |
|    learning_rate        | 0.00191      |
|    loss                 | 1.11e+03     |
|    n_updates            | 250          |
|    policy_gradient_loss | 5e-05        |
|    value_loss           | 2.83e+03     |
------------------------------------------
Eval num_timesteps=55000, episode_reward=329.92 +/- 128.77
Episode length: 19.30 +/- 4.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.3        |
|    mean_reward          | 330         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.008537055 |
|    clip_fraction        | 0.00488     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0265     |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.00191     |
|    loss                 | 1.2e+03     |
|    n_updates            | 260         |
|    policy_gradient_loss | 2.98e-05    |
|    value_loss           | 2.89e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 219      |
| time/              |          |
|    fps             | 854      |
|    iterations      | 27       |
|    time_elapsed    | 64       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | 221          |
| time/                   |              |
|    fps                  | 854          |
|    iterations           | 28           |
|    time_elapsed         | 67           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0010536348 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0196      |
|    explained_variance   | 0.491        |
|    learning_rate        | 0.00191      |
|    loss                 | 974          |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000135    |
|    value_loss           | 3e+03        |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16           |
|    ep_rew_mean          | 224          |
| time/                   |              |
|    fps                  | 855          |
|    iterations           | 29           |
|    time_elapsed         | 69           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0025093658 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0164      |
|    explained_variance   | 0.549        |
|    learning_rate        | 0.00191      |
|    loss                 | 736          |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00019     |
|    value_loss           | 2.67e+03     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=189.72 +/- 171.88
Episode length: 15.00 +/- 4.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 190          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0055652666 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0134      |
|    explained_variance   | 0.496        |
|    learning_rate        | 0.00191      |
|    loss                 | 868          |
|    n_updates            | 290          |
|    policy_gradient_loss | 0.000566     |
|    value_loss           | 2.85e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | 208      |
| time/              |          |
|    fps             | 854      |
|    iterations      | 30       |
|    time_elapsed    | 71       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:13:15,470] Trial 2 finished with value: 187.9169295 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.001907696701908466, 'gamma': 0.9657198005779274, 'gae_lambda': 0.8526356397750227}. Best is trial 1 with value: 266.347252.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.4     |
|    ep_rew_mean     | -89.7    |
| time/              |          |
|    fps             | 1195     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.6        |
|    ep_rew_mean          | -58.6       |
| time/                   |             |
|    fps                  | 1070        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.030302443 |
|    clip_fraction        | 0.683       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.000169    |
|    learning_rate        | 0.00191     |
|    loss                 | 252         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.108       |
|    value_loss           | 743         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=243.59 +/- 207.25
Episode length: 16.20 +/- 5.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.2       |
|    mean_reward          | 244        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.11292331 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.172      |
|    learning_rate        | 0.00191    |
|    loss                 | 234        |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.0081     |
|    value_loss           | 692        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -5.8     |
| time/              |          |
|    fps             | 1018     |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.4       |
|    ep_rew_mean          | -4.64      |
| time/                   |            |
|    fps                  | 1007       |
|    iterations           | 4          |
|    time_elapsed         | 8          |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.06700891 |
|    clip_fraction        | 0.519      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | 0.188      |
|    learning_rate        | 0.00191    |
|    loss                 | 297        |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0241     |
|    value_loss           | 683        |
----------------------------------------
Eval num_timesteps=10000, episode_reward=312.18 +/- 161.12
Episode length: 18.40 +/- 4.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.4        |
|    mean_reward          | 312         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.054363146 |
|    clip_fraction        | 0.493       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.235       |
|    learning_rate        | 0.00191     |
|    loss                 | 287         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00852     |
|    value_loss           | 812         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.2     |
|    ep_rew_mean     | 4.65     |
| time/              |          |
|    fps             | 982      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.1       |
|    ep_rew_mean          | -9.39      |
| time/                   |            |
|    fps                  | 977        |
|    iterations           | 6          |
|    time_elapsed         | 12         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.08754037 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | 0.268      |
|    learning_rate        | 0.00191    |
|    loss                 | 402        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.002     |
|    value_loss           | 929        |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 23.4        |
| time/                   |             |
|    fps                  | 976         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.040622286 |
|    clip_fraction        | 0.451       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.202       |
|    learning_rate        | 0.00191     |
|    loss                 | 249         |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.0135      |
|    value_loss           | 814         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=180.08 +/- 141.94
Episode length: 14.40 +/- 3.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.4       |
|    mean_reward          | 180        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.04481958 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.56      |
|    explained_variance   | 0.326      |
|    learning_rate        | 0.00191    |
|    loss                 | 358        |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.00487   |
|    value_loss           | 976        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 58.3     |
| time/              |          |
|    fps             | 967      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.6        |
|    ep_rew_mean          | 96.8        |
| time/                   |             |
|    fps                  | 968         |
|    iterations           | 9           |
|    time_elapsed         | 19          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.059397697 |
|    clip_fraction        | 0.427       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.00191     |
|    loss                 | 400         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00631    |
|    value_loss           | 1.24e+03    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=243.35 +/- 200.82
Episode length: 17.00 +/- 5.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 243         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.058025554 |
|    clip_fraction        | 0.415       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.935      |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.00191     |
|    loss                 | 551         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.000456    |
|    value_loss           | 1.49e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 145      |
| time/              |          |
|    fps             | 962      |
|    iterations      | 10       |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.4        |
|    ep_rew_mean          | 187         |
| time/                   |             |
|    fps                  | 963         |
|    iterations           | 11          |
|    time_elapsed         | 23          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.040235326 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.494      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.00191     |
|    loss                 | 972         |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.000113    |
|    value_loss           | 2.62e+03    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15.5       |
|    ep_rew_mean          | 195        |
| time/                   |            |
|    fps                  | 963        |
|    iterations           | 12         |
|    time_elapsed         | 25         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.03830287 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.424     |
|    explained_variance   | 0.434      |
|    learning_rate        | 0.00191    |
|    loss                 | 803        |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00371   |
|    value_loss           | 2.72e+03   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=206.21 +/- 153.06
Episode length: 15.70 +/- 5.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.077465974 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.492       |
|    learning_rate        | 0.00191     |
|    loss                 | 1.05e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.00348     |
|    value_loss           | 2.82e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 205      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.1         |
|    ep_rew_mean          | 220          |
| time/                   |              |
|    fps                  | 962          |
|    iterations           | 14           |
|    time_elapsed         | 29           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0075785033 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0868      |
|    explained_variance   | 0.391        |
|    learning_rate        | 0.00191      |
|    loss                 | 1.19e+03     |
|    n_updates            | 130          |
|    policy_gradient_loss | -3.63e-05    |
|    value_loss           | 3.22e+03     |
------------------------------------------
Eval num_timesteps=30000, episode_reward=144.46 +/- 101.24
Episode length: 13.20 +/- 2.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.2         |
|    mean_reward          | 144          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0047816616 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0663      |
|    explained_variance   | 0.451        |
|    learning_rate        | 0.00191      |
|    loss                 | 993          |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 2.79e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | 229      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.8        |
|    ep_rew_mean          | 205         |
| time/                   |             |
|    fps                  | 961         |
|    iterations           | 16          |
|    time_elapsed         | 34          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.013818522 |
|    clip_fraction        | 0.0184      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0968     |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.00191     |
|    loss                 | 1.02e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00403    |
|    value_loss           | 2.77e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16          |
|    ep_rew_mean          | 226         |
| time/                   |             |
|    fps                  | 962         |
|    iterations           | 17          |
|    time_elapsed         | 36          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.028412562 |
|    clip_fraction        | 0.0496      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.00191     |
|    loss                 | 692         |
|    n_updates            | 160         |
|    policy_gradient_loss | 0.00662     |
|    value_loss           | 2.53e+03    |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=185.87 +/- 160.04
Episode length: 14.60 +/- 3.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 186          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0074351286 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0624      |
|    explained_variance   | 0.427        |
|    learning_rate        | 0.00191      |
|    loss                 | 1.06e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000268    |
|    value_loss           | 3.23e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 226      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.5         |
|    ep_rew_mean          | 207          |
| time/                   |              |
|    fps                  | 960          |
|    iterations           | 19           |
|    time_elapsed         | 40           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0021277342 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0369      |
|    explained_variance   | 0.481        |
|    learning_rate        | 0.00191      |
|    loss                 | 990          |
|    n_updates            | 180          |
|    policy_gradient_loss | 0.000181     |
|    value_loss           | 3.06e+03     |
------------------------------------------
Eval num_timesteps=40000, episode_reward=174.56 +/- 159.12
Episode length: 14.10 +/- 4.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.1        |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.001338583 |
|    clip_fraction        | 0.00166     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0152     |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.00191     |
|    loss                 | 1.04e+03    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.000696   |
|    value_loss           | 2.8e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | 204      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 20       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.5        |
|    ep_rew_mean          | 247         |
| time/                   |             |
|    fps                  | 960         |
|    iterations           | 21          |
|    time_elapsed         | 44          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.002799874 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0245     |
|    explained_variance   | 0.433       |
|    learning_rate        | 0.00191     |
|    loss                 | 980         |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.000427    |
|    value_loss           | 2.8e+03     |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=293.72 +/- 282.20
Episode length: 17.90 +/- 8.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.9         |
|    mean_reward          | 294          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0013562993 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0222      |
|    explained_variance   | 0.394        |
|    learning_rate        | 0.00191      |
|    loss                 | 997          |
|    n_updates            | 210          |
|    policy_gradient_loss | -3.78e-05    |
|    value_loss           | 2.78e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 253      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 22       |
|    time_elapsed    | 47       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | 240          |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 23           |
|    time_elapsed         | 49           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0030096816 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0.437        |
|    learning_rate        | 0.00191      |
|    loss                 | 1.12e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | 0.000675     |
|    value_loss           | 2.92e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | 214          |
| time/                   |              |
|    fps                  | 959          |
|    iterations           | 24           |
|    time_elapsed         | 51           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 9.340103e-05 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0139      |
|    explained_variance   | 0.484        |
|    learning_rate        | 0.00191      |
|    loss                 | 1.09e+03     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.000174    |
|    value_loss           | 2.81e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=221.77 +/- 168.13
Episode length: 15.70 +/- 4.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | 222          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0026596212 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0271      |
|    explained_variance   | 0.388        |
|    learning_rate        | 0.00191      |
|    loss                 | 1.09e+03     |
|    n_updates            | 240          |
|    policy_gradient_loss | 0.000693     |
|    value_loss           | 2.9e+03      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | 210      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 25       |
|    time_elapsed    | 53       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.7         |
|    ep_rew_mean          | 248          |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 26           |
|    time_elapsed         | 55           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 6.574576e-05 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0215      |
|    explained_variance   | 0.506        |
|    learning_rate        | 0.00191      |
|    loss                 | 855          |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.000834    |
|    value_loss           | 2.72e+03     |
------------------------------------------
Eval num_timesteps=55000, episode_reward=172.64 +/- 214.37
Episode length: 15.40 +/- 6.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 173          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0015765913 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0242      |
|    explained_variance   | 0.469        |
|    learning_rate        | 0.00191      |
|    loss                 | 854          |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000518    |
|    value_loss           | 2.61e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 265      |
| time/              |          |
|    fps             | 956      |
|    iterations      | 27       |
|    time_elapsed    | 57       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | 212          |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 28           |
|    time_elapsed         | 59           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0035467036 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0411      |
|    explained_variance   | 0.523        |
|    learning_rate        | 0.00191      |
|    loss                 | 974          |
|    n_updates            | 270          |
|    policy_gradient_loss | 0.000839     |
|    value_loss           | 2.79e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.1         |
|    ep_rew_mean          | 194          |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 29           |
|    time_elapsed         | 62           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0064707743 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0313      |
|    explained_variance   | 0.406        |
|    learning_rate        | 0.00191      |
|    loss                 | 883          |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 2.74e+03     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=245.43 +/- 162.86
Episode length: 16.30 +/- 3.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | 245          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0004218319 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0401      |
|    explained_variance   | 0.41         |
|    learning_rate        | 0.00191      |
|    loss                 | 948          |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000669    |
|    value_loss           | 2.62e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 243      |
| time/              |          |
|    fps             | 956      |
|    iterations      | 30       |
|    time_elapsed    | 64       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:14:20,436] Trial 3 finished with value: 216.6817352 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.0019113686838677464, 'gamma': 0.9246706789491205, 'gae_lambda': 0.909808494203562}. Best is trial 1 with value: 266.347252.
Using cuda device
Eval num_timesteps=5000, episode_reward=163.71 +/- 133.96
Episode length: 16.90 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.5     |
|    ep_rew_mean     | -86.3    |
| time/              |          |
|    fps             | 1155     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=211.97 +/- 125.38
Episode length: 15.90 +/- 3.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | 212         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010726893 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 5.63e-05    |
|    learning_rate        | 2.28e-05    |
|    loss                 | 903         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00836    |
|    value_loss           | 2.22e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=276.92 +/- 265.10
Episode length: 17.10 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | 277      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | -84.5    |
| time/              |          |
|    fps             | 1043     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=133.20 +/- 120.27
Episode length: 13.70 +/- 2.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.7        |
|    mean_reward          | 133         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011331325 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.000171    |
|    learning_rate        | 2.28e-05    |
|    loss                 | 567         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 1.27e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | -55.5    |
| time/              |          |
|    fps             | 1013     |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=245.97 +/- 176.78
Episode length: 16.60 +/- 4.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | 246         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.014527994 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.00114     |
|    learning_rate        | 2.28e-05    |
|    loss                 | 733         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 1.57e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=278.17 +/- 230.69
Episode length: 17.60 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | -27.7    |
| time/              |          |
|    fps             | 996      |
|    iterations      | 4        |
|    time_elapsed    | 32       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=274.66 +/- 211.19
Episode length: 17.40 +/- 5.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17.4      |
|    mean_reward          | 275       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.0107359 |
|    clip_fraction        | 0.192     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.94     |
|    explained_variance   | 0.0138    |
|    learning_rate        | 2.28e-05  |
|    loss                 | 1.17e+03  |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0146   |
|    value_loss           | 2.26e+03  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=255.23 +/- 220.76
Episode length: 16.80 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 255      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | -3.75    |
| time/              |          |
|    fps             | 986      |
|    iterations      | 5        |
|    time_elapsed    | 41       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=216.79 +/- 158.86
Episode length: 15.70 +/- 4.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 217         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.014948054 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.0245      |
|    learning_rate        | 2.28e-05    |
|    loss                 | 1.36e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 2.76e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 982      |
|    iterations      | 6        |
|    time_elapsed    | 50       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=241.25 +/- 175.27
Episode length: 16.20 +/- 4.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 241         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010988212 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.0255      |
|    learning_rate        | 2.28e-05    |
|    loss                 | 1.59e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00973    |
|    value_loss           | 3.37e+03    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=190.41 +/- 203.61
Episode length: 16.00 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 22.6     |
| time/              |          |
|    fps             | 977      |
|    iterations      | 7        |
|    time_elapsed    | 58       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=272.80 +/- 148.12
Episode length: 17.30 +/- 4.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.014482378 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.079       |
|    learning_rate        | 2.28e-05    |
|    loss                 | 1.35e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 3.14e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=206.20 +/- 156.84
Episode length: 15.60 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 206      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 56.6     |
| time/              |          |
|    fps             | 971      |
|    iterations      | 8        |
|    time_elapsed    | 67       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:15:30,203] Trial 4 finished with value: 179.7951249 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 2.277585664528122e-05, 'gamma': 0.9875303775788007, 'gae_lambda': 0.9733393458065628}. Best is trial 1 with value: 266.347252.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | -94.8    |
| time/              |          |
|    fps             | 1180     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-59.17 +/- 115.37
Episode length: 29.10 +/- 14.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.1       |
|    mean_reward          | -59.2      |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.09600715 |
|    clip_fraction        | 0.655      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.73      |
|    explained_variance   | -1.53e-05  |
|    learning_rate        | 0.0028     |
|    loss                 | 323        |
|    n_updates            | 10         |
|    policy_gradient_loss | 0.121      |
|    value_loss           | 599        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.8     |
|    ep_rew_mean     | -78.6    |
| time/              |          |
|    fps             | 977      |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-115.97 +/- 0.03
Episode length: 27.40 +/- 11.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 27.4      |
|    mean_reward          | -116      |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.6165855 |
|    clip_fraction        | 0.585     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.9      |
|    explained_variance   | 0.256     |
|    learning_rate        | 0.0028    |
|    loss                 | 240       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.0604    |
|    value_loss           | 488       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.3     |
|    ep_rew_mean     | -47.2    |
| time/              |          |
|    fps             | 933      |
|    iterations      | 3        |
|    time_elapsed    | 13       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-60.35 +/- 70.63
Episode length: 28.60 +/- 11.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 28.6      |
|    mean_reward          | -60.3     |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.3713675 |
|    clip_fraction        | 0.658     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.62     |
|    explained_variance   | 0.402     |
|    learning_rate        | 0.0028    |
|    loss                 | 189       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.0904    |
|    value_loss           | 437       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | -39.9    |
| time/              |          |
|    fps             | 907      |
|    iterations      | 4        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=106.14 +/- 126.10
Episode length: 17.60 +/- 7.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.6       |
|    mean_reward          | 106        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.11530904 |
|    clip_fraction        | 0.595      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | 0.399      |
|    learning_rate        | 0.0028     |
|    loss                 | 154        |
|    n_updates            | 40         |
|    policy_gradient_loss | 0.0473     |
|    value_loss           | 452        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.2     |
|    ep_rew_mean     | 5.31     |
| time/              |          |
|    fps             | 895      |
|    iterations      | 5        |
|    time_elapsed    | 22       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.1       |
|    ep_rew_mean          | 20.8       |
| time/                   |            |
|    fps                  | 894        |
|    iterations           | 6          |
|    time_elapsed         | 27         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.09782227 |
|    clip_fraction        | 0.576      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | 0.451      |
|    learning_rate        | 0.0028     |
|    loss                 | 165        |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.0306     |
|    value_loss           | 533        |
----------------------------------------
Eval num_timesteps=25000, episode_reward=63.39 +/- 100.04
Episode length: 16.00 +/- 4.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16         |
|    mean_reward          | 63.4       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.09651314 |
|    clip_fraction        | 0.552      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.5       |
|    explained_variance   | 0.49       |
|    learning_rate        | 0.0028     |
|    loss                 | 145        |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.0143     |
|    value_loss           | 666        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | 76.7     |
| time/              |          |
|    fps             | 886      |
|    iterations      | 7        |
|    time_elapsed    | 32       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=111.88 +/- 147.67
Episode length: 23.70 +/- 9.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 23.7     |
|    mean_reward          | 112      |
| time/                   |          |
|    total_timesteps      | 30000    |
| train/                  |          |
|    approx_kl            | 0.100606 |
|    clip_fraction        | 0.555    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.31    |
|    explained_variance   | 0.553    |
|    learning_rate        | 0.0028   |
|    loss                 | 298      |
|    n_updates            | 70       |
|    policy_gradient_loss | 0.021    |
|    value_loss           | 848      |
--------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 102      |
| time/              |          |
|    fps             | 882      |
|    iterations      | 8        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=283.44 +/- 77.32
Episode length: 32.40 +/- 4.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 32.4       |
|    mean_reward          | 283        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.22825447 |
|    clip_fraction        | 0.584      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.52       |
|    learning_rate        | 0.0028     |
|    loss                 | 222        |
|    n_updates            | 80         |
|    policy_gradient_loss | 0.0281     |
|    value_loss           | 993        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.1     |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 876      |
|    iterations      | 9        |
|    time_elapsed    | 42       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=247.78 +/- 69.57
Episode length: 33.60 +/- 5.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 33.6       |
|    mean_reward          | 248        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.17173797 |
|    clip_fraction        | 0.575      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0028     |
|    loss                 | 259        |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0362     |
|    value_loss           | 1.14e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.8     |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 871      |
|    iterations      | 10       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=228.56 +/- 60.72
Episode length: 37.30 +/- 12.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 37.3       |
|    mean_reward          | 229        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.55699635 |
|    clip_fraction        | 0.676      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.0028     |
|    loss                 | 260        |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.0732     |
|    value_loss           | 1.23e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.4     |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 867      |
|    iterations      | 11       |
|    time_elapsed    | 51       |
|    total_timesteps | 45056    |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 30.8     |
|    ep_rew_mean          | 228      |
| time/                   |          |
|    fps                  | 866      |
|    iterations           | 12       |
|    time_elapsed         | 56       |
|    total_timesteps      | 49152    |
| train/                  |          |
|    approx_kl            | 1.144597 |
|    clip_fraction        | 0.672    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.02    |
|    explained_variance   | 0.728    |
|    learning_rate        | 0.0028   |
|    loss                 | 335      |
|    n_updates            | 110      |
|    policy_gradient_loss | 0.0576   |
|    value_loss           | 1.13e+03 |
--------------------------------------
Eval num_timesteps=50000, episode_reward=283.76 +/- 127.69
Episode length: 28.40 +/- 8.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 28.4      |
|    mean_reward          | 284       |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.6109192 |
|    clip_fraction        | 0.618     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.832    |
|    explained_variance   | 0.738     |
|    learning_rate        | 0.0028    |
|    loss                 | 352       |
|    n_updates            | 120       |
|    policy_gradient_loss | 0.0541    |
|    value_loss           | 1.23e+03  |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 863      |
|    iterations      | 13       |
|    time_elapsed    | 61       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=234.43 +/- 145.71
Episode length: 29.70 +/- 7.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 29.7      |
|    mean_reward          | 234       |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.5765532 |
|    clip_fraction        | 0.531     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.596    |
|    explained_variance   | 0.682     |
|    learning_rate        | 0.0028    |
|    loss                 | 440       |
|    n_updates            | 130       |
|    policy_gradient_loss | 0.0405    |
|    value_loss           | 1.67e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.1     |
|    ep_rew_mean     | 198      |
| time/              |          |
|    fps             | 860      |
|    iterations      | 14       |
|    time_elapsed    | 66       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=120.45 +/- 156.48
Episode length: 28.20 +/- 11.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 28.2      |
|    mean_reward          | 120       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 1.3850749 |
|    clip_fraction        | 0.593     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.655    |
|    explained_variance   | 0.756     |
|    learning_rate        | 0.0028    |
|    loss                 | 391       |
|    n_updates            | 140       |
|    policy_gradient_loss | 0.0668    |
|    value_loss           | 1.47e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 144      |
| time/              |          |
|    fps             | 858      |
|    iterations      | 15       |
|    time_elapsed    | 71       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:16:43,383] Trial 5 finished with value: -17.0115569 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.0027977425277281222, 'gamma': 0.9892429157185669, 'gae_lambda': 0.8280653849220686}. Best is trial 1 with value: 266.347252.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | -91      |
| time/              |          |
|    fps             | 1179     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=352.88 +/- 231.23
Episode length: 19.50 +/- 6.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.5        |
|    mean_reward          | 353         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010995903 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000113   |
|    learning_rate        | 8.77e-05    |
|    loss                 | 295         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 723         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | -81.5    |
| time/              |          |
|    fps             | 1055     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=202.52 +/- 132.10
Episode length: 15.20 +/- 4.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012898192 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.0169      |
|    learning_rate        | 8.77e-05    |
|    loss                 | 173         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 557         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -68.6    |
| time/              |          |
|    fps             | 1019     |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=144.97 +/- 128.66
Episode length: 14.20 +/- 3.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.2        |
|    mean_reward          | 145         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.018097496 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.0942      |
|    learning_rate        | 8.77e-05    |
|    loss                 | 379         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0259     |
|    value_loss           | 603         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | -60      |
| time/              |          |
|    fps             | 1002     |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=213.76 +/- 189.04
Episode length: 16.10 +/- 5.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.014087442 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.222       |
|    learning_rate        | 8.77e-05    |
|    loss                 | 262         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 543         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | -49      |
| time/              |          |
|    fps             | 990      |
|    iterations      | 5        |
|    time_elapsed    | 20       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | -28.7      |
| time/                   |            |
|    fps                  | 987        |
|    iterations           | 6          |
|    time_elapsed         | 24         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.01572874 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.9       |
|    explained_variance   | 0.351      |
|    learning_rate        | 8.77e-05   |
|    loss                 | 264        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0298    |
|    value_loss           | 538        |
----------------------------------------
Eval num_timesteps=25000, episode_reward=260.45 +/- 148.44
Episode length: 17.00 +/- 4.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 260         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015562559 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.398       |
|    learning_rate        | 8.77e-05    |
|    loss                 | 274         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 581         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | -2.97    |
| time/              |          |
|    fps             | 980      |
|    iterations      | 7        |
|    time_elapsed    | 29       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=145.79 +/- 118.42
Episode length: 13.80 +/- 2.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.8        |
|    mean_reward          | 146         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.018055877 |
|    clip_fraction        | 0.409       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.409       |
|    learning_rate        | 8.77e-05    |
|    loss                 | 294         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0413     |
|    value_loss           | 710         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 26.9     |
| time/              |          |
|    fps             | 977      |
|    iterations      | 8        |
|    time_elapsed    | 33       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=190.83 +/- 153.67
Episode length: 15.30 +/- 4.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.018913168 |
|    clip_fraction        | 0.406       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.371       |
|    learning_rate        | 8.77e-05    |
|    loss                 | 440         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0398     |
|    value_loss           | 882         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | 45.1     |
| time/              |          |
|    fps             | 973      |
|    iterations      | 9        |
|    time_elapsed    | 37       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=255.55 +/- 223.33
Episode length: 17.00 +/- 6.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17         |
|    mean_reward          | 256        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.02480539 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.401      |
|    learning_rate        | 8.77e-05   |
|    loss                 | 568        |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0374    |
|    value_loss           | 985        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 65       |
| time/              |          |
|    fps             | 970      |
|    iterations      | 10       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=217.84 +/- 157.47
Episode length: 15.70 +/- 3.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | 218        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.03150592 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.967     |
|    explained_variance   | 0.389      |
|    learning_rate        | 8.77e-05   |
|    loss                 | 620        |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0308    |
|    value_loss           | 1.31e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 969      |
|    iterations      | 11       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.4        |
|    ep_rew_mean          | 151         |
| time/                   |             |
|    fps                  | 969         |
|    iterations           | 12          |
|    time_elapsed         | 50          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.016316859 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.754      |
|    explained_variance   | 0.368       |
|    learning_rate        | 8.77e-05    |
|    loss                 | 908         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 1.8e+03     |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=233.75 +/- 173.26
Episode length: 16.50 +/- 4.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | 234         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015483161 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.584      |
|    explained_variance   | 0.424       |
|    learning_rate        | 8.77e-05    |
|    loss                 | 794         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00764    |
|    value_loss           | 2.24e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | 151      |
| time/              |          |
|    fps             | 969      |
|    iterations      | 13       |
|    time_elapsed    | 54       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=216.77 +/- 160.34
Episode length: 15.60 +/- 4.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | 217         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.009569178 |
|    clip_fraction        | 0.0892      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.457      |
|    explained_variance   | 0.43        |
|    learning_rate        | 8.77e-05    |
|    loss                 | 1.15e+03    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00775    |
|    value_loss           | 2.63e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 967      |
|    iterations      | 14       |
|    time_elapsed    | 59       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=209.28 +/- 162.98
Episode length: 15.10 +/- 4.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 209         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.005570856 |
|    clip_fraction        | 0.0645      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.331      |
|    explained_variance   | 0.489       |
|    learning_rate        | 8.77e-05    |
|    loss                 | 1.49e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00647    |
|    value_loss           | 2.74e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 200      |
| time/              |          |
|    fps             | 966      |
|    iterations      | 15       |
|    time_elapsed    | 63       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:17:48,124] Trial 6 finished with value: 281.81199780000003 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 8.773645486148636e-05, 'gamma': 0.9443941419014311, 'gae_lambda': 0.8030252894165435}. Best is trial 6 with value: 281.81199780000003.
Using cuda device
Eval num_timesteps=5000, episode_reward=224.31 +/- 197.31
Episode length: 15.70 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | -91.6    |
| time/              |          |
|    fps             | 1171     |
|    iterations      | 1        |
|    time_elapsed    | 6        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-64.48 +/- 36.86
Episode length: 21.80 +/- 6.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.8        |
|    mean_reward          | -64.5       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.013283797 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 4.4e-05     |
|    learning_rate        | 1.43e-05    |
|    loss                 | 150         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 727         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-54.96 +/- 43.72
Episode length: 16.70 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -55      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29       |
|    ep_rew_mean     | -82.4    |
| time/              |          |
|    fps             | 878      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=304.52 +/- 152.96
Episode length: 18.20 +/- 4.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.2        |
|    mean_reward          | 305         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.013242725 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.00688     |
|    learning_rate        | 1.43e-05    |
|    loss                 | 216         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 664         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -72.8    |
| time/              |          |
|    fps             | 816      |
|    iterations      | 3        |
|    time_elapsed    | 30       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=299.32 +/- 120.12
Episode length: 18.60 +/- 3.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.6        |
|    mean_reward          | 299         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012711935 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.0279      |
|    learning_rate        | 1.43e-05    |
|    loss                 | 372         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 657         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=206.96 +/- 149.01
Episode length: 15.40 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | -64.8    |
| time/              |          |
|    fps             | 781      |
|    iterations      | 4        |
|    time_elapsed    | 41       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=222.71 +/- 158.44
Episode length: 15.40 +/- 3.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 223         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012343476 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.0867      |
|    learning_rate        | 1.43e-05    |
|    loss                 | 290         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 657         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=264.14 +/- 228.96
Episode length: 17.80 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | -43.1    |
| time/              |          |
|    fps             | 762      |
|    iterations      | 5        |
|    time_elapsed    | 53       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=301.06 +/- 223.13
Episode length: 18.10 +/- 5.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.1        |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.014039421 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.187       |
|    learning_rate        | 1.43e-05    |
|    loss                 | 267         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 668         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | -25.1    |
| time/              |          |
|    fps             | 753      |
|    iterations      | 6        |
|    time_elapsed    | 65       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=178.70 +/- 152.51
Episode length: 14.80 +/- 4.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015140394 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.27        |
|    learning_rate        | 1.43e-05    |
|    loss                 | 303         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 726         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=182.60 +/- 164.18
Episode length: 14.80 +/- 4.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | -0.835   |
| time/              |          |
|    fps             | 747      |
|    iterations      | 7        |
|    time_elapsed    | 76       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=177.35 +/- 211.24
Episode length: 14.60 +/- 5.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.6        |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.015339224 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.312       |
|    learning_rate        | 1.43e-05    |
|    loss                 | 236         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 820         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=256.51 +/- 158.53
Episode length: 17.30 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 15.2     |
| time/              |          |
|    fps             | 740      |
|    iterations      | 8        |
|    time_elapsed    | 88       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:19:21,711] Trial 7 finished with value: 334.35086079999996 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 1.433792152364836e-05, 'gamma': 0.9163891350148526, 'gae_lambda': 0.8395106088411898}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.1     |
|    ep_rew_mean     | -83.4    |
| time/              |          |
|    fps             | 1199     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 26.9      |
|    ep_rew_mean          | -22.2     |
| time/                   |           |
|    fps                  | 1017      |
|    iterations           | 2         |
|    time_elapsed         | 4         |
|    total_timesteps      | 4096      |
| train/                  |           |
|    approx_kl            | 0.4432432 |
|    clip_fraction        | 0.648     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.82     |
|    explained_variance   | -0.000281 |
|    learning_rate        | 0.00343   |
|    loss                 | 170       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.0679    |
|    value_loss           | 589       |
---------------------------------------
Eval num_timesteps=5000, episode_reward=103.78 +/- 139.95
Episode length: 18.40 +/- 3.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.4       |
|    mean_reward          | 104        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.24937658 |
|    clip_fraction        | 0.61       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.67      |
|    explained_variance   | -0.0569    |
|    learning_rate        | 0.00343    |
|    loss                 | 198        |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.0759     |
|    value_loss           | 574        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | -36.7    |
| time/              |          |
|    fps             | 949      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 18.3      |
|    ep_rew_mean          | 10.9      |
| time/                   |           |
|    fps                  | 933       |
|    iterations           | 4         |
|    time_elapsed         | 8         |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.5379434 |
|    clip_fraction        | 0.728     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.5      |
|    explained_variance   | 0.188     |
|    learning_rate        | 0.00343   |
|    loss                 | 179       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.106     |
|    value_loss           | 630       |
---------------------------------------
Eval num_timesteps=10000, episode_reward=245.02 +/- 147.69
Episode length: 17.10 +/- 4.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17.1      |
|    mean_reward          | 245       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.2058945 |
|    clip_fraction        | 0.641     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.28     |
|    explained_variance   | 0.21      |
|    learning_rate        | 0.00343   |
|    loss                 | 314       |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.0407    |
|    value_loss           | 792       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 45.8     |
| time/              |          |
|    fps             | 905      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.5       |
|    ep_rew_mean          | 52.3       |
| time/                   |            |
|    fps                  | 898        |
|    iterations           | 6          |
|    time_elapsed         | 13         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.14079893 |
|    clip_fraction        | 0.588      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.241      |
|    learning_rate        | 0.00343    |
|    loss                 | 315        |
|    n_updates            | 50         |
|    policy_gradient_loss | 0.0379     |
|    value_loss           | 1.04e+03   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.3       |
|    ep_rew_mean          | 42.1       |
| time/                   |            |
|    fps                  | 893        |
|    iterations           | 7          |
|    time_elapsed         | 16         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.08716669 |
|    clip_fraction        | 0.572      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.201      |
|    learning_rate        | 0.00343    |
|    loss                 | 469        |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.0205     |
|    value_loss           | 1.16e+03   |
----------------------------------------
Eval num_timesteps=15000, episode_reward=256.56 +/- 159.73
Episode length: 16.70 +/- 4.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.7       |
|    mean_reward          | 257        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.12841558 |
|    clip_fraction        | 0.542      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.203      |
|    learning_rate        | 0.00343    |
|    loss                 | 314        |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.0294     |
|    value_loss           | 1.37e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | 106      |
| time/              |          |
|    fps             | 884      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.2       |
|    ep_rew_mean          | 104        |
| time/                   |            |
|    fps                  | 884        |
|    iterations           | 9          |
|    time_elapsed         | 20         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.16950455 |
|    clip_fraction        | 0.522      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.878     |
|    explained_variance   | 0.203      |
|    learning_rate        | 0.00343    |
|    loss                 | 727        |
|    n_updates            | 80         |
|    policy_gradient_loss | 0.0347     |
|    value_loss           | 1.55e+03   |
----------------------------------------
Eval num_timesteps=20000, episode_reward=182.45 +/- 131.64
Episode length: 14.50 +/- 3.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.121024415 |
|    clip_fraction        | 0.373       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.794      |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.00343     |
|    loss                 | 441         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.0194      |
|    value_loss           | 1.89e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 113      |
| time/              |          |
|    fps             | 880      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 17.1      |
|    ep_rew_mean          | 152       |
| time/                   |           |
|    fps                  | 880       |
|    iterations           | 11        |
|    time_elapsed         | 25        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.2904033 |
|    clip_fraction        | 0.413     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.682    |
|    explained_variance   | 0.192     |
|    learning_rate        | 0.00343   |
|    loss                 | 771       |
|    n_updates            | 100       |
|    policy_gradient_loss | 0.0318    |
|    value_loss           | 2.11e+03  |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.8       |
|    ep_rew_mean          | 164        |
| time/                   |            |
|    fps                  | 879        |
|    iterations           | 12         |
|    time_elapsed         | 27         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.11472415 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.538     |
|    explained_variance   | 0.414      |
|    learning_rate        | 0.00343    |
|    loss                 | 741        |
|    n_updates            | 110        |
|    policy_gradient_loss | 0.0221     |
|    value_loss           | 2.16e+03   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=252.54 +/- 145.27
Episode length: 17.70 +/- 4.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 253        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.33785617 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.398     |
|    explained_variance   | 0.478      |
|    learning_rate        | 0.00343    |
|    loss                 | 976        |
|    n_updates            | 120        |
|    policy_gradient_loss | 0.0248     |
|    value_loss           | 2.77e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | 208      |
| time/              |          |
|    fps             | 875      |
|    iterations      | 13       |
|    time_elapsed    | 30       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.5       |
|    ep_rew_mean          | 189        |
| time/                   |            |
|    fps                  | 874        |
|    iterations           | 14         |
|    time_elapsed         | 32         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.24683346 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.272     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.00343    |
|    loss                 | 1.03e+03   |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.0153     |
|    value_loss           | 3.35e+03   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=185.66 +/- 129.77
Episode length: 15.50 +/- 4.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.5       |
|    mean_reward          | 186        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.12042181 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.231     |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.00343    |
|    loss                 | 1.29e+03   |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.0387     |
|    value_loss           | 2.86e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 169      |
| time/              |          |
|    fps             | 871      |
|    iterations      | 15       |
|    time_elapsed    | 35       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.3        |
|    ep_rew_mean          | 172         |
| time/                   |             |
|    fps                  | 871         |
|    iterations           | 16          |
|    time_elapsed         | 37          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.095418796 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.299      |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.00343     |
|    loss                 | 1.03e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.00942     |
|    value_loss           | 2.94e+03    |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 14.3      |
|    ep_rew_mean          | 160       |
| time/                   |           |
|    fps                  | 871       |
|    iterations           | 17        |
|    time_elapsed         | 39        |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.3134548 |
|    clip_fraction        | 0.164     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.241    |
|    explained_variance   | 0.555     |
|    learning_rate        | 0.00343   |
|    loss                 | 1.5e+03   |
|    n_updates            | 160       |
|    policy_gradient_loss | 0.0152    |
|    value_loss           | 3.18e+03  |
---------------------------------------
Eval num_timesteps=35000, episode_reward=258.73 +/- 177.02
Episode length: 17.00 +/- 4.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 259         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.029881459 |
|    clip_fraction        | 0.0529      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0946     |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.00343     |
|    loss                 | 962         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00073    |
|    value_loss           | 2.76e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 868      |
|    iterations      | 18       |
|    time_elapsed    | 42       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.8        |
|    ep_rew_mean          | 205         |
| time/                   |             |
|    fps                  | 867         |
|    iterations           | 19          |
|    time_elapsed         | 44          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.055444285 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.061      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.00343     |
|    loss                 | 1.33e+03    |
|    n_updates            | 180         |
|    policy_gradient_loss | 0.00147     |
|    value_loss           | 3.16e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=252.95 +/- 155.14
Episode length: 17.00 +/- 4.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 253         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.033649605 |
|    clip_fraction        | 0.0535      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0671     |
|    explained_variance   | 0.55        |
|    learning_rate        | 0.00343     |
|    loss                 | 1.56e+03    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.000111   |
|    value_loss           | 3.13e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 204      |
| time/              |          |
|    fps             | 866      |
|    iterations      | 20       |
|    time_elapsed    | 47       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.6        |
|    ep_rew_mean          | 200         |
| time/                   |             |
|    fps                  | 865         |
|    iterations           | 21          |
|    time_elapsed         | 49          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.028096233 |
|    clip_fraction        | 0.0612      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0956     |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.00343     |
|    loss                 | 656         |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.000199    |
|    value_loss           | 2.95e+03    |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=180.58 +/- 166.09
Episode length: 15.50 +/- 4.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.036875043 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.599       |
|    learning_rate        | 0.00343     |
|    loss                 | 1.41e+03    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 3.31e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 201      |
| time/              |          |
|    fps             | 864      |
|    iterations      | 22       |
|    time_elapsed    | 52       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.1        |
|    ep_rew_mean          | 181         |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 23          |
|    time_elapsed         | 54          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.061738566 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.00343     |
|    loss                 | 1.06e+03    |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.000597    |
|    value_loss           | 3.16e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.5        |
|    ep_rew_mean          | 178         |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 24          |
|    time_elapsed         | 56          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.061971623 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.204      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.00343     |
|    loss                 | 1.73e+03    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00662    |
|    value_loss           | 3.27e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=268.13 +/- 220.90
Episode length: 17.60 +/- 6.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.6       |
|    mean_reward          | 268        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.29190433 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.216     |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.00343    |
|    loss                 | 958        |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.00709   |
|    value_loss           | 2.85e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 862      |
|    iterations      | 25       |
|    time_elapsed    | 59       |
|    total_timesteps | 51200    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 15.8      |
|    ep_rew_mean          | 210       |
| time/                   |           |
|    fps                  | 862       |
|    iterations           | 26        |
|    time_elapsed         | 61        |
|    total_timesteps      | 53248     |
| train/                  |           |
|    approx_kl            | 0.2137632 |
|    clip_fraction        | 0.187     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.251    |
|    explained_variance   | 0.603     |
|    learning_rate        | 0.00343   |
|    loss                 | 1.43e+03  |
|    n_updates            | 250       |
|    policy_gradient_loss | 0.014     |
|    value_loss           | 3.03e+03  |
---------------------------------------
Eval num_timesteps=55000, episode_reward=285.86 +/- 219.98
Episode length: 18.10 +/- 6.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.1       |
|    mean_reward          | 286        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.10187335 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.17      |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.00343    |
|    loss                 | 1.22e+03   |
|    n_updates            | 260        |
|    policy_gradient_loss | 0.00515    |
|    value_loss           | 3.3e+03    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 861      |
|    iterations      | 27       |
|    time_elapsed    | 64       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15.2       |
|    ep_rew_mean          | 173        |
| time/                   |            |
|    fps                  | 861        |
|    iterations           | 28         |
|    time_elapsed         | 66         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.16820925 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.175     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.00343    |
|    loss                 | 1.18e+03   |
|    n_updates            | 270        |
|    policy_gradient_loss | 0.00917    |
|    value_loss           | 3.11e+03   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.6       |
|    ep_rew_mean          | 198        |
| time/                   |            |
|    fps                  | 861        |
|    iterations           | 29         |
|    time_elapsed         | 68         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.19380118 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.222     |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.00343    |
|    loss                 | 1.95e+03   |
|    n_updates            | 280        |
|    policy_gradient_loss | 0.0121     |
|    value_loss           | 3.53e+03   |
----------------------------------------
Eval num_timesteps=60000, episode_reward=165.35 +/- 171.25
Episode length: 16.30 +/- 5.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 165        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.32220072 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.594      |
|    learning_rate        | 0.00343    |
|    loss                 | 1.43e+03   |
|    n_updates            | 290        |
|    policy_gradient_loss | 0.0244     |
|    value_loss           | 3.39e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 201      |
| time/              |          |
|    fps             | 860      |
|    iterations      | 30       |
|    time_elapsed    | 71       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:20:34,118] Trial 8 finished with value: 181.1685273 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0034347782711234296, 'gamma': 0.9820064265583961, 'gae_lambda': 0.8317361679734313}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.91 +/- 0.10
Episode length: 21.90 +/- 9.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.9     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.5     |
|    ep_rew_mean     | -91.1    |
| time/              |          |
|    fps             | 1165     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=177.79 +/- 146.57
Episode length: 16.50 +/- 5.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.033655368 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 3.89e-05    |
|    learning_rate        | 0.00107     |
|    loss                 | 271         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0103      |
|    value_loss           | 626         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=195.24 +/- 142.12
Episode length: 18.00 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | -83.2    |
| time/              |          |
|    fps             | 986      |
|    iterations      | 2        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=230.18 +/- 135.59
Episode length: 15.70 +/- 3.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | 230        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.03125598 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2         |
|    explained_variance   | 0.365      |
|    learning_rate        | 0.00107    |
|    loss                 | 158        |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00305   |
|    value_loss           | 496        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.4     |
|    ep_rew_mean     | -46.3    |
| time/              |          |
|    fps             | 940      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=263.58 +/- 160.01
Episode length: 16.40 +/- 4.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | 264         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.028176121 |
|    clip_fraction        | 0.393       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.00107     |
|    loss                 | 190         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 475         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=292.85 +/- 137.75
Episode length: 18.20 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 293      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -33.8    |
| time/              |          |
|    fps             | 913      |
|    iterations      | 4        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=252.37 +/- 203.49
Episode length: 16.40 +/- 6.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | 252         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.029977035 |
|    clip_fraction        | 0.394       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.00107     |
|    loss                 | 134         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 528         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=229.63 +/- 177.32
Episode length: 16.30 +/- 5.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 900      |
|    iterations      | 5        |
|    time_elapsed    | 45       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=229.50 +/- 125.57
Episode length: 15.50 +/- 3.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | 230         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.031564727 |
|    clip_fraction        | 0.384       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.00107     |
|    loss                 | 234         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 574         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 11.6     |
| time/              |          |
|    fps             | 892      |
|    iterations      | 6        |
|    time_elapsed    | 55       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=396.30 +/- 201.23
Episode length: 20.20 +/- 6.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.2        |
|    mean_reward          | 396         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.032882586 |
|    clip_fraction        | 0.399       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.00107     |
|    loss                 | 224         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00955    |
|    value_loss           | 664         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=255.99 +/- 179.98
Episode length: 16.50 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | 256      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 44       |
| time/              |          |
|    fps             | 883      |
|    iterations      | 7        |
|    time_elapsed    | 64       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=323.76 +/- 158.81
Episode length: 18.50 +/- 4.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.5       |
|    mean_reward          | 324        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.04083525 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.00107    |
|    loss                 | 223        |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.00681   |
|    value_loss           | 807        |
----------------------------------------
Eval num_timesteps=65000, episode_reward=270.68 +/- 235.12
Episode length: 16.90 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 44.4     |
| time/              |          |
|    fps             | 878      |
|    iterations      | 8        |
|    time_elapsed    | 74       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:21:51,657] Trial 9 finished with value: 297.06456769999994 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.0010674517507648226, 'gamma': 0.9395139943404727, 'gae_lambda': 0.8823400596614882}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.98 +/- 0.00
Episode length: 28.80 +/- 10.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.8     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.7     |
|    ep_rew_mean     | -89.9    |
| time/              |          |
|    fps             | 1157     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=279.11 +/- 217.79
Episode length: 16.90 +/- 5.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014749777 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 5.6e-06     |
|    learning_rate        | 1.1e-05     |
|    loss                 | 329         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 902         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=146.11 +/- 94.81
Episode length: 12.80 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.8     |
|    mean_reward     | 146      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.3     |
|    ep_rew_mean     | -73.7    |
| time/              |          |
|    fps             | 866      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=157.57 +/- 133.41
Episode length: 14.50 +/- 3.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 158         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011280929 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.00615     |
|    learning_rate        | 1.1e-05     |
|    loss                 | 591         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 887         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | -65.3    |
| time/              |          |
|    fps             | 803      |
|    iterations      | 3        |
|    time_elapsed    | 30       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=327.73 +/- 204.27
Episode length: 18.70 +/- 6.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.7        |
|    mean_reward          | 328         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012059998 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.0167      |
|    learning_rate        | 1.1e-05     |
|    loss                 | 330         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 936         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=253.08 +/- 226.86
Episode length: 16.70 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | 253      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -54.3    |
| time/              |          |
|    fps             | 771      |
|    iterations      | 4        |
|    time_elapsed    | 42       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=241.53 +/- 160.44
Episode length: 16.70 +/- 4.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.7       |
|    mean_reward          | 242        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.01220781 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.0368     |
|    learning_rate        | 1.1e-05    |
|    loss                 | 426        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0197    |
|    value_loss           | 962        |
----------------------------------------
Eval num_timesteps=40000, episode_reward=254.68 +/- 154.79
Episode length: 16.30 +/- 3.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | 255      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -34.2    |
| time/              |          |
|    fps             | 755      |
|    iterations      | 5        |
|    time_elapsed    | 54       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=181.23 +/- 118.96
Episode length: 15.40 +/- 3.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.014778311 |
|    clip_fraction        | 0.264       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.0756      |
|    learning_rate        | 1.1e-05     |
|    loss                 | 643         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 1.06e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -2.79    |
| time/              |          |
|    fps             | 745      |
|    iterations      | 6        |
|    time_elapsed    | 65       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=223.23 +/- 153.17
Episode length: 15.80 +/- 4.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | 223         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.012196137 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.157       |
|    learning_rate        | 1.1e-05     |
|    loss                 | 636         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 1.19e+03    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=217.95 +/- 153.88
Episode length: 15.60 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | -5.05    |
| time/              |          |
|    fps             | 739      |
|    iterations      | 7        |
|    time_elapsed    | 77       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=151.52 +/- 148.72
Episode length: 14.50 +/- 3.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 152         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.014089235 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.24        |
|    learning_rate        | 1.1e-05     |
|    loss                 | 661         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 1.24e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=284.13 +/- 184.81
Episode length: 17.90 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 732      |
|    iterations      | 8        |
|    time_elapsed    | 89       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:23:26,240] Trial 10 finished with value: 244.64088120000002 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 1.0957182476731935e-05, 'gamma': 0.9074134521104158, 'gae_lambda': 0.9345452803229009}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=69.53 +/- 153.66
Episode length: 17.60 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | 69.5     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.8     |
|    ep_rew_mean     | -90.4    |
| time/              |          |
|    fps             | 1175     |
|    iterations      | 1        |
|    time_elapsed    | 6        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=329.60 +/- 202.29
Episode length: 18.90 +/- 5.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.9        |
|    mean_reward          | 330         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.028464751 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 8.94e-05    |
|    learning_rate        | 0.000418    |
|    loss                 | 272         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 622         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=198.50 +/- 161.11
Episode length: 16.00 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.4     |
|    ep_rew_mean     | -56.9    |
| time/              |          |
|    fps             | 893      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=27.01 +/- 65.27
Episode length: 15.30 +/- 5.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | 27          |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.032090902 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.247       |
|    learning_rate        | 0.000418    |
|    loss                 | 327         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 553         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -33.3    |
| time/              |          |
|    fps             | 822      |
|    iterations      | 3        |
|    time_elapsed    | 29       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=53.45 +/- 55.94
Episode length: 16.70 +/- 3.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | 53.4        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.027928032 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.000418    |
|    loss                 | 206         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00932    |
|    value_loss           | 563         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=64.38 +/- 88.98
Episode length: 20.20 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.2     |
|    mean_reward     | 64.4     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -22.3    |
| time/              |          |
|    fps             | 785      |
|    iterations      | 4        |
|    time_elapsed    | 41       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=221.61 +/- 142.46
Episode length: 15.40 +/- 3.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.025305882 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.000418    |
|    loss                 | 214         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 685         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=240.24 +/- 163.94
Episode length: 16.70 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 769      |
|    iterations      | 5        |
|    time_elapsed    | 53       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=310.32 +/- 141.38
Episode length: 18.60 +/- 4.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.6        |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.035153314 |
|    clip_fraction        | 0.37        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000418    |
|    loss                 | 190         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 738         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 45.6     |
| time/              |          |
|    fps             | 755      |
|    iterations      | 6        |
|    time_elapsed    | 65       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=233.58 +/- 155.89
Episode length: 16.70 +/- 4.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | 234         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.031188786 |
|    clip_fraction        | 0.38        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.000418    |
|    loss                 | 268         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 892         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=177.59 +/- 168.53
Episode length: 14.90 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.7     |
|    ep_rew_mean     | 58.9     |
| time/              |          |
|    fps             | 740      |
|    iterations      | 7        |
|    time_elapsed    | 77       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=212.47 +/- 145.41
Episode length: 15.70 +/- 3.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | 212        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.02866666 |
|    clip_fraction        | 0.348      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.329      |
|    learning_rate        | 0.000418   |
|    loss                 | 614        |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0063    |
|    value_loss           | 1.17e+03   |
----------------------------------------
Eval num_timesteps=65000, episode_reward=283.93 +/- 152.84
Episode length: 17.40 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 733      |
|    iterations      | 8        |
|    time_elapsed    | 89       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:25:00,520] Trial 11 finished with value: 215.043689 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0004184166997959868, 'gamma': 0.9360762852299169, 'gae_lambda': 0.877546264027335}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.98 +/- 0.00
Episode length: 18.70 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.7     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | -87.4    |
| time/              |          |
|    fps             | 1136     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-44.60 +/- 52.19
Episode length: 37.60 +/- 11.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.6        |
|    mean_reward          | -44.6       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.026551824 |
|    clip_fraction        | 0.342       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | -1.87e-05   |
|    learning_rate        | 0.000506    |
|    loss                 | 134         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.00637     |
|    value_loss           | 561         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-68.12 +/- 41.55
Episode length: 45.20 +/- 24.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.2     |
|    mean_reward     | -68.1    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.9     |
|    ep_rew_mean     | -75      |
| time/              |          |
|    fps             | 850      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=85.04 +/- 127.16
Episode length: 14.10 +/- 3.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.1        |
|    mean_reward          | 85          |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.031258967 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.000506    |
|    loss                 | 242         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00012    |
|    value_loss           | 422         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -54.6    |
| time/              |          |
|    fps             | 803      |
|    iterations      | 3        |
|    time_elapsed    | 30       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=146.62 +/- 120.48
Episode length: 14.00 +/- 3.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14          |
|    mean_reward          | 147         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.030835997 |
|    clip_fraction        | 0.389       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.464       |
|    learning_rate        | 0.000506    |
|    loss                 | 139         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00377    |
|    value_loss           | 422         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=278.16 +/- 185.88
Episode length: 18.00 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | -3.43    |
| time/              |          |
|    fps             | 772      |
|    iterations      | 4        |
|    time_elapsed    | 42       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=155.52 +/- 128.18
Episode length: 13.90 +/- 3.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 13.9       |
|    mean_reward          | 156        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.03551527 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.8       |
|    explained_variance   | 0.455      |
|    learning_rate        | 0.000506   |
|    loss                 | 164        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0161    |
|    value_loss           | 518        |
----------------------------------------
Eval num_timesteps=40000, episode_reward=271.46 +/- 225.26
Episode length: 17.80 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 24.8     |
| time/              |          |
|    fps             | 759      |
|    iterations      | 5        |
|    time_elapsed    | 53       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=168.88 +/- 148.76
Episode length: 14.50 +/- 3.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.038649254 |
|    clip_fraction        | 0.458       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.000506    |
|    loss                 | 147         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 705         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 63       |
| time/              |          |
|    fps             | 749      |
|    iterations      | 6        |
|    time_elapsed    | 65       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=211.74 +/- 157.44
Episode length: 15.50 +/- 3.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.5       |
|    mean_reward          | 212        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.03498683 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.399      |
|    learning_rate        | 0.000506   |
|    loss                 | 164        |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0114    |
|    value_loss           | 906        |
----------------------------------------
Eval num_timesteps=55000, episode_reward=239.56 +/- 130.56
Episode length: 16.30 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 103      |
| time/              |          |
|    fps             | 739      |
|    iterations      | 7        |
|    time_elapsed    | 77       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=171.98 +/- 138.93
Episode length: 14.50 +/- 3.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.035790935 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.000506    |
|    loss                 | 415         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00957    |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=220.34 +/- 148.41
Episode length: 16.10 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | 220      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 111      |
| time/              |          |
|    fps             | 732      |
|    iterations      | 8        |
|    time_elapsed    | 89       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:26:34,649] Trial 12 finished with value: 244.43934629999998 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0005061312585823765, 'gamma': 0.9566774020817209, 'gae_lambda': 0.8546254557153423}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.95 +/- 0.03
Episode length: 26.30 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.3     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | -81.8    |
| time/              |          |
|    fps             | 1116     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=236.87 +/- 140.98
Episode length: 16.80 +/- 4.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011758693 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000108   |
|    learning_rate        | 0.000115    |
|    loss                 | 226         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00952    |
|    value_loss           | 816         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=187.64 +/- 141.65
Episode length: 14.80 +/- 3.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.8     |
|    ep_rew_mean     | -71.2    |
| time/              |          |
|    fps             | 960      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=140.43 +/- 99.52
Episode length: 13.80 +/- 2.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.8        |
|    mean_reward          | 140         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012630368 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.227       |
|    learning_rate        | 0.000115    |
|    loss                 | 315         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 710         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -67.1    |
| time/              |          |
|    fps             | 925      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=244.64 +/- 159.65
Episode length: 16.40 +/- 4.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | 245         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010659263 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.000115    |
|    loss                 | 380         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 627         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=207.29 +/- 150.16
Episode length: 15.10 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -51.7    |
| time/              |          |
|    fps             | 901      |
|    iterations      | 4        |
|    time_elapsed    | 36       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=304.40 +/- 262.22
Episode length: 17.30 +/- 6.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 304         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012441173 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.000115    |
|    loss                 | 293         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 705         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=263.49 +/- 139.91
Episode length: 16.80 +/- 3.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | -34.9    |
| time/              |          |
|    fps             | 888      |
|    iterations      | 5        |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=213.90 +/- 219.90
Episode length: 16.00 +/- 6.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.013614298 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.000115    |
|    loss                 | 423         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 783         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | -23.9    |
| time/              |          |
|    fps             | 883      |
|    iterations      | 6        |
|    time_elapsed    | 55       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=212.05 +/- 218.61
Episode length: 16.00 +/- 6.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 212         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015103927 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.000115    |
|    loss                 | 443         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 852         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=202.87 +/- 120.34
Episode length: 15.60 +/- 3.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 203      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | -0.251   |
| time/              |          |
|    fps             | 876      |
|    iterations      | 7        |
|    time_elapsed    | 65       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=177.17 +/- 134.39
Episode length: 14.20 +/- 3.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.2        |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.016570365 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.32        |
|    learning_rate        | 0.000115    |
|    loss                 | 734         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=258.72 +/- 188.79
Episode length: 17.90 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 873      |
|    iterations      | 8        |
|    time_elapsed    | 74       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:27:52,526] Trial 13 finished with value: 217.53432149999998 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.00011506498163653022, 'gamma': 0.9285447012507858, 'gae_lambda': 0.9186079687300844}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=26.63 +/- 46.21
Episode length: 16.60 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 26.6     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.2     |
|    ep_rew_mean     | -89.8    |
| time/              |          |
|    fps             | 1167     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-115.96 +/- 0.03
Episode length: 59.90 +/- 47.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59.9        |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.035693064 |
|    clip_fraction        | 0.402       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | -5.65e-05   |
|    learning_rate        | 0.000946    |
|    loss                 | 296         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0157      |
|    value_loss           | 539         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-115.92 +/- 0.08
Episode length: 33.00 +/- 30.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | -90.3    |
| time/              |          |
|    fps             | 856      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=56.22 +/- 78.47
Episode length: 26.90 +/- 24.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.9        |
|    mean_reward          | 56.2        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.056939937 |
|    clip_fraction        | 0.433       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.000946    |
|    loss                 | 174         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.0212      |
|    value_loss           | 398         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.5     |
|    ep_rew_mean     | -44.5    |
| time/              |          |
|    fps             | 793      |
|    iterations      | 3        |
|    time_elapsed    | 30       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=158.72 +/- 138.98
Episode length: 14.90 +/- 5.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.9       |
|    mean_reward          | 159        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.07311547 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.86      |
|    explained_variance   | 0.301      |
|    learning_rate        | 0.000946   |
|    loss                 | 182        |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0149     |
|    value_loss           | 397        |
----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=183.29 +/- 164.53
Episode length: 14.50 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -17      |
| time/              |          |
|    fps             | 766      |
|    iterations      | 4        |
|    time_elapsed    | 42       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=183.89 +/- 135.80
Episode length: 15.10 +/- 4.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.059514314 |
|    clip_fraction        | 0.468       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.000946    |
|    loss                 | 135         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.000421    |
|    value_loss           | 448         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=214.27 +/- 148.86
Episode length: 15.70 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 5.84     |
| time/              |          |
|    fps             | 749      |
|    iterations      | 5        |
|    time_elapsed    | 54       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=202.36 +/- 135.27
Episode length: 15.70 +/- 4.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | 202        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.05412797 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | 0.227      |
|    learning_rate        | 0.000946   |
|    loss                 | 271        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0017    |
|    value_loss           | 542        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 23.8     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 6        |
|    time_elapsed    | 66       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=341.91 +/- 264.75
Episode length: 19.20 +/- 7.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.2       |
|    mean_reward          | 342        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.05447713 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.47      |
|    explained_variance   | 0.286      |
|    learning_rate        | 0.000946   |
|    loss                 | 148        |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.00308    |
|    value_loss           | 599        |
----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=205.68 +/- 122.56
Episode length: 15.60 +/- 3.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 206      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 54       |
| time/              |          |
|    fps             | 735      |
|    iterations      | 7        |
|    time_elapsed    | 78       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=126.81 +/- 118.02
Episode length: 13.50 +/- 3.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.5        |
|    mean_reward          | 127         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.058753155 |
|    clip_fraction        | 0.499       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.286       |
|    learning_rate        | 0.000946    |
|    loss                 | 114         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00897    |
|    value_loss           | 742         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=205.65 +/- 156.57
Episode length: 15.00 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 206      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | 60       |
| time/              |          |
|    fps             | 732      |
|    iterations      | 8        |
|    time_elapsed    | 89       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:29:26,954] Trial 14 finished with value: 168.03204320000003 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0009457495940934272, 'gamma': 0.9022615055685319, 'gae_lambda': 0.8668609419884623}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.22 +/- 2.17
Episode length: 24.40 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.6     |
|    ep_rew_mean     | -92.7    |
| time/              |          |
|    fps             | 1152     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-82.89 +/- 39.29
Episode length: 22.20 +/- 8.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 22.2      |
|    mean_reward          | -82.9     |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 2.2573285 |
|    clip_fraction        | 0.826     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.3      |
|    explained_variance   | 9.71e-05  |
|    learning_rate        | 0.00663   |
|    loss                 | 244       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.246     |
|    value_loss           | 538       |
---------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-90.60 +/- 31.56
Episode length: 20.40 +/- 9.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.4     |
|    mean_reward     | -90.6    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -21.1    |
| time/              |          |
|    fps             | 968      |
|    iterations      | 2        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=203.23 +/- 163.96
Episode length: 16.70 +/- 5.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.7      |
|    mean_reward          | 203       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.9498192 |
|    clip_fraction        | 0.749     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.21     |
|    explained_variance   | 0.0137    |
|    learning_rate        | 0.00663   |
|    loss                 | 234       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.109     |
|    value_loss           | 646       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.7     |
|    ep_rew_mean     | 76.7     |
| time/              |          |
|    fps             | 925      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=208.98 +/- 140.14
Episode length: 15.40 +/- 3.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 15.4      |
|    mean_reward          | 209       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.9436046 |
|    clip_fraction        | 0.588     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.288    |
|    explained_variance   | 0.0803    |
|    learning_rate        | 0.00663   |
|    loss                 | 510       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.058     |
|    value_loss           | 1.08e+03  |
---------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=123.53 +/- 123.09
Episode length: 13.60 +/- 3.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.6     |
|    mean_reward     | 124      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 904      |
|    iterations      | 4        |
|    time_elapsed    | 36       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=180.51 +/- 146.30
Episode length: 14.60 +/- 4.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 14.6      |
|    mean_reward          | 181       |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 3.1497877 |
|    clip_fraction        | 0.338     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0153   |
|    explained_variance   | 0.137     |
|    learning_rate        | 0.00663   |
|    loss                 | 717       |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.0205    |
|    value_loss           | 2.05e+03  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=248.92 +/- 221.66
Episode length: 16.50 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | 249      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | 212      |
| time/              |          |
|    fps             | 890      |
|    iterations      | 5        |
|    time_elapsed    | 45       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=204.68 +/- 167.66
Episode length: 15.00 +/- 3.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | 205          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0073425374 |
|    clip_fraction        | 0.00139      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000405    |
|    explained_variance   | 0.406        |
|    learning_rate        | 0.00663      |
|    loss                 | 755          |
|    n_updates            | 50           |
|    policy_gradient_loss | 5.38e-06     |
|    value_loss           | 2.38e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 228      |
| time/              |          |
|    fps             | 882      |
|    iterations      | 6        |
|    time_elapsed    | 55       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=246.89 +/- 149.19
Episode length: 16.30 +/- 4.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.002762867 |
|    clip_fraction        | 0.000598    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000508   |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.00663     |
|    loss                 | 1.24e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | 3.78e-05    |
|    value_loss           | 2.76e+03    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=422.25 +/- 245.65
Episode length: 20.70 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.7     |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 264      |
| time/              |          |
|    fps             | 874      |
|    iterations      | 7        |
|    time_elapsed    | 65       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=260.20 +/- 198.87
Episode length: 16.90 +/- 5.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | 260         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.000348783 |
|    clip_fraction        | 0.000183    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000379   |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.00663     |
|    loss                 | 1.67e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -3.94e-05   |
|    value_loss           | 3e+03       |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=196.36 +/- 124.96
Episode length: 15.20 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 240      |
| time/              |          |
|    fps             | 868      |
|    iterations      | 8        |
|    time_elapsed    | 75       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:30:45,162] Trial 15 finished with value: 264.58142399999997 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 0.006627175001644583, 'gamma': 0.948274557432727, 'gae_lambda': 0.8035817859183467}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.32 +/- 1.94
Episode length: 29.80 +/- 29.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 29.8     |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.1     |
|    ep_rew_mean     | -83.6    |
| time/              |          |
|    fps             | 1144     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=187.19 +/- 161.31
Episode length: 18.20 +/- 5.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.2        |
|    mean_reward          | 187         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014872022 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | -0.000304   |
|    learning_rate        | 0.00017     |
|    loss                 | 307         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 576         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=109.26 +/- 134.65
Episode length: 15.20 +/- 3.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 109      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.7     |
|    ep_rew_mean     | -72.9    |
| time/              |          |
|    fps             | 875      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=250.20 +/- 107.35
Episode length: 16.10 +/- 2.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | 250         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.016269073 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.382       |
|    learning_rate        | 0.00017     |
|    loss                 | 149         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 421         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.8     |
|    ep_rew_mean     | -70.3    |
| time/              |          |
|    fps             | 811      |
|    iterations      | 3        |
|    time_elapsed    | 30       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=260.98 +/- 134.65
Episode length: 17.30 +/- 4.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.016481023 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.00017     |
|    loss                 | 278         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 468         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=237.72 +/- 218.10
Episode length: 16.30 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | 238      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -46.3    |
| time/              |          |
|    fps             | 782      |
|    iterations      | 4        |
|    time_elapsed    | 41       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=228.60 +/- 140.41
Episode length: 16.20 +/- 4.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 229         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.018430736 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.495       |
|    learning_rate        | 0.00017     |
|    loss                 | 246         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 455         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=173.52 +/- 151.27
Episode length: 15.10 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | -37.2    |
| time/              |          |
|    fps             | 761      |
|    iterations      | 5        |
|    time_elapsed    | 53       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=279.17 +/- 144.95
Episode length: 17.70 +/- 4.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.020195162 |
|    clip_fraction        | 0.332       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.00017     |
|    loss                 | 174         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 557         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | -14.7    |
| time/              |          |
|    fps             | 753      |
|    iterations      | 6        |
|    time_elapsed    | 65       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=273.59 +/- 151.57
Episode length: 17.00 +/- 3.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.019298164 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.00017     |
|    loss                 | 148         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 642         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=161.65 +/- 155.73
Episode length: 14.60 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 23.6     |
| time/              |          |
|    fps             | 743      |
|    iterations      | 7        |
|    time_elapsed    | 77       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=197.62 +/- 127.81
Episode length: 14.70 +/- 3.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.7        |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.025239764 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.00017     |
|    loss                 | 328         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 731         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=281.84 +/- 160.40
Episode length: 17.40 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | 282      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 46       |
| time/              |          |
|    fps             | 729      |
|    iterations      | 8        |
|    time_elapsed    | 89       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:32:20,104] Trial 16 finished with value: 133.572113 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 0.0001703745199268485, 'gamma': 0.9168461512851319, 'gae_lambda': 0.8364793317093434}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.94 +/- 0.02
Episode length: 22.30 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.9     |
|    ep_rew_mean     | -90.3    |
| time/              |          |
|    fps             | 1159     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=66.39 +/- 107.14
Episode length: 15.40 +/- 4.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 66.4        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011897205 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000146   |
|    learning_rate        | 3.59e-05    |
|    loss                 | 547         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 944         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=81.21 +/- 74.45
Episode length: 17.70 +/- 3.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | 81.2     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27       |
|    ep_rew_mean     | -83.7    |
| time/              |          |
|    fps             | 979      |
|    iterations      | 2        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=133.58 +/- 128.06
Episode length: 14.20 +/- 3.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.2        |
|    mean_reward          | 134         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010578164 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.0162      |
|    learning_rate        | 3.59e-05    |
|    loss                 | 354         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 796         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.3     |
|    ep_rew_mean     | -71.3    |
| time/              |          |
|    fps             | 934      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=236.52 +/- 157.30
Episode length: 16.40 +/- 3.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.4       |
|    mean_reward          | 237        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01129281 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.02      |
|    explained_variance   | 0.0808     |
|    learning_rate        | 3.59e-05   |
|    loss                 | 404        |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0186    |
|    value_loss           | 760        |
----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=187.45 +/- 138.10
Episode length: 15.00 +/- 3.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -51.3    |
| time/              |          |
|    fps             | 912      |
|    iterations      | 4        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=279.72 +/- 170.95
Episode length: 17.60 +/- 4.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.6        |
|    mean_reward          | 280         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012955129 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.202       |
|    learning_rate        | 3.59e-05    |
|    loss                 | 355         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 750         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=225.63 +/- 228.07
Episode length: 15.60 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 226      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -40.9    |
| time/              |          |
|    fps             | 898      |
|    iterations      | 5        |
|    time_elapsed    | 45       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=279.91 +/- 217.39
Episode length: 17.40 +/- 6.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.4        |
|    mean_reward          | 280         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.013977736 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.289       |
|    learning_rate        | 3.59e-05    |
|    loss                 | 412         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 834         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 890      |
|    iterations      | 6        |
|    time_elapsed    | 55       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=175.06 +/- 193.24
Episode length: 14.50 +/- 5.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015964817 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.329       |
|    learning_rate        | 3.59e-05    |
|    loss                 | 507         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0272     |
|    value_loss           | 942         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=229.57 +/- 222.72
Episode length: 16.20 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 8.22     |
| time/              |          |
|    fps             | 883      |
|    iterations      | 7        |
|    time_elapsed    | 64       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=135.97 +/- 105.77
Episode length: 13.50 +/- 2.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.5        |
|    mean_reward          | 136         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.014185341 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.367       |
|    learning_rate        | 3.59e-05    |
|    loss                 | 442         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 1.14e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=294.54 +/- 137.70
Episode length: 17.90 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | 295      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 36.9     |
| time/              |          |
|    fps             | 877      |
|    iterations      | 8        |
|    time_elapsed    | 74       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:33:37,575] Trial 17 finished with value: 164.4545747 and parameters: {'n_steps': 8192, 'batch_size': 64, 'learning_rate': 3.593415303612652e-05, 'gamma': 0.9390618089390655, 'gae_lambda': 0.9029557469441736}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -94.2    |
| time/              |          |
|    fps             | 1177     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.2        |
|    ep_rew_mean          | -79.2       |
| time/                   |             |
|    fps                  | 1013        |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.032384913 |
|    clip_fraction        | 0.569       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | -5.96e-05   |
|    learning_rate        | 0.0008      |
|    loss                 | 424         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.0775      |
|    value_loss           | 956         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-115.96 +/- 0.02
Episode length: 26.60 +/- 9.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.6        |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.031319104 |
|    clip_fraction        | 0.361       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.0008      |
|    loss                 | 383         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00192     |
|    value_loss           | 740         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | -71.6    |
| time/              |          |
|    fps             | 910      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.8        |
|    ep_rew_mean          | -63.6       |
| time/                   |             |
|    fps                  | 905         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.020852277 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.233       |
|    learning_rate        | 0.0008      |
|    loss                 | 251         |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.00457     |
|    value_loss           | 537         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=-115.94 +/- 0.03
Episode length: 23.40 +/- 7.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.4        |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.025491718 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.0008      |
|    loss                 | 230         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 617         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -50      |
| time/              |          |
|    fps             | 880      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.2        |
|    ep_rew_mean          | -39.2       |
| time/                   |             |
|    fps                  | 876         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.039149977 |
|    clip_fraction        | 0.376       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.176       |
|    learning_rate        | 0.0008      |
|    loss                 | 234         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 793         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.2        |
|    ep_rew_mean          | -44.2       |
| time/                   |             |
|    fps                  | 874         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.024022385 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.0008      |
|    loss                 | 346         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00805    |
|    value_loss           | 806         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-39.47 +/- 41.39
Episode length: 17.70 +/- 6.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | -39.5       |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.022503195 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.189       |
|    learning_rate        | 0.0008      |
|    loss                 | 235         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 906         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | -34.2    |
| time/              |          |
|    fps             | 866      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.7       |
|    ep_rew_mean          | -1.19      |
| time/                   |            |
|    fps                  | 868        |
|    iterations           | 9          |
|    time_elapsed         | 21         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.02611503 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.66      |
|    explained_variance   | 0.104      |
|    learning_rate        | 0.0008     |
|    loss                 | 288        |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.00286   |
|    value_loss           | 689        |
----------------------------------------
Eval num_timesteps=20000, episode_reward=197.71 +/- 244.03
Episode length: 16.60 +/- 7.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.027356701 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.175       |
|    learning_rate        | 0.0008      |
|    loss                 | 323         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 1.15e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.2       |
|    ep_rew_mean          | 19.9       |
| time/                   |            |
|    fps                  | 863        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.04209934 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.48      |
|    explained_variance   | 0.111      |
|    learning_rate        | 0.0008     |
|    loss                 | 470        |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.00653    |
|    value_loss           | 1.58e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 57.7        |
| time/                   |             |
|    fps                  | 865         |
|    iterations           | 12          |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.035535224 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.136       |
|    learning_rate        | 0.0008      |
|    loss                 | 465         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=236.03 +/- 225.37
Episode length: 16.10 +/- 6.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | 236         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.030284775 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.131       |
|    learning_rate        | 0.0008      |
|    loss                 | 658         |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 1.59e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.7     |
|    ep_rew_mean     | 42.9     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 13       |
|    time_elapsed    | 30       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 79.2       |
| time/                   |            |
|    fps                  | 864        |
|    iterations           | 14         |
|    time_elapsed         | 33         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.03274472 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.172      |
|    learning_rate        | 0.0008     |
|    loss                 | 461        |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.00248   |
|    value_loss           | 1.37e+03   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=195.26 +/- 139.09
Episode length: 14.90 +/- 3.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.023293136 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.223       |
|    learning_rate        | 0.0008      |
|    loss                 | 540         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00795    |
|    value_loss           | 1.96e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18       |
|    ep_rew_mean     | 81.5     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 15       |
|    time_elapsed    | 35       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.6       |
|    ep_rew_mean          | 66.5       |
| time/                   |            |
|    fps                  | 862        |
|    iterations           | 16         |
|    time_elapsed         | 37         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.03672593 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.0688     |
|    learning_rate        | 0.0008     |
|    loss                 | 805        |
|    n_updates            | 150        |
|    policy_gradient_loss | 0.00367    |
|    value_loss           | 2.38e+03   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 17.5      |
|    ep_rew_mean          | 68        |
| time/                   |           |
|    fps                  | 861       |
|    iterations           | 17        |
|    time_elapsed         | 40        |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.0443841 |
|    clip_fraction        | 0.275     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.15     |
|    explained_variance   | 0.224     |
|    learning_rate        | 0.0008    |
|    loss                 | 469       |
|    n_updates            | 160       |
|    policy_gradient_loss | 0.000664  |
|    value_loss           | 1.86e+03  |
---------------------------------------
Eval num_timesteps=35000, episode_reward=296.38 +/- 234.49
Episode length: 17.50 +/- 6.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | 296         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.029757654 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.142       |
|    learning_rate        | 0.0008      |
|    loss                 | 1.03e+03    |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00755    |
|    value_loss           | 2.02e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | 84.1     |
| time/              |          |
|    fps             | 858      |
|    iterations      | 18       |
|    time_elapsed    | 42       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.8       |
|    ep_rew_mean          | 128        |
| time/                   |            |
|    fps                  | 858        |
|    iterations           | 19         |
|    time_elapsed         | 45         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.04305669 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.178      |
|    learning_rate        | 0.0008     |
|    loss                 | 643        |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.00196   |
|    value_loss           | 2.38e+03   |
----------------------------------------
Eval num_timesteps=40000, episode_reward=197.70 +/- 145.51
Episode length: 14.90 +/- 4.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.045903184 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.179       |
|    learning_rate        | 0.0008      |
|    loss                 | 747         |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00371    |
|    value_loss           | 2.78e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 856      |
|    iterations      | 20       |
|    time_elapsed    | 47       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.9        |
|    ep_rew_mean          | 173         |
| time/                   |             |
|    fps                  | 857         |
|    iterations           | 21          |
|    time_elapsed         | 50          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.054708015 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.686      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0008      |
|    loss                 | 887         |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.00114     |
|    value_loss           | 3e+03       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=218.18 +/- 118.68
Episode length: 15.70 +/- 3.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.033760726 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.568      |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0008      |
|    loss                 | 913         |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00261    |
|    value_loss           | 3.73e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 207      |
| time/              |          |
|    fps             | 856      |
|    iterations      | 22       |
|    time_elapsed    | 52       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.1        |
|    ep_rew_mean          | 163         |
| time/                   |             |
|    fps                  | 857         |
|    iterations           | 23          |
|    time_elapsed         | 54          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.024156332 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.443      |
|    explained_variance   | 0.25        |
|    learning_rate        | 0.0008      |
|    loss                 | 1.48e+03    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 4.71e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.7        |
|    ep_rew_mean          | 183         |
| time/                   |             |
|    fps                  | 856         |
|    iterations           | 24          |
|    time_elapsed         | 57          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.011404596 |
|    clip_fraction        | 0.0932      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.42       |
|    explained_variance   | 0.243       |
|    learning_rate        | 0.0008      |
|    loss                 | 1.51e+03    |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.00158     |
|    value_loss           | 4.17e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=301.54 +/- 189.36
Episode length: 18.20 +/- 5.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.2        |
|    mean_reward          | 302         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010167696 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.397      |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.0008      |
|    loss                 | 1.46e+03    |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.00232     |
|    value_loss           | 3.96e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 195      |
| time/              |          |
|    fps             | 854      |
|    iterations      | 25       |
|    time_elapsed    | 59       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15.5       |
|    ep_rew_mean          | 196        |
| time/                   |            |
|    fps                  | 855        |
|    iterations           | 26         |
|    time_elapsed         | 62         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.02834282 |
|    clip_fraction        | 0.0863     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.282     |
|    explained_variance   | 0.407      |
|    learning_rate        | 0.0008     |
|    loss                 | 1.17e+03   |
|    n_updates            | 250        |
|    policy_gradient_loss | 0.00177    |
|    value_loss           | 3.87e+03   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=227.12 +/- 211.13
Episode length: 16.20 +/- 6.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 227         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.008044381 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.246      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0008      |
|    loss                 | 1.21e+03    |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.00232     |
|    value_loss           | 4.36e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 205      |
| time/              |          |
|    fps             | 853      |
|    iterations      | 27       |
|    time_elapsed    | 64       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.7        |
|    ep_rew_mean          | 198         |
| time/                   |             |
|    fps                  | 854         |
|    iterations           | 28          |
|    time_elapsed         | 67          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.009485714 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.264      |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0008      |
|    loss                 | 1.15e+03    |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.00113     |
|    value_loss           | 4.53e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | 199          |
| time/                   |              |
|    fps                  | 855          |
|    iterations           | 29           |
|    time_elapsed         | 69           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0074570305 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.225       |
|    explained_variance   | 0.398        |
|    learning_rate        | 0.0008       |
|    loss                 | 1.14e+03     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00274     |
|    value_loss           | 4.28e+03     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=244.42 +/- 244.55
Episode length: 16.40 +/- 6.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 244          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0075578145 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.174       |
|    explained_variance   | 0.418        |
|    learning_rate        | 0.0008       |
|    loss                 | 1.47e+03     |
|    n_updates            | 290          |
|    policy_gradient_loss | -1.97e-05    |
|    value_loss           | 4.44e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 220      |
| time/              |          |
|    fps             | 853      |
|    iterations      | 30       |
|    time_elapsed    | 71       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:34:50,547] Trial 18 finished with value: 217.6020646 and parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0008002502589341042, 'gamma': 0.9560887274695655, 'gae_lambda': 0.9457024863318715}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.2     |
|    ep_rew_mean     | -87.5    |
| time/              |          |
|    fps             | 1170     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=249.94 +/- 145.52
Episode length: 16.30 +/- 3.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | 250         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011073854 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.000205    |
|    learning_rate        | 0.00019     |
|    loss                 | 603         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 950         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32       |
|    ep_rew_mean     | -76.4    |
| time/              |          |
|    fps             | 870      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=165.32 +/- 135.31
Episode length: 14.30 +/- 3.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.3        |
|    mean_reward          | 165         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.023304854 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.00019     |
|    loss                 | 406         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 710         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | -64.7    |
| time/              |          |
|    fps             | 805      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=281.50 +/- 242.10
Episode length: 17.70 +/- 6.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 281         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.018780103 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.00019     |
|    loss                 | 284         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 641         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -47.6    |
| time/              |          |
|    fps             | 776      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=156.91 +/- 135.38
Episode length: 14.40 +/- 3.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 157         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.021761285 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.204       |
|    learning_rate        | 0.00019     |
|    loss                 | 202         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 494         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.7     |
|    ep_rew_mean     | -31.5    |
| time/              |          |
|    fps             | 762      |
|    iterations      | 5        |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.1        |
|    ep_rew_mean          | -20.4       |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 6           |
|    time_elapsed         | 32          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.018395048 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.00019     |
|    loss                 | 409         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 565         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=257.68 +/- 164.13
Episode length: 16.40 +/- 4.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | 258         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.020707801 |
|    clip_fraction        | 0.362       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.00019     |
|    loss                 | 426         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 683         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | -2.34    |
| time/              |          |
|    fps             | 747      |
|    iterations      | 7        |
|    time_elapsed    | 38       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=245.01 +/- 236.53
Episode length: 16.50 +/- 6.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | 245         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.021105625 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.289       |
|    learning_rate        | 0.00019     |
|    loss                 | 529         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 965         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 35       |
| time/              |          |
|    fps             | 742      |
|    iterations      | 8        |
|    time_elapsed    | 44       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=274.22 +/- 224.43
Episode length: 16.50 +/- 6.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.022094555 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.49       |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.00019     |
|    loss                 | 646         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 1.31e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 60.8     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 9        |
|    time_elapsed    | 50       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=183.51 +/- 132.75
Episode length: 14.40 +/- 2.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.023540383 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.00019     |
|    loss                 | 858         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 1.82e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 90.1     |
| time/              |          |
|    fps             | 731      |
|    iterations      | 10       |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=210.54 +/- 148.09
Episode length: 15.60 +/- 4.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | 211         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.021827567 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.00019     |
|    loss                 | 1.44e+03    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 2.69e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 728      |
|    iterations      | 11       |
|    time_elapsed    | 61       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.9       |
|    ep_rew_mean          | 151        |
| time/                   |            |
|    fps                  | 728        |
|    iterations           | 12         |
|    time_elapsed         | 67         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.02123345 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.808     |
|    explained_variance   | 0.373      |
|    learning_rate        | 0.00019    |
|    loss                 | 965        |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00952   |
|    value_loss           | 3.91e+03   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=255.71 +/- 290.47
Episode length: 16.60 +/- 8.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.6       |
|    mean_reward          | 256        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.01760497 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.64      |
|    explained_variance   | 0.402      |
|    learning_rate        | 0.00019    |
|    loss                 | 2.98e+03   |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.00274   |
|    value_loss           | 4.74e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 726      |
|    iterations      | 13       |
|    time_elapsed    | 73       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=172.75 +/- 142.83
Episode length: 14.30 +/- 3.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.3       |
|    mean_reward          | 173        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.02026178 |
|    clip_fraction        | 0.0971     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.501     |
|    explained_variance   | 0.507      |
|    learning_rate        | 0.00019    |
|    loss                 | 1.65e+03   |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.00284   |
|    value_loss           | 5.06e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 199      |
| time/              |          |
|    fps             | 724      |
|    iterations      | 14       |
|    time_elapsed    | 79       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=177.43 +/- 136.83
Episode length: 14.40 +/- 3.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.023753421 |
|    clip_fraction        | 0.0704      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.358      |
|    explained_variance   | 0.519       |
|    learning_rate        | 0.00019     |
|    loss                 | 2.14e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00424    |
|    value_loss           | 5.58e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 723      |
|    iterations      | 15       |
|    time_elapsed    | 84       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:36:18,243] Trial 19 finished with value: 170.9193953 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 0.00019022737345167183, 'gamma': 0.999725509911805, 'gae_lambda': 0.8863829203465203}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=118.15 +/- 128.88
Episode length: 15.40 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 118      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.3     |
|    ep_rew_mean     | -87.1    |
| time/              |          |
|    fps             | 1160     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=205.16 +/- 166.78
Episode length: 16.50 +/- 3.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | 205         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011248015 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 7.56e-05    |
|    learning_rate        | 4.03e-05    |
|    loss                 | 484         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 754         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=190.86 +/- 178.76
Episode length: 15.80 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -81.7    |
| time/              |          |
|    fps             | 875      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-88.56 +/- 53.70
Episode length: 26.50 +/- 5.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.5        |
|    mean_reward          | -88.6       |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012960945 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.0632      |
|    learning_rate        | 4.03e-05    |
|    loss                 | 340         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 586         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | -71.3    |
| time/              |          |
|    fps             | 806      |
|    iterations      | 3        |
|    time_elapsed    | 30       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=246.40 +/- 206.47
Episode length: 17.90 +/- 7.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.9        |
|    mean_reward          | 246         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.013166122 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.261       |
|    learning_rate        | 4.03e-05    |
|    loss                 | 408         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 573         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=156.67 +/- 117.39
Episode length: 13.90 +/- 2.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.9     |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30       |
|    ep_rew_mean     | -50.8    |
| time/              |          |
|    fps             | 782      |
|    iterations      | 4        |
|    time_elapsed    | 41       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=217.54 +/- 227.51
Episode length: 15.80 +/- 6.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014698217 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.358       |
|    learning_rate        | 4.03e-05    |
|    loss                 | 163         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 543         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=225.43 +/- 155.20
Episode length: 15.70 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.8     |
|    ep_rew_mean     | -11.3    |
| time/              |          |
|    fps             | 765      |
|    iterations      | 5        |
|    time_elapsed    | 53       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=179.68 +/- 146.06
Episode length: 14.40 +/- 3.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 180          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0143859815 |
|    clip_fraction        | 0.312        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.349        |
|    learning_rate        | 4.03e-05     |
|    loss                 | 166          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0299      |
|    value_loss           | 684          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 0.313    |
| time/              |          |
|    fps             | 753      |
|    iterations      | 6        |
|    time_elapsed    | 65       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=351.09 +/- 212.18
Episode length: 19.10 +/- 6.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.1        |
|    mean_reward          | 351         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015539501 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.352       |
|    learning_rate        | 4.03e-05    |
|    loss                 | 416         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 823         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=227.85 +/- 160.78
Episode length: 16.30 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 20.4     |
| time/              |          |
|    fps             | 746      |
|    iterations      | 7        |
|    time_elapsed    | 76       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=198.58 +/- 130.85
Episode length: 15.30 +/- 3.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | 199         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.016382955 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.339       |
|    learning_rate        | 4.03e-05    |
|    loss                 | 362         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0299     |
|    value_loss           | 957         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=213.41 +/- 155.73
Episode length: 15.10 +/- 3.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 46.2     |
| time/              |          |
|    fps             | 738      |
|    iterations      | 8        |
|    time_elapsed    | 88       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:37:51,965] Trial 20 finished with value: 183.900975 and parameters: {'n_steps': 8192, 'batch_size': 32, 'learning_rate': 4.0255008605433735e-05, 'gamma': 0.9296731771816653, 'gae_lambda': 0.858430221306759}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.2     |
|    ep_rew_mean     | -87.7    |
| time/              |          |
|    fps             | 1183     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=189.17 +/- 142.10
Episode length: 14.80 +/- 4.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.014281655 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -4.1e-05    |
|    learning_rate        | 1.21e-05    |
|    loss                 | 422         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00996    |
|    value_loss           | 833         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -76.8    |
| time/              |          |
|    fps             | 1052     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=207.93 +/- 124.62
Episode length: 15.10 +/- 3.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 208         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014099307 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | -0.000141   |
|    learning_rate        | 1.21e-05    |
|    loss                 | 287         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 669         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | -69.7    |
| time/              |          |
|    fps             | 1017     |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=324.91 +/- 253.07
Episode length: 19.30 +/- 7.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.3        |
|    mean_reward          | 325         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012327118 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.00143     |
|    learning_rate        | 1.21e-05    |
|    loss                 | 297         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 700         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | -59.5    |
| time/              |          |
|    fps             | 996      |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=237.76 +/- 133.33
Episode length: 16.10 +/- 3.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | 238         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.017821176 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.0059      |
|    learning_rate        | 1.21e-05    |
|    loss                 | 323         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 711         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | -36.6    |
| time/              |          |
|    fps             | 987      |
|    iterations      | 5        |
|    time_elapsed    | 20       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | -30.4       |
| time/                   |             |
|    fps                  | 986         |
|    iterations           | 6           |
|    time_elapsed         | 24          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.012579528 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.00898     |
|    learning_rate        | 1.21e-05    |
|    loss                 | 312         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 765         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=142.05 +/- 115.26
Episode length: 13.50 +/- 3.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 13.5       |
|    mean_reward          | 142        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01577789 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.81      |
|    explained_variance   | 0.0141     |
|    learning_rate        | 1.21e-05   |
|    loss                 | 460        |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0292    |
|    value_loss           | 818        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    fps             | 980      |
|    iterations      | 7        |
|    time_elapsed    | 29       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=195.38 +/- 165.97
Episode length: 15.40 +/- 4.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.013780443 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.0202      |
|    learning_rate        | 1.21e-05    |
|    loss                 | 601         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 974         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 26.7     |
| time/              |          |
|    fps             | 976      |
|    iterations      | 8        |
|    time_elapsed    | 33       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=223.46 +/- 144.80
Episode length: 16.30 +/- 4.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | 223         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.018447582 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.0254      |
|    learning_rate        | 1.21e-05    |
|    loss                 | 463         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 1.24e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 72.6     |
| time/              |          |
|    fps             | 974      |
|    iterations      | 9        |
|    time_elapsed    | 37       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=293.53 +/- 141.62
Episode length: 17.20 +/- 3.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.2        |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.026982464 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.0242      |
|    learning_rate        | 1.21e-05    |
|    loss                 | 867         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 1.6e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.5     |
|    ep_rew_mean     | 102      |
| time/              |          |
|    fps             | 971      |
|    iterations      | 10       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=138.57 +/- 142.31
Episode length: 13.80 +/- 4.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.8        |
|    mean_reward          | 139         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.022470934 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.0272      |
|    learning_rate        | 1.21e-05    |
|    loss                 | 926         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 2.04e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 124      |
| time/              |          |
|    fps             | 969      |
|    iterations      | 11       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.3       |
|    ep_rew_mean          | 127        |
| time/                   |            |
|    fps                  | 969        |
|    iterations           | 12         |
|    time_elapsed         | 50         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.01061048 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.872     |
|    explained_variance   | 0.0316     |
|    learning_rate        | 1.21e-05   |
|    loss                 | 1.1e+03    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 2.64e+03   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=301.13 +/- 229.83
Episode length: 17.40 +/- 5.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.4       |
|    mean_reward          | 301        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.00784444 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.763     |
|    explained_variance   | 0.0332     |
|    learning_rate        | 1.21e-05   |
|    loss                 | 1.82e+03   |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.00942   |
|    value_loss           | 3.05e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | 109      |
| time/              |          |
|    fps             | 966      |
|    iterations      | 13       |
|    time_elapsed    | 55       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=289.94 +/- 136.15
Episode length: 17.30 +/- 3.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | 290          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0046303705 |
|    clip_fraction        | 0.069        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.661       |
|    explained_variance   | 0.0392       |
|    learning_rate        | 1.21e-05     |
|    loss                 | 1.37e+03     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00467     |
|    value_loss           | 3.28e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | 175      |
| time/              |          |
|    fps             | 965      |
|    iterations      | 14       |
|    time_elapsed    | 59       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=204.49 +/- 214.15
Episode length: 14.90 +/- 6.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.9         |
|    mean_reward          | 204          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0043608686 |
|    clip_fraction        | 0.0704       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.56        |
|    explained_variance   | 0.0452       |
|    learning_rate        | 1.21e-05     |
|    loss                 | 1.62e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0056      |
|    value_loss           | 3.77e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 176      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 15       |
|    time_elapsed    | 63       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:38:57,052] Trial 21 finished with value: 317.4420731 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 1.212057190036814e-05, 'gamma': 0.9469492161796369, 'gae_lambda': 0.80831619343111}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.9     |
|    ep_rew_mean     | -95.6    |
| time/              |          |
|    fps             | 1184     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=266.15 +/- 149.28
Episode length: 17.10 +/- 3.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.1        |
|    mean_reward          | 266         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012533296 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.000196    |
|    learning_rate        | 1.16e-05    |
|    loss                 | 346         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 812         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | -82.9    |
| time/              |          |
|    fps             | 1054     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=124.41 +/- 100.84
Episode length: 13.20 +/- 3.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.2        |
|    mean_reward          | 124         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011827351 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.000683    |
|    learning_rate        | 1.16e-05    |
|    loss                 | 254         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 664         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | -72.5    |
| time/              |          |
|    fps             | 1019     |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=125.14 +/- 90.39
Episode length: 12.90 +/- 1.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 12.9        |
|    mean_reward          | 125         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012440755 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.00391     |
|    learning_rate        | 1.16e-05    |
|    loss                 | 352         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 659         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -57.5    |
| time/              |          |
|    fps             | 1001     |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=254.82 +/- 192.26
Episode length: 16.40 +/- 5.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | 255         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.013937386 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.00691     |
|    learning_rate        | 1.16e-05    |
|    loss                 | 345         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 659         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | -56.8    |
| time/              |          |
|    fps             | 991      |
|    iterations      | 5        |
|    time_elapsed    | 20       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | -24.4       |
| time/                   |             |
|    fps                  | 991         |
|    iterations           | 6           |
|    time_elapsed         | 24          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014236266 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.0122      |
|    learning_rate        | 1.16e-05    |
|    loss                 | 417         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 703         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=304.67 +/- 145.12
Episode length: 18.60 +/- 4.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.6        |
|    mean_reward          | 305         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011762279 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.0158      |
|    learning_rate        | 1.16e-05    |
|    loss                 | 524         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 870         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | -9.14    |
| time/              |          |
|    fps             | 984      |
|    iterations      | 7        |
|    time_elapsed    | 29       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=200.85 +/- 153.20
Episode length: 14.90 +/- 3.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.016036913 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.0218      |
|    learning_rate        | 1.16e-05    |
|    loss                 | 402         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 888         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 20.2     |
| time/              |          |
|    fps             | 979      |
|    iterations      | 8        |
|    time_elapsed    | 33       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=132.95 +/- 88.73
Episode length: 13.80 +/- 2.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.8        |
|    mean_reward          | 133         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.016690522 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.0294      |
|    learning_rate        | 1.16e-05    |
|    loss                 | 469         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 1.1e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 32       |
| time/              |          |
|    fps             | 976      |
|    iterations      | 9        |
|    time_elapsed    | 37       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=148.89 +/- 191.13
Episode length: 14.30 +/- 5.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.3        |
|    mean_reward          | 149         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.025388554 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.0347      |
|    learning_rate        | 1.16e-05    |
|    loss                 | 705         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 1.34e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.6     |
|    ep_rew_mean     | 77.5     |
| time/              |          |
|    fps             | 973      |
|    iterations      | 10       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=311.13 +/- 154.82
Episode length: 17.90 +/- 4.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.9        |
|    mean_reward          | 311         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.028680235 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.0392      |
|    learning_rate        | 1.16e-05    |
|    loss                 | 719         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 1.7e+03     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 970      |
|    iterations      | 11       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15.8       |
|    ep_rew_mean          | 112        |
| time/                   |            |
|    fps                  | 971        |
|    iterations           | 12         |
|    time_elapsed         | 50         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.02100686 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.866     |
|    explained_variance   | 0.0406     |
|    learning_rate        | 1.16e-05   |
|    loss                 | 1.26e+03   |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 2.15e+03   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=248.78 +/- 220.08
Episode length: 16.00 +/- 6.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 249         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008759746 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.0489      |
|    learning_rate        | 1.16e-05    |
|    loss                 | 1.17e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 2.47e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 156      |
| time/              |          |
|    fps             | 970      |
|    iterations      | 13       |
|    time_elapsed    | 54       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=349.25 +/- 273.83
Episode length: 19.60 +/- 8.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.6        |
|    mean_reward          | 349         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.005104605 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.0423      |
|    learning_rate        | 1.16e-05    |
|    loss                 | 1.13e+03    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00833    |
|    value_loss           | 2.96e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 158      |
| time/              |          |
|    fps             | 967      |
|    iterations      | 14       |
|    time_elapsed    | 59       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=98.96 +/- 84.29
Episode length: 12.60 +/- 2.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.6         |
|    mean_reward          | 99           |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0038839816 |
|    clip_fraction        | 0.0815       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.511       |
|    explained_variance   | 0.0477       |
|    learning_rate        | 1.16e-05     |
|    loss                 | 1.97e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00584     |
|    value_loss           | 3.22e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 171      |
| time/              |          |
|    fps             | 966      |
|    iterations      | 15       |
|    time_elapsed    | 63       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:40:01,709] Trial 22 finished with value: 91.46517779999999 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 1.158974368128003e-05, 'gamma': 0.9150034204875954, 'gae_lambda': 0.8160976629149719}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.3     |
|    ep_rew_mean     | -89      |
| time/              |          |
|    fps             | 1174     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-97.81 +/- 24.91
Episode length: 20.10 +/- 8.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.1        |
|    mean_reward          | -97.8       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.014492551 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.000166    |
|    learning_rate        | 1.93e-05    |
|    loss                 | 434         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 819         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.4     |
|    ep_rew_mean     | -81      |
| time/              |          |
|    fps             | 1053     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=145.29 +/- 129.23
Episode length: 14.40 +/- 4.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 145         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.013402861 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.06       |
|    explained_variance   | 0.00445     |
|    learning_rate        | 1.93e-05    |
|    loss                 | 352         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 666         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.3     |
|    ep_rew_mean     | -71.1    |
| time/              |          |
|    fps             | 1018     |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=266.21 +/- 210.28
Episode length: 17.20 +/- 5.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.2        |
|    mean_reward          | 266         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.015429379 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.0112      |
|    learning_rate        | 1.93e-05    |
|    loss                 | 383         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 695         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -59.5    |
| time/              |          |
|    fps             | 1003     |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=279.71 +/- 224.03
Episode length: 17.20 +/- 6.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.2        |
|    mean_reward          | 280         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.017104393 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.00744     |
|    learning_rate        | 1.93e-05    |
|    loss                 | 333         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 759         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | -47.5    |
| time/              |          |
|    fps             | 991      |
|    iterations      | 5        |
|    time_elapsed    | 20       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | -27.4       |
| time/                   |             |
|    fps                  | 989         |
|    iterations           | 6           |
|    time_elapsed         | 24          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.011890473 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.0199      |
|    learning_rate        | 1.93e-05    |
|    loss                 | 399         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 774         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=221.71 +/- 156.92
Episode length: 15.70 +/- 4.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015202351 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.0255      |
|    learning_rate        | 1.93e-05    |
|    loss                 | 413         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 873         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | -5.02    |
| time/              |          |
|    fps             | 983      |
|    iterations      | 7        |
|    time_elapsed    | 29       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=296.89 +/- 226.15
Episode length: 17.70 +/- 5.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.014784673 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.0338      |
|    learning_rate        | 1.93e-05    |
|    loss                 | 471         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 1.12e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 23.7     |
| time/              |          |
|    fps             | 979      |
|    iterations      | 8        |
|    time_elapsed    | 33       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=208.56 +/- 155.33
Episode length: 15.40 +/- 3.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 209         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.015581596 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.0441      |
|    learning_rate        | 1.93e-05    |
|    loss                 | 654         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 1.23e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 48.3     |
| time/              |          |
|    fps             | 975      |
|    iterations      | 9        |
|    time_elapsed    | 37       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=181.47 +/- 132.60
Episode length: 14.20 +/- 3.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.2        |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.018908586 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.0567      |
|    learning_rate        | 1.93e-05    |
|    loss                 | 809         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 1.48e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | 60.7     |
| time/              |          |
|    fps             | 972      |
|    iterations      | 10       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=200.01 +/- 159.89
Episode length: 15.00 +/- 3.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.023485918 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.085       |
|    learning_rate        | 1.93e-05    |
|    loss                 | 755         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 1.69e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 83.6     |
| time/              |          |
|    fps             | 969      |
|    iterations      | 11       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.2        |
|    ep_rew_mean          | 114         |
| time/                   |             |
|    fps                  | 969         |
|    iterations           | 12          |
|    time_elapsed         | 50          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.018805634 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.116       |
|    learning_rate        | 1.93e-05    |
|    loss                 | 1.5e+03     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 2.22e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=208.42 +/- 131.86
Episode length: 15.00 +/- 3.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 208         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.012187529 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.853      |
|    explained_variance   | 0.112       |
|    learning_rate        | 1.93e-05    |
|    loss                 | 1.43e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 2.93e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | 108      |
| time/              |          |
|    fps             | 968      |
|    iterations      | 13       |
|    time_elapsed    | 54       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=180.14 +/- 123.56
Episode length: 14.80 +/- 3.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 180         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.008163727 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.736      |
|    explained_variance   | 0.143       |
|    learning_rate        | 1.93e-05    |
|    loss                 | 1.55e+03    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 3.25e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | 138      |
| time/              |          |
|    fps             | 967      |
|    iterations      | 14       |
|    time_elapsed    | 59       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=314.85 +/- 173.07
Episode length: 17.90 +/- 4.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.9         |
|    mean_reward          | 315          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0054672766 |
|    clip_fraction        | 0.122        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.608       |
|    explained_variance   | 0.169        |
|    learning_rate        | 1.93e-05     |
|    loss                 | 2.07e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00993     |
|    value_loss           | 3.63e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 965      |
|    iterations      | 15       |
|    time_elapsed    | 63       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:41:06,474] Trial 23 finished with value: 198.19464269999997 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 1.9325175889577218e-05, 'gamma': 0.9577202412596982, 'gae_lambda': 0.8417550195678007}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.3     |
|    ep_rew_mean     | -94.8    |
| time/              |          |
|    fps             | 1182     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=107.63 +/- 54.64
Episode length: 12.90 +/- 1.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 12.9      |
|    mean_reward          | 108       |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 0.0113463 |
|    clip_fraction        | 0.161     |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.07     |
|    explained_variance   | -0.000132 |
|    learning_rate        | 5.32e-05  |
|    loss                 | 320       |
|    n_updates            | 10        |
|    policy_gradient_loss | -0.0145   |
|    value_loss           | 733       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -84.7    |
| time/              |          |
|    fps             | 1057     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=261.36 +/- 214.16
Episode length: 16.50 +/- 5.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012475384 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.00812     |
|    learning_rate        | 5.32e-05    |
|    loss                 | 297         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 671         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.9     |
|    ep_rew_mean     | -70.7    |
| time/              |          |
|    fps             | 1022     |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=187.41 +/- 111.20
Episode length: 14.80 +/- 2.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 187         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012268273 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.0222      |
|    learning_rate        | 5.32e-05    |
|    loss                 | 332         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 645         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | -66.9    |
| time/              |          |
|    fps             | 1002     |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=179.14 +/- 124.76
Episode length: 14.70 +/- 3.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.7       |
|    mean_reward          | 179        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.01429613 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.97      |
|    explained_variance   | 0.0486     |
|    learning_rate        | 5.32e-05   |
|    loss                 | 279        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0207    |
|    value_loss           | 580        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -33.3    |
| time/              |          |
|    fps             | 992      |
|    iterations      | 5        |
|    time_elapsed    | 20       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | -21.5       |
| time/                   |             |
|    fps                  | 987         |
|    iterations           | 6           |
|    time_elapsed         | 24          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.018533766 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.108       |
|    learning_rate        | 5.32e-05    |
|    loss                 | 345         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 651         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=300.80 +/- 176.78
Episode length: 18.20 +/- 4.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.2        |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015456639 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.242       |
|    learning_rate        | 5.32e-05    |
|    loss                 | 339         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 660         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | -7.48    |
| time/              |          |
|    fps             | 980      |
|    iterations      | 7        |
|    time_elapsed    | 29       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=233.09 +/- 245.55
Episode length: 16.20 +/- 6.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 233         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.013843469 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.303       |
|    learning_rate        | 5.32e-05    |
|    loss                 | 407         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 780         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 38.6     |
| time/              |          |
|    fps             | 975      |
|    iterations      | 8        |
|    time_elapsed    | 33       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=307.62 +/- 188.94
Episode length: 18.30 +/- 5.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 308         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.019012427 |
|    clip_fraction        | 0.426       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.46       |
|    explained_variance   | 0.297       |
|    learning_rate        | 5.32e-05    |
|    loss                 | 471         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0394     |
|    value_loss           | 1.02e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 61.8     |
| time/              |          |
|    fps             | 970      |
|    iterations      | 9        |
|    time_elapsed    | 37       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=160.61 +/- 130.28
Episode length: 14.10 +/- 3.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.1        |
|    mean_reward          | 161         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.027208883 |
|    clip_fraction        | 0.379       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.309       |
|    learning_rate        | 5.32e-05    |
|    loss                 | 542         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 1.23e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 83.8     |
| time/              |          |
|    fps             | 967      |
|    iterations      | 10       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=187.60 +/- 132.64
Episode length: 14.40 +/- 3.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.019668907 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.992      |
|    explained_variance   | 0.274       |
|    learning_rate        | 5.32e-05    |
|    loss                 | 876         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 1.71e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | 98.9     |
| time/              |          |
|    fps             | 965      |
|    iterations      | 11       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.3        |
|    ep_rew_mean          | 151         |
| time/                   |             |
|    fps                  | 965         |
|    iterations           | 12          |
|    time_elapsed         | 50          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.020616557 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.768      |
|    explained_variance   | 0.29        |
|    learning_rate        | 5.32e-05    |
|    loss                 | 937         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 2.07e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=266.30 +/- 168.85
Episode length: 16.30 +/- 4.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | 266          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0074815038 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.61        |
|    explained_variance   | 0.326        |
|    learning_rate        | 5.32e-05     |
|    loss                 | 1.38e+03     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0104      |
|    value_loss           | 2.47e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 189      |
| time/              |          |
|    fps             | 964      |
|    iterations      | 13       |
|    time_elapsed    | 55       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=123.31 +/- 61.81
Episode length: 12.90 +/- 1.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.9         |
|    mean_reward          | 123          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0062280865 |
|    clip_fraction        | 0.0914       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.493       |
|    explained_variance   | 0.391        |
|    learning_rate        | 5.32e-05     |
|    loss                 | 1.18e+03     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00852     |
|    value_loss           | 2.82e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 964      |
|    iterations      | 14       |
|    time_elapsed    | 59       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=242.26 +/- 162.34
Episode length: 16.00 +/- 4.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 242         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.003598115 |
|    clip_fraction        | 0.0673      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.433       |
|    learning_rate        | 5.32e-05    |
|    loss                 | 1.47e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0046     |
|    value_loss           | 3.04e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 223      |
| time/              |          |
|    fps             | 963      |
|    iterations      | 15       |
|    time_elapsed    | 63       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:42:11,325] Trial 24 finished with value: 201.68667449999998 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 5.321834065613494e-05, 'gamma': 0.9369026937503399, 'gae_lambda': 0.8185010671484332}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -87.2    |
| time/              |          |
|    fps             | 1188     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-115.94 +/- 0.05
Episode length: 26.90 +/- 11.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.9        |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.016486404 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000177   |
|    learning_rate        | 2.13e-05    |
|    loss                 | 382         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 839         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -88.5    |
| time/              |          |
|    fps             | 990      |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=159.90 +/- 114.54
Episode length: 14.30 +/- 3.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.3        |
|    mean_reward          | 160         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.015449687 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.00714     |
|    learning_rate        | 2.13e-05    |
|    loss                 | 271         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 750         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | -58.3    |
| time/              |          |
|    fps             | 940      |
|    iterations      | 3        |
|    time_elapsed    | 13       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=246.87 +/- 158.55
Episode length: 16.30 +/- 3.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | 247         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.011683379 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.00904     |
|    learning_rate        | 2.13e-05    |
|    loss                 | 291         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 740         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | -66.2    |
| time/              |          |
|    fps             | 914      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=193.54 +/- 140.69
Episode length: 14.60 +/- 4.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.6        |
|    mean_reward          | 194         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010986147 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.0284      |
|    learning_rate        | 2.13e-05    |
|    loss                 | 373         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 831         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | -41.2    |
| time/              |          |
|    fps             | 900      |
|    iterations      | 5        |
|    time_elapsed    | 22       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.8       |
|    ep_rew_mean          | -14.4      |
| time/                   |            |
|    fps                  | 897        |
|    iterations           | 6          |
|    time_elapsed         | 27         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.01688813 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.88      |
|    explained_variance   | 0.0283     |
|    learning_rate        | 2.13e-05   |
|    loss                 | 345        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0238    |
|    value_loss           | 900        |
----------------------------------------
Eval num_timesteps=25000, episode_reward=229.94 +/- 212.98
Episode length: 16.20 +/- 6.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 230         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.013901666 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.0574      |
|    learning_rate        | 2.13e-05    |
|    loss                 | 566         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 954         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 888      |
|    iterations      | 7        |
|    time_elapsed    | 32       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=145.12 +/- 140.22
Episode length: 14.20 +/- 4.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.2        |
|    mean_reward          | 145         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.018629808 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.0941      |
|    learning_rate        | 2.13e-05    |
|    loss                 | 825         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 1.12e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 882      |
|    iterations      | 8        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=257.73 +/- 203.92
Episode length: 17.00 +/- 6.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 258         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.017348774 |
|    clip_fraction        | 0.342       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.164       |
|    learning_rate        | 2.13e-05    |
|    loss                 | 680         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 1.21e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 49.6     |
| time/              |          |
|    fps             | 876      |
|    iterations      | 9        |
|    time_elapsed    | 42       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=215.36 +/- 139.29
Episode length: 15.30 +/- 4.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.3       |
|    mean_reward          | 215        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.01928282 |
|    clip_fraction        | 0.354      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.203      |
|    learning_rate        | 2.13e-05   |
|    loss                 | 688        |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0306    |
|    value_loss           | 1.47e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 56       |
| time/              |          |
|    fps             | 873      |
|    iterations      | 10       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=229.49 +/- 142.73
Episode length: 16.20 +/- 4.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 229         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.019829134 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.195       |
|    learning_rate        | 2.13e-05    |
|    loss                 | 712         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 1.78e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 104      |
| time/              |          |
|    fps             | 870      |
|    iterations      | 11       |
|    time_elapsed    | 51       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.8        |
|    ep_rew_mean          | 139         |
| time/                   |             |
|    fps                  | 867         |
|    iterations           | 12          |
|    time_elapsed         | 56          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.014283408 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.948      |
|    explained_variance   | 0.229       |
|    learning_rate        | 2.13e-05    |
|    loss                 | 1.1e+03     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 2.25e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=215.07 +/- 154.08
Episode length: 15.30 +/- 3.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | 215         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010493172 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.809      |
|    explained_variance   | 0.251       |
|    learning_rate        | 2.13e-05    |
|    loss                 | 1.41e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 2.47e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 866      |
|    iterations      | 13       |
|    time_elapsed    | 61       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=215.51 +/- 231.23
Episode length: 15.50 +/- 5.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | 216          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0077581108 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.654       |
|    explained_variance   | 0.277        |
|    learning_rate        | 2.13e-05     |
|    loss                 | 1.34e+03     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.0122      |
|    value_loss           | 2.88e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 157      |
| time/              |          |
|    fps             | 865      |
|    iterations      | 14       |
|    time_elapsed    | 66       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=274.36 +/- 243.84
Episode length: 16.90 +/- 6.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.005281821 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.536      |
|    explained_variance   | 0.282       |
|    learning_rate        | 2.13e-05    |
|    loss                 | 1.29e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00813    |
|    value_loss           | 3.22e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 864      |
|    iterations      | 15       |
|    time_elapsed    | 71       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:43:23,956] Trial 25 finished with value: 263.2592819 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 2.13475885200386e-05, 'gamma': 0.9243329823301847, 'gae_lambda': 0.8716093516745418}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=29.62 +/- 127.18
Episode length: 17.00 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 29.6     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.9     |
|    ep_rew_mean     | -90.6    |
| time/              |          |
|    fps             | 1163     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=114.14 +/- 106.39
Episode length: 19.30 +/- 6.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.3       |
|    mean_reward          | 114        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01413005 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.07      |
|    explained_variance   | -0.000104  |
|    learning_rate        | 0.000274   |
|    loss                 | 315        |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 650        |
----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=113.25 +/- 106.00
Episode length: 17.20 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | 113      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -77.8    |
| time/              |          |
|    fps             | 1047     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=189.68 +/- 167.96
Episode length: 14.90 +/- 4.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.014977476 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.000274    |
|    loss                 | 188         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 490         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | -68.5    |
| time/              |          |
|    fps             | 1017     |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=285.79 +/- 149.27
Episode length: 17.20 +/- 4.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.2        |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.016710795 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.000274    |
|    loss                 | 154         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 455         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=227.17 +/- 219.11
Episode length: 16.20 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 227      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | -44.9    |
| time/              |          |
|    fps             | 998      |
|    iterations      | 4        |
|    time_elapsed    | 32       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=159.41 +/- 123.28
Episode length: 13.60 +/- 2.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.6        |
|    mean_reward          | 159         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.016432913 |
|    clip_fraction        | 0.331       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.000274    |
|    loss                 | 220         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 483         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=212.54 +/- 174.31
Episode length: 15.30 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | -15.1    |
| time/              |          |
|    fps             | 988      |
|    iterations      | 5        |
|    time_elapsed    | 41       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=248.69 +/- 152.76
Episode length: 16.80 +/- 4.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | 249         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.019763745 |
|    clip_fraction        | 0.38        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.417       |
|    learning_rate        | 0.000274    |
|    loss                 | 309         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0316     |
|    value_loss           | 615         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 6.95     |
| time/              |          |
|    fps             | 982      |
|    iterations      | 6        |
|    time_elapsed    | 50       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=235.09 +/- 223.36
Episode length: 15.90 +/- 6.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | 235         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.020505782 |
|    clip_fraction        | 0.394       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.000274    |
|    loss                 | 366         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 756         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=201.34 +/- 136.73
Episode length: 15.30 +/- 3.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 56.5     |
| time/              |          |
|    fps             | 976      |
|    iterations      | 7        |
|    time_elapsed    | 58       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=168.52 +/- 124.69
Episode length: 13.90 +/- 3.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.9        |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.025198678 |
|    clip_fraction        | 0.466       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.372       |
|    learning_rate        | 0.000274    |
|    loss                 | 495         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0376     |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=131.51 +/- 144.89
Episode length: 12.60 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.6     |
|    mean_reward     | 132      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 54.6     |
| time/              |          |
|    fps             | 971      |
|    iterations      | 8        |
|    time_elapsed    | 67       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:44:33,170] Trial 26 finished with value: 263.2878402 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 0.0002740562668990454, 'gamma': 0.9426431948087463, 'gae_lambda': 0.8170109207496263}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.3     |
|    ep_rew_mean     | -85.4    |
| time/              |          |
|    fps             | 1194     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-86.76 +/- 40.89
Episode length: 26.10 +/- 11.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.1        |
|    mean_reward          | -86.8       |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.020920265 |
|    clip_fraction        | 0.399       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | -8.23e-06   |
|    learning_rate        | 0.000928    |
|    loss                 | 213         |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.000961    |
|    value_loss           | 654         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | -70.2    |
| time/              |          |
|    fps             | 989      |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=70.56 +/- 88.52
Episode length: 14.90 +/- 3.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | 70.6        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.017705763 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.000928    |
|    loss                 | 250         |
|    n_updates            | 20          |
|    policy_gradient_loss | -4.89e-06   |
|    value_loss           | 519         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | -61.9    |
| time/              |          |
|    fps             | 942      |
|    iterations      | 3        |
|    time_elapsed    | 13       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=249.60 +/- 206.33
Episode length: 16.90 +/- 6.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | 250         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.023679867 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.416       |
|    learning_rate        | 0.000928    |
|    loss                 | 214         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00803    |
|    value_loss           | 425         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -59.7    |
| time/              |          |
|    fps             | 920      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=210.99 +/- 146.99
Episode length: 15.60 +/- 3.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.6       |
|    mean_reward          | 211        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.02939023 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.95      |
|    explained_variance   | 0.459      |
|    learning_rate        | 0.000928   |
|    loss                 | 217        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00629   |
|    value_loss           | 411        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -54.4    |
| time/              |          |
|    fps             | 904      |
|    iterations      | 5        |
|    time_elapsed    | 22       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23          |
|    ep_rew_mean          | -21.9       |
| time/                   |             |
|    fps                  | 898         |
|    iterations           | 6           |
|    time_elapsed         | 27          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.026424017 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.372       |
|    learning_rate        | 0.000928    |
|    loss                 | 108         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00997    |
|    value_loss           | 445         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=242.19 +/- 179.86
Episode length: 15.70 +/- 5.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | 242        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.03709593 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.78      |
|    explained_variance   | 0.396      |
|    learning_rate        | 0.000928   |
|    loss                 | 172        |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00885   |
|    value_loss           | 515        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | -0.289   |
| time/              |          |
|    fps             | 890      |
|    iterations      | 7        |
|    time_elapsed    | 32       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=289.12 +/- 219.18
Episode length: 17.90 +/- 6.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.9        |
|    mean_reward          | 289         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.050334927 |
|    clip_fraction        | 0.414       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.335       |
|    learning_rate        | 0.000928    |
|    loss                 | 137         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 583         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 26.6     |
| time/              |          |
|    fps             | 883      |
|    iterations      | 8        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=245.89 +/- 210.02
Episode length: 16.90 +/- 6.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.9       |
|    mean_reward          | 246        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.02891586 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | 0.394      |
|    learning_rate        | 0.000928   |
|    loss                 | 168        |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.00721   |
|    value_loss           | 688        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 56.6     |
| time/              |          |
|    fps             | 879      |
|    iterations      | 9        |
|    time_elapsed    | 41       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=192.40 +/- 164.50
Episode length: 14.80 +/- 5.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 192         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.033401884 |
|    clip_fraction        | 0.384       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.309       |
|    learning_rate        | 0.000928    |
|    loss                 | 150         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00608    |
|    value_loss           | 854         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.2     |
|    ep_rew_mean     | 45.8     |
| time/              |          |
|    fps             | 875      |
|    iterations      | 10       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=275.88 +/- 126.59
Episode length: 16.70 +/- 3.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.7       |
|    mean_reward          | 276        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.03635126 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.38       |
|    learning_rate        | 0.000928   |
|    loss                 | 170        |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.00745   |
|    value_loss           | 933        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 72.1     |
| time/              |          |
|    fps             | 872      |
|    iterations      | 11       |
|    time_elapsed    | 51       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.9        |
|    ep_rew_mean          | 80          |
| time/                   |             |
|    fps                  | 868         |
|    iterations           | 12          |
|    time_elapsed         | 56          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.029779367 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.000928    |
|    loss                 | 275         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00275    |
|    value_loss           | 971         |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=269.96 +/- 182.31
Episode length: 17.50 +/- 4.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | 270         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.036635604 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.377       |
|    learning_rate        | 0.000928    |
|    loss                 | 732         |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.0032      |
|    value_loss           | 1.24e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 103      |
| time/              |          |
|    fps             | 863      |
|    iterations      | 13       |
|    time_elapsed    | 61       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=200.56 +/- 137.53
Episode length: 14.50 +/- 2.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.5       |
|    mean_reward          | 201        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.04015689 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1         |
|    explained_variance   | 0.39       |
|    learning_rate        | 0.000928   |
|    loss                 | 488        |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.00812   |
|    value_loss           | 1.44e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | 110      |
| time/              |          |
|    fps             | 862      |
|    iterations      | 14       |
|    time_elapsed    | 66       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=137.74 +/- 130.56
Episode length: 13.80 +/- 3.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 13.8       |
|    mean_reward          | 138        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.04114119 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.992     |
|    explained_variance   | 0.462      |
|    learning_rate        | 0.000928   |
|    loss                 | 456        |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.00467   |
|    value_loss           | 1.42e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 860      |
|    iterations      | 15       |
|    time_elapsed    | 71       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:45:46,204] Trial 27 finished with value: 258.747876 and parameters: {'n_steps': 4096, 'batch_size': 64, 'learning_rate': 0.0009280458654612435, 'gamma': 0.9677078500371512, 'gae_lambda': 0.8459159534408769}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.4     |
|    ep_rew_mean     | -91      |
| time/              |          |
|    fps             | 1181     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 24.7      |
|    ep_rew_mean          | -87.1     |
| time/                   |           |
|    fps                  | 891       |
|    iterations           | 2         |
|    time_elapsed         | 4         |
|    total_timesteps      | 4096      |
| train/                  |           |
|    approx_kl            | 4.8623333 |
|    clip_fraction        | 0.915     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.938    |
|    explained_variance   | 0.000128  |
|    learning_rate        | 0.00759   |
|    loss                 | 157       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.33      |
|    value_loss           | 626       |
---------------------------------------
Eval num_timesteps=5000, episode_reward=31.77 +/- 146.55
Episode length: 17.00 +/- 4.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17        |
|    mean_reward          | 31.8      |
| time/                   |           |
|    total_timesteps      | 5000      |
| train/                  |           |
|    approx_kl            | 3.6415272 |
|    clip_fraction        | 0.573     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.368    |
|    explained_variance   | 0.34      |
|    learning_rate        | 0.00759   |
|    loss                 | 339       |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.16      |
|    value_loss           | 452       |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.8     |
|    ep_rew_mean     | -6.85    |
| time/              |          |
|    fps             | 804      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.9      |
|    ep_rew_mean          | -64.5     |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 4         |
|    time_elapsed         | 10        |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 1.6446383 |
|    clip_fraction        | 0.661     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.237    |
|    explained_variance   | 0.471     |
|    learning_rate        | 0.00759   |
|    loss                 | 318       |
|    n_updates            | 30        |
|    policy_gradient_loss | 0.124     |
|    value_loss           | 647       |
---------------------------------------
Eval num_timesteps=10000, episode_reward=-99.42 +/- 38.13
Episode length: 23.50 +/- 10.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 23.5      |
|    mean_reward          | -99.4     |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 1.9220881 |
|    clip_fraction        | 0.243     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.137    |
|    explained_variance   | 0.558     |
|    learning_rate        | 0.00759   |
|    loss                 | 174       |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.12      |
|    value_loss           | 445       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | -92.7    |
| time/              |          |
|    fps             | 759      |
|    iterations      | 5        |
|    time_elapsed    | 13       |
|    total_timesteps | 10240    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.4      |
|    ep_rew_mean          | -73       |
| time/                   |           |
|    fps                  | 753       |
|    iterations           | 6         |
|    time_elapsed         | 16        |
|    total_timesteps      | 12288     |
| train/                  |           |
|    approx_kl            | 0.8677432 |
|    clip_fraction        | 0.145     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0499   |
|    explained_variance   | 0.544     |
|    learning_rate        | 0.00759   |
|    loss                 | 94.4      |
|    n_updates            | 50        |
|    policy_gradient_loss | 0.0297    |
|    value_loss           | 367       |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.4      |
|    ep_rew_mean          | -54.3     |
| time/                   |           |
|    fps                  | 749       |
|    iterations           | 7         |
|    time_elapsed         | 19        |
|    total_timesteps      | 14336     |
| train/                  |           |
|    approx_kl            | 0.4954551 |
|    clip_fraction        | 0.0667    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0265   |
|    explained_variance   | 0.615     |
|    learning_rate        | 0.00759   |
|    loss                 | 240       |
|    n_updates            | 60        |
|    policy_gradient_loss | 0.0121    |
|    value_loss           | 425       |
---------------------------------------
Eval num_timesteps=15000, episode_reward=-83.51 +/- 83.83
Episode length: 19.40 +/- 5.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.4       |
|    mean_reward          | -83.5      |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.62161565 |
|    clip_fraction        | 0.0478     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0188    |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.00759    |
|    loss                 | 512        |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.0105     |
|    value_loss           | 610        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | -75.3    |
| time/              |          |
|    fps             | 738      |
|    iterations      | 8        |
|    time_elapsed    | 22       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 14.6       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 9          |
|    time_elapsed         | 25         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.55365765 |
|    clip_fraction        | 0.0955     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0219    |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.00759    |
|    loss                 | 45.2       |
|    n_updates            | 80         |
|    policy_gradient_loss | 0.0225     |
|    value_loss           | 539        |
----------------------------------------
Eval num_timesteps=20000, episode_reward=284.90 +/- 121.10
Episode length: 17.20 +/- 2.89
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 17.2     |
|    mean_reward          | 285      |
| time/                   |          |
|    total_timesteps      | 20000    |
| train/                  |          |
|    approx_kl            | 17.17157 |
|    clip_fraction        | 0.46     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0201  |
|    explained_variance   | 0.588    |
|    learning_rate        | 0.00759  |
|    loss                 | 590      |
|    n_updates            | 90       |
|    policy_gradient_loss | -0.0081  |
|    value_loss           | 1.31e+03 |
--------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | 197      |
| time/              |          |
|    fps             | 728      |
|    iterations      | 10       |
|    time_elapsed    | 28       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 16.4      |
|    ep_rew_mean          | 223       |
| time/                   |           |
|    fps                  | 725       |
|    iterations           | 11        |
|    time_elapsed         | 31        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.8315387 |
|    clip_fraction        | 0.0625    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0147   |
|    explained_variance   | 0.222     |
|    learning_rate        | 0.00759   |
|    loss                 | 641       |
|    n_updates            | 100       |
|    policy_gradient_loss | 0.0134    |
|    value_loss           | 2.4e+03   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 16.1      |
|    ep_rew_mean          | 192       |
| time/                   |           |
|    fps                  | 722       |
|    iterations           | 12        |
|    time_elapsed         | 34        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 1.6002605 |
|    clip_fraction        | 0.0917    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.019    |
|    explained_variance   | 0.379     |
|    learning_rate        | 0.00759   |
|    loss                 | 995       |
|    n_updates            | 110       |
|    policy_gradient_loss | 0.00817   |
|    value_loss           | 2.31e+03  |
---------------------------------------
Eval num_timesteps=25000, episode_reward=150.32 +/- 145.63
Episode length: 16.60 +/- 4.74
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 16.6     |
|    mean_reward          | 150      |
| time/                   |          |
|    total_timesteps      | 25000    |
| train/                  |          |
|    approx_kl            | 2.273474 |
|    clip_fraction        | 0.149    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0335  |
|    explained_variance   | 0.435    |
|    learning_rate        | 0.00759  |
|    loss                 | 853      |
|    n_updates            | 120      |
|    policy_gradient_loss | 0.0147   |
|    value_loss           | 2.34e+03 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 717      |
|    iterations      | 13       |
|    time_elapsed    | 37       |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 15.8      |
|    ep_rew_mean          | 106       |
| time/                   |           |
|    fps                  | 715       |
|    iterations           | 14        |
|    time_elapsed         | 40        |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 2.4912052 |
|    clip_fraction        | 0.171     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0352   |
|    explained_variance   | 0.581     |
|    learning_rate        | 0.00759   |
|    loss                 | 1.17e+03  |
|    n_updates            | 130       |
|    policy_gradient_loss | 0.0292    |
|    value_loss           | 2.31e+03  |
---------------------------------------
Eval num_timesteps=30000, episode_reward=127.50 +/- 138.26
Episode length: 15.30 +/- 3.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 15.3      |
|    mean_reward          | 127       |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 1.6643306 |
|    clip_fraction        | 0.116     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0194   |
|    explained_variance   | 0.634     |
|    learning_rate        | 0.00759   |
|    loss                 | 889       |
|    n_updates            | 140       |
|    policy_gradient_loss | 0.0153    |
|    value_loss           | 2.38e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | 124      |
| time/              |          |
|    fps             | 713      |
|    iterations      | 15       |
|    time_elapsed    | 43       |
|    total_timesteps | 30720    |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 16.6     |
|    ep_rew_mean          | 117      |
| time/                   |          |
|    fps                  | 712      |
|    iterations           | 16       |
|    time_elapsed         | 45       |
|    total_timesteps      | 32768    |
| train/                  |          |
|    approx_kl            | 2.431001 |
|    clip_fraction        | 0.132    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.028   |
|    explained_variance   | 0.67     |
|    learning_rate        | 0.00759  |
|    loss                 | 450      |
|    n_updates            | 150      |
|    policy_gradient_loss | 0.0271   |
|    value_loss           | 2.36e+03 |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 18.1      |
|    ep_rew_mean          | 56.9      |
| time/                   |           |
|    fps                  | 713       |
|    iterations           | 17        |
|    time_elapsed         | 48        |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 2.5800223 |
|    clip_fraction        | 0.107     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0166   |
|    explained_variance   | 0.691     |
|    learning_rate        | 0.00759   |
|    loss                 | 546       |
|    n_updates            | 160       |
|    policy_gradient_loss | 0.00491   |
|    value_loss           | 2.52e+03  |
---------------------------------------
Eval num_timesteps=35000, episode_reward=22.52 +/- 158.70
Episode length: 24.50 +/- 19.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 24.5      |
|    mean_reward          | 22.5      |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 1.7724686 |
|    clip_fraction        | 0.125     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0245   |
|    explained_variance   | 0.695     |
|    learning_rate        | 0.00759   |
|    loss                 | 941       |
|    n_updates            | 170       |
|    policy_gradient_loss | 0.0208    |
|    value_loss           | 1.83e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | 33.9     |
| time/              |          |
|    fps             | 713      |
|    iterations      | 18       |
|    time_elapsed    | 51       |
|    total_timesteps | 36864    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.7      |
|    ep_rew_mean          | 33        |
| time/                   |           |
|    fps                  | 713       |
|    iterations           | 19        |
|    time_elapsed         | 54        |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 2.3683133 |
|    clip_fraction        | 0.103     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00633  |
|    explained_variance   | 0.719     |
|    learning_rate        | 0.00759   |
|    loss                 | 887       |
|    n_updates            | 180       |
|    policy_gradient_loss | 0.0131    |
|    value_loss           | 1.57e+03  |
---------------------------------------
Eval num_timesteps=40000, episode_reward=63.03 +/- 190.10
Episode length: 20.90 +/- 4.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 20.9      |
|    mean_reward          | 63        |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.4436962 |
|    clip_fraction        | 0.0292    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0049   |
|    explained_variance   | 0.72      |
|    learning_rate        | 0.00759   |
|    loss                 | 1.06e+03  |
|    n_updates            | 190       |
|    policy_gradient_loss | 0.00276   |
|    value_loss           | 2.05e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 711      |
|    iterations      | 20       |
|    time_elapsed    | 57       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 18.6       |
| time/                   |            |
|    fps                  | 710        |
|    iterations           | 21         |
|    time_elapsed         | 60         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.72249043 |
|    clip_fraction        | 0.0277     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00526   |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.00759    |
|    loss                 | 421        |
|    n_updates            | 200        |
|    policy_gradient_loss | 0.00304    |
|    value_loss           | 1.87e+03   |
----------------------------------------
Eval num_timesteps=45000, episode_reward=-31.22 +/- 128.84
Episode length: 19.50 +/- 5.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.5       |
|    mean_reward          | -31.2      |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.59641594 |
|    clip_fraction        | 0.025      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00362   |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.00759    |
|    loss                 | 1.1e+03    |
|    n_updates            | 210        |
|    policy_gradient_loss | 0.00492    |
|    value_loss           | 1.89e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 34.5     |
| time/              |          |
|    fps             | 708      |
|    iterations      | 22       |
|    time_elapsed    | 63       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 18.6      |
|    ep_rew_mean          | -9.05     |
| time/                   |           |
|    fps                  | 708       |
|    iterations           | 23        |
|    time_elapsed         | 66        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.3156711 |
|    clip_fraction        | 0.018     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0028   |
|    explained_variance   | 0.727     |
|    learning_rate        | 0.00759   |
|    loss                 | 1.34e+03  |
|    n_updates            | 220       |
|    policy_gradient_loss | 0.00414   |
|    value_loss           | 1.7e+03   |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20.6     |
|    ep_rew_mean          | -41.2    |
| time/                   |          |
|    fps                  | 707      |
|    iterations           | 24       |
|    time_elapsed         | 69       |
|    total_timesteps      | 49152    |
| train/                  |          |
|    approx_kl            | 0.463075 |
|    clip_fraction        | 0.0217   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00404 |
|    explained_variance   | 0.708    |
|    learning_rate        | 0.00759  |
|    loss                 | 1.38e+03 |
|    n_updates            | 230      |
|    policy_gradient_loss | 0.0015   |
|    value_loss           | 1.42e+03 |
--------------------------------------
Eval num_timesteps=50000, episode_reward=85.12 +/- 165.93
Episode length: 16.10 +/- 3.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.1      |
|    mean_reward          | 85.1      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.3502308 |
|    clip_fraction        | 0.0188    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00277  |
|    explained_variance   | 0.7       |
|    learning_rate        | 0.00759   |
|    loss                 | 825       |
|    n_updates            | 240       |
|    policy_gradient_loss | 0.00227   |
|    value_loss           | 1.1e+03   |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 4.2      |
| time/              |          |
|    fps             | 706      |
|    iterations      | 25       |
|    time_elapsed    | 72       |
|    total_timesteps | 51200    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19        |
|    ep_rew_mean          | -24.6     |
| time/                   |           |
|    fps                  | 706       |
|    iterations           | 26        |
|    time_elapsed         | 75        |
|    total_timesteps      | 53248     |
| train/                  |           |
|    approx_kl            | 0.8119034 |
|    clip_fraction        | 0.0388    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00399  |
|    explained_variance   | 0.676     |
|    learning_rate        | 0.00759   |
|    loss                 | 757       |
|    n_updates            | 250       |
|    policy_gradient_loss | 0.00619   |
|    value_loss           | 1.7e+03   |
---------------------------------------
Eval num_timesteps=55000, episode_reward=2.35 +/- 184.18
Episode length: 20.90 +/- 4.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 20.9      |
|    mean_reward          | 2.35      |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.6737771 |
|    clip_fraction        | 0.0192    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00277  |
|    explained_variance   | 0.706     |
|    learning_rate        | 0.00759   |
|    loss                 | 312       |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.00181  |
|    value_loss           | 1.47e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | -28.1    |
| time/              |          |
|    fps             | 705      |
|    iterations      | 27       |
|    time_elapsed    | 78       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | -43.7      |
| time/                   |            |
|    fps                  | 704        |
|    iterations           | 28         |
|    time_elapsed         | 81         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.46905372 |
|    clip_fraction        | 0.0172     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00251   |
|    explained_variance   | 0.701      |
|    learning_rate        | 0.00759    |
|    loss                 | 1.26e+03   |
|    n_updates            | 270        |
|    policy_gradient_loss | 0.00333    |
|    value_loss           | 1.24e+03   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.6      |
|    ep_rew_mean          | -76       |
| time/                   |           |
|    fps                  | 704       |
|    iterations           | 29        |
|    time_elapsed         | 84        |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 1.0146227 |
|    clip_fraction        | 0.0317    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00364  |
|    explained_variance   | 0.72      |
|    learning_rate        | 0.00759   |
|    loss                 | 143       |
|    n_updates            | 280       |
|    policy_gradient_loss | 0.00732   |
|    value_loss           | 1.07e+03  |
---------------------------------------
Eval num_timesteps=60000, episode_reward=-114.92 +/- 3.14
Episode length: 24.40 +/- 8.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 24.4       |
|    mean_reward          | -115       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.26656443 |
|    clip_fraction        | 0.0217     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00449   |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.00759    |
|    loss                 | 162        |
|    n_updates            | 290        |
|    policy_gradient_loss | 0.00354    |
|    value_loss           | 841        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | -81.1    |
| time/              |          |
|    fps             | 703      |
|    iterations      | 30       |
|    time_elapsed    | 87       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:47:15,221] Trial 28 finished with value: -63.974205000000005 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 0.00759219589052604, 'gamma': 0.9510106323339204, 'gae_lambda': 0.8267990659047846}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.96 +/- 0.03
Episode length: 25.20 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | -87.8    |
| time/              |          |
|    fps             | 1165     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-115.91 +/- 0.07
Episode length: 18.90 +/- 6.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.9        |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.017725775 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.000101    |
|    learning_rate        | 1.63e-05    |
|    loss                 | 536         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 1e+03       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-115.96 +/- 0.02
Episode length: 23.30 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | -79.4    |
| time/              |          |
|    fps             | 1043     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=256.60 +/- 234.02
Episode length: 16.40 +/- 6.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | 257         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.013452949 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.00548     |
|    learning_rate        | 1.63e-05    |
|    loss                 | 409         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 992         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.3     |
|    ep_rew_mean     | -69.7    |
| time/              |          |
|    fps             | 1019     |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=206.66 +/- 250.70
Episode length: 15.80 +/- 6.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.8       |
|    mean_reward          | 207        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01314677 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2         |
|    explained_variance   | 0.00806    |
|    learning_rate        | 1.63e-05   |
|    loss                 | 556        |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 1.07e+03   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=228.79 +/- 144.56
Episode length: 16.10 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.9     |
|    ep_rew_mean     | -42      |
| time/              |          |
|    fps             | 1002     |
|    iterations      | 4        |
|    time_elapsed    | 32       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=311.82 +/- 219.12
Episode length: 18.20 +/- 6.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | 312          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0121137565 |
|    clip_fraction        | 0.18         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.00843      |
|    learning_rate        | 1.63e-05     |
|    loss                 | 639          |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0191      |
|    value_loss           | 1.29e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=207.54 +/- 157.88
Episode length: 15.30 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | -33.1    |
| time/              |          |
|    fps             | 993      |
|    iterations      | 5        |
|    time_elapsed    | 41       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=204.06 +/- 141.91
Episode length: 15.10 +/- 4.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.015501957 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.0256      |
|    learning_rate        | 1.63e-05    |
|    loss                 | 619         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 1.38e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -8.25    |
| time/              |          |
|    fps             | 988      |
|    iterations      | 6        |
|    time_elapsed    | 49       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=200.22 +/- 183.57
Episode length: 15.00 +/- 4.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15         |
|    mean_reward          | 200        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.01472518 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.8       |
|    explained_variance   | 0.0537     |
|    learning_rate        | 1.63e-05   |
|    loss                 | 655        |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.019     |
|    value_loss           | 1.51e+03   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=214.20 +/- 140.10
Episode length: 15.20 +/- 3.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.8     |
|    ep_rew_mean     | -14.5    |
| time/              |          |
|    fps             | 981      |
|    iterations      | 7        |
|    time_elapsed    | 58       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=174.91 +/- 135.51
Episode length: 14.80 +/- 3.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.016023617 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.1         |
|    learning_rate        | 1.63e-05    |
|    loss                 | 768         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=334.12 +/- 249.65
Episode length: 18.50 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.5     |
|    mean_reward     | 334      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 16.1     |
| time/              |          |
|    fps             | 978      |
|    iterations      | 8        |
|    time_elapsed    | 66       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:48:24,010] Trial 29 finished with value: 134.8264389 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 1.627177492719803e-05, 'gamma': 0.9112961264117679, 'gae_lambda': 0.9584594389522396}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.94 +/- 0.04
Episode length: 24.50 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | -88.2    |
| time/              |          |
|    fps             | 1147     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-115.94 +/- 0.05
Episode length: 26.40 +/- 6.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.4        |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009329125 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -7.33e-05   |
|    learning_rate        | 4.7e-05     |
|    loss                 | 514         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00628    |
|    value_loss           | 1.26e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-115.96 +/- 0.02
Episode length: 31.10 +/- 7.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.1     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | -83.2    |
| time/              |          |
|    fps             | 1040     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=321.49 +/- 274.15
Episode length: 18.30 +/- 7.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 321         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011938148 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.00961     |
|    learning_rate        | 4.7e-05     |
|    loss                 | 663         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 1.16e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | -64.6    |
| time/              |          |
|    fps             | 1015     |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=256.30 +/- 200.41
Episode length: 17.10 +/- 5.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.1        |
|    mean_reward          | 256         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015603806 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.0476      |
|    learning_rate        | 4.7e-05     |
|    loss                 | 635         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=141.24 +/- 140.44
Episode length: 13.90 +/- 3.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.9     |
|    mean_reward     | 141      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | -55.9    |
| time/              |          |
|    fps             | 1001     |
|    iterations      | 4        |
|    time_elapsed    | 32       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=154.23 +/- 155.11
Episode length: 13.70 +/- 4.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.7        |
|    mean_reward          | 154         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014550321 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.106       |
|    learning_rate        | 4.7e-05     |
|    loss                 | 779         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 1.46e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=347.30 +/- 212.42
Episode length: 19.60 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.6     |
|    mean_reward     | 347      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | -43.2    |
| time/              |          |
|    fps             | 991      |
|    iterations      | 5        |
|    time_elapsed    | 41       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=331.22 +/- 160.64
Episode length: 18.30 +/- 4.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 331         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.011429217 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.175       |
|    learning_rate        | 4.7e-05     |
|    loss                 | 701         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 1.42e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -13.9    |
| time/              |          |
|    fps             | 986      |
|    iterations      | 6        |
|    time_elapsed    | 49       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=237.48 +/- 154.09
Episode length: 16.10 +/- 4.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.013229242 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.268       |
|    learning_rate        | 4.7e-05     |
|    loss                 | 833         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 1.58e+03    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=215.26 +/- 159.50
Episode length: 15.40 +/- 3.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 980      |
|    iterations      | 7        |
|    time_elapsed    | 58       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=214.22 +/- 147.04
Episode length: 15.30 +/- 3.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.013472877 |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.229       |
|    learning_rate        | 4.7e-05     |
|    loss                 | 1e+03       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 2.06e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=265.47 +/- 170.77
Episode length: 16.60 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.7     |
|    ep_rew_mean     | 16.7     |
| time/              |          |
|    fps             | 975      |
|    iterations      | 8        |
|    time_elapsed    | 67       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:49:33,010] Trial 30 finished with value: 302.8906219 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 4.697779271456615e-05, 'gamma': 0.9317841602796741, 'gae_lambda': 0.9855416159540753}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=2.18 +/- 109.41
Episode length: 18.20 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | 2.18     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.1     |
|    ep_rew_mean     | -89.2    |
| time/              |          |
|    fps             | 1158     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=194.81 +/- 158.54
Episode length: 15.10 +/- 3.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009306355 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -4.73e-05   |
|    learning_rate        | 5.42e-05    |
|    loss                 | 435         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00893    |
|    value_loss           | 988         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=241.97 +/- 224.07
Episode length: 16.20 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -87.3    |
| time/              |          |
|    fps             | 1048     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=289.04 +/- 153.32
Episode length: 17.80 +/- 4.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.8        |
|    mean_reward          | 289         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012694653 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.0433      |
|    learning_rate        | 5.42e-05    |
|    loss                 | 303         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 782         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -60      |
| time/              |          |
|    fps             | 1018     |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=270.16 +/- 217.32
Episode length: 17.50 +/- 6.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | 270         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012752021 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.188       |
|    learning_rate        | 5.42e-05    |
|    loss                 | 374         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 749         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=144.49 +/- 137.52
Episode length: 13.90 +/- 3.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.9     |
|    mean_reward     | 144      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | -40.9    |
| time/              |          |
|    fps             | 998      |
|    iterations      | 4        |
|    time_elapsed    | 32       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=197.74 +/- 151.66
Episode length: 15.10 +/- 3.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 198         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.011390573 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.27        |
|    learning_rate        | 5.42e-05    |
|    loss                 | 393         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 817         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=338.43 +/- 161.04
Episode length: 18.50 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.5     |
|    mean_reward     | 338      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -25.1    |
| time/              |          |
|    fps             | 986      |
|    iterations      | 5        |
|    time_elapsed    | 41       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=315.85 +/- 214.55
Episode length: 18.50 +/- 6.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.5        |
|    mean_reward          | 316         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.014480469 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.306       |
|    learning_rate        | 5.42e-05    |
|    loss                 | 419         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 932         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | -17.3    |
| time/              |          |
|    fps             | 980      |
|    iterations      | 6        |
|    time_elapsed    | 50       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=182.35 +/- 160.23
Episode length: 15.00 +/- 5.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.013893529 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.336       |
|    learning_rate        | 5.42e-05    |
|    loss                 | 390         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 984         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=217.31 +/- 211.64
Episode length: 15.60 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | -2.39    |
| time/              |          |
|    fps             | 975      |
|    iterations      | 7        |
|    time_elapsed    | 58       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=244.97 +/- 248.55
Episode length: 16.80 +/- 6.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | 245         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.015069824 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.319       |
|    learning_rate        | 5.42e-05    |
|    loss                 | 465         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 1.19e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=141.69 +/- 130.78
Episode length: 13.30 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.3     |
|    mean_reward     | 142      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 22.7     |
| time/              |          |
|    fps             | 972      |
|    iterations      | 8        |
|    time_elapsed    | 67       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:50:42,212] Trial 31 finished with value: 288.92539980000004 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 5.4180327177966224e-05, 'gamma': 0.9314050907316455, 'gae_lambda': 0.9245162243532998}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.98 +/- 0.00
Episode length: 19.80 +/- 2.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.8     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | -81      |
| time/              |          |
|    fps             | 1161     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=245.05 +/- 155.86
Episode length: 17.50 +/- 4.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | 245         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011884786 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.000119    |
|    learning_rate        | 3.58e-05    |
|    loss                 | 697         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 1.4e+03     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=185.42 +/- 151.49
Episode length: 14.90 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.9     |
|    ep_rew_mean     | -81.7    |
| time/              |          |
|    fps             | 1051     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=286.47 +/- 151.23
Episode length: 17.50 +/- 4.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011113107 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.0109      |
|    learning_rate        | 3.58e-05    |
|    loss                 | 533         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 1.19e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -69.2    |
| time/              |          |
|    fps             | 1023     |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=279.81 +/- 149.48
Episode length: 16.70 +/- 3.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | 280          |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0117961615 |
|    clip_fraction        | 0.17         |
|    clip_range           | 0.2          |
|    entropy_loss         | -2           |
|    explained_variance   | 0.0308       |
|    learning_rate        | 3.58e-05     |
|    loss                 | 684          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.015       |
|    value_loss           | 1.48e+03     |
------------------------------------------
Eval num_timesteps=30000, episode_reward=128.69 +/- 100.11
Episode length: 13.10 +/- 2.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.1     |
|    mean_reward     | 129      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -44.1    |
| time/              |          |
|    fps             | 1006     |
|    iterations      | 4        |
|    time_elapsed    | 32       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=151.70 +/- 117.73
Episode length: 14.00 +/- 4.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14          |
|    mean_reward          | 152         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012164449 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.0644      |
|    learning_rate        | 3.58e-05    |
|    loss                 | 755         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=223.95 +/- 146.29
Episode length: 16.50 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -27.2    |
| time/              |          |
|    fps             | 995      |
|    iterations      | 5        |
|    time_elapsed    | 41       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=189.90 +/- 200.58
Episode length: 14.80 +/- 6.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.013883493 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.131       |
|    learning_rate        | 3.58e-05    |
|    loss                 | 731         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 1.6e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | -21      |
| time/              |          |
|    fps             | 991      |
|    iterations      | 6        |
|    time_elapsed    | 49       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=196.56 +/- 180.63
Episode length: 15.00 +/- 4.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15         |
|    mean_reward          | 197        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.01329824 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.76      |
|    explained_variance   | 0.19       |
|    learning_rate        | 3.58e-05   |
|    loss                 | 950        |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 1.79e+03   |
----------------------------------------
Eval num_timesteps=55000, episode_reward=130.63 +/- 94.34
Episode length: 13.20 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.2     |
|    mean_reward     | 131      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | -6.92    |
| time/              |          |
|    fps             | 986      |
|    iterations      | 7        |
|    time_elapsed    | 58       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=273.66 +/- 168.34
Episode length: 17.50 +/- 4.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.014877757 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.275       |
|    learning_rate        | 3.58e-05    |
|    loss                 | 898         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 1.89e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=277.79 +/- 188.09
Episode length: 17.00 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 26.6     |
| time/              |          |
|    fps             | 981      |
|    iterations      | 8        |
|    time_elapsed    | 66       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:51:50,821] Trial 32 finished with value: 175.8693893 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 3.578156823319888e-05, 'gamma': 0.9222737688951923, 'gae_lambda': 0.9884324495792022}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-87.24 +/- 42.04
Episode length: 26.40 +/- 9.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | -87.2    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | -83.2    |
| time/              |          |
|    fps             | 1149     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=286.91 +/- 179.46
Episode length: 17.50 +/- 4.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | 287         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012686182 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000137   |
|    learning_rate        | 1.39e-05    |
|    loss                 | 428         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 949         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=164.26 +/- 114.46
Episode length: 14.50 +/- 3.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.8     |
|    ep_rew_mean     | -80      |
| time/              |          |
|    fps             | 1047     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=167.31 +/- 154.35
Episode length: 13.60 +/- 4.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.6        |
|    mean_reward          | 167         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012239395 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.00191     |
|    learning_rate        | 1.39e-05    |
|    loss                 | 441         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 825         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -68.6    |
| time/              |          |
|    fps             | 1017     |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=253.85 +/- 141.64
Episode length: 16.10 +/- 3.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.1       |
|    mean_reward          | 254        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01568631 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.02      |
|    explained_variance   | 0.0115     |
|    learning_rate        | 1.39e-05   |
|    loss                 | 386        |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0212    |
|    value_loss           | 821        |
----------------------------------------
Eval num_timesteps=30000, episode_reward=222.02 +/- 188.31
Episode length: 16.40 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | -53.7    |
| time/              |          |
|    fps             | 1000     |
|    iterations      | 4        |
|    time_elapsed    | 32       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=213.70 +/- 151.77
Episode length: 15.60 +/- 3.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.015612105 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.0169      |
|    learning_rate        | 1.39e-05    |
|    loss                 | 439         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 828         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=230.57 +/- 156.39
Episode length: 15.90 +/- 3.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | -49.5    |
| time/              |          |
|    fps             | 990      |
|    iterations      | 5        |
|    time_elapsed    | 41       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=264.59 +/- 189.16
Episode length: 16.60 +/- 4.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | 265         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.012647303 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.0277      |
|    learning_rate        | 1.39e-05    |
|    loss                 | 498         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 951         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | -17.4    |
| time/              |          |
|    fps             | 985      |
|    iterations      | 6        |
|    time_elapsed    | 49       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=163.33 +/- 182.27
Episode length: 13.90 +/- 5.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.9        |
|    mean_reward          | 163         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.013446755 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.05        |
|    learning_rate        | 1.39e-05    |
|    loss                 | 619         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=216.72 +/- 158.28
Episode length: 15.10 +/- 3.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | -2.94    |
| time/              |          |
|    fps             | 979      |
|    iterations      | 7        |
|    time_elapsed    | 58       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=341.61 +/- 192.90
Episode length: 19.00 +/- 5.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 342         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.014448615 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.0747      |
|    learning_rate        | 1.39e-05    |
|    loss                 | 684         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 1.32e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=65000, episode_reward=320.59 +/- 157.41
Episode length: 17.90 +/- 3.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -2.79    |
| time/              |          |
|    fps             | 974      |
|    iterations      | 8        |
|    time_elapsed    | 67       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:52:59,867] Trial 33 finished with value: 110.70233160000001 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 1.3897218800632893e-05, 'gamma': 0.9328805448111763, 'gae_lambda': 0.8891226099513861}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-115.77 +/- 0.61
Episode length: 27.80 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | -86.7    |
| time/              |          |
|    fps             | 1154     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=133.85 +/- 123.81
Episode length: 13.50 +/- 3.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.5        |
|    mean_reward          | 134         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010738244 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.000137    |
|    learning_rate        | 2.63e-05    |
|    loss                 | 381         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 897         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=204.38 +/- 141.29
Episode length: 15.50 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -90      |
| time/              |          |
|    fps             | 1051     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=254.27 +/- 199.66
Episode length: 16.70 +/- 5.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | 254         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012618962 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.00631     |
|    learning_rate        | 2.63e-05    |
|    loss                 | 370         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 792         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | -63.1    |
| time/              |          |
|    fps             | 1022     |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=141.93 +/- 91.92
Episode length: 13.50 +/- 2.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.5        |
|    mean_reward          | 142         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.014091527 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.0203      |
|    learning_rate        | 2.63e-05    |
|    loss                 | 366         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 802         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=276.94 +/- 133.44
Episode length: 17.70 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | 277      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -60.2    |
| time/              |          |
|    fps             | 1004     |
|    iterations      | 4        |
|    time_elapsed    | 32       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=197.10 +/- 137.69
Episode length: 15.40 +/- 4.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014629109 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.0417      |
|    learning_rate        | 2.63e-05    |
|    loss                 | 380         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 841         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=189.59 +/- 159.69
Episode length: 15.30 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | -51.8    |
| time/              |          |
|    fps             | 992      |
|    iterations      | 5        |
|    time_elapsed    | 41       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=105.59 +/- 44.40
Episode length: 13.10 +/- 2.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.1        |
|    mean_reward          | 106         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.013268204 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.08        |
|    learning_rate        | 2.63e-05    |
|    loss                 | 329         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 851         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | -23.1    |
| time/              |          |
|    fps             | 989      |
|    iterations      | 6        |
|    time_elapsed    | 49       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=288.10 +/- 151.95
Episode length: 17.20 +/- 4.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.2        |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.014038199 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.152       |
|    learning_rate        | 2.63e-05    |
|    loss                 | 452         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0295     |
|    value_loss           | 915         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=152.82 +/- 164.52
Episode length: 14.30 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.3     |
|    mean_reward     | 153      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | -7.41    |
| time/              |          |
|    fps             | 983      |
|    iterations      | 7        |
|    time_elapsed    | 58       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=169.63 +/- 124.33
Episode length: 14.00 +/- 2.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14          |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.015819032 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.238       |
|    learning_rate        | 2.63e-05    |
|    loss                 | 609         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 1.01e+03    |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=236.94 +/- 162.30
Episode length: 16.40 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 23.1     |
| time/              |          |
|    fps             | 978      |
|    iterations      | 8        |
|    time_elapsed    | 66       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:54:09,119] Trial 34 finished with value: 281.9552794 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 2.633934840665336e-05, 'gamma': 0.9212479919997246, 'gae_lambda': 0.8984167032868986}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
Eval num_timesteps=5000, episode_reward=-50.25 +/- 50.97
Episode length: 15.20 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -50.2    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | -88.6    |
| time/              |          |
|    fps             | 1158     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=102.70 +/- 56.61
Episode length: 12.80 +/- 2.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 12.8        |
|    mean_reward          | 103         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011569593 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 5.08e-05    |
|    learning_rate        | 8.36e-05    |
|    loss                 | 254         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 708         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=335.81 +/- 229.45
Episode length: 18.70 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.7     |
|    mean_reward     | 336      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | -73.5    |
| time/              |          |
|    fps             | 1047     |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=239.73 +/- 258.99
Episode length: 15.90 +/- 7.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | 240         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011671825 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.105       |
|    learning_rate        | 8.36e-05    |
|    loss                 | 253         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 586         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | -53.5    |
| time/              |          |
|    fps             | 1020     |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=168.53 +/- 171.15
Episode length: 13.90 +/- 4.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.9        |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012688704 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.277       |
|    learning_rate        | 8.36e-05    |
|    loss                 | 343         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 618         |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=336.54 +/- 203.16
Episode length: 18.60 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -40.7    |
| time/              |          |
|    fps             | 1002     |
|    iterations      | 4        |
|    time_elapsed    | 32       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=209.30 +/- 184.60
Episode length: 15.50 +/- 6.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | 209         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.015221661 |
|    clip_fraction        | 0.29        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.347       |
|    learning_rate        | 8.36e-05    |
|    loss                 | 271         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 635         |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=244.34 +/- 128.79
Episode length: 16.60 +/- 3.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | 244      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | -28.1    |
| time/              |          |
|    fps             | 993      |
|    iterations      | 5        |
|    time_elapsed    | 41       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=280.77 +/- 176.69
Episode length: 17.30 +/- 4.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 281        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.01453138 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.81      |
|    explained_variance   | 0.383      |
|    learning_rate        | 8.36e-05   |
|    loss                 | 353        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0249    |
|    value_loss           | 726        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 987      |
|    iterations      | 6        |
|    time_elapsed    | 49       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=274.77 +/- 201.79
Episode length: 17.50 +/- 5.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | 275         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.015678927 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.423       |
|    learning_rate        | 8.36e-05    |
|    loss                 | 440         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0287     |
|    value_loss           | 809         |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=95.41 +/- 105.61
Episode length: 12.90 +/- 2.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 12.9     |
|    mean_reward     | 95.4     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 981      |
|    iterations      | 7        |
|    time_elapsed    | 58       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=226.92 +/- 126.38
Episode length: 16.10 +/- 3.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | 227         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.016284337 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.351       |
|    learning_rate        | 8.36e-05    |
|    loss                 | 368         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0291     |
|    value_loss           | 956         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=202.85 +/- 142.59
Episode length: 15.50 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | 203      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.7     |
|    ep_rew_mean     | 49.4     |
| time/              |          |
|    fps             | 977      |
|    iterations      | 8        |
|    time_elapsed    | 67       |
|    total_timesteps | 65536    |
---------------------------------
[I 2024-07-20 22:55:17,986] Trial 35 finished with value: 247.42360080000003 and parameters: {'n_steps': 8192, 'batch_size': 128, 'learning_rate': 8.35915442701744e-05, 'gamma': 0.9002357135742762, 'gae_lambda': 0.8624603461436829}. Best is trial 7 with value: 334.35086079999996.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.9     |
|    ep_rew_mean     | -89      |
| time/              |          |
|    fps             | 1174     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 25.8         |
|    ep_rew_mean          | -85.3        |
| time/                   |              |
|    fps                  | 1072         |
|    iterations           | 2            |
|    time_elapsed         | 3            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0010988242 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.08        |
|    explained_variance   | 3.73e-05     |
|    learning_rate        | 1.06e-05     |
|    loss                 | 673          |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00283     |
|    value_loss           | 1.19e+03     |
------------------------------------------
Eval num_timesteps=5000, episode_reward=307.36 +/- 215.46
Episode length: 18.60 +/- 5.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.6        |
|    mean_reward          | 307         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010092371 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.000102    |
|    learning_rate        | 1.06e-05    |
|    loss                 | 601         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0094     |
|    value_loss           | 1.2e+03     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | -71.6    |
| time/              |          |
|    fps             | 1019     |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.7        |
|    ep_rew_mean          | -49.1       |
| time/                   |             |
|    fps                  | 1008        |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.013256504 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | -0.00246    |
|    learning_rate        | 1.06e-05    |
|    loss                 | 438         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0098     |
|    value_loss           | 935         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=149.85 +/- 106.31
Episode length: 13.80 +/- 2.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.8        |
|    mean_reward          | 150         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011337433 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.00171     |
|    learning_rate        | 1.06e-05    |
|    loss                 | 419         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 959         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | -50.5    |
| time/              |          |
|    fps             | 992      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 21.4         |
|    ep_rew_mean          | -30          |
| time/                   |              |
|    fps                  | 990          |
|    iterations           | 6            |
|    time_elapsed         | 12           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0135345645 |
|    clip_fraction        | 0.15         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -0.00062     |
|    learning_rate        | 1.06e-05     |
|    loss                 | 406          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0168      |
|    value_loss           | 906          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | -6.79       |
| time/                   |             |
|    fps                  | 988         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.014927111 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.000477    |
|    learning_rate        | 1.06e-05    |
|    loss                 | 635         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=206.75 +/- 162.14
Episode length: 15.10 +/- 4.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 207         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.013350284 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.00717     |
|    learning_rate        | 1.06e-05    |
|    loss                 | 682         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 1.3e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.7     |
|    ep_rew_mean     | 20.5     |
| time/              |          |
|    fps             | 979      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | -17.2       |
| time/                   |             |
|    fps                  | 978         |
|    iterations           | 9           |
|    time_elapsed         | 18          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.014144662 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.00367     |
|    learning_rate        | 1.06e-05    |
|    loss                 | 720         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.0321      |
|    value_loss           | 1.86e+03    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=261.48 +/- 133.11
Episode length: 16.50 +/- 3.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.014263166 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.00707     |
|    learning_rate        | 1.06e-05    |
|    loss                 | 605         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 1.35e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 15       |
| time/              |          |
|    fps             | 971      |
|    iterations      | 10       |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | -14.9       |
| time/                   |             |
|    fps                  | 971         |
|    iterations           | 11          |
|    time_elapsed         | 23          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.007979312 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.0101      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 721         |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.0171      |
|    value_loss           | 1.6e+03     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 16.5        |
| time/                   |             |
|    fps                  | 970         |
|    iterations           | 12          |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014202852 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.0134      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 868         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 1.39e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=158.69 +/- 102.66
Episode length: 14.50 +/- 3.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 159         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009141421 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.0163      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 968         |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.0216      |
|    value_loss           | 1.85e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 0.66     |
| time/              |          |
|    fps             | 966      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.8        |
|    ep_rew_mean          | 33.1        |
| time/                   |             |
|    fps                  | 966         |
|    iterations           | 14          |
|    time_elapsed         | 29          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.016136032 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.0169      |
|    learning_rate        | 1.06e-05    |
|    loss                 | 776         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 1.71e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=251.75 +/- 227.05
Episode length: 16.40 +/- 6.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.4       |
|    mean_reward          | 252        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.01755127 |
|    clip_fraction        | 0.0422     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.63      |
|    explained_variance   | 0.0223     |
|    learning_rate        | 1.06e-05   |
|    loss                 | 985        |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.00339    |
|    value_loss           | 2.04e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 62.6     |
| time/              |          |
|    fps             | 963      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.1       |
|    ep_rew_mean          | 163        |
| time/                   |            |
|    fps                  | 963        |
|    iterations           | 16         |
|    time_elapsed         | 34         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.17148273 |
|    clip_fraction        | 0.667      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.0213     |
|    learning_rate        | 1.06e-05   |
|    loss                 | 1.02e+03   |
|    n_updates            | 150        |
|    policy_gradient_loss | 0.0161     |
|    value_loss           | 2.39e+03   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15.5       |
|    ep_rew_mean          | 202        |
| time/                   |            |
|    fps                  | 962        |
|    iterations           | 17         |
|    time_elapsed         | 36         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.31203458 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.401     |
|    explained_variance   | 0.0101     |
|    learning_rate        | 1.06e-05   |
|    loss                 | 2.11e+03   |
|    n_updates            | 160        |
|    policy_gradient_loss | 0.00777    |
|    value_loss           | 5.14e+03   |
----------------------------------------
Eval num_timesteps=35000, episode_reward=308.00 +/- 170.43
Episode length: 18.20 +/- 4.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.2        |
|    mean_reward          | 308         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.034644812 |
|    clip_fraction        | 0.0121      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0206     |
|    explained_variance   | 0.00199     |
|    learning_rate        | 1.06e-05    |
|    loss                 | 3.56e+03    |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.000292   |
|    value_loss           | 6.62e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | 237      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | 213          |
| time/                   |              |
|    fps                  | 960          |
|    iterations           | 19           |
|    time_elapsed         | 40           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 3.859168e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000878    |
|    explained_variance   | 0.000478     |
|    learning_rate        | 1.06e-05     |
|    loss                 | 4.35e+03     |
|    n_updates            | 180          |
|    policy_gradient_loss | -6.91e-06    |
|    value_loss           | 8.62e+03     |
------------------------------------------
Eval num_timesteps=40000, episode_reward=214.82 +/- 122.25
Episode length: 16.20 +/- 3.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.2      |
|    mean_reward          | 215       |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000171 |
|    explained_variance   | 0.000703  |
|    learning_rate        | 1.06e-05  |
|    loss                 | 4.51e+03  |
|    n_updates            | 190       |
|    policy_gradient_loss | -3.22e-07 |
|    value_loss           | 8.69e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | 236      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 20       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 16        |
|    ep_rew_mean          | 229       |
| time/                   |           |
|    fps                  | 959       |
|    iterations           | 21        |
|    time_elapsed         | 44        |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -9.03e-05 |
|    explained_variance   | 0.000381  |
|    learning_rate        | 1.06e-05  |
|    loss                 | 5.38e+03  |
|    n_updates            | 200       |
|    policy_gradient_loss | -2.12e-07 |
|    value_loss           | 9.54e+03  |
---------------------------------------
Eval num_timesteps=45000, episode_reward=238.47 +/- 198.98
Episode length: 16.30 +/- 5.68
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 16.3           |
|    mean_reward          | 238            |
| time/                   |                |
|    total_timesteps      | 45000          |
| train/                  |                |
|    approx_kl            | -4.0745363e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -8.67e-05      |
|    explained_variance   | 0.00451        |
|    learning_rate        | 1.06e-05       |
|    loss                 | 4.95e+03       |
|    n_updates            | 210            |
|    policy_gradient_loss | 6.25e-09       |
|    value_loss           | 9.35e+03       |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 250      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 22       |
|    time_elapsed    | 47       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 16.6      |
|    ep_rew_mean          | 248       |
| time/                   |           |
|    fps                  | 958       |
|    iterations           | 23        |
|    time_elapsed         | 49        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.41e-05 |
|    explained_variance   | 0.00402   |
|    learning_rate        | 1.06e-05  |
|    loss                 | 5.01e+03  |
|    n_updates            | 220       |
|    policy_gradient_loss | -1.26e-07 |
|    value_loss           | 9.8e+03   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 15.2      |
|    ep_rew_mean          | 201       |
| time/                   |           |
|    fps                  | 959       |
|    iterations           | 24        |
|    time_elapsed         | 51        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.71e-05 |
|    explained_variance   | 0.0056    |
|    learning_rate        | 1.06e-05  |
|    loss                 | 5.71e+03  |
|    n_updates            | 230       |
|    policy_gradient_loss | -7.17e-08 |
|    value_loss           | 1.06e+04  |
---------------------------------------
Eval num_timesteps=50000, episode_reward=229.88 +/- 159.78
Episode length: 15.90 +/- 4.61
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 15.9           |
|    mean_reward          | 230            |
| time/                   |                |
|    total_timesteps      | 50000          |
| train/                  |                |
|    approx_kl            | -1.9033905e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.53e-05      |
|    explained_variance   | 0.0122         |
|    learning_rate        | 1.06e-05       |
|    loss                 | 4.77e+03       |
|    n_updates            | 240            |
|    policy_gradient_loss | 2.48e-08       |
|    value_loss           | 9.9e+03        |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | 217      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 25       |
|    time_elapsed    | 53       |
|    total_timesteps | 51200    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 16.8      |
|    ep_rew_mean          | 259       |
| time/                   |           |
|    fps                  | 958       |
|    iterations           | 26        |
|    time_elapsed         | 55        |
|    total_timesteps      | 53248     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.88e-05 |
|    explained_variance   | 0.012     |
|    learning_rate        | 1.06e-05  |
|    loss                 | 4.7e+03   |
|    n_updates            | 250       |
|    policy_gradient_loss | 1.11e-08  |
|    value_loss           | 9.6e+03   |
---------------------------------------
Eval num_timesteps=55000, episode_reward=194.34 +/- 160.54
Episode length: 15.10 +/- 4.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 15.1      |
|    mean_reward          | 194       |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.62e-05 |
|    explained_variance   | 0.0113    |
|    learning_rate        | 1.06e-05  |
|    loss                 | 5.27e+03  |
|    n_updates            | 260       |
|    policy_gradient_loss | -3.19e-08 |
|    value_loss           | 9.95e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 234      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 27       |
|    time_elapsed    | 57       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 15.7      |
|    ep_rew_mean          | 217       |
| time/                   |           |
|    fps                  | 958       |
|    iterations           | 28        |
|    time_elapsed         | 59        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.48e-05 |
|    explained_variance   | 0.0192    |
|    learning_rate        | 1.06e-05  |
|    loss                 | 5.29e+03  |
|    n_updates            | 270       |
|    policy_gradient_loss | 1.26e-08  |
|    value_loss           | 9.96e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 16.2      |
|    ep_rew_mean          | 233       |
| time/                   |           |
|    fps                  | 958       |
|    iterations           | 29        |
|    time_elapsed         | 61        |
|    total_timesteps      | 59392     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.31e-05 |
|    explained_variance   | 0.0215    |
|    learning_rate        | 1.06e-05  |
|    loss                 | 4.63e+03  |
|    n_updates            | 280       |
|    policy_gradient_loss | -1.09e-08 |
|    value_loss           | 9.8e+03   |
---------------------------------------
Eval num_timesteps=60000, episode_reward=208.22 +/- 203.19
Episode length: 15.10 +/- 4.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 15.1      |
|    mean_reward          | 208       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.17e-05 |
|    explained_variance   | 0.0262    |
|    learning_rate        | 1.06e-05  |
|    loss                 | 5.28e+03  |
|    n_updates            | 290       |
|    policy_gradient_loss | -7.65e-10 |
|    value_loss           | 9.63e+03  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 223      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 30       |
|    time_elapsed    | 64       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:56:22,945] Trial 36 finished with value: 346.24260259999994 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 1.0633435587786441e-05, 'gamma': 0.963521923100097, 'gae_lambda': 0.8794084939045616}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.7     |
|    ep_rew_mean     | -88.2    |
| time/              |          |
|    fps             | 1161     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.2        |
|    ep_rew_mean          | -83.9       |
| time/                   |             |
|    fps                  | 1061        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.014224308 |
|    clip_fraction        | 0.0664      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.000183    |
|    learning_rate        | 2.75e-05    |
|    loss                 | 1.62e+03    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 3.49e+03    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=171.87 +/- 116.61
Episode length: 14.30 +/- 3.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.3        |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012516385 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | -0.00301    |
|    learning_rate        | 2.75e-05    |
|    loss                 | 1e+03       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00613    |
|    value_loss           | 1.72e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | -71.5    |
| time/              |          |
|    fps             | 1006     |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.4        |
|    ep_rew_mean          | -58         |
| time/                   |             |
|    fps                  | 999         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.010091074 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.00419     |
|    learning_rate        | 2.75e-05    |
|    loss                 | 705         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00976    |
|    value_loss           | 1.65e+03    |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=286.13 +/- 238.22
Episode length: 17.20 +/- 6.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.2        |
|    mean_reward          | 286         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012618944 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.00505     |
|    learning_rate        | 2.75e-05    |
|    loss                 | 1.3e+03     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 2.27e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | -30.9    |
| time/              |          |
|    fps             | 981      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22          |
|    ep_rew_mean          | -12.1       |
| time/                   |             |
|    fps                  | 981         |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.013725401 |
|    clip_fraction        | 0.0659      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.00639     |
|    learning_rate        | 2.75e-05    |
|    loss                 | 1.5e+03     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00133    |
|    value_loss           | 3.41e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 979         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.013618442 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.00637     |
|    learning_rate        | 2.75e-05    |
|    loss                 | 1.26e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 2.99e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=254.91 +/- 229.35
Episode length: 17.10 +/- 6.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.1        |
|    mean_reward          | 255         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.005940258 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.00878     |
|    learning_rate        | 2.75e-05    |
|    loss                 | 2.07e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.0151      |
|    value_loss           | 3.9e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 4.1      |
| time/              |          |
|    fps             | 972      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 43.9        |
| time/                   |             |
|    fps                  | 972         |
|    iterations           | 9           |
|    time_elapsed         | 18          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.016667787 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.0115      |
|    learning_rate        | 2.75e-05    |
|    loss                 | 1.59e+03    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00413    |
|    value_loss           | 3.67e+03    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=231.49 +/- 139.42
Episode length: 15.60 +/- 3.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | 231         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.056757636 |
|    clip_fraction        | 0.534       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.47       |
|    explained_variance   | 0.0246      |
|    learning_rate        | 2.75e-05    |
|    loss                 | 2.29e+03    |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.018       |
|    value_loss           | 5.06e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.5     |
|    ep_rew_mean     | 72.2     |
| time/              |          |
|    fps             | 967      |
|    iterations      | 10       |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 16.7      |
|    ep_rew_mean          | 79.9      |
| time/                   |           |
|    fps                  | 967       |
|    iterations           | 11        |
|    time_elapsed         | 23        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.0244377 |
|    clip_fraction        | 0.142     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.27     |
|    explained_variance   | 0.0266    |
|    learning_rate        | 2.75e-05  |
|    loss                 | 3.62e+03  |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.000569 |
|    value_loss           | 6.09e+03  |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17          |
|    ep_rew_mean          | 98          |
| time/                   |             |
|    fps                  | 967         |
|    iterations           | 12          |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.005856443 |
|    clip_fraction        | 0.0798      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.0379      |
|    learning_rate        | 2.75e-05    |
|    loss                 | 2.88e+03    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 5.72e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=253.01 +/- 142.20
Episode length: 17.30 +/- 3.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | 253          |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0056811697 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0.0357       |
|    learning_rate        | 2.75e-05     |
|    loss                 | 4.16e+03     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00343     |
|    value_loss           | 8.24e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.7     |
|    ep_rew_mean     | 123      |
| time/              |          |
|    fps             | 962      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.8         |
|    ep_rew_mean          | 122          |
| time/                   |              |
|    fps                  | 963          |
|    iterations           | 14           |
|    time_elapsed         | 29           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0056439065 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.961       |
|    explained_variance   | 0.0506       |
|    learning_rate        | 2.75e-05     |
|    loss                 | 4.57e+03     |
|    n_updates            | 130          |
|    policy_gradient_loss | 0.000707     |
|    value_loss           | 9.31e+03     |
------------------------------------------
Eval num_timesteps=30000, episode_reward=146.08 +/- 125.18
Episode length: 14.10 +/- 3.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.1       |
|    mean_reward          | 146        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.01615779 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.879     |
|    explained_variance   | 0.057      |
|    learning_rate        | 2.75e-05   |
|    loss                 | 4.41e+03   |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.000344   |
|    value_loss           | 9.11e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 961      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 17.1         |
|    ep_rew_mean          | 149          |
| time/                   |              |
|    fps                  | 961          |
|    iterations           | 16           |
|    time_elapsed         | 34           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0036197705 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.806       |
|    explained_variance   | 0.0502       |
|    learning_rate        | 2.75e-05     |
|    loss                 | 6.63e+03     |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000312    |
|    value_loss           | 1.26e+04     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.5        |
|    ep_rew_mean          | 160         |
| time/                   |             |
|    fps                  | 962         |
|    iterations           | 17          |
|    time_elapsed         | 36          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.001416353 |
|    clip_fraction        | 0.0748      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.0632      |
|    learning_rate        | 2.75e-05    |
|    loss                 | 5.56e+03    |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00158    |
|    value_loss           | 1.04e+04    |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=246.33 +/- 204.84
Episode length: 17.10 +/- 5.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.1         |
|    mean_reward          | 246          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0059280493 |
|    clip_fraction        | 0.072        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.761       |
|    explained_variance   | 0.0458       |
|    learning_rate        | 2.75e-05     |
|    loss                 | 8.02e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | 0.000907     |
|    value_loss           | 1.46e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.6     |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16          |
|    ep_rew_mean          | 150         |
| time/                   |             |
|    fps                  | 961         |
|    iterations           | 19          |
|    time_elapsed         | 40          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.005080859 |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.643      |
|    explained_variance   | 0.0868      |
|    learning_rate        | 2.75e-05    |
|    loss                 | 5.08e+03    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00213    |
|    value_loss           | 1.2e+04     |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=172.88 +/- 134.76
Episode length: 14.10 +/- 3.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.1       |
|    mean_reward          | 173        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.00345397 |
|    clip_fraction        | 0.0904     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.55      |
|    explained_variance   | 0.0895     |
|    learning_rate        | 2.75e-05   |
|    loss                 | 6.25e+03   |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.00332   |
|    value_loss           | 1.18e+04   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 155      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 20       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.1         |
|    ep_rew_mean          | 171          |
| time/                   |              |
|    fps                  | 960          |
|    iterations           | 21           |
|    time_elapsed         | 44           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0057333848 |
|    clip_fraction        | 0.0766       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.513       |
|    explained_variance   | 0.104        |
|    learning_rate        | 2.75e-05     |
|    loss                 | 5.84e+03     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00227     |
|    value_loss           | 1.21e+04     |
------------------------------------------
Eval num_timesteps=45000, episode_reward=180.78 +/- 134.65
Episode length: 14.50 +/- 3.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.5         |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0022242223 |
|    clip_fraction        | 0.0461       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.491       |
|    explained_variance   | 0.094        |
|    learning_rate        | 2.75e-05     |
|    loss                 | 6.67e+03     |
|    n_updates            | 210          |
|    policy_gradient_loss | 0.000198     |
|    value_loss           | 1.36e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | 155      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 22       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.8         |
|    ep_rew_mean          | 189          |
| time/                   |              |
|    fps                  | 959          |
|    iterations           | 23           |
|    time_elapsed         | 49           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0020124258 |
|    clip_fraction        | 0.059        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.451       |
|    explained_variance   | 0.0832       |
|    learning_rate        | 2.75e-05     |
|    loss                 | 7.14e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00367     |
|    value_loss           | 1.54e+04     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.8        |
|    ep_rew_mean          | 154         |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 24          |
|    time_elapsed         | 51          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.005315339 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.431      |
|    explained_variance   | 0.146       |
|    learning_rate        | 2.75e-05    |
|    loss                 | 5.77e+03    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00298    |
|    value_loss           | 1.17e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=177.05 +/- 140.10
Episode length: 14.70 +/- 3.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 177          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0020191674 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.447       |
|    explained_variance   | 0.161        |
|    learning_rate        | 2.75e-05     |
|    loss                 | 5.24e+03     |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000229    |
|    value_loss           | 1.01e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 25       |
|    time_elapsed    | 53       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | 209          |
| time/                   |              |
|    fps                  | 959          |
|    iterations           | 26           |
|    time_elapsed         | 55           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0063184183 |
|    clip_fraction        | 0.0584       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.352       |
|    explained_variance   | 0.14         |
|    learning_rate        | 2.75e-05     |
|    loss                 | 6.13e+03     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 1.31e+04     |
------------------------------------------
Eval num_timesteps=55000, episode_reward=211.99 +/- 120.19
Episode length: 15.90 +/- 4.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | 212         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.001944326 |
|    clip_fraction        | 0.0454      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.285      |
|    explained_variance   | 0.171       |
|    learning_rate        | 2.75e-05    |
|    loss                 | 5.08e+03    |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 1.34e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 27       |
|    time_elapsed    | 57       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.6         |
|    ep_rew_mean          | 216          |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 28           |
|    time_elapsed         | 59           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0023337458 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.277       |
|    explained_variance   | 0.17         |
|    learning_rate        | 2.75e-05     |
|    loss                 | 8.19e+03     |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 1.39e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | 192          |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 29           |
|    time_elapsed         | 61           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0008462014 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.227       |
|    explained_variance   | 0.174        |
|    learning_rate        | 2.75e-05     |
|    loss                 | 5.59e+03     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.000204    |
|    value_loss           | 1.41e+04     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=246.44 +/- 225.00
Episode length: 16.10 +/- 5.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | 246         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.003605264 |
|    clip_fraction        | 0.0244      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | 0.192       |
|    learning_rate        | 2.75e-05    |
|    loss                 | 5.18e+03    |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.000451   |
|    value_loss           | 1.23e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 213      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 30       |
|    time_elapsed    | 64       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:57:27,907] Trial 37 finished with value: 230.94007249999999 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 2.7472639056219906e-05, 'gamma': 0.9731941056767868, 'gae_lambda': 0.9893812901719536}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.3     |
|    ep_rew_mean     | -87.2    |
| time/              |          |
|    fps             | 1199     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 43.1         |
|    ep_rew_mean          | -84          |
| time/                   |              |
|    fps                  | 1086         |
|    iterations           | 2            |
|    time_elapsed         | 3            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0018692166 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.08        |
|    explained_variance   | 1.97e-06     |
|    learning_rate        | 1.01e-05     |
|    loss                 | 476          |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00443     |
|    value_loss           | 707          |
------------------------------------------
Eval num_timesteps=5000, episode_reward=198.87 +/- 176.53
Episode length: 15.40 +/- 4.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | 199          |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0118261175 |
|    clip_fraction        | 0.0949       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.07        |
|    explained_variance   | -0.000108    |
|    learning_rate        | 1.01e-05     |
|    loss                 | 380          |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 773          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -79.7    |
| time/              |          |
|    fps             | 1030     |
|    iterations      | 3        |
|    time_elapsed    | 5        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.2        |
|    ep_rew_mean          | -62.4       |
| time/                   |             |
|    fps                  | 1016        |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012307739 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.00119     |
|    learning_rate        | 1.01e-05    |
|    loss                 | 317         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 817         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=239.10 +/- 171.32
Episode length: 15.70 +/- 3.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 239         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.017343702 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.00377     |
|    learning_rate        | 1.01e-05    |
|    loss                 | 441         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 820         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | -55.9    |
| time/              |          |
|    fps             | 997      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | -29         |
| time/                   |             |
|    fps                  | 994         |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.009808561 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.000414    |
|    learning_rate        | 1.01e-05    |
|    loss                 | 397         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 686         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.6        |
|    ep_rew_mean          | -22.8       |
| time/                   |             |
|    fps                  | 991         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.013508607 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.000444    |
|    learning_rate        | 1.01e-05    |
|    loss                 | 370         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 894         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=118.04 +/- 73.58
Episode length: 12.80 +/- 1.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 12.8        |
|    mean_reward          | 118         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.015834874 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.00204     |
|    learning_rate        | 1.01e-05    |
|    loss                 | 505         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 977         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 1.88     |
| time/              |          |
|    fps             | 983      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 27.8        |
| time/                   |             |
|    fps                  | 981         |
|    iterations           | 9           |
|    time_elapsed         | 18          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.011611087 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.00947     |
|    learning_rate        | 1.01e-05    |
|    loss                 | 583         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.000359   |
|    value_loss           | 1.19e+03    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=260.44 +/- 224.60
Episode length: 16.80 +/- 5.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | 260         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.014863897 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.00809     |
|    learning_rate        | 1.01e-05    |
|    loss                 | 567         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.0237      |
|    value_loss           | 1.51e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 21.7     |
| time/              |          |
|    fps             | 974      |
|    iterations      | 10       |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18         |
|    ep_rew_mean          | 38.1       |
| time/                   |            |
|    fps                  | 973        |
|    iterations           | 11         |
|    time_elapsed         | 23         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.01658338 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | 0.0084     |
|    learning_rate        | 1.01e-05   |
|    loss                 | 837        |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0194    |
|    value_loss           | 1.62e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.2        |
|    ep_rew_mean          | 52.4        |
| time/                   |             |
|    fps                  | 974         |
|    iterations           | 12          |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.023941826 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.43       |
|    explained_variance   | 0.00894     |
|    learning_rate        | 1.01e-05    |
|    loss                 | 702         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 1.82e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=163.47 +/- 172.24
Episode length: 15.00 +/- 5.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 163         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.037431963 |
|    clip_fraction        | 0.4         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.0123      |
|    learning_rate        | 1.01e-05    |
|    loss                 | 1.16e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 2.22e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | 103      |
| time/              |          |
|    fps             | 970      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.2       |
|    ep_rew_mean          | 157        |
| time/                   |            |
|    fps                  | 971        |
|    iterations           | 14         |
|    time_elapsed         | 29         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.06861982 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.762     |
|    explained_variance   | 0.00397    |
|    learning_rate        | 1.01e-05   |
|    loss                 | 1.78e+03   |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.00339    |
|    value_loss           | 3.18e+03   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=187.04 +/- 186.22
Episode length: 14.40 +/- 4.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 187         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.019390605 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.478      |
|    explained_variance   | 0.00379     |
|    learning_rate        | 1.01e-05    |
|    loss                 | 2.38e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 4.23e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | 161      |
| time/              |          |
|    fps             | 968      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16          |
|    ep_rew_mean          | 191         |
| time/                   |             |
|    fps                  | 969         |
|    iterations           | 16          |
|    time_elapsed         | 33          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.005831129 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.323      |
|    explained_variance   | 0.00379     |
|    learning_rate        | 1.01e-05    |
|    loss                 | 2.33e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 5.32e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.4         |
|    ep_rew_mean          | 204          |
| time/                   |              |
|    fps                  | 969          |
|    iterations           | 17           |
|    time_elapsed         | 35           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0026739663 |
|    clip_fraction        | 0.04         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.315       |
|    explained_variance   | 0.00613      |
|    learning_rate        | 1.01e-05     |
|    loss                 | 3.33e+03     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00231     |
|    value_loss           | 5.95e+03     |
------------------------------------------
Eval num_timesteps=35000, episode_reward=173.91 +/- 127.24
Episode length: 14.20 +/- 3.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 174          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0045758905 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.249       |
|    explained_variance   | 0.00453      |
|    learning_rate        | 1.01e-05     |
|    loss                 | 3.7e+03      |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00322     |
|    value_loss           | 6.51e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | 235      |
| time/              |          |
|    fps             | 966      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.2         |
|    ep_rew_mean          | 209          |
| time/                   |              |
|    fps                  | 967          |
|    iterations           | 19           |
|    time_elapsed         | 40           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0007904548 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.201       |
|    explained_variance   | 0.00336      |
|    learning_rate        | 1.01e-05     |
|    loss                 | 3.99e+03     |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00069     |
|    value_loss           | 7.28e+03     |
------------------------------------------
Eval num_timesteps=40000, episode_reward=195.85 +/- 156.30
Episode length: 14.70 +/- 4.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0016589111 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.19        |
|    explained_variance   | 0.00426      |
|    learning_rate        | 1.01e-05     |
|    loss                 | 3.63e+03     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.000119    |
|    value_loss           | 7.47e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 965      |
|    iterations      | 20       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15           |
|    ep_rew_mean          | 184          |
| time/                   |              |
|    fps                  | 965          |
|    iterations           | 21           |
|    time_elapsed         | 44           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0016983359 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.0035       |
|    learning_rate        | 1.01e-05     |
|    loss                 | 4.17e+03     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00161     |
|    value_loss           | 7.78e+03     |
------------------------------------------
Eval num_timesteps=45000, episode_reward=314.37 +/- 230.14
Episode length: 18.40 +/- 6.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.4         |
|    mean_reward          | 314          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0005256073 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.0066       |
|    learning_rate        | 1.01e-05     |
|    loss                 | 3.4e+03      |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000492    |
|    value_loss           | 7.79e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 235      |
| time/              |          |
|    fps             | 962      |
|    iterations      | 22       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | 201          |
| time/                   |              |
|    fps                  | 962          |
|    iterations           | 23           |
|    time_elapsed         | 48           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0003690261 |
|    clip_fraction        | 0.00737      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0963      |
|    explained_variance   | 0.0109       |
|    learning_rate        | 1.01e-05     |
|    loss                 | 3.44e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000385    |
|    value_loss           | 7.73e+03     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.2        |
|    ep_rew_mean          | 196         |
| time/                   |             |
|    fps                  | 962         |
|    iterations           | 24          |
|    time_elapsed         | 51          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.000363073 |
|    clip_fraction        | 0.00664     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0862     |
|    explained_variance   | 0.0141      |
|    learning_rate        | 1.01e-05    |
|    loss                 | 4.94e+03    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.000991   |
|    value_loss           | 8.03e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=152.27 +/- 122.54
Episode length: 14.10 +/- 3.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.1          |
|    mean_reward          | 152           |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 0.00062883255 |
|    clip_fraction        | 0.00703       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0765       |
|    explained_variance   | 0.0162        |
|    learning_rate        | 1.01e-05      |
|    loss                 | 3.6e+03       |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.000751     |
|    value_loss           | 7.61e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 218      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 25       |
|    time_elapsed    | 53       |
|    total_timesteps | 51200    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 16.2          |
|    ep_rew_mean          | 236           |
| time/                   |               |
|    fps                  | 961           |
|    iterations           | 26            |
|    time_elapsed         | 55            |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 0.00026912935 |
|    clip_fraction        | 0.0043        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0611       |
|    explained_variance   | 0.0149        |
|    learning_rate        | 1.01e-05      |
|    loss                 | 4.53e+03      |
|    n_updates            | 250           |
|    policy_gradient_loss | -0.000599     |
|    value_loss           | 7.86e+03      |
-------------------------------------------
Eval num_timesteps=55000, episode_reward=165.73 +/- 108.83
Episode length: 13.90 +/- 3.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.9          |
|    mean_reward          | 166           |
| time/                   |               |
|    total_timesteps      | 55000         |
| train/                  |               |
|    approx_kl            | 0.00030259843 |
|    clip_fraction        | 0.00576       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0476       |
|    explained_variance   | 0.0153        |
|    learning_rate        | 1.01e-05      |
|    loss                 | 3.27e+03      |
|    n_updates            | 260           |
|    policy_gradient_loss | -0.000552     |
|    value_loss           | 8.23e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 229      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 27       |
|    time_elapsed    | 57       |
|    total_timesteps | 55296    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 15.6          |
|    ep_rew_mean          | 209           |
| time/                   |               |
|    fps                  | 960           |
|    iterations           | 28            |
|    time_elapsed         | 59            |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00023799145 |
|    clip_fraction        | 0.00278       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0373       |
|    explained_variance   | 0.0242        |
|    learning_rate        | 1.01e-05      |
|    loss                 | 3.37e+03      |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.000575     |
|    value_loss           | 8.25e+03      |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 15.9          |
|    ep_rew_mean          | 222           |
| time/                   |               |
|    fps                  | 960           |
|    iterations           | 29            |
|    time_elapsed         | 61            |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 1.3726356e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0377       |
|    explained_variance   | 0.0313        |
|    learning_rate        | 1.01e-05      |
|    loss                 | 4.07e+03      |
|    n_updates            | 280           |
|    policy_gradient_loss | 5.57e-05      |
|    value_loss           | 7.78e+03      |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=261.84 +/- 141.27
Episode length: 16.30 +/- 4.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.3          |
|    mean_reward          | 262           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 0.00029985484 |
|    clip_fraction        | 0.00298       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0297       |
|    explained_variance   | 0.0293        |
|    learning_rate        | 1.01e-05      |
|    loss                 | 5.13e+03      |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.000338     |
|    value_loss           | 8.01e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 217      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 30       |
|    time_elapsed    | 64       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:58:32,734] Trial 38 finished with value: 197.8266357 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 1.0072699056053218e-05, 'gamma': 0.9610960590632964, 'gae_lambda': 0.8461510997727336}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | -88.8    |
| time/              |          |
|    fps             | 1185     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.8        |
|    ep_rew_mean          | -80.8       |
| time/                   |             |
|    fps                  | 1072        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.005038875 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | -7.52e-05   |
|    learning_rate        | 1.63e-05    |
|    loss                 | 374         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 776         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=294.87 +/- 168.57
Episode length: 17.80 +/- 4.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.8        |
|    mean_reward          | 295         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011438962 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.00193     |
|    learning_rate        | 1.63e-05    |
|    loss                 | 264         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 791         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.8     |
|    ep_rew_mean     | -71.7    |
| time/              |          |
|    fps             | 1018     |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 26.1        |
|    ep_rew_mean          | -54.6       |
| time/                   |             |
|    fps                  | 1008        |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012197705 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.00204     |
|    learning_rate        | 1.63e-05    |
|    loss                 | 432         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 677         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=253.26 +/- 164.58
Episode length: 16.70 +/- 4.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | 253         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014279004 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.00375     |
|    learning_rate        | 1.63e-05    |
|    loss                 | 361         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 750         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | -38.1    |
| time/              |          |
|    fps             | 988      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | -20.6       |
| time/                   |             |
|    fps                  | 986         |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.017931212 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.00217     |
|    learning_rate        | 1.63e-05    |
|    loss                 | 462         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 845         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | -5.15       |
| time/                   |             |
|    fps                  | 984         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.011493336 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.00856     |
|    learning_rate        | 1.63e-05    |
|    loss                 | 434         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 905         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=135.24 +/- 112.07
Episode length: 13.90 +/- 3.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.9        |
|    mean_reward          | 135         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.013302725 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.0125      |
|    learning_rate        | 1.63e-05    |
|    loss                 | 385         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 976         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 978      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 17.6         |
|    ep_rew_mean          | 6.77         |
| time/                   |              |
|    fps                  | 977          |
|    iterations           | 9            |
|    time_elapsed         | 18           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0035379478 |
|    clip_fraction        | 0.0448       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.014        |
|    learning_rate        | 1.63e-05     |
|    loss                 | 561          |
|    n_updates            | 80           |
|    policy_gradient_loss | 0.0107       |
|    value_loss           | 1.05e+03     |
------------------------------------------
Eval num_timesteps=20000, episode_reward=178.75 +/- 154.18
Episode length: 14.30 +/- 3.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.3       |
|    mean_reward          | 179        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.02032918 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.56      |
|    explained_variance   | 0.0205     |
|    learning_rate        | 1.63e-05   |
|    loss                 | 729        |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0269    |
|    value_loss           | 1.27e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.7     |
|    ep_rew_mean     | 55.8     |
| time/              |          |
|    fps             | 972      |
|    iterations      | 10       |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.1       |
|    ep_rew_mean          | 75.5       |
| time/                   |            |
|    fps                  | 972        |
|    iterations           | 11         |
|    time_elapsed         | 23         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.03468909 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.0251     |
|    learning_rate        | 1.63e-05   |
|    loss                 | 853        |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 1.65e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16          |
|    ep_rew_mean          | 109         |
| time/                   |             |
|    fps                  | 972         |
|    iterations           | 12          |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.039434485 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.93       |
|    explained_variance   | 0.0271      |
|    learning_rate        | 1.63e-05    |
|    loss                 | 954         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 1.94e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=150.42 +/- 148.82
Episode length: 13.50 +/- 3.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.5        |
|    mean_reward          | 150         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009087841 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.777      |
|    explained_variance   | 0.0208      |
|    learning_rate        | 1.63e-05    |
|    loss                 | 1.17e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00815    |
|    value_loss           | 2.56e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 131      |
| time/              |          |
|    fps             | 969      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | 141          |
| time/                   |              |
|    fps                  | 969          |
|    iterations           | 14           |
|    time_elapsed         | 29           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0063013416 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.663       |
|    explained_variance   | 0.0196       |
|    learning_rate        | 1.63e-05     |
|    loss                 | 1.45e+03     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00712     |
|    value_loss           | 3.05e+03     |
------------------------------------------
Eval num_timesteps=30000, episode_reward=195.80 +/- 144.66
Episode length: 15.60 +/- 4.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 196          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0044324095 |
|    clip_fraction        | 0.0766       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.578       |
|    explained_variance   | 0.0238       |
|    learning_rate        | 1.63e-05     |
|    loss                 | 1.74e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00485     |
|    value_loss           | 3.49e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 967      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15           |
|    ep_rew_mean          | 155          |
| time/                   |              |
|    fps                  | 967          |
|    iterations           | 16           |
|    time_elapsed         | 33           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0047347983 |
|    clip_fraction        | 0.0644       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.467       |
|    explained_variance   | 0.0291       |
|    learning_rate        | 1.63e-05     |
|    loss                 | 2.18e+03     |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00473     |
|    value_loss           | 3.93e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15           |
|    ep_rew_mean          | 159          |
| time/                   |              |
|    fps                  | 967          |
|    iterations           | 17           |
|    time_elapsed         | 36           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0038577942 |
|    clip_fraction        | 0.0533       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.389       |
|    explained_variance   | 0.0289       |
|    learning_rate        | 1.63e-05     |
|    loss                 | 2.4e+03      |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00402     |
|    value_loss           | 4.18e+03     |
------------------------------------------
Eval num_timesteps=35000, episode_reward=205.50 +/- 149.62
Episode length: 15.20 +/- 4.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | 206          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0017544434 |
|    clip_fraction        | 0.0415       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.311       |
|    explained_variance   | 0.0332       |
|    learning_rate        | 1.63e-05     |
|    loss                 | 2.25e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00351     |
|    value_loss           | 4.62e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 193      |
| time/              |          |
|    fps             | 964      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.9        |
|    ep_rew_mean          | 200         |
| time/                   |             |
|    fps                  | 963         |
|    iterations           | 19          |
|    time_elapsed         | 40          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.001282423 |
|    clip_fraction        | 0.0357      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.263      |
|    explained_variance   | 0.0399      |
|    learning_rate        | 1.63e-05    |
|    loss                 | 2.35e+03    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00201    |
|    value_loss           | 4.54e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=164.96 +/- 136.54
Episode length: 13.50 +/- 4.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.5          |
|    mean_reward          | 165           |
| time/                   |               |
|    total_timesteps      | 40000         |
| train/                  |               |
|    approx_kl            | 0.00091438694 |
|    clip_fraction        | 0.0207        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.235        |
|    explained_variance   | 0.0435        |
|    learning_rate        | 1.63e-05      |
|    loss                 | 2.26e+03      |
|    n_updates            | 190           |
|    policy_gradient_loss | 2.08e-05      |
|    value_loss           | 4.71e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 961      |
|    iterations      | 20       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | 202          |
| time/                   |              |
|    fps                  | 962          |
|    iterations           | 21           |
|    time_elapsed         | 44           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0010986013 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.2         |
|    explained_variance   | 0.0607       |
|    learning_rate        | 1.63e-05     |
|    loss                 | 2.39e+03     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 4.65e+03     |
------------------------------------------
Eval num_timesteps=45000, episode_reward=250.96 +/- 177.21
Episode length: 16.40 +/- 4.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 251          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0017365825 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.174       |
|    explained_variance   | 0.0535       |
|    learning_rate        | 1.63e-05     |
|    loss                 | 2.41e+03     |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.0023      |
|    value_loss           | 5.3e+03      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 224      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 22       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | 225          |
| time/                   |              |
|    fps                  | 960          |
|    iterations           | 23           |
|    time_elapsed         | 49           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0011162962 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.141       |
|    explained_variance   | 0.0564       |
|    learning_rate        | 1.63e-05     |
|    loss                 | 2.39e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 5.22e+03     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.1        |
|    ep_rew_mean          | 188         |
| time/                   |             |
|    fps                  | 960         |
|    iterations           | 24          |
|    time_elapsed         | 51          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.001107407 |
|    clip_fraction        | 0.0162      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.058       |
|    learning_rate        | 1.63e-05    |
|    loss                 | 2.33e+03    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.000735   |
|    value_loss           | 5.35e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=295.95 +/- 141.13
Episode length: 17.60 +/- 3.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 296          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0013593085 |
|    clip_fraction        | 0.00972      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0838      |
|    explained_variance   | 0.101        |
|    learning_rate        | 1.63e-05     |
|    loss                 | 2.78e+03     |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00021     |
|    value_loss           | 5.25e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | 208      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 25       |
|    time_elapsed    | 53       |
|    total_timesteps | 51200    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 15.4          |
|    ep_rew_mean          | 198           |
| time/                   |               |
|    fps                  | 959           |
|    iterations           | 26            |
|    time_elapsed         | 55            |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 0.00043551013 |
|    clip_fraction        | 0.00513       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0627       |
|    explained_variance   | 0.105         |
|    learning_rate        | 1.63e-05      |
|    loss                 | 2.13e+03      |
|    n_updates            | 250           |
|    policy_gradient_loss | -8.22e-05     |
|    value_loss           | 5.24e+03      |
-------------------------------------------
Eval num_timesteps=55000, episode_reward=215.90 +/- 154.05
Episode length: 15.60 +/- 4.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 216          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0011106611 |
|    clip_fraction        | 0.0064       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0429      |
|    explained_variance   | 0.121        |
|    learning_rate        | 1.63e-05     |
|    loss                 | 2.49e+03     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000702    |
|    value_loss           | 5.15e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 255      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 27       |
|    time_elapsed    | 57       |
|    total_timesteps | 55296    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 15.9          |
|    ep_rew_mean          | 225           |
| time/                   |               |
|    fps                  | 958           |
|    iterations           | 28            |
|    time_elapsed         | 59            |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00014472465 |
|    clip_fraction        | 0.00171       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0321       |
|    explained_variance   | 0.12          |
|    learning_rate        | 1.63e-05      |
|    loss                 | 2.67e+03      |
|    n_updates            | 270           |
|    policy_gradient_loss | -0.000185     |
|    value_loss           | 5.3e+03       |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 16.1          |
|    ep_rew_mean          | 235           |
| time/                   |               |
|    fps                  | 959           |
|    iterations           | 29            |
|    time_elapsed         | 61            |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00018802658 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0262       |
|    explained_variance   | 0.141         |
|    learning_rate        | 1.63e-05      |
|    loss                 | 2.39e+03      |
|    n_updates            | 280           |
|    policy_gradient_loss | -0.000219     |
|    value_loss           | 5.37e+03      |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=148.12 +/- 118.70
Episode length: 13.60 +/- 3.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.6          |
|    mean_reward          | 148           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 1.9865925e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0259       |
|    explained_variance   | 0.175         |
|    learning_rate        | 1.63e-05      |
|    loss                 | 2.59e+03      |
|    n_updates            | 290           |
|    policy_gradient_loss | -4.6e-05      |
|    value_loss           | 5.04e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | 228      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 30       |
|    time_elapsed    | 64       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 22:59:37,608] Trial 39 finished with value: 191.58493500000003 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 1.629566089101022e-05, 'gamma': 0.9484150325165066, 'gae_lambda': 0.8054541847423503}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | -83.8    |
| time/              |          |
|    fps             | 1187     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34          |
|    ep_rew_mean          | -82.6       |
| time/                   |             |
|    fps                  | 1071        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.019887872 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -8.11e-06   |
|    learning_rate        | 6.01e-05    |
|    loss                 | 637         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 1.86e+03    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-115.95 +/- 0.03
Episode length: 28.00 +/- 8.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28          |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009236848 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | -0.00149    |
|    learning_rate        | 6.01e-05    |
|    loss                 | 651         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00776    |
|    value_loss           | 1.23e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.9     |
|    ep_rew_mean     | -69.5    |
| time/              |          |
|    fps             | 1005     |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.1        |
|    ep_rew_mean          | -54.2       |
| time/                   |             |
|    fps                  | 995         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.015453745 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | -0.0177     |
|    learning_rate        | 6.01e-05    |
|    loss                 | 750         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 1.48e+03    |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=274.35 +/- 157.62
Episode length: 17.20 +/- 4.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.2        |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008687241 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.00397     |
|    learning_rate        | 6.01e-05    |
|    loss                 | 932         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 1.79e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | -35.3    |
| time/              |          |
|    fps             | 981      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.9        |
|    ep_rew_mean          | -29.3       |
| time/                   |             |
|    fps                  | 981         |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011354351 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.000153    |
|    learning_rate        | 6.01e-05    |
|    loss                 | 1.29e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 2.69e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 21.4         |
|    ep_rew_mean          | -1.68        |
| time/                   |              |
|    fps                  | 981          |
|    iterations           | 7            |
|    time_elapsed         | 14           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0154942805 |
|    clip_fraction        | 0.208        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.00927      |
|    learning_rate        | 6.01e-05     |
|    loss                 | 1.1e+03      |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0149      |
|    value_loss           | 2.12e+03     |
------------------------------------------
Eval num_timesteps=15000, episode_reward=166.64 +/- 105.95
Episode length: 14.50 +/- 3.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.5       |
|    mean_reward          | 167        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01311158 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.75      |
|    explained_variance   | 0.029      |
|    learning_rate        | 6.01e-05   |
|    loss                 | 1.26e+03   |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 2.68e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 974      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.5       |
|    ep_rew_mean          | 34.7       |
| time/                   |            |
|    fps                  | 973        |
|    iterations           | 9          |
|    time_elapsed         | 18         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.01335679 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.62      |
|    explained_variance   | 0.0442     |
|    learning_rate        | 6.01e-05   |
|    loss                 | 1.66e+03   |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 3.45e+03   |
----------------------------------------
Eval num_timesteps=20000, episode_reward=201.46 +/- 127.37
Episode length: 15.40 +/- 4.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.4       |
|    mean_reward          | 201        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.01953467 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.44      |
|    explained_variance   | 0.0472     |
|    learning_rate        | 6.01e-05   |
|    loss                 | 1.89e+03   |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0246    |
|    value_loss           | 4.46e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 63.6     |
| time/              |          |
|    fps             | 968      |
|    iterations      | 10       |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.9       |
|    ep_rew_mean          | 96.5       |
| time/                   |            |
|    fps                  | 969        |
|    iterations           | 11         |
|    time_elapsed         | 23         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.02284033 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.132      |
|    learning_rate        | 6.01e-05   |
|    loss                 | 1.67e+03   |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0189    |
|    value_loss           | 4.38e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.9        |
|    ep_rew_mean          | 121         |
| time/                   |             |
|    fps                  | 969         |
|    iterations           | 12          |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009918124 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.0975      |
|    learning_rate        | 6.01e-05    |
|    loss                 | 3.3e+03     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 6.47e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=222.12 +/- 137.65
Episode length: 16.00 +/- 3.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011448925 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.869      |
|    explained_variance   | 0.119       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 3.61e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00673    |
|    value_loss           | 7.06e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 138      |
| time/              |          |
|    fps             | 965      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.8        |
|    ep_rew_mean          | 161         |
| time/                   |             |
|    fps                  | 965         |
|    iterations           | 14          |
|    time_elapsed         | 29          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.007098148 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.761      |
|    explained_variance   | 0.121       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 4.51e+03    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00744    |
|    value_loss           | 9.08e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=175.16 +/- 156.21
Episode length: 14.50 +/- 4.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.010966929 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.707      |
|    explained_variance   | 0.187       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 3.34e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00495    |
|    value_loss           | 8.26e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 151      |
| time/              |          |
|    fps             | 963      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.8        |
|    ep_rew_mean          | 153         |
| time/                   |             |
|    fps                  | 963         |
|    iterations           | 16          |
|    time_elapsed         | 34          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.003082024 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.645      |
|    explained_variance   | 0.182       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 5.09e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 8.66e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.1         |
|    ep_rew_mean          | 168          |
| time/                   |              |
|    fps                  | 963          |
|    iterations           | 17           |
|    time_elapsed         | 36           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0068374574 |
|    clip_fraction        | 0.087        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.542       |
|    explained_variance   | 0.197        |
|    learning_rate        | 6.01e-05     |
|    loss                 | 4.24e+03     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00672     |
|    value_loss           | 9.59e+03     |
------------------------------------------
Eval num_timesteps=35000, episode_reward=277.39 +/- 210.57
Episode length: 17.10 +/- 5.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.1         |
|    mean_reward          | 277          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0018859867 |
|    clip_fraction        | 0.0733       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.451       |
|    explained_variance   | 0.181        |
|    learning_rate        | 6.01e-05     |
|    loss                 | 4.64e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00393     |
|    value_loss           | 1.06e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | 168      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.8        |
|    ep_rew_mean          | 181         |
| time/                   |             |
|    fps                  | 961         |
|    iterations           | 19          |
|    time_elapsed         | 40          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.003068967 |
|    clip_fraction        | 0.0389      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.41       |
|    explained_variance   | 0.248       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 4.25e+03    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00281    |
|    value_loss           | 9.06e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=184.79 +/- 146.85
Episode length: 14.40 +/- 3.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.4       |
|    mean_reward          | 185        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.00230929 |
|    clip_fraction        | 0.0412     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.217      |
|    learning_rate        | 6.01e-05   |
|    loss                 | 6.06e+03   |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.00312   |
|    value_loss           | 1.16e+04   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | 221      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 20       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.2        |
|    ep_rew_mean          | 212         |
| time/                   |             |
|    fps                  | 960         |
|    iterations           | 21          |
|    time_elapsed         | 44          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.003285071 |
|    clip_fraction        | 0.0483      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.279      |
|    explained_variance   | 0.253       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 5.53e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0038     |
|    value_loss           | 1.23e+04    |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=271.84 +/- 248.17
Episode length: 16.50 +/- 7.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | 272          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0028401345 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | 0.275        |
|    learning_rate        | 6.01e-05     |
|    loss                 | 5.23e+03     |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00159     |
|    value_loss           | 1.14e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 204      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 22       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.3        |
|    ep_rew_mean          | 184         |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 23          |
|    time_elapsed         | 49          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.002364649 |
|    clip_fraction        | 0.0243      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.285       |
|    learning_rate        | 6.01e-05    |
|    loss                 | 6.03e+03    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00218    |
|    value_loss           | 1.14e+04    |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 15.4          |
|    ep_rew_mean          | 194           |
| time/                   |               |
|    fps                  | 959           |
|    iterations           | 24            |
|    time_elapsed         | 51            |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.00066744327 |
|    clip_fraction        | 0.0122        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.178        |
|    explained_variance   | 0.31          |
|    learning_rate        | 6.01e-05      |
|    loss                 | 5.18e+03      |
|    n_updates            | 230           |
|    policy_gradient_loss | -0.000764     |
|    value_loss           | 9.74e+03      |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=239.18 +/- 233.18
Episode length: 16.30 +/- 6.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | 239          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0006719386 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0.372        |
|    learning_rate        | 6.01e-05     |
|    loss                 | 3.36e+03     |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000696    |
|    value_loss           | 9.11e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 238      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 25       |
|    time_elapsed    | 53       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.6        |
|    ep_rew_mean          | 204         |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 26          |
|    time_elapsed         | 55          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.002284737 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.4         |
|    learning_rate        | 6.01e-05    |
|    loss                 | 4.13e+03    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00048    |
|    value_loss           | 1.02e+04    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=128.84 +/- 98.37
Episode length: 13.30 +/- 3.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.3          |
|    mean_reward          | 129           |
| time/                   |               |
|    total_timesteps      | 55000         |
| train/                  |               |
|    approx_kl            | 0.00049219653 |
|    clip_fraction        | 0.00679       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.127        |
|    explained_variance   | 0.317         |
|    learning_rate        | 6.01e-05      |
|    loss                 | 5.44e+03      |
|    n_updates            | 260           |
|    policy_gradient_loss | -0.000178     |
|    value_loss           | 9.54e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | 249      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 27       |
|    time_elapsed    | 57       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.4         |
|    ep_rew_mean          | 234          |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 28           |
|    time_elapsed         | 59           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0006507983 |
|    clip_fraction        | 0.00723      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.105       |
|    explained_variance   | 0.356        |
|    learning_rate        | 6.01e-05     |
|    loss                 | 5.6e+03      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000263    |
|    value_loss           | 1.11e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | 241          |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 29           |
|    time_elapsed         | 62           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0003789469 |
|    clip_fraction        | 0.00884      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0678      |
|    explained_variance   | 0.393        |
|    learning_rate        | 6.01e-05     |
|    loss                 | 5.23e+03     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.000856    |
|    value_loss           | 1.02e+04     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=310.20 +/- 213.73
Episode length: 17.80 +/- 6.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.8         |
|    mean_reward          | 310          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0009572478 |
|    clip_fraction        | 0.00942      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0522      |
|    explained_variance   | 0.377        |
|    learning_rate        | 6.01e-05     |
|    loss                 | 4.51e+03     |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000835    |
|    value_loss           | 1.15e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | 235      |
| time/              |          |
|    fps             | 956      |
|    iterations      | 30       |
|    time_elapsed    | 64       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 23:00:42,623] Trial 40 finished with value: 336.5600312 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 6.006526482585863e-05, 'gamma': 0.978131507824099, 'gae_lambda': 0.9694739602981245}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.2     |
|    ep_rew_mean     | -92.4    |
| time/              |          |
|    fps             | 1180     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.2        |
|    ep_rew_mean          | -87.9       |
| time/                   |             |
|    fps                  | 1072        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008766973 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000113   |
|    learning_rate        | 5.26e-05    |
|    loss                 | 735         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00371    |
|    value_loss           | 1.94e+03    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=226.81 +/- 174.18
Episode length: 15.60 +/- 4.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.6       |
|    mean_reward          | 227        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01076079 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.07      |
|    explained_variance   | -0.0111    |
|    learning_rate        | 5.26e-05   |
|    loss                 | 634        |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00513   |
|    value_loss           | 1.12e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.3     |
|    ep_rew_mean     | -71.5    |
| time/              |          |
|    fps             | 1022     |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 26.8        |
|    ep_rew_mean          | -57.2       |
| time/                   |             |
|    fps                  | 1012        |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.015325125 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.00441     |
|    learning_rate        | 5.26e-05    |
|    loss                 | 682         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=156.82 +/- 138.70
Episode length: 14.40 +/- 3.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 157         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.015434535 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.0169      |
|    learning_rate        | 5.26e-05    |
|    loss                 | 689         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 1.37e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -47.5    |
| time/              |          |
|    fps             | 995      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | -31.2       |
| time/                   |             |
|    fps                  | 992         |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.016745154 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.0137      |
|    learning_rate        | 5.26e-05    |
|    loss                 | 844         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 1.65e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 6.82        |
| time/                   |             |
|    fps                  | 990         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.015464054 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.0377      |
|    learning_rate        | 5.26e-05    |
|    loss                 | 764         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=229.45 +/- 133.24
Episode length: 16.00 +/- 4.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16         |
|    mean_reward          | 229        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01754194 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0.0376     |
|    learning_rate        | 5.26e-05   |
|    loss                 | 1.76e+03   |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.00661   |
|    value_loss           | 3.3e+03    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 26       |
| time/              |          |
|    fps             | 981      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.7        |
|    ep_rew_mean          | 40.1        |
| time/                   |             |
|    fps                  | 980         |
|    iterations           | 9           |
|    time_elapsed         | 18          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.015302984 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.051       |
|    learning_rate        | 5.26e-05    |
|    loss                 | 1.79e+03    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 3.7e+03     |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=165.16 +/- 144.94
Episode length: 14.50 +/- 4.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 165         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.017056335 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.0929      |
|    learning_rate        | 5.26e-05    |
|    loss                 | 1.61e+03    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 3.56e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.2     |
|    ep_rew_mean     | 84.5     |
| time/              |          |
|    fps             | 975      |
|    iterations      | 10       |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 106         |
| time/                   |             |
|    fps                  | 976         |
|    iterations           | 11          |
|    time_elapsed         | 23          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.022239886 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.168       |
|    learning_rate        | 5.26e-05    |
|    loss                 | 1.78e+03    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00755    |
|    value_loss           | 4.16e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.4        |
|    ep_rew_mean          | 111         |
| time/                   |             |
|    fps                  | 975         |
|    iterations           | 12          |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.008398274 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.119       |
|    learning_rate        | 5.26e-05    |
|    loss                 | 3.49e+03    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00587    |
|    value_loss           | 6.28e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=335.86 +/- 266.76
Episode length: 19.10 +/- 7.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.1         |
|    mean_reward          | 336          |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0090764165 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.91        |
|    explained_variance   | 0.142        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 3.69e+03     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00704     |
|    value_loss           | 6.92e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | 153      |
| time/              |          |
|    fps             | 969      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.6        |
|    ep_rew_mean          | 156         |
| time/                   |             |
|    fps                  | 970         |
|    iterations           | 14          |
|    time_elapsed         | 29          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.007492894 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.787      |
|    explained_variance   | 0.139       |
|    learning_rate        | 5.26e-05    |
|    loss                 | 3.8e+03     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00599    |
|    value_loss           | 8.93e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=302.26 +/- 211.92
Episode length: 17.60 +/- 4.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | 302          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0057642073 |
|    clip_fraction        | 0.0878       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.663       |
|    explained_variance   | 0.168        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 4.76e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00474     |
|    value_loss           | 9.68e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 147      |
| time/              |          |
|    fps             | 966      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.1         |
|    ep_rew_mean          | 166          |
| time/                   |              |
|    fps                  | 966          |
|    iterations           | 16           |
|    time_elapsed         | 33           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0022273595 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.574       |
|    explained_variance   | 0.182        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 4.29e+03     |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00274     |
|    value_loss           | 8.34e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.2         |
|    ep_rew_mean          | 159          |
| time/                   |              |
|    fps                  | 967          |
|    iterations           | 17           |
|    time_elapsed         | 35           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0038842661 |
|    clip_fraction        | 0.0648       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.491       |
|    explained_variance   | 0.165        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 4.64e+03     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.0032      |
|    value_loss           | 1.05e+04     |
------------------------------------------
Eval num_timesteps=35000, episode_reward=158.02 +/- 139.86
Episode length: 14.40 +/- 3.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 158          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0042963084 |
|    clip_fraction        | 0.0477       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.43        |
|    explained_variance   | 0.217        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 4.51e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00397     |
|    value_loss           | 9.23e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | 167      |
| time/              |          |
|    fps             | 965      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 17           |
|    ep_rew_mean          | 221          |
| time/                   |              |
|    fps                  | 949          |
|    iterations           | 19           |
|    time_elapsed         | 40           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0021817274 |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.364       |
|    explained_variance   | 0.253        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 3.58e+03     |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00221     |
|    value_loss           | 8.74e+03     |
------------------------------------------
Eval num_timesteps=40000, episode_reward=181.20 +/- 127.76
Episode length: 14.50 +/- 3.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.5         |
|    mean_reward          | 181          |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0028328388 |
|    clip_fraction        | 0.0376       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.266       |
|    explained_variance   | 0.234        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 5.41e+03     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 1.29e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 197      |
| time/              |          |
|    fps             | 947      |
|    iterations      | 20       |
|    time_elapsed    | 43       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.1         |
|    ep_rew_mean          | 217          |
| time/                   |              |
|    fps                  | 949          |
|    iterations           | 21           |
|    time_elapsed         | 45           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0039189123 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.215       |
|    explained_variance   | 0.291        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 4.67e+03     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 1.16e+04     |
------------------------------------------
Eval num_timesteps=45000, episode_reward=159.63 +/- 154.53
Episode length: 13.70 +/- 4.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.7        |
|    mean_reward          | 160         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.001417756 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.32        |
|    learning_rate        | 5.26e-05    |
|    loss                 | 5.77e+03    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 1.03e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | 205      |
| time/              |          |
|    fps             | 947      |
|    iterations      | 22       |
|    time_elapsed    | 47       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | 197          |
| time/                   |              |
|    fps                  | 947          |
|    iterations           | 23           |
|    time_elapsed         | 49           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0012891544 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.162       |
|    explained_variance   | 0.328        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 5.38e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.001       |
|    value_loss           | 9.93e+03     |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 16.6          |
|    ep_rew_mean          | 234           |
| time/                   |               |
|    fps                  | 948           |
|    iterations           | 24            |
|    time_elapsed         | 51            |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.00091333233 |
|    clip_fraction        | 0.0181        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.153        |
|    explained_variance   | 0.314         |
|    learning_rate        | 5.26e-05      |
|    loss                 | 4.66e+03      |
|    n_updates            | 230           |
|    policy_gradient_loss | -0.000823     |
|    value_loss           | 1.01e+04      |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=299.99 +/- 150.21
Episode length: 18.30 +/- 4.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 300         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.000990158 |
|    clip_fraction        | 0.0109      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.339       |
|    learning_rate        | 5.26e-05    |
|    loss                 | 3.98e+03    |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00089    |
|    value_loss           | 9.73e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 224      |
| time/              |          |
|    fps             | 947      |
|    iterations      | 25       |
|    time_elapsed    | 54       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | 211          |
| time/                   |              |
|    fps                  | 947          |
|    iterations           | 26           |
|    time_elapsed         | 56           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0004056226 |
|    clip_fraction        | 0.00923      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.113       |
|    explained_variance   | 0.392        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 4.91e+03     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.000859    |
|    value_loss           | 8.42e+03     |
------------------------------------------
Eval num_timesteps=55000, episode_reward=305.44 +/- 161.20
Episode length: 18.50 +/- 5.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.5         |
|    mean_reward          | 305          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0015856023 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0891      |
|    explained_variance   | 0.386        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 4.87e+03     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 8.96e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 204      |
| time/              |          |
|    fps             | 946      |
|    iterations      | 27       |
|    time_elapsed    | 58       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.2         |
|    ep_rew_mean          | 227          |
| time/                   |              |
|    fps                  | 947          |
|    iterations           | 28           |
|    time_elapsed         | 60           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0005081111 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0831      |
|    explained_variance   | 0.314        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 4.92e+03     |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00032     |
|    value_loss           | 9.89e+03     |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 15.8          |
|    ep_rew_mean          | 213           |
| time/                   |               |
|    fps                  | 947           |
|    iterations           | 29            |
|    time_elapsed         | 62            |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00085302786 |
|    clip_fraction        | 0.0104        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0733       |
|    explained_variance   | 0.309         |
|    learning_rate        | 5.26e-05      |
|    loss                 | 5.21e+03      |
|    n_updates            | 280           |
|    policy_gradient_loss | -0.0012       |
|    value_loss           | 1.14e+04      |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=298.35 +/- 231.69
Episode length: 17.90 +/- 6.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.9         |
|    mean_reward          | 298          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0004312645 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0664      |
|    explained_variance   | 0.313        |
|    learning_rate        | 5.26e-05     |
|    loss                 | 5.22e+03     |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00041     |
|    value_loss           | 1.17e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | 182      |
| time/              |          |
|    fps             | 946      |
|    iterations      | 30       |
|    time_elapsed    | 64       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 23:01:48,301] Trial 41 finished with value: 301.7854705 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 5.264279758563532e-05, 'gamma': 0.9713147841918751, 'gae_lambda': 0.9742197735694413}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.8     |
|    ep_rew_mean     | -85.9    |
| time/              |          |
|    fps             | 1187     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30         |
|    ep_rew_mean          | -87.3      |
| time/                   |            |
|    fps                  | 1086       |
|    iterations           | 2          |
|    time_elapsed         | 3          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.01680897 |
|    clip_fraction        | 0.09       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.07      |
|    explained_variance   | -0.000168  |
|    learning_rate        | 2.73e-05   |
|    loss                 | 830        |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00614   |
|    value_loss           | 2.14e+03   |
----------------------------------------
Eval num_timesteps=5000, episode_reward=287.80 +/- 163.71
Episode length: 17.70 +/- 4.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010093216 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | -0.000628   |
|    learning_rate        | 2.73e-05    |
|    loss                 | 577         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0095     |
|    value_loss           | 1.28e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -64.6    |
| time/              |          |
|    fps             | 1031     |
|    iterations      | 3        |
|    time_elapsed    | 5        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.8        |
|    ep_rew_mean          | -54.5       |
| time/                   |             |
|    fps                  | 1020        |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012933359 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.000995    |
|    learning_rate        | 2.73e-05    |
|    loss                 | 784         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 1.44e+03    |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=130.72 +/- 113.22
Episode length: 13.40 +/- 2.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.4        |
|    mean_reward          | 131         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011377636 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.00846     |
|    learning_rate        | 2.73e-05    |
|    loss                 | 803         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 1.33e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | -48.3    |
| time/              |          |
|    fps             | 1002     |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | -34.3       |
| time/                   |             |
|    fps                  | 1000        |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012835013 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.00209     |
|    learning_rate        | 2.73e-05    |
|    loss                 | 542         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0224     |
|    value_loss           | 1.41e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 19.8         |
|    ep_rew_mean          | 1.52         |
| time/                   |              |
|    fps                  | 995          |
|    iterations           | 7            |
|    time_elapsed         | 14           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0141521795 |
|    clip_fraction        | 0.266        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.84        |
|    explained_variance   | 0.0118       |
|    learning_rate        | 2.73e-05     |
|    loss                 | 642          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0228      |
|    value_loss           | 1.32e+03     |
------------------------------------------
Eval num_timesteps=15000, episode_reward=269.46 +/- 179.70
Episode length: 17.60 +/- 5.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.6        |
|    mean_reward          | 269         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012965782 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.0153      |
|    learning_rate        | 2.73e-05    |
|    loss                 | 1.22e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.000694    |
|    value_loss           | 2.55e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 7.86     |
| time/              |          |
|    fps             | 984      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 33.1        |
| time/                   |             |
|    fps                  | 984         |
|    iterations           | 9           |
|    time_elapsed         | 18          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.014478986 |
|    clip_fraction        | 0.0706      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.024       |
|    learning_rate        | 2.73e-05    |
|    loss                 | 1.61e+03    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00607    |
|    value_loss           | 2.29e+03    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=177.30 +/- 156.90
Episode length: 14.50 +/- 4.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.5       |
|    mean_reward          | 177        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.04038403 |
|    clip_fraction        | 0.643      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.67      |
|    explained_variance   | 0.0246     |
|    learning_rate        | 2.73e-05   |
|    loss                 | 1.55e+03   |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0515     |
|    value_loss           | 3.62e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | 87.6     |
| time/              |          |
|    fps             | 976      |
|    iterations      | 10       |
|    time_elapsed    | 20       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 15.5       |
|    ep_rew_mean          | 95.2       |
| time/                   |            |
|    fps                  | 975        |
|    iterations           | 11         |
|    time_elapsed         | 23         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.13875845 |
|    clip_fraction        | 0.555      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.904     |
|    explained_variance   | 0.0205     |
|    learning_rate        | 2.73e-05   |
|    loss                 | 2.29e+03   |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.0292     |
|    value_loss           | 6.34e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.3        |
|    ep_rew_mean          | 144         |
| time/                   |             |
|    fps                  | 975         |
|    iterations           | 12          |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009911543 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.899      |
|    explained_variance   | 0.0225      |
|    learning_rate        | 2.73e-05    |
|    loss                 | 4.02e+03    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 7.62e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=270.25 +/- 166.72
Episode length: 16.90 +/- 4.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | 270         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010299229 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.85       |
|    explained_variance   | 0.0361      |
|    learning_rate        | 2.73e-05    |
|    loss                 | 4.25e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00231    |
|    value_loss           | 8.89e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | 165      |
| time/              |          |
|    fps             | 971      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.8        |
|    ep_rew_mean          | 147         |
| time/                   |             |
|    fps                  | 971         |
|    iterations           | 14          |
|    time_elapsed         | 29          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.004205617 |
|    clip_fraction        | 0.0541      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.733      |
|    explained_variance   | 0.0384      |
|    learning_rate        | 2.73e-05    |
|    loss                 | 5.28e+03    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 1.06e+04    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=267.50 +/- 156.07
Episode length: 16.60 +/- 4.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0107761845 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.603       |
|    explained_variance   | 0.0432       |
|    learning_rate        | 2.73e-05     |
|    loss                 | 7.43e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00104     |
|    value_loss           | 1.1e+04      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 968      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.4        |
|    ep_rew_mean          | 199         |
| time/                   |             |
|    fps                  | 968         |
|    iterations           | 16          |
|    time_elapsed         | 33          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.007521117 |
|    clip_fraction        | 0.0646      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.0589      |
|    learning_rate        | 2.73e-05    |
|    loss                 | 4.58e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.000954   |
|    value_loss           | 1.06e+04    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.5         |
|    ep_rew_mean          | 183          |
| time/                   |              |
|    fps                  | 969          |
|    iterations           | 17           |
|    time_elapsed         | 35           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0052832095 |
|    clip_fraction        | 0.0644       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.344       |
|    explained_variance   | 0.065        |
|    learning_rate        | 2.73e-05     |
|    loss                 | 7.81e+03     |
|    n_updates            | 160          |
|    policy_gradient_loss | 0.00145      |
|    value_loss           | 1.37e+04     |
------------------------------------------
Eval num_timesteps=35000, episode_reward=193.14 +/- 168.30
Episode length: 15.10 +/- 4.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.004389373 |
|    clip_fraction        | 0.0443      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.28       |
|    explained_variance   | 0.0785      |
|    learning_rate        | 2.73e-05    |
|    loss                 | 5e+03       |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 1.32e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 207      |
| time/              |          |
|    fps             | 966      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | 206          |
| time/                   |              |
|    fps                  | 966          |
|    iterations           | 19           |
|    time_elapsed         | 40           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0025540679 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.22        |
|    explained_variance   | 0.101        |
|    learning_rate        | 2.73e-05     |
|    loss                 | 5.96e+03     |
|    n_updates            | 180          |
|    policy_gradient_loss | 0.000754     |
|    value_loss           | 1.26e+04     |
------------------------------------------
Eval num_timesteps=40000, episode_reward=311.35 +/- 141.64
Episode length: 18.80 +/- 4.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.8         |
|    mean_reward          | 311          |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0004199141 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.183       |
|    explained_variance   | 0.116        |
|    learning_rate        | 2.73e-05     |
|    loss                 | 6.49e+03     |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.000856     |
|    value_loss           | 1.36e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 213      |
| time/              |          |
|    fps             | 963      |
|    iterations      | 20       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.5         |
|    ep_rew_mean          | 193          |
| time/                   |              |
|    fps                  | 964          |
|    iterations           | 21           |
|    time_elapsed         | 44           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0034640967 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.161       |
|    explained_variance   | 0.0965       |
|    learning_rate        | 2.73e-05     |
|    loss                 | 6.39e+03     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.000436    |
|    value_loss           | 1.53e+04     |
------------------------------------------
Eval num_timesteps=45000, episode_reward=267.12 +/- 169.61
Episode length: 16.60 +/- 4.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 267          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0007887923 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.162        |
|    learning_rate        | 2.73e-05     |
|    loss                 | 6.27e+03     |
|    n_updates            | 210          |
|    policy_gradient_loss | 0.000267     |
|    value_loss           | 1.17e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | 197      |
| time/              |          |
|    fps             | 962      |
|    iterations      | 22       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 15.7          |
|    ep_rew_mean          | 207           |
| time/                   |               |
|    fps                  | 963           |
|    iterations           | 23            |
|    time_elapsed         | 48            |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.00083019334 |
|    clip_fraction        | 0.0129        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.106        |
|    explained_variance   | 0.149         |
|    learning_rate        | 2.73e-05      |
|    loss                 | 6.27e+03      |
|    n_updates            | 220           |
|    policy_gradient_loss | -0.00115      |
|    value_loss           | 1.29e+04      |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 16            |
|    ep_rew_mean          | 217           |
| time/                   |               |
|    fps                  | 963           |
|    iterations           | 24            |
|    time_elapsed         | 51            |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.00090665295 |
|    clip_fraction        | 0.0113        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0905       |
|    explained_variance   | 0.153         |
|    learning_rate        | 2.73e-05      |
|    loss                 | 5.68e+03      |
|    n_updates            | 230           |
|    policy_gradient_loss | -0.000264     |
|    value_loss           | 1.26e+04      |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=210.96 +/- 163.54
Episode length: 15.40 +/- 3.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | 211           |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 0.00074671465 |
|    clip_fraction        | 0.0118        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0852       |
|    explained_variance   | 0.219         |
|    learning_rate        | 2.73e-05      |
|    loss                 | 5.08e+03      |
|    n_updates            | 240           |
|    policy_gradient_loss | -6.01e-05     |
|    value_loss           | 1.06e+04      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 252      |
| time/              |          |
|    fps             | 961      |
|    iterations      | 25       |
|    time_elapsed    | 53       |
|    total_timesteps | 51200    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 15.9          |
|    ep_rew_mean          | 218           |
| time/                   |               |
|    fps                  | 962           |
|    iterations           | 26            |
|    time_elapsed         | 55            |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 0.00050874543 |
|    clip_fraction        | 0.00981       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0766       |
|    explained_variance   | 0.199         |
|    learning_rate        | 2.73e-05      |
|    loss                 | 6.75e+03      |
|    n_updates            | 250           |
|    policy_gradient_loss | 2.84e-06      |
|    value_loss           | 1.31e+04      |
-------------------------------------------
Eval num_timesteps=55000, episode_reward=218.56 +/- 228.34
Episode length: 16.10 +/- 6.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | 219          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0012831066 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0574      |
|    explained_variance   | 0.29         |
|    learning_rate        | 2.73e-05     |
|    loss                 | 4.67e+03     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000719    |
|    value_loss           | 1.02e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 27       |
|    time_elapsed    | 57       |
|    total_timesteps | 55296    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 16.2          |
|    ep_rew_mean          | 226           |
| time/                   |               |
|    fps                  | 961           |
|    iterations           | 28            |
|    time_elapsed         | 59            |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 9.1007794e-05 |
|    clip_fraction        | 0.00313       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0598       |
|    explained_variance   | 0.236         |
|    learning_rate        | 2.73e-05      |
|    loss                 | 5.38e+03      |
|    n_updates            | 270           |
|    policy_gradient_loss | 4.64e-05      |
|    value_loss           | 1.31e+04      |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16           |
|    ep_rew_mean          | 217          |
| time/                   |              |
|    fps                  | 961          |
|    iterations           | 29           |
|    time_elapsed         | 61           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0007713967 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0559      |
|    explained_variance   | 0.217        |
|    learning_rate        | 2.73e-05     |
|    loss                 | 6.33e+03     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.000284    |
|    value_loss           | 1.33e+04     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=262.88 +/- 201.49
Episode length: 16.60 +/- 5.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | 263           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 0.00044654624 |
|    clip_fraction        | 0.00547       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0635       |
|    explained_variance   | 0.236         |
|    learning_rate        | 2.73e-05      |
|    loss                 | 5.52e+03      |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.000234     |
|    value_loss           | 1.36e+04      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 222      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 30       |
|    time_elapsed    | 63       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 23:02:53,028] Trial 42 finished with value: 168.99991749999998 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 2.7326375411731923e-05, 'gamma': 0.9821234486082998, 'gae_lambda': 0.9562740761745663}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | -90.4    |
| time/              |          |
|    fps             | 1165     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 33.5         |
|    ep_rew_mean          | -86          |
| time/                   |              |
|    fps                  | 1069         |
|    iterations           | 2            |
|    time_elapsed         | 3            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0016200852 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.08        |
|    explained_variance   | -9.85e-05    |
|    learning_rate        | 1.44e-05     |
|    loss                 | 1.19e+03     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00232     |
|    value_loss           | 2.89e+03     |
------------------------------------------
Eval num_timesteps=5000, episode_reward=242.32 +/- 168.80
Episode length: 17.00 +/- 4.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 242         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009967621 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000194   |
|    learning_rate        | 1.44e-05    |
|    loss                 | 746         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 2e+03       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | -72.8    |
| time/              |          |
|    fps             | 1017     |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.1        |
|    ep_rew_mean          | -65.5       |
| time/                   |             |
|    fps                  | 1006        |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012576039 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | -0.00308    |
|    learning_rate        | 1.44e-05    |
|    loss                 | 889         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00799    |
|    value_loss           | 1.62e+03    |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=270.19 +/- 187.81
Episode length: 17.70 +/- 5.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 270         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.020141767 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | -0.00213    |
|    learning_rate        | 1.44e-05    |
|    loss                 | 584         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 1.2e+03     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | -51.9    |
| time/              |          |
|    fps             | 984      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.3        |
|    ep_rew_mean          | -16.1       |
| time/                   |             |
|    fps                  | 981         |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012998469 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.00593     |
|    learning_rate        | 1.44e-05    |
|    loss                 | 841         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 1.53e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | -9.34       |
| time/                   |             |
|    fps                  | 978         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.011867788 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.00195     |
|    learning_rate        | 1.44e-05    |
|    loss                 | 1.43e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00466     |
|    value_loss           | 2.95e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=225.02 +/- 149.72
Episode length: 16.20 +/- 4.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | 225         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.014478931 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.00593     |
|    learning_rate        | 1.44e-05    |
|    loss                 | 1.25e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 2.68e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 22       |
| time/              |          |
|    fps             | 971      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.8       |
|    ep_rew_mean          | -36.3      |
| time/                   |            |
|    fps                  | 971        |
|    iterations           | 9          |
|    time_elapsed         | 18         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.06345121 |
|    clip_fraction        | 0.535      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.92      |
|    explained_variance   | 0.00528    |
|    learning_rate        | 1.44e-05   |
|    loss                 | 1.75e+03   |
|    n_updates            | 80         |
|    policy_gradient_loss | 0.0487     |
|    value_loss           | 3.92e+03   |
----------------------------------------
Eval num_timesteps=20000, episode_reward=110.61 +/- 76.40
Episode length: 13.10 +/- 1.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 13.1       |
|    mean_reward          | 111        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.29573184 |
|    clip_fraction        | 0.536      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.72      |
|    explained_variance   | 0.00498    |
|    learning_rate        | 1.44e-05   |
|    loss                 | 1.02e+03   |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0412     |
|    value_loss           | 2.46e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | 55.1     |
| time/              |          |
|    fps             | 968      |
|    iterations      | 10       |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.4       |
|    ep_rew_mean          | -22.6      |
| time/                   |            |
|    fps                  | 969        |
|    iterations           | 11         |
|    time_elapsed         | 23         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.26903093 |
|    clip_fraction        | 0.712      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.69      |
|    explained_variance   | 0.00652    |
|    learning_rate        | 1.44e-05   |
|    loss                 | 2.43e+03   |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.119      |
|    value_loss           | 5.34e+03   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.9       |
|    ep_rew_mean          | 60.4       |
| time/                   |            |
|    fps                  | 969        |
|    iterations           | 12         |
|    time_elapsed         | 25         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.20998225 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.73      |
|    explained_variance   | 0.00678    |
|    learning_rate        | 1.44e-05   |
|    loss                 | 1.36e+03   |
|    n_updates            | 110        |
|    policy_gradient_loss | 0.0255     |
|    value_loss           | 2.52e+03   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=215.42 +/- 164.52
Episode length: 15.50 +/- 4.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.5       |
|    mean_reward          | 215        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.21311224 |
|    clip_fraction        | 0.712      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.76      |
|    explained_variance   | 0.00832    |
|    learning_rate        | 1.44e-05   |
|    loss                 | 2.65e+03   |
|    n_updates            | 120        |
|    policy_gradient_loss | 0.094      |
|    value_loss           | 6.15e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | -27.7    |
| time/              |          |
|    fps             | 966      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.8       |
|    ep_rew_mean          | 32.4       |
| time/                   |            |
|    fps                  | 967        |
|    iterations           | 14         |
|    time_elapsed         | 29         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.14017193 |
|    clip_fraction        | 0.416      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.79      |
|    explained_variance   | 0.00509    |
|    learning_rate        | 1.44e-05   |
|    loss                 | 1.4e+03    |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.0151     |
|    value_loss           | 2.77e+03   |
----------------------------------------
Eval num_timesteps=30000, episode_reward=202.37 +/- 178.00
Episode length: 15.50 +/- 4.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.5       |
|    mean_reward          | 202        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.07342771 |
|    clip_fraction        | 0.547      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.79      |
|    explained_variance   | 0.00981    |
|    learning_rate        | 1.44e-05   |
|    loss                 | 1.9e+03    |
|    n_updates            | 140        |
|    policy_gradient_loss | 0.0567     |
|    value_loss           | 3.75e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | -12.8    |
| time/              |          |
|    fps             | 964      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | -6.02       |
| time/                   |             |
|    fps                  | 964         |
|    iterations           | 16          |
|    time_elapsed         | 33          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.016410887 |
|    clip_fraction        | 0.419       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.00997     |
|    learning_rate        | 1.44e-05    |
|    loss                 | 1.25e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.00836     |
|    value_loss           | 2.41e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 44.8        |
| time/                   |             |
|    fps                  | 964         |
|    iterations           | 17          |
|    time_elapsed         | 36          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.015613552 |
|    clip_fraction        | 0.0932      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.0186      |
|    learning_rate        | 1.44e-05    |
|    loss                 | 1e+03       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 2.49e+03    |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=351.80 +/- 270.63
Episode length: 19.90 +/- 7.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.9         |
|    mean_reward          | 352          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0053493534 |
|    clip_fraction        | 0.369        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.0215       |
|    learning_rate        | 1.44e-05     |
|    loss                 | 1.84e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | 0.0342       |
|    value_loss           | 4.28e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.5     |
|    ep_rew_mean     | 30.4     |
| time/              |          |
|    fps             | 961      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.2        |
|    ep_rew_mean          | 66          |
| time/                   |             |
|    fps                  | 961         |
|    iterations           | 19          |
|    time_elapsed         | 40          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.028396979 |
|    clip_fraction        | 0.547       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | 0.0156      |
|    learning_rate        | 1.44e-05    |
|    loss                 | 2.14e+03    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 4.41e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=209.76 +/- 197.38
Episode length: 15.10 +/- 4.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 210         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.037889283 |
|    clip_fraction        | 0.368       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.0254      |
|    learning_rate        | 1.44e-05    |
|    loss                 | 2.45e+03    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00606    |
|    value_loss           | 5e+03       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 85.6     |
| time/              |          |
|    fps             | 959      |
|    iterations      | 20       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.5        |
|    ep_rew_mean          | 121         |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 21          |
|    time_elapsed         | 44          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.012466113 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.973      |
|    explained_variance   | 0.0228      |
|    learning_rate        | 1.44e-05    |
|    loss                 | 3.29e+03    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00567    |
|    value_loss           | 6.5e+03     |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=279.08 +/- 205.57
Episode length: 17.50 +/- 5.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.015321532 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.799      |
|    explained_variance   | 0.0209      |
|    learning_rate        | 1.44e-05    |
|    loss                 | 4.04e+03    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00788    |
|    value_loss           | 8.04e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 152      |
| time/              |          |
|    fps             | 957      |
|    iterations      | 22       |
|    time_elapsed    | 47       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.3        |
|    ep_rew_mean          | 147         |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 23          |
|    time_elapsed         | 49          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.010367742 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.0222      |
|    learning_rate        | 1.44e-05    |
|    loss                 | 6.37e+03    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00591    |
|    value_loss           | 1.08e+04    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.6        |
|    ep_rew_mean          | 157         |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 24          |
|    time_elapsed         | 51          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.003904095 |
|    clip_fraction        | 0.0791      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.581      |
|    explained_variance   | 0.0195      |
|    learning_rate        | 1.44e-05    |
|    loss                 | 6.47e+03    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 1.31e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=322.49 +/- 240.73
Episode length: 18.50 +/- 6.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.5         |
|    mean_reward          | 322          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0031675266 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.52        |
|    explained_variance   | 0.0255       |
|    learning_rate        | 1.44e-05     |
|    loss                 | 5.8e+03      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00319     |
|    value_loss           | 1.22e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 209      |
| time/              |          |
|    fps             | 956      |
|    iterations      | 25       |
|    time_elapsed    | 53       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.9        |
|    ep_rew_mean          | 182         |
| time/                   |             |
|    fps                  | 956         |
|    iterations           | 26          |
|    time_elapsed         | 55          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.002391791 |
|    clip_fraction        | 0.0387      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.0342      |
|    learning_rate        | 1.44e-05    |
|    loss                 | 5.85e+03    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 1.31e+04    |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=158.47 +/- 119.63
Episode length: 14.40 +/- 3.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | 158          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0013386938 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.386       |
|    explained_variance   | 0.032        |
|    learning_rate        | 1.44e-05     |
|    loss                 | 6.99e+03     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00172     |
|    value_loss           | 1.27e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 184      |
| time/              |          |
|    fps             | 955      |
|    iterations      | 27       |
|    time_elapsed    | 57       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | 198          |
| time/                   |              |
|    fps                  | 955          |
|    iterations           | 28           |
|    time_elapsed         | 60           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0017641217 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.33        |
|    explained_variance   | 0.0369       |
|    learning_rate        | 1.44e-05     |
|    loss                 | 6.87e+03     |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00121     |
|    value_loss           | 1.36e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.2         |
|    ep_rew_mean          | 194          |
| time/                   |              |
|    fps                  | 956          |
|    iterations           | 29           |
|    time_elapsed         | 62           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0012816598 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | 0.0517       |
|    learning_rate        | 1.44e-05     |
|    loss                 | 6.79e+03     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00145     |
|    value_loss           | 1.24e+04     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=102.64 +/- 120.55
Episode length: 12.60 +/- 2.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 12.6         |
|    mean_reward          | 103          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0015079557 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.25        |
|    explained_variance   | 0.0461       |
|    learning_rate        | 1.44e-05     |
|    loss                 | 7.05e+03     |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 1.41e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 231      |
| time/              |          |
|    fps             | 954      |
|    iterations      | 30       |
|    time_elapsed    | 64       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 23:03:58,147] Trial 43 finished with value: 208.3067871 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 1.438279444619316e-05, 'gamma': 0.9757452959548226, 'gae_lambda': 0.9720144787091721}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | -87.9    |
| time/              |          |
|    fps             | 1183     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.3        |
|    ep_rew_mean          | -83.2       |
| time/                   |             |
|    fps                  | 1066        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008263649 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.000223    |
|    learning_rate        | 0.000124    |
|    loss                 | 634         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.000682   |
|    value_loss           | 1.58e+03    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=193.03 +/- 144.20
Episode length: 15.30 +/- 3.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | 193         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.015133322 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.00376     |
|    learning_rate        | 0.000124    |
|    loss                 | 553         |
|    n_updates            | 20          |
|    policy_gradient_loss | 2.17e-05    |
|    value_loss           | 1.14e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.9     |
|    ep_rew_mean     | -67.4    |
| time/              |          |
|    fps             | 1015     |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.4        |
|    ep_rew_mean          | -52.6       |
| time/                   |             |
|    fps                  | 1010        |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.013113953 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.0217      |
|    learning_rate        | 0.000124    |
|    loss                 | 775         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00861    |
|    value_loss           | 1.7e+03     |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=185.58 +/- 218.83
Episode length: 15.00 +/- 5.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 186         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.016262291 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.0429      |
|    learning_rate        | 0.000124    |
|    loss                 | 626         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 1.41e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | -44.1    |
| time/              |          |
|    fps             | 996      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.5        |
|    ep_rew_mean          | -32.1       |
| time/                   |             |
|    fps                  | 995         |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.015622219 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.0783      |
|    learning_rate        | 0.000124    |
|    loss                 | 719         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 1.8e+03     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.8        |
|    ep_rew_mean          | -11.4       |
| time/                   |             |
|    fps                  | 993         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.012541089 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.0884      |
|    learning_rate        | 0.000124    |
|    loss                 | 738         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 1.94e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=275.70 +/- 175.71
Episode length: 17.70 +/- 4.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 276         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.008844645 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.000124    |
|    loss                 | 1.46e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 2.55e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 9.16     |
| time/              |          |
|    fps             | 983      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.6        |
|    ep_rew_mean          | 57.4        |
| time/                   |             |
|    fps                  | 982         |
|    iterations           | 9           |
|    time_elapsed         | 18          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.014083663 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.136       |
|    learning_rate        | 0.000124    |
|    loss                 | 963         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 2.56e+03    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=281.77 +/- 163.77
Episode length: 17.50 +/- 4.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17.5      |
|    mean_reward          | 282       |
| time/                   |           |
|    total_timesteps      | 20000     |
| train/                  |           |
|    approx_kl            | 0.0193415 |
|    clip_fraction        | 0.285     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.52     |
|    explained_variance   | 0.21      |
|    learning_rate        | 0.000124  |
|    loss                 | 1.37e+03  |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.0074   |
|    value_loss           | 3.31e+03  |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 31.2     |
| time/              |          |
|    fps             | 976      |
|    iterations      | 10       |
|    time_elapsed    | 20       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.2        |
|    ep_rew_mean          | 47.2        |
| time/                   |             |
|    fps                  | 975         |
|    iterations           | 11          |
|    time_elapsed         | 23          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.018748267 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.213       |
|    learning_rate        | 0.000124    |
|    loss                 | 1.25e+03    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 2.71e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.9        |
|    ep_rew_mean          | 82.2        |
| time/                   |             |
|    fps                  | 975         |
|    iterations           | 12          |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014078262 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.154       |
|    learning_rate        | 0.000124    |
|    loss                 | 1.91e+03    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 3.84e+03    |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=110.12 +/- 104.97
Episode length: 12.70 +/- 2.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 12.7       |
|    mean_reward          | 110        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01228955 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.138      |
|    learning_rate        | 0.000124   |
|    loss                 | 3.15e+03   |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.00342   |
|    value_loss           | 5.54e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | 78       |
| time/              |          |
|    fps             | 971      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.6        |
|    ep_rew_mean          | 118         |
| time/                   |             |
|    fps                  | 971         |
|    iterations           | 14          |
|    time_elapsed         | 29          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.019789826 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.172       |
|    learning_rate        | 0.000124    |
|    loss                 | 2.82e+03    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 5.01e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=390.78 +/- 164.81
Episode length: 20.80 +/- 5.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.8        |
|    mean_reward          | 391         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.015254074 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.829      |
|    explained_variance   | 0.164       |
|    learning_rate        | 0.000124    |
|    loss                 | 3.17e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00725    |
|    value_loss           | 6.79e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.5     |
|    ep_rew_mean     | 173      |
| time/              |          |
|    fps             | 967      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.8        |
|    ep_rew_mean          | 163         |
| time/                   |             |
|    fps                  | 968         |
|    iterations           | 16          |
|    time_elapsed         | 33          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.013131688 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.72       |
|    explained_variance   | 0.246       |
|    learning_rate        | 0.000124    |
|    loss                 | 3.53e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 7.47e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.5        |
|    ep_rew_mean          | 169         |
| time/                   |             |
|    fps                  | 968         |
|    iterations           | 17          |
|    time_elapsed         | 35          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.010409215 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.683      |
|    explained_variance   | 0.223       |
|    learning_rate        | 0.000124    |
|    loss                 | 4.48e+03    |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 8.34e+03    |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=198.40 +/- 156.27
Episode length: 15.50 +/- 4.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | 198          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0062206984 |
|    clip_fraction        | 0.0934       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.499       |
|    explained_variance   | 0.298        |
|    learning_rate        | 0.000124     |
|    loss                 | 3.31e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00542     |
|    value_loss           | 7.9e+03      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 195      |
| time/              |          |
|    fps             | 965      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.2        |
|    ep_rew_mean          | 173         |
| time/                   |             |
|    fps                  | 965         |
|    iterations           | 19          |
|    time_elapsed         | 40          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.007852655 |
|    clip_fraction        | 0.048       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.437      |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.000124    |
|    loss                 | 3.85e+03    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 9.22e+03    |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=168.24 +/- 126.76
Episode length: 14.30 +/- 2.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.3         |
|    mean_reward          | 168          |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0026054822 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.327       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.000124     |
|    loss                 | 4.6e+03      |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.000808    |
|    value_loss           | 7.9e+03      |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 190      |
| time/              |          |
|    fps             | 962      |
|    iterations      | 20       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | 201          |
| time/                   |              |
|    fps                  | 962          |
|    iterations           | 21           |
|    time_elapsed         | 44           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0018428722 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.268       |
|    explained_variance   | 0.373        |
|    learning_rate        | 0.000124     |
|    loss                 | 3.49e+03     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.000207    |
|    value_loss           | 7.19e+03     |
------------------------------------------
Eval num_timesteps=45000, episode_reward=325.96 +/- 267.43
Episode length: 18.40 +/- 7.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.4        |
|    mean_reward          | 326         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.001784586 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.000124    |
|    loss                 | 5.14e+03    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00242    |
|    value_loss           | 8.55e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 22       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.9         |
|    ep_rew_mean          | 202          |
| time/                   |              |
|    fps                  | 961          |
|    iterations           | 23           |
|    time_elapsed         | 48           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0010838392 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.000124     |
|    loss                 | 4.65e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000396    |
|    value_loss           | 8.12e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | 200          |
| time/                   |              |
|    fps                  | 961          |
|    iterations           | 24           |
|    time_elapsed         | 51           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0015370846 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.162       |
|    explained_variance   | 0.365        |
|    learning_rate        | 0.000124     |
|    loss                 | 3.84e+03     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00177     |
|    value_loss           | 8.54e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=177.59 +/- 99.52
Episode length: 14.60 +/- 2.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | 178          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0013888946 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.135       |
|    explained_variance   | 0.404        |
|    learning_rate        | 0.000124     |
|    loss                 | 4.09e+03     |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 8.41e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 229      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 25       |
|    time_elapsed    | 53       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | 230          |
| time/                   |              |
|    fps                  | 960          |
|    iterations           | 26           |
|    time_elapsed         | 55           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0024807348 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.000124     |
|    loss                 | 3.63e+03     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00069     |
|    value_loss           | 8.98e+03     |
------------------------------------------
Eval num_timesteps=55000, episode_reward=220.60 +/- 144.70
Episode length: 15.60 +/- 3.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | 221          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0016405196 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.091       |
|    explained_variance   | 0.397        |
|    learning_rate        | 0.000124     |
|    loss                 | 4.88e+03     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000904    |
|    value_loss           | 9.91e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 210      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 27       |
|    time_elapsed    | 57       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.1         |
|    ep_rew_mean          | 226          |
| time/                   |              |
|    fps                  | 959          |
|    iterations           | 28           |
|    time_elapsed         | 59           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0003862067 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.09        |
|    explained_variance   | 0.317        |
|    learning_rate        | 0.000124     |
|    loss                 | 5.4e+03      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000726    |
|    value_loss           | 1.01e+04     |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 16.3          |
|    ep_rew_mean          | 225           |
| time/                   |               |
|    fps                  | 960           |
|    iterations           | 29            |
|    time_elapsed         | 61            |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00045685665 |
|    clip_fraction        | 0.00806       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0686       |
|    explained_variance   | 0.392         |
|    learning_rate        | 0.000124      |
|    loss                 | 5.93e+03      |
|    n_updates            | 280           |
|    policy_gradient_loss | -0.000491     |
|    value_loss           | 9.91e+03      |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=175.37 +/- 123.93
Episode length: 14.30 +/- 3.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.3          |
|    mean_reward          | 175           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 0.00045964017 |
|    clip_fraction        | 0.0063        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0651       |
|    explained_variance   | 0.309         |
|    learning_rate        | 0.000124      |
|    loss                 | 4.65e+03      |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.000761     |
|    value_loss           | 9.72e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | 211      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 30       |
|    time_elapsed    | 64       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 23:05:02,979] Trial 44 finished with value: 240.55235739999998 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 0.00012400173010989704, 'gamma': 0.9623122930440845, 'gae_lambda': 0.9824503437121291}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.6     |
|    ep_rew_mean     | -94.3    |
| time/              |          |
|    fps             | 1188     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.4        |
|    ep_rew_mean          | -80.8       |
| time/                   |             |
|    fps                  | 883         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011180447 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000108   |
|    learning_rate        | 6.39e-05    |
|    loss                 | 935         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0069     |
|    value_loss           | 1.32e+03    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=160.46 +/- 163.13
Episode length: 14.70 +/- 4.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | 160          |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0109983925 |
|    clip_fraction        | 0.201        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.0216       |
|    learning_rate        | 6.39e-05     |
|    loss                 | 444          |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00695     |
|    value_loss           | 943          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.3     |
|    ep_rew_mean     | -76.4    |
| time/              |          |
|    fps             | 799      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.4        |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 782         |
|    iterations           | 4           |
|    time_elapsed         | 10          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.013282175 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.0703      |
|    learning_rate        | 6.39e-05    |
|    loss                 | 399         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00699    |
|    value_loss           | 736         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=238.43 +/- 139.00
Episode length: 15.70 +/- 3.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 238         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.016918017 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.98       |
|    explained_variance   | 0.118       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 310         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 782         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | -39.8    |
| time/              |          |
|    fps             | 762      |
|    iterations      | 5        |
|    time_elapsed    | 13       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.7        |
|    ep_rew_mean          | -26.4       |
| time/                   |             |
|    fps                  | 750         |
|    iterations           | 6           |
|    time_elapsed         | 16          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.019764788 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.134       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 716         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 1.19e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.6        |
|    ep_rew_mean          | 26.6        |
| time/                   |             |
|    fps                  | 741         |
|    iterations           | 7           |
|    time_elapsed         | 19          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.015034212 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.132       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 694         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 1.2e+03     |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=178.68 +/- 129.43
Episode length: 14.80 +/- 3.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.018902924 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.209       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 961         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0252     |
|    value_loss           | 1.84e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | 39.1     |
| time/              |          |
|    fps             | 732      |
|    iterations      | 8        |
|    time_elapsed    | 22       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.5        |
|    ep_rew_mean          | 48.1        |
| time/                   |             |
|    fps                  | 727         |
|    iterations           | 9           |
|    time_elapsed         | 25          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.018415894 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.17        |
|    learning_rate        | 6.39e-05    |
|    loss                 | 796         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 2.75e+03    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=170.90 +/- 162.14
Episode length: 14.50 +/- 3.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 171         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.019598715 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.194       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 1.71e+03    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 3.07e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | 115      |
| time/              |          |
|    fps             | 722      |
|    iterations      | 10       |
|    time_elapsed    | 28       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.2        |
|    ep_rew_mean          | 115         |
| time/                   |             |
|    fps                  | 723         |
|    iterations           | 11          |
|    time_elapsed         | 31          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.018710744 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.942      |
|    explained_variance   | 0.238       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 1.73e+03    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00749    |
|    value_loss           | 4.55e+03    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.3       |
|    ep_rew_mean          | 136        |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 12         |
|    time_elapsed         | 34         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.01144455 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.804     |
|    explained_variance   | 0.303      |
|    learning_rate        | 6.39e-05   |
|    loss                 | 2.38e+03   |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.00731   |
|    value_loss           | 4.82e+03   |
----------------------------------------
Eval num_timesteps=25000, episode_reward=245.50 +/- 158.72
Episode length: 16.80 +/- 4.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | 246         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011215503 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.696      |
|    explained_variance   | 0.268       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 1.98e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00667    |
|    value_loss           | 5.17e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 172      |
| time/              |          |
|    fps             | 719      |
|    iterations      | 13       |
|    time_elapsed    | 37       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15          |
|    ep_rew_mean          | 163         |
| time/                   |             |
|    fps                  | 719         |
|    iterations           | 14          |
|    time_elapsed         | 39          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.010069046 |
|    clip_fraction        | 0.0778      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.337       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 3.15e+03    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0032     |
|    value_loss           | 5.98e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=316.03 +/- 120.24
Episode length: 19.00 +/- 3.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19           |
|    mean_reward          | 316          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0037425319 |
|    clip_fraction        | 0.046        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.354       |
|    explained_variance   | 0.363        |
|    learning_rate        | 6.39e-05     |
|    loss                 | 3.02e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000235    |
|    value_loss           | 7.4e+03      |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 178      |
| time/              |          |
|    fps             | 717      |
|    iterations      | 15       |
|    time_elapsed    | 42       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 15.7        |
|    ep_rew_mean          | 191         |
| time/                   |             |
|    fps                  | 715         |
|    iterations           | 16          |
|    time_elapsed         | 45          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.004793181 |
|    clip_fraction        | 0.0452      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.322      |
|    explained_variance   | 0.381       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 3.37e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 7.73e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.5         |
|    ep_rew_mean          | 229          |
| time/                   |              |
|    fps                  | 715          |
|    iterations           | 17           |
|    time_elapsed         | 48           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0032413555 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.192       |
|    explained_variance   | 0.363        |
|    learning_rate        | 6.39e-05     |
|    loss                 | 4.8e+03      |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 7.85e+03     |
------------------------------------------
Eval num_timesteps=35000, episode_reward=230.30 +/- 252.41
Episode length: 16.30 +/- 7.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | 230          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0032781297 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.152       |
|    explained_variance   | 0.453        |
|    learning_rate        | 6.39e-05     |
|    loss                 | 3.25e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000669    |
|    value_loss           | 6.67e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 714      |
|    iterations      | 18       |
|    time_elapsed    | 51       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.3         |
|    ep_rew_mean          | 228          |
| time/                   |              |
|    fps                  | 714          |
|    iterations           | 19           |
|    time_elapsed         | 54           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0015171496 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.505        |
|    learning_rate        | 6.39e-05     |
|    loss                 | 2.54e+03     |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.000357    |
|    value_loss           | 7.54e+03     |
------------------------------------------
Eval num_timesteps=40000, episode_reward=159.86 +/- 104.56
Episode length: 14.10 +/- 2.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.1        |
|    mean_reward          | 160         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.004682517 |
|    clip_fraction        | 0.0202      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0995     |
|    explained_variance   | 0.439       |
|    learning_rate        | 6.39e-05    |
|    loss                 | 4.69e+03    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 8.9e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | 216      |
| time/              |          |
|    fps             | 713      |
|    iterations      | 20       |
|    time_elapsed    | 57       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.3         |
|    ep_rew_mean          | 205          |
| time/                   |              |
|    fps                  | 714          |
|    iterations           | 21           |
|    time_elapsed         | 60           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0056176647 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0633      |
|    explained_variance   | 0.411        |
|    learning_rate        | 6.39e-05     |
|    loss                 | 5.62e+03     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.000842    |
|    value_loss           | 8.1e+03      |
------------------------------------------
Eval num_timesteps=45000, episode_reward=253.05 +/- 165.26
Episode length: 17.20 +/- 4.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.2          |
|    mean_reward          | 253           |
| time/                   |               |
|    total_timesteps      | 45000         |
| train/                  |               |
|    approx_kl            | 0.00046102068 |
|    clip_fraction        | 0.00444       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0316       |
|    explained_variance   | 0.48          |
|    learning_rate        | 6.39e-05      |
|    loss                 | 2.33e+03      |
|    n_updates            | 210           |
|    policy_gradient_loss | -0.000907     |
|    value_loss           | 7.62e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | 236      |
| time/              |          |
|    fps             | 713      |
|    iterations      | 22       |
|    time_elapsed    | 63       |
|    total_timesteps | 45056    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 15.8          |
|    ep_rew_mean          | 222           |
| time/                   |               |
|    fps                  | 714           |
|    iterations           | 23            |
|    time_elapsed         | 65            |
|    total_timesteps      | 47104         |
| train/                  |               |
|    approx_kl            | 0.00096864405 |
|    clip_fraction        | 0.00269       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0.427         |
|    learning_rate        | 6.39e-05      |
|    loss                 | 2.87e+03      |
|    n_updates            | 220           |
|    policy_gradient_loss | -0.00043      |
|    value_loss           | 7.89e+03      |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.6         |
|    ep_rew_mean          | 210          |
| time/                   |              |
|    fps                  | 713          |
|    iterations           | 24           |
|    time_elapsed         | 68           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 5.551323e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0156      |
|    explained_variance   | 0.459        |
|    learning_rate        | 6.39e-05     |
|    loss                 | 5.27e+03     |
|    n_updates            | 230          |
|    policy_gradient_loss | -3.35e-05    |
|    value_loss           | 7.99e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=222.24 +/- 138.41
Episode length: 16.40 +/- 4.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | 222          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 9.518815e-06 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0205      |
|    explained_variance   | 0.464        |
|    learning_rate        | 6.39e-05     |
|    loss                 | 3.64e+03     |
|    n_updates            | 240          |
|    policy_gradient_loss | -2.42e-05    |
|    value_loss           | 7.59e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 223      |
| time/              |          |
|    fps             | 711      |
|    iterations      | 25       |
|    time_elapsed    | 71       |
|    total_timesteps | 51200    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 16.1          |
|    ep_rew_mean          | 232           |
| time/                   |               |
|    fps                  | 711           |
|    iterations           | 26            |
|    time_elapsed         | 74            |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 0.00034937312 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0214       |
|    explained_variance   | 0.501         |
|    learning_rate        | 6.39e-05      |
|    loss                 | 2.66e+03      |
|    n_updates            | 250           |
|    policy_gradient_loss | -0.000258     |
|    value_loss           | 7.02e+03      |
-------------------------------------------
Eval num_timesteps=55000, episode_reward=254.04 +/- 171.26
Episode length: 16.50 +/- 4.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | 254          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0003922337 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0182      |
|    explained_variance   | 0.463        |
|    learning_rate        | 6.39e-05     |
|    loss                 | 4.82e+03     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.0004      |
|    value_loss           | 7.73e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 238      |
| time/              |          |
|    fps             | 710      |
|    iterations      | 27       |
|    time_elapsed    | 77       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.8         |
|    ep_rew_mean          | 228          |
| time/                   |              |
|    fps                  | 710          |
|    iterations           | 28           |
|    time_elapsed         | 80           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0016299295 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0165      |
|    explained_variance   | 0.486        |
|    learning_rate        | 6.39e-05     |
|    loss                 | 5.42e+03     |
|    n_updates            | 270          |
|    policy_gradient_loss | 0.000199     |
|    value_loss           | 8.5e+03      |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 16.4          |
|    ep_rew_mean          | 238           |
| time/                   |               |
|    fps                  | 710           |
|    iterations           | 29            |
|    time_elapsed         | 83            |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 2.3138215e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00772      |
|    explained_variance   | 0.486         |
|    learning_rate        | 6.39e-05      |
|    loss                 | 2.62e+03      |
|    n_updates            | 280           |
|    policy_gradient_loss | -3.09e-05     |
|    value_loss           | 8.5e+03       |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=235.14 +/- 147.48
Episode length: 16.10 +/- 3.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | 235           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 0.00018905065 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0103       |
|    explained_variance   | 0.512         |
|    learning_rate        | 6.39e-05      |
|    loss                 | 6.27e+03      |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.000109     |
|    value_loss           | 7.32e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | 239      |
| time/              |          |
|    fps             | 709      |
|    iterations      | 30       |
|    time_elapsed    | 86       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 23:06:31,129] Trial 45 finished with value: 184.02407680000002 and parameters: {'n_steps': 2048, 'batch_size': 32, 'learning_rate': 6.387166829513581e-05, 'gamma': 0.9794325314369672, 'gae_lambda': 0.9401342055849607}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.6     |
|    ep_rew_mean     | -91.2    |
| time/              |          |
|    fps             | 1178     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=168.92 +/- 157.56
Episode length: 14.30 +/- 4.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.3        |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.006551964 |
|    clip_fraction        | 0.032       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 2.65e-05    |
|    learning_rate        | 1.02e-05    |
|    loss                 | 1.34e+03    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00412    |
|    value_loss           | 3.06e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.6     |
|    ep_rew_mean     | -80.6    |
| time/              |          |
|    fps             | 1054     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=204.77 +/- 168.49
Episode length: 15.70 +/- 4.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 205         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.012904204 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.00236     |
|    learning_rate        | 1.02e-05    |
|    loss                 | 771         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 1.8e+03     |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -70.2    |
| time/              |          |
|    fps             | 1018     |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=398.91 +/- 165.44
Episode length: 21.10 +/- 5.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.1        |
|    mean_reward          | 399         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.015814774 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | -0.00667    |
|    learning_rate        | 1.02e-05    |
|    loss                 | 640         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 1.42e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.6     |
|    ep_rew_mean     | -60.6    |
| time/              |          |
|    fps             | 999      |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=122.79 +/- 106.57
Episode length: 12.80 +/- 2.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 12.8        |
|    mean_reward          | 123         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.015528354 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | -0.0041     |
|    learning_rate        | 1.02e-05    |
|    loss                 | 843         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 1.56e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -23.8    |
| time/              |          |
|    fps             | 991      |
|    iterations      | 5        |
|    time_elapsed    | 20       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.6        |
|    ep_rew_mean          | -8.08       |
| time/                   |             |
|    fps                  | 989         |
|    iterations           | 6           |
|    time_elapsed         | 24          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.017076591 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -0.0029     |
|    learning_rate        | 1.02e-05    |
|    loss                 | 814         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 2.1e+03     |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=230.54 +/- 164.62
Episode length: 16.00 +/- 4.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 231         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011772946 |
|    clip_fraction        | 0.0825      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.00666     |
|    learning_rate        | 1.02e-05    |
|    loss                 | 1.65e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.000915    |
|    value_loss           | 2.9e+03     |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 8.1      |
| time/              |          |
|    fps             | 983      |
|    iterations      | 7        |
|    time_elapsed    | 29       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=215.03 +/- 172.12
Episode length: 15.90 +/- 4.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | 215          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0135362465 |
|    clip_fraction        | 0.131        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.00502      |
|    learning_rate        | 1.02e-05     |
|    loss                 | 1.28e+03     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.0136      |
|    value_loss           | 3.82e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 30.1     |
| time/              |          |
|    fps             | 977      |
|    iterations      | 8        |
|    time_elapsed    | 33       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=182.77 +/- 145.27
Episode length: 14.70 +/- 3.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.7        |
|    mean_reward          | 183         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014233095 |
|    clip_fraction        | 0.503       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.00788     |
|    learning_rate        | 1.02e-05    |
|    loss                 | 2.34e+03    |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.0536      |
|    value_loss           | 4.33e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 4.82     |
| time/              |          |
|    fps             | 974      |
|    iterations      | 9        |
|    time_elapsed    | 37       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=169.56 +/- 120.01
Episode length: 14.60 +/- 3.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.6        |
|    mean_reward          | 170         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.018458953 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.00513     |
|    learning_rate        | 1.02e-05    |
|    loss                 | 1.9e+03     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 3.74e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 43.6     |
| time/              |          |
|    fps             | 970      |
|    iterations      | 10       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=304.98 +/- 104.24
Episode length: 18.00 +/- 3.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 305         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.030276924 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.00858     |
|    learning_rate        | 1.02e-05    |
|    loss                 | 3.14e+03    |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.00551     |
|    value_loss           | 5.39e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 86.9     |
| time/              |          |
|    fps             | 968      |
|    iterations      | 11       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.6       |
|    ep_rew_mean          | 133        |
| time/                   |            |
|    fps                  | 968        |
|    iterations           | 12         |
|    time_elapsed         | 50         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.10942224 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.00759    |
|    learning_rate        | 1.02e-05   |
|    loss                 | 3.66e+03   |
|    n_updates            | 110        |
|    policy_gradient_loss | 0.00226    |
|    value_loss           | 7.34e+03   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=293.09 +/- 147.32
Episode length: 17.70 +/- 4.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 293        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.04189463 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.792     |
|    explained_variance   | 0.007      |
|    learning_rate        | 1.02e-05   |
|    loss                 | 4.96e+03   |
|    n_updates            | 120        |
|    policy_gradient_loss | 1.52e-05   |
|    value_loss           | 1.06e+04   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | 140      |
| time/              |          |
|    fps             | 966      |
|    iterations      | 13       |
|    time_elapsed    | 55       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=189.80 +/- 155.21
Episode length: 14.30 +/- 2.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.3         |
|    mean_reward          | 190          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0065571684 |
|    clip_fraction        | 0.0866       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.671       |
|    explained_variance   | 0.00602      |
|    learning_rate        | 1.02e-05     |
|    loss                 | 5.34e+03     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.0074      |
|    value_loss           | 1.12e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 160      |
| time/              |          |
|    fps             | 964      |
|    iterations      | 14       |
|    time_elapsed    | 59       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=150.59 +/- 133.15
Episode length: 13.90 +/- 3.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.9        |
|    mean_reward          | 151         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.004777018 |
|    clip_fraction        | 0.0497      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.591      |
|    explained_variance   | 0.0116      |
|    learning_rate        | 1.02e-05    |
|    loss                 | 7.11e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00403    |
|    value_loss           | 1.26e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 185      |
| time/              |          |
|    fps             | 962      |
|    iterations      | 15       |
|    time_elapsed    | 63       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 23:07:36,053] Trial 46 finished with value: 167.56777340000002 and parameters: {'n_steps': 4096, 'batch_size': 128, 'learning_rate': 1.0207339419674276e-05, 'gamma': 0.991466693469466, 'gae_lambda': 0.962885008397504}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.6     |
|    ep_rew_mean     | -86.3    |
| time/              |          |
|    fps             | 1173     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37          |
|    ep_rew_mean          | -75.2       |
| time/                   |             |
|    fps                  | 1069        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012917258 |
|    clip_fraction        | 0.0438      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 1.2e-05     |
|    learning_rate        | 2.14e-05    |
|    loss                 | 799         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00632    |
|    value_loss           | 1.82e+03    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-113.59 +/- 6.91
Episode length: 19.80 +/- 5.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.8        |
|    mean_reward          | -114        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.014436555 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | -0.00204    |
|    learning_rate        | 2.14e-05    |
|    loss                 | 542         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 1.26e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | -68.3    |
| time/              |          |
|    fps             | 1013     |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.1        |
|    ep_rew_mean          | -62.1       |
| time/                   |             |
|    fps                  | 1005        |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.015817735 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.00492     |
|    learning_rate        | 2.14e-05    |
|    loss                 | 426         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 1.16e+03    |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=223.01 +/- 173.45
Episode length: 15.90 +/- 4.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.9       |
|    mean_reward          | 223        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01214998 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.94      |
|    explained_variance   | 0.00904    |
|    learning_rate        | 2.14e-05   |
|    loss                 | 590        |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 1.06e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | -37.1    |
| time/              |          |
|    fps             | 987      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | -25.2       |
| time/                   |             |
|    fps                  | 985         |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.016463244 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.0106      |
|    learning_rate        | 2.14e-05    |
|    loss                 | 738         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 1.41e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | 16          |
| time/                   |             |
|    fps                  | 983         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.013636073 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.0201      |
|    learning_rate        | 2.14e-05    |
|    loss                 | 794         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 1.53e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=297.24 +/- 164.46
Episode length: 17.90 +/- 5.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.9        |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.008510349 |
|    clip_fraction        | 0.366       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.00805     |
|    learning_rate        | 2.14e-05    |
|    loss                 | 1.28e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.0411      |
|    value_loss           | 2.45e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -2.61    |
| time/              |          |
|    fps             | 974      |
|    iterations      | 8        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.6        |
|    ep_rew_mean          | 10.8        |
| time/                   |             |
|    fps                  | 974         |
|    iterations           | 9           |
|    time_elapsed         | 18          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.011555016 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.0118      |
|    learning_rate        | 2.14e-05    |
|    loss                 | 1.55e+03    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 2.32e+03    |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=188.73 +/- 221.77
Episode length: 14.90 +/- 5.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.013159922 |
|    clip_fraction        | 0.0197      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.017       |
|    learning_rate        | 2.14e-05    |
|    loss                 | 922         |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00307    |
|    value_loss           | 2.22e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 33.7     |
| time/              |          |
|    fps             | 969      |
|    iterations      | 10       |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.4        |
|    ep_rew_mean          | 83.2        |
| time/                   |             |
|    fps                  | 970         |
|    iterations           | 11          |
|    time_elapsed         | 23          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.045340255 |
|    clip_fraction        | 0.417       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.42       |
|    explained_variance   | 0.0265      |
|    learning_rate        | 2.14e-05    |
|    loss                 | 1.42e+03    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00178    |
|    value_loss           | 3.22e+03    |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 16.6      |
|    ep_rew_mean          | 91.1      |
| time/                   |           |
|    fps                  | 970       |
|    iterations           | 12        |
|    time_elapsed         | 25        |
|    total_timesteps      | 24576     |
| train/                  |           |
|    approx_kl            | 0.0758582 |
|    clip_fraction        | 0.194     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.11     |
|    explained_variance   | 0.0257    |
|    learning_rate        | 2.14e-05  |
|    loss                 | 2.36e+03  |
|    n_updates            | 110       |
|    policy_gradient_loss | -0.00383  |
|    value_loss           | 4.49e+03  |
---------------------------------------
Eval num_timesteps=25000, episode_reward=295.57 +/- 206.95
Episode length: 18.40 +/- 5.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.4        |
|    mean_reward          | 296         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010899804 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.961      |
|    explained_variance   | 0.0284      |
|    learning_rate        | 2.14e-05    |
|    loss                 | 2.43e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 5.07e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 105      |
| time/              |          |
|    fps             | 964      |
|    iterations      | 13       |
|    time_elapsed    | 27       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.8        |
|    ep_rew_mean          | 141         |
| time/                   |             |
|    fps                  | 965         |
|    iterations           | 14          |
|    time_elapsed         | 29          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.010375043 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.831      |
|    explained_variance   | 0.0406      |
|    learning_rate        | 2.14e-05    |
|    loss                 | 2.27e+03    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 5.76e+03    |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=208.33 +/- 207.74
Episode length: 15.80 +/- 6.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | 208         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.014286425 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.0406      |
|    learning_rate        | 2.14e-05    |
|    loss                 | 4.15e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 7.45e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 962      |
|    iterations      | 15       |
|    time_elapsed    | 31       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.3         |
|    ep_rew_mean          | 167          |
| time/                   |              |
|    fps                  | 963          |
|    iterations           | 16           |
|    time_elapsed         | 34           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0039404454 |
|    clip_fraction        | 0.053        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.661       |
|    explained_variance   | 0.0341       |
|    learning_rate        | 2.14e-05     |
|    loss                 | 4.36e+03     |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00109     |
|    value_loss           | 9e+03        |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 17.4         |
|    ep_rew_mean          | 222          |
| time/                   |              |
|    fps                  | 964          |
|    iterations           | 17           |
|    time_elapsed         | 36           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0036803691 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.593       |
|    explained_variance   | 0.0491       |
|    learning_rate        | 2.14e-05     |
|    loss                 | 4.43e+03     |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000746    |
|    value_loss           | 9.14e+03     |
------------------------------------------
Eval num_timesteps=35000, episode_reward=200.54 +/- 243.36
Episode length: 15.80 +/- 6.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.003282372 |
|    clip_fraction        | 0.0523      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.0256      |
|    learning_rate        | 2.14e-05    |
|    loss                 | 7.61e+03    |
|    n_updates            | 170         |
|    policy_gradient_loss | 0.000525    |
|    value_loss           | 1.42e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 180      |
| time/              |          |
|    fps             | 961      |
|    iterations      | 18       |
|    time_elapsed    | 38       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.7         |
|    ep_rew_mean          | 180          |
| time/                   |              |
|    fps                  | 961          |
|    iterations           | 19           |
|    time_elapsed         | 40           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0076983785 |
|    clip_fraction        | 0.0545       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.427       |
|    explained_variance   | 0.0661       |
|    learning_rate        | 2.14e-05     |
|    loss                 | 4.67e+03     |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00341     |
|    value_loss           | 1.08e+04     |
------------------------------------------
Eval num_timesteps=40000, episode_reward=167.89 +/- 113.06
Episode length: 14.20 +/- 2.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.2         |
|    mean_reward          | 168          |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0010496103 |
|    clip_fraction        | 0.0406       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.358       |
|    explained_variance   | 0.0902       |
|    learning_rate        | 2.14e-05     |
|    loss                 | 4.93e+03     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 9.18e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | 223      |
| time/              |          |
|    fps             | 960      |
|    iterations      | 20       |
|    time_elapsed    | 42       |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 17           |
|    ep_rew_mean          | 226          |
| time/                   |              |
|    fps                  | 960          |
|    iterations           | 21           |
|    time_elapsed         | 44           |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0014237831 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.351       |
|    explained_variance   | 0.0946       |
|    learning_rate        | 2.14e-05     |
|    loss                 | 5.63e+03     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 9.58e+03     |
------------------------------------------
Eval num_timesteps=45000, episode_reward=284.62 +/- 171.99
Episode length: 18.00 +/- 5.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.002614601 |
|    clip_fraction        | 0.0506      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.291      |
|    explained_variance   | 0.0939      |
|    learning_rate        | 2.14e-05    |
|    loss                 | 5.47e+03    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 1.08e+04    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 242      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 22       |
|    time_elapsed    | 46       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.7         |
|    ep_rew_mean          | 233          |
| time/                   |              |
|    fps                  | 960          |
|    iterations           | 23           |
|    time_elapsed         | 49           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0007520423 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.276       |
|    explained_variance   | 0.0928       |
|    learning_rate        | 2.14e-05     |
|    loss                 | 5.47e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | -6.37e-05    |
|    value_loss           | 1.18e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16           |
|    ep_rew_mean          | 207          |
| time/                   |              |
|    fps                  | 960          |
|    iterations           | 24           |
|    time_elapsed         | 51           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0016464773 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.209       |
|    explained_variance   | 0.136        |
|    learning_rate        | 2.14e-05     |
|    loss                 | 5.48e+03     |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.000781    |
|    value_loss           | 1.12e+04     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=260.40 +/- 233.85
Episode length: 16.60 +/- 5.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | 260          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0010277068 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0.157        |
|    learning_rate        | 2.14e-05     |
|    loss                 | 4.99e+03     |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000418    |
|    value_loss           | 9.92e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 204      |
| time/              |          |
|    fps             | 959      |
|    iterations      | 25       |
|    time_elapsed    | 53       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.4         |
|    ep_rew_mean          | 224          |
| time/                   |              |
|    fps                  | 959          |
|    iterations           | 26           |
|    time_elapsed         | 55           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0013592928 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0.164        |
|    learning_rate        | 2.14e-05     |
|    loss                 | 5.17e+03     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00055     |
|    value_loss           | 9.82e+03     |
------------------------------------------
Eval num_timesteps=55000, episode_reward=137.18 +/- 116.44
Episode length: 13.50 +/- 3.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.5         |
|    mean_reward          | 137          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0005888848 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.157       |
|    explained_variance   | 0.167        |
|    learning_rate        | 2.14e-05     |
|    loss                 | 5.08e+03     |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000777    |
|    value_loss           | 1.03e+04     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 232      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 27       |
|    time_elapsed    | 57       |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16           |
|    ep_rew_mean          | 215          |
| time/                   |              |
|    fps                  | 959          |
|    iterations           | 28           |
|    time_elapsed         | 59           |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0006848546 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.178        |
|    learning_rate        | 2.14e-05     |
|    loss                 | 5.37e+03     |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 1.09e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 15.2         |
|    ep_rew_mean          | 192          |
| time/                   |              |
|    fps                  | 960          |
|    iterations           | 29           |
|    time_elapsed         | 61           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0015176854 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.201        |
|    learning_rate        | 2.14e-05     |
|    loss                 | 4.49e+03     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 9.88e+03     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=239.69 +/- 163.55
Episode length: 16.20 +/- 4.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | 240           |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 0.00030420424 |
|    clip_fraction        | 0.00713       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.111        |
|    explained_variance   | 0.2           |
|    learning_rate        | 2.14e-05      |
|    loss                 | 5.1e+03       |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.000352     |
|    value_loss           | 9.74e+03      |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | 224      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 30       |
|    time_elapsed    | 64       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 23:08:41,361] Trial 47 finished with value: 293.7355208 and parameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 2.1366714043179895e-05, 'gamma': 0.9673026110930657, 'gae_lambda': 0.947594535178392}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.8     |
|    ep_rew_mean     | -85      |
| time/              |          |
|    fps             | 1189     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-115.94 +/- 0.05
Episode length: 28.80 +/- 7.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.8        |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010052979 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000254   |
|    learning_rate        | 3.34e-05    |
|    loss                 | 618         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 1.01e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.2     |
|    ep_rew_mean     | -87.7    |
| time/              |          |
|    fps             | 867      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=58.96 +/- 77.60
Episode length: 15.70 +/- 4.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 59          |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009220639 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.003       |
|    learning_rate        | 3.34e-05    |
|    loss                 | 350         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 879         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | -73      |
| time/              |          |
|    fps             | 800      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=243.47 +/- 170.62
Episode length: 15.60 +/- 3.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | 243         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.013456771 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.0308      |
|    learning_rate        | 3.34e-05    |
|    loss                 | 272         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 804         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | -57.2    |
| time/              |          |
|    fps             | 771      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=153.42 +/- 154.22
Episode length: 14.30 +/- 4.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.3        |
|    mean_reward          | 153         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011628859 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.0831      |
|    learning_rate        | 3.34e-05    |
|    loss                 | 555         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 845         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | -37.3    |
| time/              |          |
|    fps             | 752      |
|    iterations      | 5        |
|    time_elapsed    | 27       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | -28.9      |
| time/                   |            |
|    fps                  | 744        |
|    iterations           | 6          |
|    time_elapsed         | 33         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.01434559 |
|    clip_fraction        | 0.261      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.9       |
|    explained_variance   | 0.179      |
|    learning_rate        | 3.34e-05   |
|    loss                 | 616        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0239    |
|    value_loss           | 842        |
----------------------------------------
Eval num_timesteps=25000, episode_reward=182.41 +/- 136.98
Episode length: 15.10 +/- 3.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.015425426 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.251       |
|    learning_rate        | 3.34e-05    |
|    loss                 | 535         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 973         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 9.64     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 7        |
|    time_elapsed    | 39       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=383.77 +/- 255.59
Episode length: 20.60 +/- 7.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.6        |
|    mean_reward          | 384         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.017678922 |
|    clip_fraction        | 0.364       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.248       |
|    learning_rate        | 3.34e-05    |
|    loss                 | 755         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0331     |
|    value_loss           | 1.33e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.2     |
|    ep_rew_mean     | 22.1     |
| time/              |          |
|    fps             | 731      |
|    iterations      | 8        |
|    time_elapsed    | 44       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=290.72 +/- 228.90
Episode length: 18.10 +/- 6.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.1        |
|    mean_reward          | 291         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.016489912 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.254       |
|    learning_rate        | 3.34e-05    |
|    loss                 | 858         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 1.58e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 43.7     |
| time/              |          |
|    fps             | 725      |
|    iterations      | 9        |
|    time_elapsed    | 50       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=211.48 +/- 176.60
Episode length: 15.40 +/- 5.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | 211         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.019069899 |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.259       |
|    learning_rate        | 3.34e-05    |
|    loss                 | 1.54e+03    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 1.91e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | 89.9     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 10       |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=152.52 +/- 128.81
Episode length: 14.30 +/- 3.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.3        |
|    mean_reward          | 153         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.019617654 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.269       |
|    learning_rate        | 3.34e-05    |
|    loss                 | 1.26e+03    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 2.87e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 117      |
| time/              |          |
|    fps             | 717      |
|    iterations      | 11       |
|    time_elapsed    | 62       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 16.9         |
|    ep_rew_mean          | 150          |
| time/                   |              |
|    fps                  | 715          |
|    iterations           | 12           |
|    time_elapsed         | 68           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0110861445 |
|    clip_fraction        | 0.172        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.877       |
|    explained_variance   | 0.309        |
|    learning_rate        | 3.34e-05     |
|    loss                 | 1.36e+03     |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.0139      |
|    value_loss           | 3.29e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=258.10 +/- 251.41
Episode length: 16.90 +/- 6.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | 258         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.011471348 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.728      |
|    explained_variance   | 0.321       |
|    learning_rate        | 3.34e-05    |
|    loss                 | 1.34e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 3.75e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | 146      |
| time/              |          |
|    fps             | 711      |
|    iterations      | 13       |
|    time_elapsed    | 74       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=181.93 +/- 127.31
Episode length: 14.50 +/- 4.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.008388002 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.645      |
|    explained_variance   | 0.325       |
|    learning_rate        | 3.34e-05    |
|    loss                 | 1.74e+03    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00856    |
|    value_loss           | 3.95e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 710      |
|    iterations      | 14       |
|    time_elapsed    | 80       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=252.56 +/- 216.84
Episode length: 17.40 +/- 6.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | 253          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0072803875 |
|    clip_fraction        | 0.0768       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.494       |
|    explained_variance   | 0.376        |
|    learning_rate        | 3.34e-05     |
|    loss                 | 1.57e+03     |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00442     |
|    value_loss           | 5e+03        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 709      |
|    iterations      | 15       |
|    time_elapsed    | 86       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 23:10:10,507] Trial 48 finished with value: 326.70100399999995 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 3.340797805391783e-05, 'gamma': 0.9522503804594729, 'gae_lambda': 0.9123283938384938}. Best is trial 36 with value: 346.24260259999994.
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.8     |
|    ep_rew_mean     | -86.4    |
| time/              |          |
|    fps             | 1182     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=176.75 +/- 132.14
Episode length: 14.60 +/- 3.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.6        |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.013789201 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 8.85e-05    |
|    learning_rate        | 2.96e-05    |
|    loss                 | 382         |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 1.13e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | -76      |
| time/              |          |
|    fps             | 870      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=144.81 +/- 103.33
Episode length: 15.50 +/- 3.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | 145         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008832557 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.0133      |
|    learning_rate        | 2.96e-05    |
|    loss                 | 471         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 936         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.1     |
|    ep_rew_mean     | -76.4    |
| time/              |          |
|    fps             | 809      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=179.71 +/- 125.53
Episode length: 14.40 +/- 3.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.4       |
|    mean_reward          | 180        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01062515 |
|    clip_fraction        | 0.192      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.03      |
|    explained_variance   | 0.0405     |
|    learning_rate        | 2.96e-05   |
|    loss                 | 366        |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0147    |
|    value_loss           | 847        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | -66.4    |
| time/              |          |
|    fps             | 783      |
|    iterations      | 4        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=214.63 +/- 136.42
Episode length: 15.20 +/- 3.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | 215         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012622225 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.079       |
|    learning_rate        | 2.96e-05    |
|    loss                 | 373         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 761         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -43.4    |
| time/              |          |
|    fps             | 766      |
|    iterations      | 5        |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | -16.3       |
| time/                   |             |
|    fps                  | 754         |
|    iterations           | 6           |
|    time_elapsed         | 32          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014762797 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.92       |
|    explained_variance   | 0.12        |
|    learning_rate        | 2.96e-05    |
|    loss                 | 693         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 994         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=256.87 +/- 142.68
Episode length: 16.60 +/- 3.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | 257         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.013117132 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.142       |
|    learning_rate        | 2.96e-05    |
|    loss                 | 381         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 1.05e+03    |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | -9.91    |
| time/              |          |
|    fps             | 748      |
|    iterations      | 7        |
|    time_elapsed    | 38       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=189.31 +/- 175.23
Episode length: 14.80 +/- 5.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.016549205 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.189       |
|    learning_rate        | 2.96e-05    |
|    loss                 | 787         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0307     |
|    value_loss           | 1.38e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.8     |
|    ep_rew_mean     | 7.7      |
| time/              |          |
|    fps             | 741      |
|    iterations      | 8        |
|    time_elapsed    | 44       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=203.95 +/- 165.45
Episode length: 15.10 +/- 4.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.016364433 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.252       |
|    learning_rate        | 2.96e-05    |
|    loss                 | 857         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 1.58e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.7     |
|    ep_rew_mean     | 49.7     |
| time/              |          |
|    fps             | 736      |
|    iterations      | 9        |
|    time_elapsed    | 50       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=296.86 +/- 142.48
Episode length: 17.90 +/- 3.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.9       |
|    mean_reward          | 297        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.01831586 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.232      |
|    learning_rate        | 2.96e-05   |
|    loss                 | 743        |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0309    |
|    value_loss           | 2.06e+03   |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.5     |
|    ep_rew_mean     | 86.6     |
| time/              |          |
|    fps             | 731      |
|    iterations      | 10       |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=244.29 +/- 214.59
Episode length: 16.90 +/- 6.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.9       |
|    mean_reward          | 244        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.01653987 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.254      |
|    learning_rate        | 2.96e-05   |
|    loss                 | 2.03e+03   |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0181    |
|    value_loss           | 2.84e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 81.1     |
| time/              |          |
|    fps             | 729      |
|    iterations      | 11       |
|    time_elapsed    | 61       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.3       |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 12         |
|    time_elapsed         | 67         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.01807024 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.26       |
|    learning_rate        | 2.96e-05   |
|    loss                 | 1.91e+03   |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 3.49e+03   |
----------------------------------------
Eval num_timesteps=50000, episode_reward=340.67 +/- 155.66
Episode length: 18.90 +/- 4.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.9         |
|    mean_reward          | 341          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0136163905 |
|    clip_fraction        | 0.154        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.826       |
|    explained_variance   | 0.304        |
|    learning_rate        | 2.96e-05     |
|    loss                 | 2.05e+03     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0102      |
|    value_loss           | 4.12e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | 137      |
| time/              |          |
|    fps             | 724      |
|    iterations      | 13       |
|    time_elapsed    | 73       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=161.48 +/- 152.75
Episode length: 14.50 +/- 3.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.5        |
|    mean_reward          | 161         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.009839714 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.682      |
|    explained_variance   | 0.286       |
|    learning_rate        | 2.96e-05    |
|    loss                 | 3.71e+03    |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 5.01e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | 141      |
| time/              |          |
|    fps             | 722      |
|    iterations      | 14       |
|    time_elapsed    | 79       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=194.96 +/- 161.45
Episode length: 14.90 +/- 4.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.005817082 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.341       |
|    learning_rate        | 2.96e-05    |
|    loss                 | 2.79e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00553    |
|    value_loss           | 4.99e+03    |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 719      |
|    iterations      | 15       |
|    time_elapsed    | 85       |
|    total_timesteps | 61440    |
---------------------------------
[I 2024-07-20 23:11:38,681] Trial 49 finished with value: 199.297142 and parameters: {'n_steps': 4096, 'batch_size': 32, 'learning_rate': 2.9604103137552443e-05, 'gamma': 0.9635498292117279, 'gae_lambda': 0.9203158409654913}. Best is trial 36 with value: 346.24260259999994.
PPO Best trial: 346.24260259999994
PPO Best hyperparameters: {'n_steps': 2048, 'batch_size': 128, 'learning_rate': 1.0633435587786441e-05, 'gamma': 0.963521923100097, 'gae_lambda': 0.8794084939045616}
