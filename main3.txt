/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-528.74 +/- 84.78
Episode length: 52.16 +/- 16.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-520.35 +/- 69.78
Episode length: 50.28 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=-516.72 +/- 75.34
Episode length: 48.48 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-511.93 +/- 74.94
Episode length: 48.68 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | -375     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=-528.15 +/- 69.41
Episode length: 53.18 +/- 19.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.2         |
|    mean_reward          | -528         |
| time/                   |              |
|    total_timesteps      | 2500         |
| train/                  |              |
|    approx_kl            | 0.0051124482 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -2.07        |
|    explained_variance   | -0.00042     |
|    learning_rate        | 0.001        |
|    loss                 | 574          |
|    n_updates            | 1            |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 2.06e+03     |
------------------------------------------
Eval num_timesteps=3000, episode_reward=-522.16 +/- 73.05
Episode length: 50.24 +/- 15.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-542.54 +/- 65.82
Episode length: 48.06 +/- 18.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-520.34 +/- 70.30
Episode length: 54.04 +/- 19.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | -359     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=-522.75 +/- 64.40
Episode length: 49.90 +/- 16.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.9         |
|    mean_reward          | -523         |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 0.0060242177 |
|    clip_fraction        | 0.00521      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.000267     |
|    learning_rate        | 0.001        |
|    loss                 | 1.62e+03     |
|    n_updates            | 2            |
|    policy_gradient_loss | -0.0122      |
|    value_loss           | 4.18e+03     |
------------------------------------------
Eval num_timesteps=5000, episode_reward=-518.54 +/- 60.04
Episode length: 53.24 +/- 18.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-514.94 +/- 85.56
Episode length: 48.42 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-514.35 +/- 69.73
Episode length: 50.28 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.7     |
|    ep_rew_mean     | -362     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=6500, episode_reward=-514.35 +/- 76.86
Episode length: 51.82 +/- 16.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.8        |
|    mean_reward          | -514        |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.021841815 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.000479    |
|    learning_rate        | 0.001       |
|    loss                 | 3.1e+03     |
|    n_updates            | 3           |
|    policy_gradient_loss | -0.000188   |
|    value_loss           | 4.61e+03    |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=-527.55 +/- 68.69
Episode length: 49.00 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-512.54 +/- 69.79
Episode length: 48.00 +/- 15.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-499.95 +/- 80.95
Episode length: 52.36 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -500     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | -364     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 4        |
|    time_elapsed    | 34       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=8500, episode_reward=-526.35 +/- 70.34
Episode length: 52.22 +/- 16.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.2        |
|    mean_reward          | -526        |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.016825432 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.000394    |
|    learning_rate        | 0.001       |
|    loss                 | 1.13e+03    |
|    n_updates            | 4           |
|    policy_gradient_loss | 0.00282     |
|    value_loss           | 2.82e+03    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=-526.94 +/- 66.07
Episode length: 49.70 +/- 14.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-513.14 +/- 79.73
Episode length: 49.68 +/- 15.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-520.35 +/- 78.74
Episode length: 53.20 +/- 15.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.4     |
|    ep_rew_mean     | -364     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 5        |
|    time_elapsed    | 43       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=10500, episode_reward=-536.56 +/- 59.55
Episode length: 54.06 +/- 21.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.1        |
|    mean_reward          | -537        |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.027693562 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.00284     |
|    learning_rate        | 0.001       |
|    loss                 | 2.06e+03    |
|    n_updates            | 5           |
|    policy_gradient_loss | 0.000341    |
|    value_loss           | 5.21e+03    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=-530.55 +/- 59.31
Episode length: 51.42 +/- 20.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-533.56 +/- 64.58
Episode length: 57.26 +/- 19.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.3     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-523.95 +/- 58.18
Episode length: 52.16 +/- 21.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.1     |
|    ep_rew_mean     | -371     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 6        |
|    time_elapsed    | 52       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=12500, episode_reward=-508.94 +/- 86.80
Episode length: 47.72 +/- 16.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.7       |
|    mean_reward          | -509       |
| time/                   |            |
|    total_timesteps      | 12500      |
| train/                  |            |
|    approx_kl            | 0.02319623 |
|    clip_fraction        | 0.0547     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.00238    |
|    learning_rate        | 0.001      |
|    loss                 | 1.92e+03   |
|    n_updates            | 6          |
|    policy_gradient_loss | 0.00572    |
|    value_loss           | 4.52e+03   |
----------------------------------------
Eval num_timesteps=13000, episode_reward=-539.55 +/- 47.50
Episode length: 53.98 +/- 18.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -540     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-508.34 +/- 72.97
Episode length: 51.72 +/- 18.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-519.14 +/- 60.06
Episode length: 47.80 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -353     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 7        |
|    time_elapsed    | 60       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=14500, episode_reward=-498.14 +/- 81.34
Episode length: 52.72 +/- 17.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.7        |
|    mean_reward          | -498        |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.015222585 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.00319     |
|    learning_rate        | 0.001       |
|    loss                 | 2.6e+03     |
|    n_updates            | 7           |
|    policy_gradient_loss | -0.0092     |
|    value_loss           | 4.39e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-498.75 +/- 85.19
Episode length: 45.80 +/- 14.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | -499     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-524.54 +/- 62.90
Episode length: 48.30 +/- 16.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-523.35 +/- 67.90
Episode length: 51.92 +/- 16.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | -325     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 8        |
|    time_elapsed    | 69       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=16500, episode_reward=-509.55 +/- 78.85
Episode length: 46.64 +/- 14.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.6        |
|    mean_reward          | -510        |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.015169343 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.979      |
|    explained_variance   | 0.00792     |
|    learning_rate        | 0.001       |
|    loss                 | 1.27e+03    |
|    n_updates            | 8           |
|    policy_gradient_loss | -0.00622    |
|    value_loss           | 2.89e+03    |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=-520.34 +/- 71.07
Episode length: 52.40 +/- 19.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-525.54 +/- 87.64
Episode length: 53.68 +/- 21.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-526.34 +/- 66.13
Episode length: 51.90 +/- 15.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.3     |
|    ep_rew_mean     | -286     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 9        |
|    time_elapsed    | 77       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=18500, episode_reward=-513.15 +/- 74.36
Episode length: 50.86 +/- 17.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.9        |
|    mean_reward          | -513        |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.021482345 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.88       |
|    explained_variance   | 0.000805    |
|    learning_rate        | 0.001       |
|    loss                 | 1.86e+03    |
|    n_updates            | 9           |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 4.49e+03    |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=-524.55 +/- 65.70
Episode length: 47.62 +/- 11.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-519.14 +/- 73.54
Episode length: 52.10 +/- 17.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-526.35 +/- 65.03
Episode length: 53.18 +/- 19.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.3     |
|    ep_rew_mean     | -300     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 10       |
|    time_elapsed    | 86       |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=-522.78 +/- 66.87
Episode length: 50.18 +/- 18.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.2         |
|    mean_reward          | -523         |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0111518465 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.81        |
|    explained_variance   | 0.00605      |
|    learning_rate        | 0.001        |
|    loss                 | 2.88e+03     |
|    n_updates            | 10           |
|    policy_gradient_loss | 0.00567      |
|    value_loss           | 4.83e+03     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=-507.18 +/- 80.05
Episode length: 49.44 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-531.02 +/- 63.87
Episode length: 47.88 +/- 14.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-510.46 +/- 86.65
Episode length: 48.60 +/- 18.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-523.76 +/- 62.20
Episode length: 48.20 +/- 13.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | -304     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 11       |
|    time_elapsed    | 96       |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=23000, episode_reward=-507.64 +/- 82.57
Episode length: 45.78 +/- 14.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 45.8       |
|    mean_reward          | -508       |
| time/                   |            |
|    total_timesteps      | 23000      |
| train/                  |            |
|    approx_kl            | 0.00926614 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.75      |
|    explained_variance   | -0.00584   |
|    learning_rate        | 0.001      |
|    loss                 | 2.9e+03    |
|    n_updates            | 11         |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 4.18e+03   |
----------------------------------------
Eval num_timesteps=23500, episode_reward=-516.18 +/- 79.52
Episode length: 52.50 +/- 17.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-542.58 +/- 59.48
Episode length: 49.50 +/- 15.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-520.39 +/- 69.52
Episode length: 49.72 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.6     |
|    ep_rew_mean     | -250     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 12       |
|    time_elapsed    | 105      |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=25000, episode_reward=-199.54 +/- 167.67
Episode length: 32.74 +/- 9.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.7        |
|    mean_reward          | -200        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.019235754 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.703      |
|    explained_variance   | -0.00296    |
|    learning_rate        | 0.001       |
|    loss                 | 3.06e+03    |
|    n_updates            | 12          |
|    policy_gradient_loss | 0.00625     |
|    value_loss           | 5.12e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=25500, episode_reward=-204.56 +/- 138.61
Episode length: 32.94 +/- 11.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-228.74 +/- 148.71
Episode length: 33.38 +/- 11.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-207.17 +/- 157.33
Episode length: 35.16 +/- 15.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | -263     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 13       |
|    time_elapsed    | 111      |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.10
Eval num_timesteps=27000, episode_reward=-522.75 +/- 83.83
Episode length: 52.36 +/- 19.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.4        |
|    mean_reward          | -523        |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.051198337 |
|    clip_fraction        | 0.5         |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.687      |
|    explained_variance   | -0.00737    |
|    learning_rate        | 0.001       |
|    loss                 | 3.63e+03    |
|    n_updates            | 13          |
|    policy_gradient_loss | 0.0169      |
|    value_loss           | 5.45e+03    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=-525.57 +/- 81.50
Episode length: 48.02 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-524.55 +/- 75.15
Episode length: 50.84 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-519.75 +/- 73.54
Episode length: 53.52 +/- 18.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.4     |
|    ep_rew_mean     | -363     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 14       |
|    time_elapsed    | 120      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=29000, episode_reward=-528.15 +/- 63.44
Episode length: 50.96 +/- 17.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51          |
|    mean_reward          | -528        |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.040362865 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.516      |
|    explained_variance   | 0.00782     |
|    learning_rate        | 0.001       |
|    loss                 | 2.96e+03    |
|    n_updates            | 14          |
|    policy_gradient_loss | 0.0179      |
|    value_loss           | 4.5e+03     |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=-528.14 +/- 65.96
Episode length: 50.14 +/- 17.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-530.54 +/- 57.46
Episode length: 45.96 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-522.15 +/- 65.23
Episode length: 50.06 +/- 15.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.2     |
|    ep_rew_mean     | -469     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 15       |
|    time_elapsed    | 128      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=31000, episode_reward=-538.34 +/- 58.44
Episode length: 47.68 +/- 17.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.7        |
|    mean_reward          | -538        |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.014063327 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.317      |
|    explained_variance   | 0.016       |
|    learning_rate        | 0.001       |
|    loss                 | 2.29e+03    |
|    n_updates            | 15          |
|    policy_gradient_loss | -0.00358    |
|    value_loss           | 4.3e+03     |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=-514.94 +/- 79.44
Episode length: 49.10 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-532.95 +/- 52.40
Episode length: 51.10 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-512.54 +/- 74.28
Episode length: 45.96 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -505     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 16       |
|    time_elapsed    | 136      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=33000, episode_reward=-519.15 +/- 57.29
Episode length: 50.62 +/- 18.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.6       |
|    mean_reward          | -519       |
| time/                   |            |
|    total_timesteps      | 33000      |
| train/                  |            |
|    approx_kl            | 0.00959829 |
|    clip_fraction        | 0.0391     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.18      |
|    explained_variance   | 0.00672    |
|    learning_rate        | 0.001      |
|    loss                 | 1.26e+03   |
|    n_updates            | 16         |
|    policy_gradient_loss | 0.00679    |
|    value_loss           | 2.68e+03   |
----------------------------------------
Eval num_timesteps=33500, episode_reward=-522.14 +/- 64.97
Episode length: 51.82 +/- 19.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-523.35 +/- 57.59
Episode length: 50.68 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-517.95 +/- 75.67
Episode length: 47.48 +/- 13.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | -503     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 17       |
|    time_elapsed    | 145      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=35000, episode_reward=-538.95 +/- 46.19
Episode length: 56.20 +/- 19.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.2        |
|    mean_reward          | -539        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.013320884 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.232      |
|    explained_variance   | 0.0124      |
|    learning_rate        | 0.001       |
|    loss                 | 705         |
|    n_updates            | 17          |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 1.86e+03    |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=-509.55 +/- 80.42
Episode length: 48.60 +/- 14.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-529.35 +/- 61.28
Episode length: 51.54 +/- 12.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-516.15 +/- 77.45
Episode length: 49.88 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | -498     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 18       |
|    time_elapsed    | 154      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=37000, episode_reward=-522.15 +/- 72.56
Episode length: 51.92 +/- 15.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.9        |
|    mean_reward          | -522        |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.032927085 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.609      |
|    explained_variance   | 0.00865     |
|    learning_rate        | 0.001       |
|    loss                 | 1.69e+03    |
|    n_updates            | 18          |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 4.04e+03    |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=-528.14 +/- 63.73
Episode length: 50.40 +/- 15.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-517.95 +/- 74.71
Episode length: 50.38 +/- 16.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-514.36 +/- 71.26
Episode length: 53.96 +/- 21.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.7     |
|    ep_rew_mean     | -447     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 19       |
|    time_elapsed    | 163      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=39000, episode_reward=-533.42 +/- 57.81
Episode length: 48.82 +/- 12.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.8        |
|    mean_reward          | -533        |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.040876824 |
|    clip_fraction        | 0.367       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.716      |
|    explained_variance   | 0.0118      |
|    learning_rate        | 0.001       |
|    loss                 | 1.92e+03    |
|    n_updates            | 19          |
|    policy_gradient_loss | 0.012       |
|    value_loss           | 5.92e+03    |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=-520.79 +/- 78.91
Episode length: 47.66 +/- 14.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-524.56 +/- 65.71
Episode length: 48.12 +/- 17.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-536.58 +/- 53.49
Episode length: 52.40 +/- 14.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | -338     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 20       |
|    time_elapsed    | 171      |
|    total_timesteps | 40960    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=41000, episode_reward=-518.58 +/- 76.86
Episode length: 47.70 +/- 14.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.7       |
|    mean_reward          | -519       |
| time/                   |            |
|    total_timesteps      | 41000      |
| train/                  |            |
|    approx_kl            | 0.03533365 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.687     |
|    explained_variance   | -0.00287   |
|    learning_rate        | 0.001      |
|    loss                 | 1.51e+03   |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 6.1e+03    |
----------------------------------------
Eval num_timesteps=41500, episode_reward=-513.71 +/- 71.67
Episode length: 46.94 +/- 13.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-511.91 +/- 78.39
Episode length: 52.20 +/- 19.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-519.01 +/- 72.09
Episode length: 51.48 +/- 14.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-526.98 +/- 65.80
Episode length: 51.44 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | -271     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 21       |
|    time_elapsed    | 181      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=43500, episode_reward=-513.18 +/- 74.59
Episode length: 49.26 +/- 18.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | -513        |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.024893802 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.571      |
|    explained_variance   | -0.0138     |
|    learning_rate        | 0.001       |
|    loss                 | 1.58e+03    |
|    n_updates            | 21          |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 4.15e+03    |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=-544.96 +/- 49.95
Episode length: 52.68 +/- 18.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -545     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-531.06 +/- 69.07
Episode length: 51.20 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-525.06 +/- 64.82
Episode length: 52.00 +/- 17.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.1     |
|    ep_rew_mean     | -260     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 22       |
|    time_elapsed    | 190      |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.17
Eval num_timesteps=45500, episode_reward=-516.00 +/- 82.80
Episode length: 53.38 +/- 18.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.4       |
|    mean_reward          | -516       |
| time/                   |            |
|    total_timesteps      | 45500      |
| train/                  |            |
|    approx_kl            | 0.04516441 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.562     |
|    explained_variance   | -0.00632   |
|    learning_rate        | 0.001      |
|    loss                 | 2.05e+03   |
|    n_updates            | 22         |
|    policy_gradient_loss | -0.00426   |
|    value_loss           | 4.34e+03   |
----------------------------------------
Eval num_timesteps=46000, episode_reward=-541.98 +/- 62.36
Episode length: 52.82 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -542     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-506.58 +/- 87.07
Episode length: 48.44 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-497.41 +/- 85.38
Episode length: 47.66 +/- 17.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | -271     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 23       |
|    time_elapsed    | 199      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=47500, episode_reward=-523.34 +/- 80.29
Episode length: 48.12 +/- 13.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.1        |
|    mean_reward          | -523        |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.054942258 |
|    clip_fraction        | 0.492       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.758      |
|    explained_variance   | 0.000618    |
|    learning_rate        | 0.001       |
|    loss                 | 3.04e+03    |
|    n_updates            | 23          |
|    policy_gradient_loss | 0.0385      |
|    value_loss           | 7.02e+03    |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=-529.34 +/- 63.58
Episode length: 50.58 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-516.13 +/- 70.65
Episode length: 48.38 +/- 18.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-498.14 +/- 74.90
Episode length: 47.40 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -498     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | -346     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 24       |
|    time_elapsed    | 207      |
|    total_timesteps | 49152    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=49500, episode_reward=-538.95 +/- 51.36
Episode length: 49.02 +/- 18.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49          |
|    mean_reward          | -539        |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.035063382 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.00118     |
|    learning_rate        | 0.001       |
|    loss                 | 3.93e+03    |
|    n_updates            | 24          |
|    policy_gradient_loss | 0.0589      |
|    value_loss           | 6.66e+03    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-510.14 +/- 78.47
Episode length: 51.26 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-511.35 +/- 68.07
Episode length: 52.92 +/- 19.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-526.96 +/- 66.62
Episode length: 49.44 +/- 15.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.7     |
|    ep_rew_mean     | -429     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 25       |
|    time_elapsed    | 216      |
|    total_timesteps | 51200    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=51500, episode_reward=-522.75 +/- 56.97
Episode length: 50.40 +/- 15.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.4        |
|    mean_reward          | -523        |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.021139544 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.00749     |
|    learning_rate        | 0.001       |
|    loss                 | 2.93e+03    |
|    n_updates            | 25          |
|    policy_gradient_loss | 0.0335      |
|    value_loss           | 4.17e+03    |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=-513.14 +/- 64.78
Episode length: 48.40 +/- 14.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-507.74 +/- 76.95
Episode length: 45.66 +/- 13.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-518.55 +/- 76.40
Episode length: 51.38 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -471     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 26       |
|    time_elapsed    | 224      |
|    total_timesteps | 53248    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=53500, episode_reward=-502.35 +/- 81.09
Episode length: 46.48 +/- 13.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.5       |
|    mean_reward          | -502       |
| time/                   |            |
|    total_timesteps      | 53500      |
| train/                  |            |
|    approx_kl            | 0.01507788 |
|    clip_fraction        | 0.0781     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.333     |
|    explained_variance   | 0.00639    |
|    learning_rate        | 0.001      |
|    loss                 | 1.26e+03   |
|    n_updates            | 26         |
|    policy_gradient_loss | 0.0301     |
|    value_loss           | 3.53e+03   |
----------------------------------------
Eval num_timesteps=54000, episode_reward=-533.55 +/- 63.18
Episode length: 53.62 +/- 19.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-543.14 +/- 57.42
Episode length: 53.16 +/- 20.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-520.34 +/- 73.30
Episode length: 48.14 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -497     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 27       |
|    time_elapsed    | 233      |
|    total_timesteps | 55296    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=55500, episode_reward=-508.95 +/- 83.84
Episode length: 50.48 +/- 17.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.5        |
|    mean_reward          | -509        |
| time/                   |             |
|    total_timesteps      | 55500       |
| train/                  |             |
|    approx_kl            | 0.009968078 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.202      |
|    explained_variance   | 0.0119      |
|    learning_rate        | 0.001       |
|    loss                 | 2.48e+03    |
|    n_updates            | 27          |
|    policy_gradient_loss | 0.0116      |
|    value_loss           | 3.93e+03    |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=-523.35 +/- 68.17
Episode length: 47.64 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-527.54 +/- 77.09
Episode length: 55.08 +/- 24.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-514.94 +/- 68.74
Episode length: 50.30 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 28       |
|    time_elapsed    | 241      |
|    total_timesteps | 57344    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=57500, episode_reward=-516.74 +/- 62.00
Episode length: 48.74 +/- 18.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.7        |
|    mean_reward          | -517        |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.012038139 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.08       |
|    explained_variance   | 0.00593     |
|    learning_rate        | 0.001       |
|    loss                 | 2.23e+03    |
|    n_updates            | 28          |
|    policy_gradient_loss | -0.00112    |
|    value_loss           | 2.5e+03     |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=-531.15 +/- 61.01
Episode length: 52.50 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-478.33 +/- 92.35
Episode length: 46.16 +/- 18.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -478     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-528.75 +/- 69.34
Episode length: 45.56 +/- 13.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 29       |
|    time_elapsed    | 250      |
|    total_timesteps | 59392    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=59500, episode_reward=-514.94 +/- 77.37
Episode length: 50.06 +/- 18.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.1        |
|    mean_reward          | -515        |
| time/                   |             |
|    total_timesteps      | 59500       |
| train/                  |             |
|    approx_kl            | 0.011225908 |
|    clip_fraction        | 0.0026      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00772    |
|    explained_variance   | 0.00963     |
|    learning_rate        | 0.001       |
|    loss                 | 912         |
|    n_updates            | 29          |
|    policy_gradient_loss | 0.00588     |
|    value_loss           | 2.32e+03    |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=-533.54 +/- 72.72
Episode length: 48.82 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-525.14 +/- 68.88
Episode length: 49.26 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-528.75 +/- 57.09
Episode length: 51.30 +/- 17.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 30       |
|    time_elapsed    | 258      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=-520.34 +/- 77.60
Episode length: 48.42 +/- 17.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.4      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.3e-06  |
|    explained_variance   | 0.0233    |
|    learning_rate        | 0.001     |
|    loss                 | 1.35e+03  |
|    n_updates            | 39        |
|    policy_gradient_loss | -5.96e-07 |
|    value_loss           | 3.62e+03  |
---------------------------------------
Eval num_timesteps=62000, episode_reward=-528.74 +/- 56.46
Episode length: 45.64 +/- 14.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-514.34 +/- 71.77
Episode length: 46.90 +/- 19.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-537.14 +/- 72.49
Episode length: 52.28 +/- 15.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 31       |
|    time_elapsed    | 267      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=-522.75 +/- 70.78
Episode length: 48.34 +/- 15.09
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 48.3           |
|    mean_reward          | -523           |
| time/                   |                |
|    total_timesteps      | 63500          |
| train/                  |                |
|    approx_kl            | 1.19325705e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -0.000613      |
|    explained_variance   | 0.026          |
|    learning_rate        | 0.001          |
|    loss                 | 2.87e+03       |
|    n_updates            | 49             |
|    policy_gradient_loss | 1.68e-06       |
|    value_loss           | 5.79e+03       |
--------------------------------------------
Eval num_timesteps=64000, episode_reward=-505.94 +/- 80.06
Episode length: 50.72 +/- 16.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -506     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-526.35 +/- 77.41
Episode length: 51.14 +/- 18.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-511.34 +/- 76.77
Episode length: 44.24 +/- 14.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.2     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-532.34 +/- 71.68
Episode length: 48.90 +/- 18.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 32       |
|    time_elapsed    | 278      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-525.15 +/- 63.15
Episode length: 52.40 +/- 16.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.4      |
|    mean_reward          | -525      |
| time/                   |           |
|    total_timesteps      | 66000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.4e-07  |
|    explained_variance   | 0.0101    |
|    learning_rate        | 0.001     |
|    loss                 | 3.02e+03  |
|    n_updates            | 59        |
|    policy_gradient_loss | -6.73e-07 |
|    value_loss           | 3.54e+03  |
---------------------------------------
Eval num_timesteps=66500, episode_reward=-524.55 +/- 69.68
Episode length: 48.24 +/- 15.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-516.75 +/- 70.44
Episode length: 50.70 +/- 15.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-518.54 +/- 74.96
Episode length: 48.38 +/- 14.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 33       |
|    time_elapsed    | 287      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-517.95 +/- 75.67
Episode length: 51.12 +/- 20.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.1         |
|    mean_reward          | -518         |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 2.788147e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00183     |
|    explained_variance   | 0.0232       |
|    learning_rate        | 0.001        |
|    loss                 | 5.84e+03     |
|    n_updates            | 69           |
|    policy_gradient_loss | 2.3e-06      |
|    value_loss           | 1.26e+04     |
------------------------------------------
Eval num_timesteps=68500, episode_reward=-518.54 +/- 65.20
Episode length: 45.34 +/- 13.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-528.75 +/- 53.52
Episode length: 56.60 +/- 20.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.6     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-540.75 +/- 54.45
Episode length: 52.64 +/- 15.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 34       |
|    time_elapsed    | 296      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-531.14 +/- 69.03
Episode length: 54.62 +/- 16.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 54.6          |
|    mean_reward          | -531          |
| time/                   |               |
|    total_timesteps      | 70000         |
| train/                  |               |
|    approx_kl            | 1.8044375e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -5.02e-06     |
|    explained_variance   | 0.00615       |
|    learning_rate        | 0.001         |
|    loss                 | 1.68e+03      |
|    n_updates            | 79            |
|    policy_gradient_loss | -4.1e-06      |
|    value_loss           | 3.39e+03      |
-------------------------------------------
Eval num_timesteps=70500, episode_reward=-528.15 +/- 69.93
Episode length: 52.90 +/- 15.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-535.93 +/- 63.51
Episode length: 51.24 +/- 20.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-507.74 +/- 77.64
Episode length: 47.78 +/- 14.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 35       |
|    time_elapsed    | 306      |
|    total_timesteps | 71680    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=72000, episode_reward=-532.94 +/- 56.68
Episode length: 53.40 +/- 15.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.4         |
|    mean_reward          | -533         |
| time/                   |              |
|    total_timesteps      | 72000        |
| train/                  |              |
|    approx_kl            | 0.0014193802 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0255      |
|    explained_variance   | 0.00909      |
|    learning_rate        | 0.001        |
|    loss                 | 1.12e+04     |
|    n_updates            | 81           |
|    policy_gradient_loss | -6.65e-05    |
|    value_loss           | 2.18e+04     |
------------------------------------------
Eval num_timesteps=72500, episode_reward=-518.55 +/- 67.92
Episode length: 47.10 +/- 13.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-511.34 +/- 71.42
Episode length: 50.46 +/- 18.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-526.95 +/- 82.76
Episode length: 49.40 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 61.8     |
|    ep_rew_mean     | -508     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 36       |
|    time_elapsed    | 315      |
|    total_timesteps | 73728    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=74000, episode_reward=-523.05 +/- 73.45
Episode length: 53.92 +/- 17.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.9       |
|    mean_reward          | -523       |
| time/                   |            |
|    total_timesteps      | 74000      |
| train/                  |            |
|    approx_kl            | 0.07123703 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.682     |
|    explained_variance   | 0.0222     |
|    learning_rate        | 0.001      |
|    loss                 | 349        |
|    n_updates            | 82         |
|    policy_gradient_loss | 0.048      |
|    value_loss           | 1.12e+03   |
----------------------------------------
Eval num_timesteps=74500, episode_reward=-517.22 +/- 76.02
Episode length: 49.66 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-532.10 +/- 67.73
Episode length: 52.14 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-519.68 +/- 86.01
Episode length: 51.60 +/- 17.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68       |
|    ep_rew_mean     | -488     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 37       |
|    time_elapsed    | 324      |
|    total_timesteps | 75776    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.32
Eval num_timesteps=76000, episode_reward=-539.73 +/- 73.38
Episode length: 51.28 +/- 19.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.3       |
|    mean_reward          | -540       |
| time/                   |            |
|    total_timesteps      | 76000      |
| train/                  |            |
|    approx_kl            | 0.16050379 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.603     |
|    explained_variance   | 0.0234     |
|    learning_rate        | 0.001      |
|    loss                 | 2.06e+03   |
|    n_updates            | 83         |
|    policy_gradient_loss | 0.046      |
|    value_loss           | 2.65e+03   |
----------------------------------------
Eval num_timesteps=76500, episode_reward=-532.88 +/- 87.89
Episode length: 46.58 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-522.84 +/- 70.08
Episode length: 46.40 +/- 12.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-518.69 +/- 63.42
Episode length: 46.52 +/- 15.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -454     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 38       |
|    time_elapsed    | 332      |
|    total_timesteps | 77824    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.28
Eval num_timesteps=78000, episode_reward=-521.33 +/- 76.22
Episode length: 48.88 +/- 17.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.9       |
|    mean_reward          | -521       |
| time/                   |            |
|    total_timesteps      | 78000      |
| train/                  |            |
|    approx_kl            | 0.14008504 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.268     |
|    explained_variance   | 0.00623    |
|    learning_rate        | 0.001      |
|    loss                 | 3.24e+03   |
|    n_updates            | 84         |
|    policy_gradient_loss | 0.0581     |
|    value_loss           | 6.14e+03   |
----------------------------------------
Eval num_timesteps=78500, episode_reward=-523.67 +/- 64.16
Episode length: 53.62 +/- 16.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-529.46 +/- 70.51
Episode length: 50.76 +/- 20.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-523.84 +/- 72.18
Episode length: 50.62 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 59       |
|    ep_rew_mean     | -464     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 39       |
|    time_elapsed    | 341      |
|    total_timesteps | 79872    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=80000, episode_reward=-528.61 +/- 75.77
Episode length: 50.38 +/- 17.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.4         |
|    mean_reward          | -529         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0071405447 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0368      |
|    explained_variance   | -0.0037      |
|    learning_rate        | 0.001        |
|    loss                 | 2.53e+03     |
|    n_updates            | 85           |
|    policy_gradient_loss | 0.0044       |
|    value_loss           | 3.79e+03     |
------------------------------------------
Eval num_timesteps=80500, episode_reward=-544.51 +/- 61.92
Episode length: 48.66 +/- 13.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -545     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-526.11 +/- 78.29
Episode length: 48.20 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-516.05 +/- 87.23
Episode length: 49.42 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.6     |
|    ep_rew_mean     | -493     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 40       |
|    time_elapsed    | 350      |
|    total_timesteps | 81920    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=82000, episode_reward=-526.37 +/- 74.48
Episode length: 49.70 +/- 15.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.7        |
|    mean_reward          | -526        |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.018051226 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00157    |
|    explained_variance   | -0.00951    |
|    learning_rate        | 0.001       |
|    loss                 | 1.64e+03    |
|    n_updates            | 86          |
|    policy_gradient_loss | 0.00115     |
|    value_loss           | 2.96e+03    |
-----------------------------------------
Eval num_timesteps=82500, episode_reward=-530.39 +/- 77.75
Episode length: 49.46 +/- 13.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-539.03 +/- 58.45
Episode length: 51.30 +/- 15.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -539     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-524.94 +/- 56.19
Episode length: 48.76 +/- 16.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 41       |
|    time_elapsed    | 359      |
|    total_timesteps | 83968    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 1.05
Eval num_timesteps=84000, episode_reward=-535.22 +/- 62.23
Episode length: 49.86 +/- 16.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.9        |
|    mean_reward          | -535        |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.061558627 |
|    clip_fraction        | 0.0588      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0315     |
|    explained_variance   | -0.0639     |
|    learning_rate        | 0.001       |
|    loss                 | 3.01e+03    |
|    n_updates            | 87          |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 4.91e+03    |
-----------------------------------------
Eval num_timesteps=84500, episode_reward=-523.97 +/- 77.77
Episode length: 50.32 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-531.76 +/- 57.89
Episode length: 50.14 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-531.05 +/- 73.72
Episode length: 46.36 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-526.97 +/- 57.63
Episode length: 51.62 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 42       |
|    time_elapsed    | 369      |
|    total_timesteps | 86016    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.90
Eval num_timesteps=86500, episode_reward=-504.17 +/- 77.86
Episode length: 50.46 +/- 16.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.5       |
|    mean_reward          | -504       |
| time/                   |            |
|    total_timesteps      | 86500      |
| train/                  |            |
|    approx_kl            | 0.44750187 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.00459    |
|    learning_rate        | 0.001      |
|    loss                 | 1.84e+03   |
|    n_updates            | 88         |
|    policy_gradient_loss | -0.00124   |
|    value_loss           | 4.96e+03   |
----------------------------------------
Eval num_timesteps=87000, episode_reward=-510.71 +/- 79.43
Episode length: 49.30 +/- 18.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-521.57 +/- 66.89
Episode length: 49.04 +/- 15.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-517.97 +/- 61.21
Episode length: 52.30 +/- 20.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 43       |
|    time_elapsed    | 378      |
|    total_timesteps | 88064    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 5.62
Eval num_timesteps=88500, episode_reward=-523.92 +/- 72.73
Episode length: 49.08 +/- 16.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.1       |
|    mean_reward          | -524       |
| time/                   |            |
|    total_timesteps      | 88500      |
| train/                  |            |
|    approx_kl            | 0.24443005 |
|    clip_fraction        | 0.000679   |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.000332  |
|    explained_variance   | 0.000351   |
|    learning_rate        | 0.001      |
|    loss                 | 1.06e+03   |
|    n_updates            | 89         |
|    policy_gradient_loss | 0.000286   |
|    value_loss           | 3.01e+03   |
----------------------------------------
Eval num_timesteps=89000, episode_reward=-516.18 +/- 80.41
Episode length: 50.96 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-516.16 +/- 75.81
Episode length: 49.26 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-540.06 +/- 59.35
Episode length: 50.26 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -540     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.7     |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 44       |
|    time_elapsed    | 387      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=-527.44 +/- 67.93
Episode length: 48.12 +/- 18.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.1      |
|    mean_reward          | -527      |
| time/                   |           |
|    total_timesteps      | 90500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.17e-36 |
|    explained_variance   | -0.028    |
|    learning_rate        | 0.001     |
|    loss                 | 4.81e+04  |
|    n_updates            | 99        |
|    policy_gradient_loss | -8.58e-09 |
|    value_loss           | 9.85e+04  |
---------------------------------------
Eval num_timesteps=91000, episode_reward=-507.11 +/- 82.12
Episode length: 50.70 +/- 19.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-520.96 +/- 70.04
Episode length: 50.26 +/- 18.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-522.13 +/- 67.91
Episode length: 54.04 +/- 17.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.2     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 45       |
|    time_elapsed    | 396      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=-514.26 +/- 69.95
Episode length: 48.68 +/- 18.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.7      |
|    mean_reward          | -514      |
| time/                   |           |
|    total_timesteps      | 92500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.57e-07 |
|    explained_variance   | -0.00895  |
|    learning_rate        | 0.001     |
|    loss                 | 4.64e+03  |
|    n_updates            | 109       |
|    policy_gradient_loss | -3.93e-11 |
|    value_loss           | 9.29e+03  |
---------------------------------------
Eval num_timesteps=93000, episode_reward=-516.77 +/- 71.43
Episode length: 48.16 +/- 14.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-509.21 +/- 88.54
Episode length: 46.20 +/- 15.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-522.66 +/- 60.98
Episode length: 53.98 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.2     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 46       |
|    time_elapsed    | 405      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=-536.56 +/- 65.60
Episode length: 51.50 +/- 15.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.5      |
|    mean_reward          | -537      |
| time/                   |           |
|    total_timesteps      | 94500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.56e-09 |
|    explained_variance   | 0.000438  |
|    learning_rate        | 0.001     |
|    loss                 | 2.17e+03  |
|    n_updates            | 119       |
|    policy_gradient_loss | 7.36e-10  |
|    value_loss           | 2.91e+03  |
---------------------------------------
Eval num_timesteps=95000, episode_reward=-541.35 +/- 53.28
Episode length: 50.64 +/- 16.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-514.24 +/- 68.57
Episode length: 51.66 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-536.57 +/- 57.07
Episode length: 55.20 +/- 21.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.8     |
|    ep_rew_mean     | -513     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 47       |
|    time_elapsed    | 415      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=-529.97 +/- 54.34
Episode length: 48.12 +/- 17.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.1      |
|    mean_reward          | -530      |
| time/                   |           |
|    total_timesteps      | 96500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.88e-07 |
|    explained_variance   | -0.00992  |
|    learning_rate        | 0.001     |
|    loss                 | 6.61e+03  |
|    n_updates            | 129       |
|    policy_gradient_loss | -2.32e-09 |
|    value_loss           | 1.23e+04  |
---------------------------------------
Eval num_timesteps=97000, episode_reward=-526.91 +/- 60.65
Episode length: 53.26 +/- 18.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-513.77 +/- 82.23
Episode length: 46.60 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-514.35 +/- 73.49
Episode length: 48.20 +/- 14.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -509     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 48       |
|    time_elapsed    | 424      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=-520.37 +/- 68.47
Episode length: 50.86 +/- 17.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.9      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.07e-09 |
|    explained_variance   | 0.00204   |
|    learning_rate        | 0.001     |
|    loss                 | 2.24e+03  |
|    n_updates            | 139       |
|    policy_gradient_loss | -1.76e-10 |
|    value_loss           | 4.17e+03  |
---------------------------------------
Eval num_timesteps=99000, episode_reward=-514.90 +/- 70.73
Episode length: 47.48 +/- 14.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-532.38 +/- 48.62
Episode length: 52.48 +/- 16.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-531.17 +/- 70.57
Episode length: 48.54 +/- 18.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 49       |
|    time_elapsed    | 433      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=-518.25 +/- 80.47
Episode length: 50.72 +/- 16.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -518      |
| time/                   |           |
|    total_timesteps      | 100500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.61e-06 |
|    explained_variance   | -0.00927  |
|    learning_rate        | 0.001     |
|    loss                 | 8.2e+03   |
|    n_updates            | 149       |
|    policy_gradient_loss | 1.89e-11  |
|    value_loss           | 1.59e+04  |
---------------------------------------
Eval num_timesteps=101000, episode_reward=-516.64 +/- 62.98
Episode length: 48.32 +/- 18.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-510.77 +/- 80.58
Episode length: 45.64 +/- 14.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-516.77 +/- 62.57
Episode length: 49.38 +/- 16.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 50       |
|    time_elapsed    | 442      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=-510.51 +/- 78.36
Episode length: 50.88 +/- 19.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.9      |
|    mean_reward          | -511      |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.85e-09 |
|    explained_variance   | 0.000762  |
|    learning_rate        | 0.001     |
|    loss                 | 2.42e+03  |
|    n_updates            | 159       |
|    policy_gradient_loss | -5.57e-10 |
|    value_loss           | 4.31e+03  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=-532.26 +/- 66.50
Episode length: 49.68 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-519.16 +/- 59.45
Episode length: 48.74 +/- 15.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-500.57 +/- 81.77
Episode length: 48.18 +/- 15.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -534     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 51       |
|    time_elapsed    | 452      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-533.56 +/- 56.87
Episode length: 51.74 +/- 13.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -534      |
| time/                   |           |
|    total_timesteps      | 104500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.54e-06 |
|    explained_variance   | -0.0143   |
|    learning_rate        | 0.001     |
|    loss                 | 8.19e+03  |
|    n_updates            | 169       |
|    policy_gradient_loss | 1.48e-09  |
|    value_loss           | 1.68e+04  |
---------------------------------------
Eval num_timesteps=105000, episode_reward=-514.37 +/- 70.25
Episode length: 49.22 +/- 16.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-504.16 +/- 77.40
Episode length: 47.18 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-532.97 +/- 52.73
Episode length: 51.30 +/- 18.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.7     |
|    ep_rew_mean     | -534     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 52       |
|    time_elapsed    | 461      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=-530.59 +/- 58.08
Episode length: 50.06 +/- 14.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -531      |
| time/                   |           |
|    total_timesteps      | 106500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.77e-09 |
|    explained_variance   | 0.000469  |
|    learning_rate        | 0.001     |
|    loss                 | 1.55e+03  |
|    n_updates            | 179       |
|    policy_gradient_loss | 1.62e-09  |
|    value_loss           | 3.79e+03  |
---------------------------------------
Eval num_timesteps=107000, episode_reward=-515.58 +/- 74.07
Episode length: 50.18 +/- 14.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-530.57 +/- 64.54
Episode length: 51.12 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-512.59 +/- 71.08
Episode length: 52.00 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-517.96 +/- 70.49
Episode length: 54.58 +/- 19.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 53       |
|    time_elapsed    | 473      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-523.80 +/- 74.77
Episode length: 52.98 +/- 16.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53        |
|    mean_reward          | -524      |
| time/                   |           |
|    total_timesteps      | 109000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.1e-06  |
|    explained_variance   | -0.0164   |
|    learning_rate        | 0.001     |
|    loss                 | 7.96e+03  |
|    n_updates            | 189       |
|    policy_gradient_loss | -2.84e-09 |
|    value_loss           | 1.61e+04  |
---------------------------------------
Eval num_timesteps=109500, episode_reward=-512.52 +/- 84.97
Episode length: 48.20 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-535.30 +/- 61.22
Episode length: 47.92 +/- 16.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-511.97 +/- 79.14
Episode length: 48.46 +/- 15.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55       |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 54       |
|    time_elapsed    | 482      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-514.92 +/- 71.83
Episode length: 44.24 +/- 13.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 44.2      |
|    mean_reward          | -515      |
| time/                   |           |
|    total_timesteps      | 111000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.29e-09 |
|    explained_variance   | 0.00097   |
|    learning_rate        | 0.001     |
|    loss                 | 3.32e+03  |
|    n_updates            | 199       |
|    policy_gradient_loss | -8.73e-11 |
|    value_loss           | 2.77e+03  |
---------------------------------------
Eval num_timesteps=111500, episode_reward=-508.36 +/- 80.46
Episode length: 45.28 +/- 15.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-505.96 +/- 74.23
Episode length: 49.46 +/- 15.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -506     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-514.96 +/- 66.88
Episode length: 45.64 +/- 14.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.5     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 55       |
|    time_elapsed    | 490      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-511.34 +/- 88.71
Episode length: 50.12 +/- 15.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 50.1          |
|    mean_reward          | -511          |
| time/                   |               |
|    total_timesteps      | 113000        |
| train/                  |               |
|    approx_kl            | -2.910383e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -4.86e-06     |
|    explained_variance   | -0.00547      |
|    learning_rate        | 0.001         |
|    loss                 | 7.07e+03      |
|    n_updates            | 209           |
|    policy_gradient_loss | 5.9e-09       |
|    value_loss           | 1.39e+04      |
-------------------------------------------
Eval num_timesteps=113500, episode_reward=-514.98 +/- 77.36
Episode length: 50.96 +/- 20.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-504.76 +/- 72.26
Episode length: 48.74 +/- 16.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-535.38 +/- 66.14
Episode length: 54.58 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 56.6     |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 56       |
|    time_elapsed    | 500      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-531.61 +/- 60.81
Episode length: 47.94 +/- 14.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.9      |
|    mean_reward          | -532      |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.69e-08 |
|    explained_variance   | -0.000449 |
|    learning_rate        | 0.001     |
|    loss                 | 1.27e+03  |
|    n_updates            | 219       |
|    policy_gradient_loss | 1.31e-09  |
|    value_loss           | 3.88e+03  |
---------------------------------------
Eval num_timesteps=115500, episode_reward=-520.33 +/- 81.13
Episode length: 50.36 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-519.92 +/- 79.99
Episode length: 50.94 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-516.06 +/- 62.62
Episode length: 49.80 +/- 14.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.5     |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 57       |
|    time_elapsed    | 510      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-521.57 +/- 71.33
Episode length: 51.26 +/- 19.19
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 51.3           |
|    mean_reward          | -522           |
| time/                   |                |
|    total_timesteps      | 117000         |
| train/                  |                |
|    approx_kl            | -2.4447218e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -6.09e-06      |
|    explained_variance   | -0.013         |
|    learning_rate        | 0.001          |
|    loss                 | 5.42e+03       |
|    n_updates            | 229            |
|    policy_gradient_loss | 1.16e-08       |
|    value_loss           | 1.04e+04       |
--------------------------------------------
Eval num_timesteps=117500, episode_reward=-512.41 +/- 78.19
Episode length: 49.50 +/- 14.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-522.77 +/- 75.95
Episode length: 52.28 +/- 20.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-532.26 +/- 61.31
Episode length: 54.72 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.9     |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 58       |
|    time_elapsed    | 519      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-531.10 +/- 59.81
Episode length: 49.10 +/- 15.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.1      |
|    mean_reward          | -531      |
| time/                   |           |
|    total_timesteps      | 119000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.05e-08 |
|    explained_variance   | 0.000296  |
|    learning_rate        | 0.001     |
|    loss                 | 2.05e+03  |
|    n_updates            | 239       |
|    policy_gradient_loss | -3.52e-08 |
|    value_loss           | 3.62e+03  |
---------------------------------------
Eval num_timesteps=119500, episode_reward=-523.32 +/- 65.82
Episode length: 51.64 +/- 17.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-531.56 +/- 67.93
Episode length: 54.38 +/- 17.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-529.27 +/- 68.70
Episode length: 50.62 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.3     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 59       |
|    time_elapsed    | 529      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-518.51 +/- 79.87
Episode length: 50.98 +/- 16.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 51            |
|    mean_reward          | -519          |
| time/                   |               |
|    total_timesteps      | 121000        |
| train/                  |               |
|    approx_kl            | -5.151378e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -8.71e-06     |
|    explained_variance   | -0.00739      |
|    learning_rate        | 0.001         |
|    loss                 | 4.27e+03      |
|    n_updates            | 249           |
|    policy_gradient_loss | 2.61e-08      |
|    value_loss           | 7.84e+03      |
-------------------------------------------
Eval num_timesteps=121500, episode_reward=-523.90 +/- 63.15
Episode length: 49.64 +/- 16.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-510.76 +/- 64.46
Episode length: 46.72 +/- 14.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-513.57 +/- 72.96
Episode length: 48.12 +/- 14.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 60       |
|    time_elapsed    | 538      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-524.57 +/- 69.69
Episode length: 51.46 +/- 15.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.5      |
|    mean_reward          | -525      |
| time/                   |           |
|    total_timesteps      | 123000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.96e-08 |
|    explained_variance   | 0.000967  |
|    learning_rate        | 0.001     |
|    loss                 | 2.09e+03  |
|    n_updates            | 259       |
|    policy_gradient_loss | -8.93e-08 |
|    value_loss           | 5.33e+03  |
---------------------------------------
Eval num_timesteps=123500, episode_reward=-494.50 +/- 87.93
Episode length: 46.58 +/- 18.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -495     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-514.31 +/- 70.98
Episode length: 52.04 +/- 20.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-523.38 +/- 64.08
Episode length: 46.14 +/- 14.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.7     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 61       |
|    time_elapsed    | 548      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-516.62 +/- 87.85
Episode length: 48.42 +/- 16.24
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 48.4           |
|    mean_reward          | -517           |
| time/                   |                |
|    total_timesteps      | 125000         |
| train/                  |                |
|    approx_kl            | -5.0873496e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -2.01e-05      |
|    explained_variance   | -0.00927       |
|    learning_rate        | 0.001          |
|    loss                 | 3.01e+03       |
|    n_updates            | 269            |
|    policy_gradient_loss | -9.03e-09      |
|    value_loss           | 5.68e+03       |
--------------------------------------------
Eval num_timesteps=125500, episode_reward=-522.14 +/- 79.95
Episode length: 46.00 +/- 15.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-526.38 +/- 66.40
Episode length: 53.88 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-514.37 +/- 59.09
Episode length: 49.30 +/- 13.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 62       |
|    time_elapsed    | 557      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-519.10 +/- 67.87
Episode length: 52.12 +/- 17.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.1      |
|    mean_reward          | -519      |
| time/                   |           |
|    total_timesteps      | 127000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.57e-08 |
|    explained_variance   | 0.00037   |
|    learning_rate        | 0.001     |
|    loss                 | 1.38e+03  |
|    n_updates            | 279       |
|    policy_gradient_loss | -7.32e-08 |
|    value_loss           | 4.18e+03  |
---------------------------------------
Eval num_timesteps=127500, episode_reward=-511.13 +/- 85.76
Episode length: 51.10 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-531.11 +/- 77.14
Episode length: 53.56 +/- 14.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-503.58 +/- 82.46
Episode length: 52.72 +/- 18.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-525.17 +/- 74.90
Episode length: 47.14 +/- 19.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 226      |
|    iterations      | 63       |
|    time_elapsed    | 568      |
|    total_timesteps | 129024   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
Eval num_timesteps=129500, episode_reward=-522.75 +/- 72.05
Episode length: 52.22 +/- 20.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.2         |
|    mean_reward          | -523         |
| time/                   |              |
|    total_timesteps      | 129500       |
| train/                  |              |
|    approx_kl            | 0.0038588159 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00336     |
|    explained_variance   | -0.0078      |
|    learning_rate        | 0.001        |
|    loss                 | 1.78e+03     |
|    n_updates            | 288          |
|    policy_gradient_loss | -8.33e-08    |
|    value_loss           | 4.33e+03     |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-501.17 +/- 77.39
Episode length: 48.04 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-514.86 +/- 68.76
Episode length: 50.10 +/- 15.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-522.04 +/- 64.47
Episode length: 50.82 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.4     |
|    ep_rew_mean     | -385     |
| time/              |          |
|    fps             | 226      |
|    iterations      | 64       |
|    time_elapsed    | 577      |
|    total_timesteps | 131072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.36
Eval num_timesteps=131500, episode_reward=-505.94 +/- 72.99
Episode length: 48.06 +/- 15.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.1       |
|    mean_reward          | -506       |
| time/                   |            |
|    total_timesteps      | 131500     |
| train/                  |            |
|    approx_kl            | 0.18224376 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.586     |
|    explained_variance   | 0.000739   |
|    learning_rate        | 0.001      |
|    loss                 | 4.2e+03    |
|    n_updates            | 289        |
|    policy_gradient_loss | 0.0421     |
|    value_loss           | 7e+03      |
----------------------------------------
Eval num_timesteps=132000, episode_reward=-511.35 +/- 73.66
Episode length: 54.00 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-520.35 +/- 72.56
Episode length: 50.32 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-510.75 +/- 86.40
Episode length: 54.48 +/- 17.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | -341     |
| time/              |          |
|    fps             | 226      |
|    iterations      | 65       |
|    time_elapsed    | 586      |
|    total_timesteps | 133120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.51
Eval num_timesteps=133500, episode_reward=-513.14 +/- 84.12
Episode length: 49.36 +/- 13.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.4       |
|    mean_reward          | -513       |
| time/                   |            |
|    total_timesteps      | 133500     |
| train/                  |            |
|    approx_kl            | 0.25636128 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.459     |
|    explained_variance   | 0.00294    |
|    learning_rate        | 0.001      |
|    loss                 | 4.33e+03   |
|    n_updates            | 290        |
|    policy_gradient_loss | 0.109      |
|    value_loss           | 7.26e+03   |
----------------------------------------
Eval num_timesteps=134000, episode_reward=-525.15 +/- 64.28
Episode length: 49.28 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-526.94 +/- 66.62
Episode length: 53.84 +/- 20.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-507.14 +/- 83.57
Episode length: 48.58 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | -432     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 66       |
|    time_elapsed    | 595      |
|    total_timesteps | 135168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.20
Eval num_timesteps=135500, episode_reward=-521.53 +/- 62.73
Episode length: 55.48 +/- 22.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 55.5       |
|    mean_reward          | -522       |
| time/                   |            |
|    total_timesteps      | 135500     |
| train/                  |            |
|    approx_kl            | 0.09865004 |
|    clip_fraction        | 0.0391     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.13      |
|    explained_variance   | 0.0068     |
|    learning_rate        | 0.001      |
|    loss                 | 2.15e+03   |
|    n_updates            | 291        |
|    policy_gradient_loss | 0.00256    |
|    value_loss           | 4.1e+03    |
----------------------------------------
Eval num_timesteps=136000, episode_reward=-514.95 +/- 78.52
Episode length: 51.32 +/- 14.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-516.14 +/- 79.75
Episode length: 51.46 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-513.74 +/- 67.58
Episode length: 50.94 +/- 21.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.2     |
|    ep_rew_mean     | -491     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 67       |
|    time_elapsed    | 604      |
|    total_timesteps | 137216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=137500, episode_reward=-522.14 +/- 62.12
Episode length: 52.36 +/- 15.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.4        |
|    mean_reward          | -522        |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.036509477 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00814    |
|    explained_variance   | 0.00479     |
|    learning_rate        | 0.001       |
|    loss                 | 3.33e+03    |
|    n_updates            | 292         |
|    policy_gradient_loss | 0.00613     |
|    value_loss           | 4.88e+03    |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=-511.94 +/- 74.22
Episode length: 49.52 +/- 17.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-508.34 +/- 71.22
Episode length: 50.82 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-533.56 +/- 62.30
Episode length: 52.78 +/- 17.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 68       |
|    time_elapsed    | 612      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-537.75 +/- 57.36
Episode length: 51.96 +/- 15.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52        |
|    mean_reward          | -538      |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.38e-07 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.001     |
|    loss                 | 894       |
|    n_updates            | 302       |
|    policy_gradient_loss | -1.86e-07 |
|    value_loss           | 3.31e+03  |
---------------------------------------
Eval num_timesteps=140000, episode_reward=-496.95 +/- 82.55
Episode length: 49.78 +/- 19.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-529.34 +/- 53.77
Episode length: 46.88 +/- 14.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-520.95 +/- 67.43
Episode length: 48.22 +/- 15.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 69       |
|    time_elapsed    | 621      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-532.35 +/- 60.20
Episode length: 50.76 +/- 20.19
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 50.8     |
|    mean_reward          | -532     |
| time/                   |          |
|    total_timesteps      | 141500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0207  |
|    learning_rate        | 0.001    |
|    loss                 | 1.16e+06 |
|    n_updates            | 312      |
|    policy_gradient_loss | 1.29e-09 |
|    value_loss           | 2.66e+06 |
--------------------------------------
Eval num_timesteps=142000, episode_reward=-517.95 +/- 78.01
Episode length: 51.28 +/- 18.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-525.75 +/- 66.45
Episode length: 50.00 +/- 14.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-517.34 +/- 77.98
Episode length: 47.82 +/- 18.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 70       |
|    time_elapsed    | 630      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-513.75 +/- 71.70
Episode length: 52.40 +/- 16.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.4      |
|    mean_reward          | -514      |
| time/                   |           |
|    total_timesteps      | 143500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.37e-37 |
|    explained_variance   | 0.0247    |
|    learning_rate        | 0.001     |
|    loss                 | 1.62e+05  |
|    n_updates            | 322       |
|    policy_gradient_loss | 3.07e-09  |
|    value_loss           | 3.36e+05  |
---------------------------------------
Eval num_timesteps=144000, episode_reward=-525.15 +/- 61.70
Episode length: 53.24 +/- 16.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-515.54 +/- 73.36
Episode length: 49.34 +/- 16.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-522.14 +/- 64.40
Episode length: 50.90 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 71       |
|    time_elapsed    | 640      |
|    total_timesteps | 145408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=145500, episode_reward=-528.75 +/- 66.43
Episode length: 54.78 +/- 19.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54.8         |
|    mean_reward          | -529         |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 0.0053195087 |
|    clip_fraction        | 0.0551       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.121       |
|    explained_variance   | 0.0219       |
|    learning_rate        | 0.001        |
|    loss                 | 6.83e+03     |
|    n_updates            | 323          |
|    policy_gradient_loss | 0.00105      |
|    value_loss           | 1.31e+04     |
------------------------------------------
Eval num_timesteps=146000, episode_reward=-535.95 +/- 60.00
Episode length: 53.48 +/- 18.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-529.95 +/- 72.50
Episode length: 49.58 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-516.14 +/- 73.65
Episode length: 52.56 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.2     |
|    ep_rew_mean     | -496     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 72       |
|    time_elapsed    | 649      |
|    total_timesteps | 147456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.36
Eval num_timesteps=147500, episode_reward=-526.34 +/- 65.03
Episode length: 46.46 +/- 15.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.5       |
|    mean_reward          | -526       |
| time/                   |            |
|    total_timesteps      | 147500     |
| train/                  |            |
|    approx_kl            | 0.17759487 |
|    clip_fraction        | 0.484      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.77      |
|    explained_variance   | 0.000588   |
|    learning_rate        | 0.001      |
|    loss                 | 4.65e+03   |
|    n_updates            | 324        |
|    policy_gradient_loss | 0.0614     |
|    value_loss           | 8.36e+03   |
----------------------------------------
Eval num_timesteps=148000, episode_reward=-508.34 +/- 82.46
Episode length: 48.04 +/- 15.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-495.74 +/- 75.09
Episode length: 47.06 +/- 19.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -496     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-525.75 +/- 75.80
Episode length: 50.06 +/- 18.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-511.35 +/- 70.66
Episode length: 49.28 +/- 18.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.3     |
|    ep_rew_mean     | -496     |
| time/              |          |
|    fps             | 226      |
|    iterations      | 73       |
|    time_elapsed    | 658      |
|    total_timesteps | 149504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.25
Eval num_timesteps=150000, episode_reward=-514.94 +/- 70.81
Episode length: 50.94 +/- 17.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.9       |
|    mean_reward          | -515       |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.12281715 |
|    clip_fraction        | 0.0469     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.228     |
|    explained_variance   | 0.000578   |
|    learning_rate        | 0.001      |
|    loss                 | 1.99e+03   |
|    n_updates            | 325        |
|    policy_gradient_loss | 0.0101     |
|    value_loss           | 3.57e+03   |
----------------------------------------
Eval num_timesteps=150500, episode_reward=-516.14 +/- 66.71
Episode length: 51.96 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-508.34 +/- 80.69
Episode length: 48.20 +/- 16.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-523.93 +/- 70.74
Episode length: 50.56 +/- 18.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 74       |
|    time_elapsed    | 667      |
|    total_timesteps | 151552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=152000, episode_reward=-521.54 +/- 70.55
Episode length: 47.92 +/- 13.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.9        |
|    mean_reward          | -522        |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.019272171 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0251     |
|    explained_variance   | 0.00241     |
|    learning_rate        | 0.001       |
|    loss                 | 2.01e+03    |
|    n_updates            | 326         |
|    policy_gradient_loss | 0.0102      |
|    value_loss           | 4.65e+03    |
-----------------------------------------
Eval num_timesteps=152500, episode_reward=-503.54 +/- 65.96
Episode length: 51.32 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-504.74 +/- 78.00
Episode length: 49.84 +/- 18.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-532.95 +/- 61.56
Episode length: 50.82 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 75       |
|    time_elapsed    | 675      |
|    total_timesteps | 153600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 32.14
Eval num_timesteps=154000, episode_reward=-511.84 +/- 69.59
Episode length: 47.84 +/- 18.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.8      |
|    mean_reward          | -512      |
| time/                   |           |
|    total_timesteps      | 154000    |
| train/                  |           |
|    approx_kl            | 5.3571477 |
|    clip_fraction        | 0.167     |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.00405  |
|    explained_variance   | 0.00717   |
|    learning_rate        | 0.001     |
|    loss                 | 1.52e+03  |
|    n_updates            | 327       |
|    policy_gradient_loss | 0.0454    |
|    value_loss           | 3.27e+03  |
---------------------------------------
Eval num_timesteps=154500, episode_reward=-523.97 +/- 61.19
Episode length: 52.08 +/- 18.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-515.58 +/- 65.04
Episode length: 54.10 +/- 20.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-526.96 +/- 70.04
Episode length: 50.00 +/- 15.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 76       |
|    time_elapsed    | 684      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-517.97 +/- 73.00
Episode length: 52.60 +/- 18.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.6      |
|    mean_reward          | -518      |
| time/                   |           |
|    total_timesteps      | 156000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-15 |
|    explained_variance   | -0.0251   |
|    learning_rate        | 0.001     |
|    loss                 | 3.03e+04  |
|    n_updates            | 337       |
|    policy_gradient_loss | 6.45e-09  |
|    value_loss           | 5.97e+04  |
---------------------------------------
Eval num_timesteps=156500, episode_reward=-525.26 +/- 65.20
Episode length: 48.26 +/- 12.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-543.67 +/- 60.46
Episode length: 48.18 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -544     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-509.43 +/- 82.86
Episode length: 46.80 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 77       |
|    time_elapsed    | 693      |
|    total_timesteps | 157696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.24
Eval num_timesteps=158000, episode_reward=-139.46 +/- 116.83
Episode length: 31.32 +/- 7.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 31.3       |
|    mean_reward          | -139       |
| time/                   |            |
|    total_timesteps      | 158000     |
| train/                  |            |
|    approx_kl            | 0.01015934 |
|    clip_fraction        | 0.04       |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.063     |
|    explained_variance   | -0.0022    |
|    learning_rate        | 0.001      |
|    loss                 | 2.63e+03   |
|    n_updates            | 338        |
|    policy_gradient_loss | 0.00344    |
|    value_loss           | 3.6e+03    |
----------------------------------------
New best mean reward!
Eval num_timesteps=158500, episode_reward=-95.37 +/- 109.23
Episode length: 32.98 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | -95.4    |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
New best mean reward!
Eval num_timesteps=159000, episode_reward=-148.74 +/- 143.25
Episode length: 33.02 +/- 8.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-94.62 +/- 153.26
Episode length: 32.94 +/- 9.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | -94.6    |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.5     |
|    ep_rew_mean     | -275     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 78       |
|    time_elapsed    | 700      |
|    total_timesteps | 159744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=160000, episode_reward=-523.97 +/- 66.55
Episode length: 50.78 +/- 15.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.8        |
|    mean_reward          | -524        |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.027749682 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.696      |
|    explained_variance   | -0.000361   |
|    learning_rate        | 0.001       |
|    loss                 | 4.46e+03    |
|    n_updates            | 339         |
|    policy_gradient_loss | 0.0531      |
|    value_loss           | 7.06e+03    |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=-532.97 +/- 58.88
Episode length: 49.78 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-538.35 +/- 61.25
Episode length: 50.64 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-526.25 +/- 72.21
Episode length: 49.18 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.1     |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 79       |
|    time_elapsed    | 709      |
|    total_timesteps | 161792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.59
Eval num_timesteps=162000, episode_reward=-528.17 +/- 61.15
Episode length: 49.80 +/- 13.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.8       |
|    mean_reward          | -528       |
| time/                   |            |
|    total_timesteps      | 162000     |
| train/                  |            |
|    approx_kl            | 0.29735982 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.397     |
|    explained_variance   | 0.000372   |
|    learning_rate        | 0.001      |
|    loss                 | 3.97e+03   |
|    n_updates            | 340        |
|    policy_gradient_loss | 0.0712     |
|    value_loss           | 9.15e+03   |
----------------------------------------
Eval num_timesteps=162500, episode_reward=-527.51 +/- 69.97
Episode length: 55.22 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-520.36 +/- 67.43
Episode length: 44.58 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-506.57 +/- 81.06
Episode length: 48.22 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.6     |
|    ep_rew_mean     | -359     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 80       |
|    time_elapsed    | 717      |
|    total_timesteps | 163840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.22
Eval num_timesteps=164000, episode_reward=-510.18 +/- 76.62
Episode length: 49.96 +/- 17.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | -510       |
| time/                   |            |
|    total_timesteps      | 164000     |
| train/                  |            |
|    approx_kl            | 0.11054057 |
|    clip_fraction        | 0.0234     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0788    |
|    explained_variance   | -0.000584  |
|    learning_rate        | 0.001      |
|    loss                 | 2.63e+03   |
|    n_updates            | 341        |
|    policy_gradient_loss | -0.00933   |
|    value_loss           | 6.29e+03   |
----------------------------------------
Eval num_timesteps=164500, episode_reward=-523.94 +/- 79.60
Episode length: 50.24 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-507.18 +/- 93.72
Episode length: 49.86 +/- 13.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-535.98 +/- 61.48
Episode length: 49.92 +/- 13.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.7     |
|    ep_rew_mean     | -456     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 81       |
|    time_elapsed    | 726      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-515.31 +/- 75.89
Episode length: 49.30 +/- 15.83
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 49.3     |
|    mean_reward          | -515     |
| time/                   |          |
|    total_timesteps      | 166000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.8e-06 |
|    explained_variance   | -0.00403 |
|    learning_rate        | 0.001    |
|    loss                 | 2.18e+03 |
|    n_updates            | 351      |
|    policy_gradient_loss | 2.9e-06  |
|    value_loss           | 2.91e+03 |
--------------------------------------
Eval num_timesteps=166500, episode_reward=-508.81 +/- 73.94
Episode length: 46.90 +/- 17.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-511.97 +/- 81.84
Episode length: 46.86 +/- 15.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-517.35 +/- 64.31
Episode length: 48.32 +/- 13.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 82       |
|    time_elapsed    | 735      |
|    total_timesteps | 167936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 3.50
Eval num_timesteps=168000, episode_reward=788.41 +/- 672.40
Episode length: 35.04 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 788       |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.1840346 |
|    clip_fraction        | 0.0526    |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.00366  |
|    explained_variance   | -0.0319   |
|    learning_rate        | 0.001     |
|    loss                 | 1.4e+05   |
|    n_updates            | 352       |
|    policy_gradient_loss | 0.0118    |
|    value_loss           | 2.96e+05  |
---------------------------------------
New best mean reward!
Eval num_timesteps=168500, episode_reward=869.98 +/- 719.36
Episode length: 35.44 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
New best mean reward!
Eval num_timesteps=169000, episode_reward=948.10 +/- 693.87
Episode length: 36.98 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
New best mean reward!
Eval num_timesteps=169500, episode_reward=922.48 +/- 704.92
Episode length: 36.08 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 183      |
| time/              |          |
|    fps             | 229      |
|    iterations      | 83       |
|    time_elapsed    | 742      |
|    total_timesteps | 169984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=170000, episode_reward=743.79 +/- 691.66
Episode length: 34.06 +/- 7.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.1        |
|    mean_reward          | 744         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.074000515 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0524     |
|    explained_variance   | -0.0354     |
|    learning_rate        | 0.001       |
|    loss                 | 6.33e+04    |
|    n_updates            | 353         |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 1.32e+05    |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=997.48 +/- 736.54
Episode length: 36.18 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 997      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
New best mean reward!
Eval num_timesteps=171000, episode_reward=588.12 +/- 516.33
Episode length: 33.50 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 588      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=838.37 +/- 697.03
Episode length: 35.68 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=859.71 +/- 689.20
Episode length: 35.64 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 731      |
| time/              |          |
|    fps             | 229      |
|    iterations      | 84       |
|    time_elapsed    | 750      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=766.36 +/- 629.96
Episode length: 35.48 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 766       |
| time/                   |           |
|    total_timesteps      | 172500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.95e-08 |
|    explained_variance   | -0.0273   |
|    learning_rate        | 0.001     |
|    loss                 | 7.17e+04  |
|    n_updates            | 363       |
|    policy_gradient_loss | 2.7e-09   |
|    value_loss           | 1.42e+05  |
---------------------------------------
Eval num_timesteps=173000, episode_reward=852.76 +/- 675.30
Episode length: 35.46 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=895.09 +/- 703.68
Episode length: 35.56 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=799.94 +/- 696.07
Episode length: 34.60 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 825      |
| time/              |          |
|    fps             | 229      |
|    iterations      | 85       |
|    time_elapsed    | 757      |
|    total_timesteps | 174080   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.30
Eval num_timesteps=174500, episode_reward=-521.47 +/- 70.07
Episode length: 52.02 +/- 15.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | -521        |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.013612011 |
|    clip_fraction        | 0.0116      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00933    |
|    explained_variance   | -0.00214    |
|    learning_rate        | 0.001       |
|    loss                 | 1.61e+04    |
|    n_updates            | 366         |
|    policy_gradient_loss | 0.00121     |
|    value_loss           | 4.31e+04    |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=-509.98 +/- 73.89
Episode length: 47.72 +/- 14.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-515.58 +/- 80.37
Episode length: 55.76 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-509.55 +/- 83.73
Episode length: 48.80 +/- 15.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 191      |
| time/              |          |
|    fps             | 229      |
|    iterations      | 86       |
|    time_elapsed    | 766      |
|    total_timesteps | 176128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.68
Eval num_timesteps=176500, episode_reward=-530.57 +/- 73.40
Episode length: 49.26 +/- 18.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.3       |
|    mean_reward          | -531       |
| time/                   |            |
|    total_timesteps      | 176500     |
| train/                  |            |
|    approx_kl            | 0.33919737 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.419     |
|    explained_variance   | 0.000418   |
|    learning_rate        | 0.001      |
|    loss                 | 4.2e+03    |
|    n_updates            | 367        |
|    policy_gradient_loss | 0.0857     |
|    value_loss           | 8.32e+03   |
----------------------------------------
Eval num_timesteps=177000, episode_reward=-520.97 +/- 69.53
Episode length: 52.20 +/- 18.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-538.98 +/- 64.41
Episode length: 52.86 +/- 18.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -539     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-518.94 +/- 77.69
Episode length: 48.64 +/- 14.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.3     |
|    ep_rew_mean     | -319     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 87       |
|    time_elapsed    | 775      |
|    total_timesteps | 178176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=178500, episode_reward=-522.17 +/- 69.77
Episode length: 48.12 +/- 15.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.1        |
|    mean_reward          | -522        |
| time/                   |             |
|    total_timesteps      | 178500      |
| train/                  |             |
|    approx_kl            | 0.030319715 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0722     |
|    explained_variance   | 0.000187    |
|    learning_rate        | 0.001       |
|    loss                 | 2.95e+03    |
|    n_updates            | 368         |
|    policy_gradient_loss | 0.0131      |
|    value_loss           | 5.92e+03    |
-----------------------------------------
Eval num_timesteps=179000, episode_reward=-530.58 +/- 72.42
Episode length: 48.34 +/- 17.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-513.18 +/- 71.64
Episode length: 51.06 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-502.71 +/- 95.43
Episode length: 51.06 +/- 17.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -503     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.1     |
|    ep_rew_mean     | -471     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 88       |
|    time_elapsed    | 783      |
|    total_timesteps | 180224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.22
Eval num_timesteps=180500, episode_reward=-524.93 +/- 73.89
Episode length: 48.96 +/- 17.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49         |
|    mean_reward          | -525       |
| time/                   |            |
|    total_timesteps      | 180500     |
| train/                  |            |
|    approx_kl            | 0.00831375 |
|    clip_fraction        | 0.000579   |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.00138   |
|    explained_variance   | -0.000176  |
|    learning_rate        | 0.001      |
|    loss                 | 2.53e+03   |
|    n_updates            | 369        |
|    policy_gradient_loss | -5.02e-05  |
|    value_loss           | 6.01e+03   |
----------------------------------------
Eval num_timesteps=181000, episode_reward=-528.18 +/- 68.63
Episode length: 51.62 +/- 14.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-528.09 +/- 70.88
Episode length: 48.68 +/- 16.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-510.15 +/- 77.32
Episode length: 47.04 +/- 17.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | -509     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 89       |
|    time_elapsed    | 792      |
|    total_timesteps | 182272   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.14
Eval num_timesteps=182500, episode_reward=-514.37 +/- 73.73
Episode length: 49.54 +/- 15.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | -514        |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.045186743 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0257     |
|    explained_variance   | -0.00402    |
|    learning_rate        | 0.001       |
|    loss                 | 1.29e+03    |
|    n_updates            | 371         |
|    policy_gradient_loss | 0.00143     |
|    value_loss           | 2.89e+03    |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=-531.77 +/- 58.48
Episode length: 50.12 +/- 15.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-535.21 +/- 64.97
Episode length: 53.10 +/- 17.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-518.18 +/- 82.59
Episode length: 47.34 +/- 17.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.8     |
|    ep_rew_mean     | -313     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 90       |
|    time_elapsed    | 801      |
|    total_timesteps | 184320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.74
Eval num_timesteps=184500, episode_reward=698.00 +/- 631.73
Episode length: 33.70 +/- 7.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 33.7       |
|    mean_reward          | 698        |
| time/                   |            |
|    total_timesteps      | 184500     |
| train/                  |            |
|    approx_kl            | 0.36811998 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.495     |
|    explained_variance   | 0.000561   |
|    learning_rate        | 0.001      |
|    loss                 | 4.2e+03    |
|    n_updates            | 372        |
|    policy_gradient_loss | 0.103      |
|    value_loss           | 7.13e+03   |
----------------------------------------
Eval num_timesteps=185000, episode_reward=844.58 +/- 721.96
Episode length: 35.02 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=912.09 +/- 701.11
Episode length: 36.48 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=830.47 +/- 750.94
Episode length: 34.32 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.5     |
|    ep_rew_mean     | 164      |
| time/              |          |
|    fps             | 230      |
|    iterations      | 91       |
|    time_elapsed    | 807      |
|    total_timesteps | 186368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=186500, episode_reward=860.17 +/- 653.89
Episode length: 35.64 +/- 6.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.6       |
|    mean_reward          | 860        |
| time/                   |            |
|    total_timesteps      | 186500     |
| train/                  |            |
|    approx_kl            | 0.04383669 |
|    clip_fraction        | 0.0625     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.209     |
|    explained_variance   | 0.00026    |
|    learning_rate        | 0.001      |
|    loss                 | 8.17e+03   |
|    n_updates            | 373        |
|    policy_gradient_loss | -2.89e-05  |
|    value_loss           | 1.5e+04    |
----------------------------------------
Eval num_timesteps=187000, episode_reward=655.90 +/- 575.36
Episode length: 33.58 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=940.19 +/- 702.19
Episode length: 36.46 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 940      |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=758.58 +/- 727.82
Episode length: 33.24 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 579      |
| time/              |          |
|    fps             | 231      |
|    iterations      | 92       |
|    time_elapsed    | 814      |
|    total_timesteps | 188416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=188500, episode_reward=987.86 +/- 768.86
Episode length: 36.08 +/- 7.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 988          |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 0.0048133414 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0716      |
|    explained_variance   | -0.00129     |
|    learning_rate        | 0.001        |
|    loss                 | 1.42e+04     |
|    n_updates            | 374          |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 4.1e+04      |
------------------------------------------
Eval num_timesteps=189000, episode_reward=948.88 +/- 722.67
Episode length: 35.98 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=879.23 +/- 709.49
Episode length: 35.28 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=878.09 +/- 664.77
Episode length: 36.42 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 765      |
| time/              |          |
|    fps             | 231      |
|    iterations      | 93       |
|    time_elapsed    | 821      |
|    total_timesteps | 190464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=190500, episode_reward=739.37 +/- 690.95
Episode length: 33.88 +/- 7.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.9        |
|    mean_reward          | 739         |
| time/                   |             |
|    total_timesteps      | 190500      |
| train/                  |             |
|    approx_kl            | 0.010650407 |
|    clip_fraction        | 0.00469     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | -0.00053    |
|    learning_rate        | 0.001       |
|    loss                 | 1.11e+04    |
|    n_updates            | 375         |
|    policy_gradient_loss | 0.0041      |
|    value_loss           | 3.14e+04    |
-----------------------------------------
Eval num_timesteps=191000, episode_reward=876.45 +/- 680.16
Episode length: 36.16 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=947.47 +/- 710.65
Episode length: 36.50 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=803.45 +/- 725.37
Episode length: 33.92 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=942.14 +/- 727.21
Episode length: 35.86 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 433      |
| time/              |          |
|    fps             | 232      |
|    iterations      | 94       |
|    time_elapsed    | 829      |
|    total_timesteps | 192512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.21
Eval num_timesteps=193000, episode_reward=831.55 +/- 681.34
Episode length: 35.16 +/- 5.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.2       |
|    mean_reward          | 832        |
| time/                   |            |
|    total_timesteps      | 193000     |
| train/                  |            |
|    approx_kl            | 0.10300115 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.000369   |
|    learning_rate        | 0.001      |
|    loss                 | 5.18e+03   |
|    n_updates            | 376        |
|    policy_gradient_loss | 0.0937     |
|    value_loss           | 1.03e+04   |
----------------------------------------
Eval num_timesteps=193500, episode_reward=852.86 +/- 721.77
Episode length: 34.90 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=725.25 +/- 620.75
Episode length: 35.00 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=936.36 +/- 776.14
Episode length: 35.00 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33       |
|    ep_rew_mean     | 12.8     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 95       |
|    time_elapsed    | 835      |
|    total_timesteps | 194560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=195000, episode_reward=892.73 +/- 722.12
Episode length: 35.80 +/- 6.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 893          |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0057915775 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.46        |
|    explained_variance   | 8.95e-05     |
|    learning_rate        | 0.001        |
|    loss                 | 2.2e+03      |
|    n_updates            | 377          |
|    policy_gradient_loss | -0.00365     |
|    value_loss           | 5.19e+03     |
------------------------------------------
Eval num_timesteps=195500, episode_reward=722.19 +/- 619.08
Episode length: 34.42 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=688.04 +/- 675.55
Episode length: 32.56 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 688      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=797.26 +/- 644.41
Episode length: 35.06 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.5     |
|    ep_rew_mean     | -77.7    |
| time/              |          |
|    fps             | 233      |
|    iterations      | 96       |
|    time_elapsed    | 842      |
|    total_timesteps | 196608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=197000, episode_reward=689.43 +/- 595.39
Episode length: 34.16 +/- 6.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 689       |
| time/                   |           |
|    total_timesteps      | 197000    |
| train/                  |           |
|    approx_kl            | 0.0159947 |
|    clip_fraction        | 0.0781    |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18     |
|    explained_variance   | 9.89e-05  |
|    learning_rate        | 0.001     |
|    loss                 | 2.65e+03  |
|    n_updates            | 378       |
|    policy_gradient_loss | 0.0024    |
|    value_loss           | 6.54e+03  |
---------------------------------------
Eval num_timesteps=197500, episode_reward=755.01 +/- 666.63
Episode length: 34.16 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=689.35 +/- 572.63
Episode length: 34.54 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=644.09 +/- 575.36
Episode length: 34.14 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.7     |
|    ep_rew_mean     | 8.01     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 97       |
|    time_elapsed    | 849      |
|    total_timesteps | 198656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=199000, episode_reward=908.62 +/- 695.66
Episode length: 36.10 +/- 6.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.1        |
|    mean_reward          | 909         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.009671391 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.000131    |
|    learning_rate        | 0.001       |
|    loss                 | 4.48e+03    |
|    n_updates            | 379         |
|    policy_gradient_loss | 0.00451     |
|    value_loss           | 7.4e+03     |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=970.36 +/- 730.40
Episode length: 36.36 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=925.35 +/- 742.82
Episode length: 35.40 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=947.23 +/- 744.58
Episode length: 35.64 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.9     |
|    ep_rew_mean     | -40.8    |
| time/              |          |
|    fps             | 234      |
|    iterations      | 98       |
|    time_elapsed    | 855      |
|    total_timesteps | 200704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=201000, episode_reward=835.57 +/- 643.40
Episode length: 35.44 +/- 6.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 836         |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.032073345 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.000163    |
|    learning_rate        | 0.001       |
|    loss                 | 5.68e+03    |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00683    |
|    value_loss           | 8.58e+03    |
-----------------------------------------
Eval num_timesteps=201500, episode_reward=834.22 +/- 709.23
Episode length: 34.76 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=789.88 +/- 648.90
Episode length: 35.98 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=899.12 +/- 732.56
Episode length: 35.46 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.9     |
|    ep_rew_mean     | 58       |
| time/              |          |
|    fps             | 235      |
|    iterations      | 99       |
|    time_elapsed    | 862      |
|    total_timesteps | 202752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=203000, episode_reward=900.46 +/- 706.51
Episode length: 36.32 +/- 6.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.3        |
|    mean_reward          | 900         |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.005992679 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.883      |
|    explained_variance   | 0.000323    |
|    learning_rate        | 0.001       |
|    loss                 | 3.8e+03     |
|    n_updates            | 381         |
|    policy_gradient_loss | 0.0101      |
|    value_loss           | 7.54e+03    |
-----------------------------------------
Eval num_timesteps=203500, episode_reward=935.73 +/- 702.27
Episode length: 36.06 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=955.35 +/- 752.87
Episode length: 35.62 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=889.66 +/- 725.81
Episode length: 35.14 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.8     |
|    ep_rew_mean     | 130      |
| time/              |          |
|    fps             | 235      |
|    iterations      | 100      |
|    time_elapsed    | 869      |
|    total_timesteps | 204800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=205000, episode_reward=836.89 +/- 651.18
Episode length: 35.92 +/- 6.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | 837         |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.014539775 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.00023     |
|    learning_rate        | 0.001       |
|    loss                 | 5.11e+03    |
|    n_updates            | 382         |
|    policy_gradient_loss | 0.0249      |
|    value_loss           | 8.39e+03    |
-----------------------------------------
Eval num_timesteps=205500, episode_reward=840.88 +/- 723.47
Episode length: 35.32 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=716.39 +/- 569.34
Episode length: 35.50 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=743.73 +/- 660.34
Episode length: 34.44 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.7     |
|    ep_rew_mean     | 4.47     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 101      |
|    time_elapsed    | 876      |
|    total_timesteps | 206848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=207000, episode_reward=893.11 +/- 670.77
Episode length: 36.22 +/- 5.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 893          |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0066373106 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.000195     |
|    learning_rate        | 0.001        |
|    loss                 | 3.89e+03     |
|    n_updates            | 383          |
|    policy_gradient_loss | 0.00501      |
|    value_loss           | 7.42e+03     |
------------------------------------------
Eval num_timesteps=207500, episode_reward=596.91 +/- 505.19
Episode length: 33.68 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=785.66 +/- 693.04
Episode length: 34.06 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=830.41 +/- 743.89
Episode length: 34.58 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.6     |
|    ep_rew_mean     | 21.3     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 102      |
|    time_elapsed    | 882      |
|    total_timesteps | 208896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=209000, episode_reward=823.36 +/- 625.05
Episode length: 35.60 +/- 6.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 823         |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.014561238 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.936      |
|    explained_variance   | 0.000436    |
|    learning_rate        | 0.001       |
|    loss                 | 3.51e+03    |
|    n_updates            | 384         |
|    policy_gradient_loss | -0.00812    |
|    value_loss           | 6.96e+03    |
-----------------------------------------
Eval num_timesteps=209500, episode_reward=844.59 +/- 706.80
Episode length: 35.48 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=906.27 +/- 660.40
Episode length: 36.82 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=907.46 +/- 723.61
Episode length: 35.74 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.9     |
|    ep_rew_mean     | 147      |
| time/              |          |
|    fps             | 237      |
|    iterations      | 103      |
|    time_elapsed    | 889      |
|    total_timesteps | 210944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=211000, episode_reward=803.34 +/- 661.84
Episode length: 35.48 +/- 6.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | 803         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.010641591 |
|    clip_fraction        | 0.0603      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.817      |
|    explained_variance   | 0.000399    |
|    learning_rate        | 0.001       |
|    loss                 | 6.09e+03    |
|    n_updates            | 385         |
|    policy_gradient_loss | 0.00423     |
|    value_loss           | 9.8e+03     |
-----------------------------------------
Eval num_timesteps=211500, episode_reward=780.75 +/- 655.25
Episode length: 35.02 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=715.84 +/- 607.68
Episode length: 34.64 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=926.03 +/- 725.48
Episode length: 35.72 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 237      |
|    iterations      | 104      |
|    time_elapsed    | 896      |
|    total_timesteps | 212992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=213000, episode_reward=962.94 +/- 765.03
Episode length: 35.02 +/- 6.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 963         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.016125374 |
|    clip_fraction        | 0.0729      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.413      |
|    explained_variance   | 0.000415    |
|    learning_rate        | 0.001       |
|    loss                 | 4.12e+03    |
|    n_updates            | 386         |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 1.45e+04    |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=654.62 +/- 629.67
Episode length: 33.40 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=933.74 +/- 722.53
Episode length: 36.32 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=826.83 +/- 669.10
Episode length: 35.10 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=958.83 +/- 714.55
Episode length: 36.56 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 481      |
| time/              |          |
|    fps             | 237      |
|    iterations      | 105      |
|    time_elapsed    | 904      |
|    total_timesteps | 215040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=215500, episode_reward=894.28 +/- 698.83
Episode length: 36.02 +/- 6.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36          |
|    mean_reward          | 894         |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.008208566 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.283      |
|    explained_variance   | 0.000207    |
|    learning_rate        | 0.001       |
|    loss                 | 5.89e+03    |
|    n_updates            | 387         |
|    policy_gradient_loss | 0.00371     |
|    value_loss           | 1.57e+04    |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=867.50 +/- 670.26
Episode length: 36.06 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=784.73 +/- 640.48
Episode length: 35.10 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=905.76 +/- 666.75
Episode length: 36.98 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 690      |
| time/              |          |
|    fps             | 238      |
|    iterations      | 106      |
|    time_elapsed    | 910      |
|    total_timesteps | 217088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=217500, episode_reward=922.84 +/- 697.29
Episode length: 36.04 +/- 6.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36          |
|    mean_reward          | 923         |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.007613051 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 3.73e-05    |
|    learning_rate        | 0.001       |
|    loss                 | 2.78e+04    |
|    n_updates            | 388         |
|    policy_gradient_loss | -0.000515   |
|    value_loss           | 3.6e+04     |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=709.56 +/- 584.01
Episode length: 34.64 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 710      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=923.60 +/- 709.65
Episode length: 35.80 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 924      |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=837.52 +/- 673.29
Episode length: 35.40 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 714      |
| time/              |          |
|    fps             | 238      |
|    iterations      | 107      |
|    time_elapsed    | 917      |
|    total_timesteps | 219136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=219500, episode_reward=808.13 +/- 688.05
Episode length: 35.04 +/- 7.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 808         |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.011818749 |
|    clip_fraction        | 0.013       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0374     |
|    explained_variance   | 3.87e-05    |
|    learning_rate        | 0.001       |
|    loss                 | 9.38e+03    |
|    n_updates            | 389         |
|    policy_gradient_loss | 0.000952    |
|    value_loss           | 2.81e+04    |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=833.97 +/- 722.20
Episode length: 34.72 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=945.15 +/- 674.62
Episode length: 37.12 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 945      |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=781.29 +/- 605.98
Episode length: 36.02 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 239      |
|    iterations      | 108      |
|    time_elapsed    | 924      |
|    total_timesteps | 221184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=221500, episode_reward=917.33 +/- 681.53
Episode length: 36.68 +/- 5.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.7        |
|    mean_reward          | 917         |
| time/                   |             |
|    total_timesteps      | 221500      |
| train/                  |             |
|    approx_kl            | 0.013979621 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00428    |
|    explained_variance   | 0.000331    |
|    learning_rate        | 0.001       |
|    loss                 | 1.71e+04    |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.000257   |
|    value_loss           | 4.03e+04    |
-----------------------------------------
Eval num_timesteps=222000, episode_reward=697.13 +/- 638.32
Episode length: 33.62 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 697      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=832.80 +/- 635.79
Episode length: 35.92 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=789.98 +/- 627.57
Episode length: 36.10 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 827      |
| time/              |          |
|    fps             | 239      |
|    iterations      | 109      |
|    time_elapsed    | 931      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=764.32 +/- 712.03
Episode length: 34.04 +/- 7.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 764       |
| time/                   |           |
|    total_timesteps      | 223500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-06 |
|    explained_variance   | 0.000587  |
|    learning_rate        | 0.001     |
|    loss                 | 2.73e+04  |
|    n_updates            | 400       |
|    policy_gradient_loss | 4.11e-07  |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=224000, episode_reward=753.30 +/- 637.02
Episode length: 34.62 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=913.18 +/- 722.48
Episode length: 36.26 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=990.37 +/- 716.78
Episode length: 36.70 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 240      |
|    iterations      | 110      |
|    time_elapsed    | 938      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=867.41 +/- 622.02
Episode length: 36.18 +/- 5.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 867       |
| time/                   |           |
|    total_timesteps      | 225500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.23e-30 |
|    explained_variance   | 0.00765   |
|    learning_rate        | 0.001     |
|    loss                 | 6.37e+03  |
|    n_updates            | 410       |
|    policy_gradient_loss | 7.97e-10  |
|    value_loss           | 2.64e+04  |
---------------------------------------
Eval num_timesteps=226000, episode_reward=895.50 +/- 691.83
Episode length: 35.48 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=893.09 +/- 695.92
Episode length: 35.84 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=738.97 +/- 638.75
Episode length: 34.38 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 926      |
| time/              |          |
|    fps             | 240      |
|    iterations      | 111      |
|    time_elapsed    | 945      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=754.20 +/- 627.43
Episode length: 35.30 +/- 6.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 754       |
| time/                   |           |
|    total_timesteps      | 227500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.49e-28 |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.001     |
|    loss                 | 4.03e+04  |
|    n_updates            | 420       |
|    policy_gradient_loss | -1.27e-09 |
|    value_loss           | 7.35e+04  |
---------------------------------------
Eval num_timesteps=228000, episode_reward=733.50 +/- 685.39
Episode length: 33.62 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=801.07 +/- 648.80
Episode length: 35.60 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=907.73 +/- 671.61
Episode length: 36.70 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 240      |
|    iterations      | 112      |
|    time_elapsed    | 953      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=891.46 +/- 705.93
Episode length: 36.12 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 891       |
| time/                   |           |
|    total_timesteps      | 229500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.44e-30 |
|    explained_variance   | 0.00648   |
|    learning_rate        | 0.001     |
|    loss                 | 2.58e+04  |
|    n_updates            | 430       |
|    policy_gradient_loss | -1.16e-09 |
|    value_loss           | 2.74e+04  |
---------------------------------------
Eval num_timesteps=230000, episode_reward=736.27 +/- 626.63
Episode length: 35.08 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=1043.65 +/- 713.54
Episode length: 37.18 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
New best mean reward!
Eval num_timesteps=231000, episode_reward=979.78 +/- 760.72
Episode length: 36.08 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 909      |
| time/              |          |
|    fps             | 240      |
|    iterations      | 113      |
|    time_elapsed    | 960      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=705.34 +/- 587.02
Episode length: 34.38 +/- 5.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 705       |
| time/                   |           |
|    total_timesteps      | 231500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.76e-27 |
|    explained_variance   | 0.0177    |
|    learning_rate        | 0.001     |
|    loss                 | 3.48e+04  |
|    n_updates            | 440       |
|    policy_gradient_loss | 1.09e-09  |
|    value_loss           | 5.78e+04  |
---------------------------------------
Eval num_timesteps=232000, episode_reward=882.16 +/- 729.11
Episode length: 35.06 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=927.96 +/- 722.83
Episode length: 35.94 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=703.88 +/- 597.87
Episode length: 34.28 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 985      |
| time/              |          |
|    fps             | 241      |
|    iterations      | 114      |
|    time_elapsed    | 968      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=959.41 +/- 743.15
Episode length: 35.38 +/- 6.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.4     |
|    mean_reward          | 959      |
| time/                   |          |
|    total_timesteps      | 233500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -9e-30   |
|    explained_variance   | 0.00924  |
|    learning_rate        | 0.001    |
|    loss                 | 2.3e+04  |
|    n_updates            | 450      |
|    policy_gradient_loss | 9.12e-10 |
|    value_loss           | 2.89e+04 |
--------------------------------------
Eval num_timesteps=234000, episode_reward=722.28 +/- 659.33
Episode length: 34.10 +/- 8.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=831.70 +/- 733.42
Episode length: 33.92 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=942.78 +/- 698.95
Episode length: 36.32 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=879.83 +/- 686.31
Episode length: 35.70 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.4     |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 115      |
|    time_elapsed    | 976      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=755.68 +/- 635.96
Episode length: 34.82 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 756       |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.26e-26 |
|    explained_variance   | 0.0232    |
|    learning_rate        | 0.001     |
|    loss                 | 1.86e+04  |
|    n_updates            | 460       |
|    policy_gradient_loss | -3.11e-09 |
|    value_loss           | 4.62e+04  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=604.22 +/- 580.49
Episode length: 32.24 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.2     |
|    mean_reward     | 604      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=715.54 +/- 656.63
Episode length: 33.48 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=746.09 +/- 622.45
Episode length: 35.02 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 116      |
|    time_elapsed    | 983      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=805.10 +/- 651.94
Episode length: 35.00 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 805       |
| time/                   |           |
|    total_timesteps      | 238000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.99e-28 |
|    explained_variance   | 0.0123    |
|    learning_rate        | 0.001     |
|    loss                 | 1.7e+04   |
|    n_updates            | 470       |
|    policy_gradient_loss | -4.37e-12 |
|    value_loss           | 2.96e+04  |
---------------------------------------
Eval num_timesteps=238500, episode_reward=908.80 +/- 707.97
Episode length: 36.12 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=860.71 +/- 680.31
Episode length: 35.62 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=878.36 +/- 710.93
Episode length: 35.70 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 241      |
|    iterations      | 117      |
|    time_elapsed    | 991      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=910.39 +/- 680.58
Episode length: 36.74 +/- 5.81
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.7     |
|    mean_reward          | 910      |
| time/                   |          |
|    total_timesteps      | 240000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5.6e-25 |
|    explained_variance   | 0.00977  |
|    learning_rate        | 0.001    |
|    loss                 | 2.93e+04 |
|    n_updates            | 480      |
|    policy_gradient_loss | 1.45e-09 |
|    value_loss           | 5.76e+04 |
--------------------------------------
Eval num_timesteps=240500, episode_reward=947.66 +/- 669.89
Episode length: 37.36 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=725.59 +/- 613.74
Episode length: 34.36 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=882.65 +/- 666.63
Episode length: 36.18 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 712      |
| time/              |          |
|    fps             | 242      |
|    iterations      | 118      |
|    time_elapsed    | 998      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=792.37 +/- 697.04
Episode length: 34.70 +/- 6.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 792       |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.03e-08 |
|    explained_variance   | 0.00688   |
|    learning_rate        | 0.001     |
|    loss                 | 2.55e+04  |
|    n_updates            | 490       |
|    policy_gradient_loss | -1.5e-09  |
|    value_loss           | 2.31e+04  |
---------------------------------------
Eval num_timesteps=242500, episode_reward=924.03 +/- 681.57
Episode length: 36.12 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 924      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=875.95 +/- 666.70
Episode length: 35.70 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=1011.55 +/- 725.10
Episode length: 36.96 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 242      |
|    iterations      | 119      |
|    time_elapsed    | 1006     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=708.27 +/- 650.08
Episode length: 33.94 +/- 7.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 708       |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.29e-10 |
|    explained_variance   | 0.00244   |
|    learning_rate        | 0.001     |
|    loss                 | 2.86e+04  |
|    n_updates            | 500       |
|    policy_gradient_loss | -1.99e-09 |
|    value_loss           | 3.82e+04  |
---------------------------------------
Eval num_timesteps=244500, episode_reward=1019.53 +/- 720.97
Episode length: 37.30 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=866.41 +/- 659.53
Episode length: 36.00 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=801.35 +/- 641.90
Episode length: 35.64 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 242      |
|    iterations      | 120      |
|    time_elapsed    | 1013     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=840.42 +/- 678.99
Episode length: 35.38 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 840       |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.53e-27 |
|    explained_variance   | 0.00746   |
|    learning_rate        | 0.001     |
|    loss                 | 2.17e+04  |
|    n_updates            | 510       |
|    policy_gradient_loss | -7.03e-10 |
|    value_loss           | 2.26e+04  |
---------------------------------------
Eval num_timesteps=246500, episode_reward=782.24 +/- 691.01
Episode length: 34.52 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=858.93 +/- 684.34
Episode length: 35.10 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=910.60 +/- 711.81
Episode length: 35.80 +/- 7.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 847      |
| time/              |          |
|    fps             | 242      |
|    iterations      | 121      |
|    time_elapsed    | 1020     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=709.62 +/- 590.90
Episode length: 34.78 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 710       |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.62e-24 |
|    explained_variance   | 0.0147    |
|    learning_rate        | 0.001     |
|    loss                 | 2.73e+04  |
|    n_updates            | 520       |
|    policy_gradient_loss | -3.23e-09 |
|    value_loss           | 5.94e+04  |
---------------------------------------
Eval num_timesteps=248500, episode_reward=1019.91 +/- 714.34
Episode length: 37.18 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=753.26 +/- 628.47
Episode length: 35.08 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=667.34 +/- 596.27
Episode length: 33.82 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 667      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 851      |
| time/              |          |
|    fps             | 243      |
|    iterations      | 122      |
|    time_elapsed    | 1028     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=797.56 +/- 648.99
Episode length: 35.06 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 798       |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.41e-26 |
|    explained_variance   | 0.00657   |
|    learning_rate        | 0.001     |
|    loss                 | 1.76e+04  |
|    n_updates            | 530       |
|    policy_gradient_loss | -4.89e-10 |
|    value_loss           | 2.87e+04  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=857.86 +/- 711.42
Episode length: 35.08 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=770.18 +/- 632.73
Episode length: 35.12 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=806.81 +/- 673.01
Episode length: 35.16 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 243      |
|    iterations      | 123      |
|    time_elapsed    | 1035     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=907.17 +/- 689.54
Episode length: 36.44 +/- 6.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 907       |
| time/                   |           |
|    total_timesteps      | 252000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.29e-22 |
|    explained_variance   | 0.000834  |
|    learning_rate        | 0.001     |
|    loss                 | 2.12e+04  |
|    n_updates            | 540       |
|    policy_gradient_loss | -7.42e-11 |
|    value_loss           | 5.2e+04   |
---------------------------------------
Eval num_timesteps=252500, episode_reward=802.33 +/- 676.51
Episode length: 35.18 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=729.78 +/- 579.36
Episode length: 35.10 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=876.91 +/- 734.40
Episode length: 34.92 +/- 7.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 771      |
| time/              |          |
|    fps             | 243      |
|    iterations      | 124      |
|    time_elapsed    | 1042     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=994.93 +/- 740.63
Episode length: 36.60 +/- 5.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 995       |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.89e-24 |
|    explained_variance   | 0.00625   |
|    learning_rate        | 0.001     |
|    loss                 | 7.75e+03  |
|    n_updates            | 550       |
|    policy_gradient_loss | -1.51e-10 |
|    value_loss           | 2.35e+04  |
---------------------------------------
Eval num_timesteps=254500, episode_reward=830.55 +/- 689.90
Episode length: 35.18 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=911.17 +/- 712.79
Episode length: 36.10 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=735.15 +/- 668.98
Episode length: 33.88 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=805.07 +/- 660.99
Episode length: 35.22 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 243      |
|    iterations      | 125      |
|    time_elapsed    | 1051     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=828.24 +/- 639.24
Episode length: 35.98 +/- 5.90
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36       |
|    mean_reward          | 828      |
| time/                   |          |
|    total_timesteps      | 256500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -9.1e-20 |
|    explained_variance   | 0.00721  |
|    learning_rate        | 0.001    |
|    loss                 | 1.64e+04 |
|    n_updates            | 560      |
|    policy_gradient_loss | 8.93e-10 |
|    value_loss           | 4.63e+04 |
--------------------------------------
Eval num_timesteps=257000, episode_reward=743.24 +/- 637.61
Episode length: 34.66 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=811.56 +/- 652.16
Episode length: 35.02 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=810.06 +/- 673.27
Episode length: 34.52 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 869      |
| time/              |          |
|    fps             | 243      |
|    iterations      | 126      |
|    time_elapsed    | 1058     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=927.32 +/- 671.87
Episode length: 36.80 +/- 5.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 927       |
| time/                   |           |
|    total_timesteps      | 258500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.35e-22 |
|    explained_variance   | 0.00757   |
|    learning_rate        | 0.001     |
|    loss                 | 1.76e+04  |
|    n_updates            | 570       |
|    policy_gradient_loss | -3.09e-10 |
|    value_loss           | 2.49e+04  |
---------------------------------------
Eval num_timesteps=259000, episode_reward=966.22 +/- 710.46
Episode length: 36.72 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 966      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=756.47 +/- 628.23
Episode length: 35.04 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=1086.39 +/- 735.96
Episode length: 37.10 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 876      |
| time/              |          |
|    fps             | 243      |
|    iterations      | 127      |
|    time_elapsed    | 1066     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=949.08 +/- 719.04
Episode length: 36.04 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 949       |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.26e-18 |
|    explained_variance   | 0.00759   |
|    learning_rate        | 0.001     |
|    loss                 | 2.91e+04  |
|    n_updates            | 580       |
|    policy_gradient_loss | 5.85e-10  |
|    value_loss           | 4.51e+04  |
---------------------------------------
Eval num_timesteps=261000, episode_reward=806.84 +/- 647.95
Episode length: 35.32 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=750.30 +/- 632.23
Episode length: 35.00 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=684.40 +/- 629.90
Episode length: 34.08 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 682      |
| time/              |          |
|    fps             | 244      |
|    iterations      | 128      |
|    time_elapsed    | 1073     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=699.76 +/- 568.78
Episode length: 35.16 +/- 6.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 700       |
| time/                   |           |
|    total_timesteps      | 262500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.69e-21 |
|    explained_variance   | 0.00116   |
|    learning_rate        | 0.001     |
|    loss                 | 1.26e+04  |
|    n_updates            | 590       |
|    policy_gradient_loss | 5.3e-10   |
|    value_loss           | 1.83e+04  |
---------------------------------------
Eval num_timesteps=263000, episode_reward=869.31 +/- 651.89
Episode length: 36.66 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=867.01 +/- 707.66
Episode length: 35.02 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=834.06 +/- 668.91
Episode length: 35.08 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 244      |
|    iterations      | 129      |
|    time_elapsed    | 1080     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=865.14 +/- 727.29
Episode length: 35.08 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 865       |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.07e-17 |
|    explained_variance   | 0.0149    |
|    learning_rate        | 0.001     |
|    loss                 | 2.19e+04  |
|    n_updates            | 600       |
|    policy_gradient_loss | 2.36e-09  |
|    value_loss           | 3.85e+04  |
---------------------------------------
Eval num_timesteps=265000, episode_reward=763.06 +/- 716.39
Episode length: 34.02 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=705.32 +/- 631.61
Episode length: 33.44 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=764.34 +/- 677.55
Episode length: 34.04 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 901      |
| time/              |          |
|    fps             | 244      |
|    iterations      | 130      |
|    time_elapsed    | 1088     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=658.49 +/- 533.83
Episode length: 34.30 +/- 5.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 658       |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.15e-19 |
|    explained_variance   | 0.0086    |
|    learning_rate        | 0.001     |
|    loss                 | 1.91e+04  |
|    n_updates            | 610       |
|    policy_gradient_loss | -5.03e-10 |
|    value_loss           | 2.64e+04  |
---------------------------------------
Eval num_timesteps=267000, episode_reward=793.82 +/- 646.91
Episode length: 34.84 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=1014.18 +/- 745.57
Episode length: 36.72 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=932.98 +/- 696.70
Episode length: 36.44 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 244      |
|    iterations      | 131      |
|    time_elapsed    | 1095     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=870.24 +/- 646.67
Episode length: 35.94 +/- 5.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 870       |
| time/                   |           |
|    total_timesteps      | 268500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.01e-16 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.001     |
|    loss                 | 2.57e+04  |
|    n_updates            | 620       |
|    policy_gradient_loss | 2.91e-09  |
|    value_loss           | 4.22e+04  |
---------------------------------------
Eval num_timesteps=269000, episode_reward=1030.52 +/- 752.36
Episode length: 36.18 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=679.99 +/- 525.06
Episode length: 34.80 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 680      |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=849.63 +/- 696.92
Episode length: 35.54 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 245      |
|    iterations      | 132      |
|    time_elapsed    | 1102     |
|    total_timesteps | 270336   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.72
Eval num_timesteps=270500, episode_reward=-528.14 +/- 60.85
Episode length: 51.00 +/- 16.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51          |
|    mean_reward          | -528        |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.029204678 |
|    clip_fraction        | 0.00826     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00936    |
|    explained_variance   | 0.0051      |
|    learning_rate        | 0.001       |
|    loss                 | 1.34e+04    |
|    n_updates            | 624         |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 2.47e+04    |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=-522.14 +/- 63.00
Episode length: 50.24 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-522.75 +/- 74.98
Episode length: 50.58 +/- 18.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=-507.15 +/- 77.30
Episode length: 51.42 +/- 17.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 92.6     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 133      |
|    time_elapsed    | 1111     |
|    total_timesteps | 272384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.24
Eval num_timesteps=272500, episode_reward=-514.95 +/- 71.05
Episode length: 49.50 +/- 16.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | -515        |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.118711896 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.328      |
|    explained_variance   | -0.00213    |
|    learning_rate        | 0.001       |
|    loss                 | 6.47e+03    |
|    n_updates            | 625         |
|    policy_gradient_loss | 0.0421      |
|    value_loss           | 1.32e+04    |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=-501.14 +/- 76.23
Episode length: 46.70 +/- 14.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-525.11 +/- 71.06
Episode length: 51.88 +/- 19.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=-516.74 +/- 66.47
Episode length: 49.72 +/- 18.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.1     |
|    ep_rew_mean     | -411     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 134      |
|    time_elapsed    | 1120     |
|    total_timesteps | 274432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=274500, episode_reward=-512.54 +/- 76.92
Episode length: 50.84 +/- 15.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.8        |
|    mean_reward          | -513        |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.007386135 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0766     |
|    explained_variance   | -0.000761   |
|    learning_rate        | 0.001       |
|    loss                 | 3.18e+03    |
|    n_updates            | 626         |
|    policy_gradient_loss | 0.000609    |
|    value_loss           | 7.88e+03    |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=-517.93 +/- 69.97
Episode length: 52.06 +/- 18.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-516.15 +/- 86.45
Episode length: 49.74 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-513.16 +/- 88.30
Episode length: 53.36 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -471     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 135      |
|    time_elapsed    | 1129     |
|    total_timesteps | 276480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.18
Eval num_timesteps=276500, episode_reward=-515.54 +/- 77.18
Episode length: 54.02 +/- 21.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54          |
|    mean_reward          | -516        |
| time/                   |             |
|    total_timesteps      | 276500      |
| train/                  |             |
|    approx_kl            | 0.046079773 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.177      |
|    explained_variance   | 0.000612    |
|    learning_rate        | 0.001       |
|    loss                 | 2.71e+03    |
|    n_updates            | 627         |
|    policy_gradient_loss | 0.0069      |
|    value_loss           | 6.11e+03    |
-----------------------------------------
Eval num_timesteps=277000, episode_reward=-520.34 +/- 57.94
Episode length: 47.24 +/- 15.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-528.15 +/- 59.35
Episode length: 52.76 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-523.35 +/- 69.24
Episode length: 52.36 +/- 17.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=-534.15 +/- 68.00
Episode length: 47.30 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -507     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 136      |
|    time_elapsed    | 1140     |
|    total_timesteps | 278528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 1.02
Eval num_timesteps=279000, episode_reward=700.39 +/- 576.27
Episode length: 34.72 +/- 6.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.7       |
|    mean_reward          | 700        |
| time/                   |            |
|    total_timesteps      | 279000     |
| train/                  |            |
|    approx_kl            | 0.51134795 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.593     |
|    explained_variance   | -8e-05     |
|    learning_rate        | 0.001      |
|    loss                 | 1.31e+03   |
|    n_updates            | 628        |
|    policy_gradient_loss | 0.197      |
|    value_loss           | 4.27e+03   |
----------------------------------------
Eval num_timesteps=279500, episode_reward=883.65 +/- 706.86
Episode length: 35.82 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=821.92 +/- 663.11
Episode length: 35.18 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=819.94 +/- 661.00
Episode length: 35.84 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.4     |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 137      |
|    time_elapsed    | 1146     |
|    total_timesteps | 280576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=281000, episode_reward=544.38 +/- 537.66
Episode length: 34.24 +/- 6.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.2        |
|    mean_reward          | 544         |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.033675633 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.935      |
|    explained_variance   | -0.000267   |
|    learning_rate        | 0.001       |
|    loss                 | 3.78e+03    |
|    n_updates            | 629         |
|    policy_gradient_loss | 0.0295      |
|    value_loss           | 7.39e+03    |
-----------------------------------------
Eval num_timesteps=281500, episode_reward=907.81 +/- 681.08
Episode length: 37.80 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=681.56 +/- 603.03
Episode length: 35.72 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=743.60 +/- 687.76
Episode length: 35.58 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32       |
|    ep_rew_mean     | -10.9    |
| time/              |          |
|    fps             | 244      |
|    iterations      | 138      |
|    time_elapsed    | 1153     |
|    total_timesteps | 282624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.44
Eval num_timesteps=283000, episode_reward=745.27 +/- 657.02
Episode length: 34.46 +/- 6.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.5       |
|    mean_reward          | 745        |
| time/                   |            |
|    total_timesteps      | 283000     |
| train/                  |            |
|    approx_kl            | 0.21908256 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | -0.000171  |
|    learning_rate        | 0.001      |
|    loss                 | 3.2e+03    |
|    n_updates            | 630        |
|    policy_gradient_loss | 0.0208     |
|    value_loss           | 7.79e+03   |
----------------------------------------
Eval num_timesteps=283500, episode_reward=911.79 +/- 732.25
Episode length: 35.16 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=833.81 +/- 674.52
Episode length: 35.58 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=782.98 +/- 722.17
Episode length: 34.24 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.6     |
|    ep_rew_mean     | 251      |
| time/              |          |
|    fps             | 245      |
|    iterations      | 139      |
|    time_elapsed    | 1160     |
|    total_timesteps | 284672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=285000, episode_reward=1007.66 +/- 723.04
Episode length: 36.92 +/- 6.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.9        |
|    mean_reward          | 1.01e+03    |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.017614633 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.573      |
|    explained_variance   | 3.2e-05     |
|    learning_rate        | 0.001       |
|    loss                 | 5.08e+03    |
|    n_updates            | 631         |
|    policy_gradient_loss | 0.0372      |
|    value_loss           | 9.61e+03    |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=736.83 +/- 688.63
Episode length: 34.30 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=903.76 +/- 687.00
Episode length: 36.48 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=835.23 +/- 710.65
Episode length: 35.18 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.3     |
|    ep_rew_mean     | 294      |
| time/              |          |
|    fps             | 245      |
|    iterations      | 140      |
|    time_elapsed    | 1166     |
|    total_timesteps | 286720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.24
Eval num_timesteps=287000, episode_reward=814.97 +/- 680.40
Episode length: 35.42 +/- 6.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.4       |
|    mean_reward          | 815        |
| time/                   |            |
|    total_timesteps      | 287000     |
| train/                  |            |
|    approx_kl            | 0.11788614 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | -0.000133  |
|    learning_rate        | 0.001      |
|    loss                 | 4.57e+03   |
|    n_updates            | 632        |
|    policy_gradient_loss | 0.13       |
|    value_loss           | 8.13e+03   |
----------------------------------------
Eval num_timesteps=287500, episode_reward=741.50 +/- 636.51
Episode length: 34.80 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=688.89 +/- 567.61
Episode length: 35.20 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=746.21 +/- 674.06
Episode length: 34.64 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33       |
|    ep_rew_mean     | -9.16    |
| time/              |          |
|    fps             | 246      |
|    iterations      | 141      |
|    time_elapsed    | 1173     |
|    total_timesteps | 288768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.16
Eval num_timesteps=289000, episode_reward=818.72 +/- 635.55
Episode length: 35.46 +/- 5.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | 819         |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.058351636 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | -1.91e-06   |
|    learning_rate        | 0.001       |
|    loss                 | 2.37e+03    |
|    n_updates            | 633         |
|    policy_gradient_loss | 0.0277      |
|    value_loss           | 5.18e+03    |
-----------------------------------------
Eval num_timesteps=289500, episode_reward=768.49 +/- 678.28
Episode length: 34.28 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=719.56 +/- 595.27
Episode length: 35.18 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=801.35 +/- 649.72
Episode length: 35.30 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.5     |
|    ep_rew_mean     | -16.1    |
| time/              |          |
|    fps             | 246      |
|    iterations      | 142      |
|    time_elapsed    | 1180     |
|    total_timesteps | 290816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.38
Eval num_timesteps=291000, episode_reward=743.02 +/- 664.50
Episode length: 34.20 +/- 6.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.2       |
|    mean_reward          | 743        |
| time/                   |            |
|    total_timesteps      | 291000     |
| train/                  |            |
|    approx_kl            | 0.18778384 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.601     |
|    explained_variance   | -1.44e-05  |
|    learning_rate        | 0.001      |
|    loss                 | 4.65e+03   |
|    n_updates            | 634        |
|    policy_gradient_loss | -0.0266    |
|    value_loss           | 8.5e+03    |
----------------------------------------
Eval num_timesteps=291500, episode_reward=798.17 +/- 650.33
Episode length: 35.26 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=681.75 +/- 591.79
Episode length: 33.92 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=878.22 +/- 704.64
Episode length: 35.04 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 431      |
| time/              |          |
|    fps             | 246      |
|    iterations      | 143      |
|    time_elapsed    | 1186     |
|    total_timesteps | 292864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=293000, episode_reward=767.05 +/- 640.28
Episode length: 34.92 +/- 6.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.9        |
|    mean_reward          | 767         |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.008018028 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 7.22e-05    |
|    learning_rate        | 0.001       |
|    loss                 | 1.6e+04     |
|    n_updates            | 635         |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 3.21e+04    |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=780.80 +/- 672.91
Episode length: 34.50 +/- 7.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=765.44 +/- 645.86
Episode length: 34.68 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=715.25 +/- 657.42
Episode length: 33.80 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 711      |
| time/              |          |
|    fps             | 247      |
|    iterations      | 144      |
|    time_elapsed    | 1193     |
|    total_timesteps | 294912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=295000, episode_reward=873.56 +/- 676.18
Episode length: 36.60 +/- 5.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.6        |
|    mean_reward          | 874         |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.013580582 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 3.99e-05    |
|    learning_rate        | 0.001       |
|    loss                 | 8.43e+03    |
|    n_updates            | 636         |
|    policy_gradient_loss | -0.000288   |
|    value_loss           | 2.43e+04    |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=784.23 +/- 634.87
Episode length: 35.94 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=844.46 +/- 669.81
Episode length: 35.74 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=789.91 +/- 630.25
Episode length: 35.18 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 614      |
| time/              |          |
|    fps             | 247      |
|    iterations      | 145      |
|    time_elapsed    | 1200     |
|    total_timesteps | 296960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=297000, episode_reward=964.09 +/- 687.82
Episode length: 36.66 +/- 5.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.7        |
|    mean_reward          | 964         |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.021138163 |
|    clip_fraction        | 0.0573      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.345      |
|    explained_variance   | -1.19e-05   |
|    learning_rate        | 0.001       |
|    loss                 | 4.79e+03    |
|    n_updates            | 637         |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 1.84e+04    |
-----------------------------------------
Eval num_timesteps=297500, episode_reward=879.21 +/- 719.09
Episode length: 35.06 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=628.47 +/- 497.31
Episode length: 34.04 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 628      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=791.74 +/- 667.85
Episode length: 35.24 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=1049.49 +/- 755.31
Episode length: 36.58 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 542      |
| time/              |          |
|    fps             | 247      |
|    iterations      | 146      |
|    time_elapsed    | 1208     |
|    total_timesteps | 299008   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=299500, episode_reward=909.87 +/- 680.62
Episode length: 36.64 +/- 5.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 36.6       |
|    mean_reward          | 910        |
| time/                   |            |
|    total_timesteps      | 299500     |
| train/                  |            |
|    approx_kl            | 0.01574683 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.122     |
|    explained_variance   | 7.16e-05   |
|    learning_rate        | 0.001      |
|    loss                 | 1.24e+04   |
|    n_updates            | 638        |
|    policy_gradient_loss | -0.00033   |
|    value_loss           | 2.11e+04   |
----------------------------------------
Eval num_timesteps=300000, episode_reward=817.36 +/- 689.53
Episode length: 35.02 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=746.61 +/- 637.46
Episode length: 34.78 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=792.15 +/- 682.37
Episode length: 34.28 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 703      |
| time/              |          |
|    fps             | 247      |
|    iterations      | 147      |
|    time_elapsed    | 1214     |
|    total_timesteps | 301056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=301500, episode_reward=770.30 +/- 649.31
Episode length: 34.64 +/- 6.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.6        |
|    mean_reward          | 770         |
| time/                   |             |
|    total_timesteps      | 301500      |
| train/                  |             |
|    approx_kl            | 0.040229652 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0294     |
|    explained_variance   | 0.000194    |
|    learning_rate        | 0.001       |
|    loss                 | 3.97e+04    |
|    n_updates            | 639         |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 6.02e+04    |
-----------------------------------------
Eval num_timesteps=302000, episode_reward=733.25 +/- 628.97
Episode length: 34.66 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=841.01 +/- 686.13
Episode length: 35.82 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=837.09 +/- 682.73
Episode length: 35.18 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 812      |
| time/              |          |
|    fps             | 248      |
|    iterations      | 148      |
|    time_elapsed    | 1221     |
|    total_timesteps | 303104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.54
Eval num_timesteps=303500, episode_reward=907.74 +/- 728.27
Episode length: 35.24 +/- 7.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.2        |
|    mean_reward          | 908         |
| time/                   |             |
|    total_timesteps      | 303500      |
| train/                  |             |
|    approx_kl            | 0.077628024 |
|    clip_fraction        | 0.00223     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.000769   |
|    explained_variance   | 0.000345    |
|    learning_rate        | 0.001       |
|    loss                 | 1.78e+04    |
|    n_updates            | 640         |
|    policy_gradient_loss | 0.000455    |
|    value_loss           | 2.84e+04    |
-----------------------------------------
Eval num_timesteps=304000, episode_reward=777.96 +/- 695.01
Episode length: 34.18 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=988.68 +/- 733.61
Episode length: 36.34 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=779.43 +/- 691.29
Episode length: 34.38 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 889      |
| time/              |          |
|    fps             | 248      |
|    iterations      | 149      |
|    time_elapsed    | 1227     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=863.42 +/- 685.05
Episode length: 35.60 +/- 6.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 863       |
| time/                   |           |
|    total_timesteps      | 305500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.77e-20 |
|    explained_variance   | 0.00221   |
|    learning_rate        | 0.001     |
|    loss                 | 1.82e+04  |
|    n_updates            | 650       |
|    policy_gradient_loss | -4.66e-11 |
|    value_loss           | 3.75e+04  |
---------------------------------------
Eval num_timesteps=306000, episode_reward=872.78 +/- 713.85
Episode length: 35.54 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=718.45 +/- 666.57
Episode length: 33.48 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=881.48 +/- 732.74
Episode length: 35.34 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 902      |
| time/              |          |
|    fps             | 248      |
|    iterations      | 150      |
|    time_elapsed    | 1235     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=745.36 +/- 630.86
Episode length: 34.34 +/- 6.59
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.3     |
|    mean_reward          | 745      |
| time/                   |          |
|    total_timesteps      | 307500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0094   |
|    learning_rate        | 0.001    |
|    loss                 | 4.08e+04 |
|    n_updates            | 660      |
|    policy_gradient_loss | 1.78e-09 |
|    value_loss           | 7.29e+04 |
--------------------------------------
Eval num_timesteps=308000, episode_reward=940.41 +/- 682.31
Episode length: 36.58 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 940      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=863.82 +/- 715.60
Episode length: 34.96 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=699.71 +/- 664.91
Episode length: 33.82 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 938      |
| time/              |          |
|    fps             | 248      |
|    iterations      | 151      |
|    time_elapsed    | 1242     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=773.03 +/- 651.38
Episode length: 34.90 +/- 6.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 773       |
| time/                   |           |
|    total_timesteps      | 309500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00818   |
|    learning_rate        | 0.001     |
|    loss                 | 9.14e+03  |
|    n_updates            | 670       |
|    policy_gradient_loss | -1.49e-09 |
|    value_loss           | 2.66e+04  |
---------------------------------------
Eval num_timesteps=310000, episode_reward=718.22 +/- 660.23
Episode length: 33.50 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=836.74 +/- 682.42
Episode length: 35.62 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=816.14 +/- 640.24
Episode length: 35.50 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 896      |
| time/              |          |
|    fps             | 249      |
|    iterations      | 152      |
|    time_elapsed    | 1249     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=707.17 +/- 597.26
Episode length: 34.34 +/- 5.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 707       |
| time/                   |           |
|    total_timesteps      | 311500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.01e-47 |
|    explained_variance   | 0.0136    |
|    learning_rate        | 0.001     |
|    loss                 | 2.28e+04  |
|    n_updates            | 680       |
|    policy_gradient_loss | -1.39e-09 |
|    value_loss           | 5.15e+04  |
---------------------------------------
Eval num_timesteps=312000, episode_reward=691.62 +/- 626.17
Episode length: 33.84 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=850.17 +/- 715.97
Episode length: 34.96 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=760.40 +/- 615.06
Episode length: 35.02 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 858      |
| time/              |          |
|    fps             | 249      |
|    iterations      | 153      |
|    time_elapsed    | 1257     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=783.14 +/- 701.73
Episode length: 34.16 +/- 7.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 783       |
| time/                   |           |
|    total_timesteps      | 313500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00534   |
|    learning_rate        | 0.001     |
|    loss                 | 1.07e+04  |
|    n_updates            | 690       |
|    policy_gradient_loss | -3.48e-10 |
|    value_loss           | 2.34e+04  |
---------------------------------------
Eval num_timesteps=314000, episode_reward=979.78 +/- 745.86
Episode length: 36.20 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=829.44 +/- 646.34
Episode length: 35.54 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=873.88 +/- 698.57
Episode length: 36.20 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 249      |
|    iterations      | 154      |
|    time_elapsed    | 1264     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=727.17 +/- 685.09
Episode length: 33.82 +/- 7.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 727       |
| time/                   |           |
|    total_timesteps      | 315500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.98e-43 |
|    explained_variance   | 0.0115    |
|    learning_rate        | 0.001     |
|    loss                 | 2.79e+04  |
|    n_updates            | 700       |
|    policy_gradient_loss | -2.33e-10 |
|    value_loss           | 4.86e+04  |
---------------------------------------
Eval num_timesteps=316000, episode_reward=887.59 +/- 692.47
Episode length: 36.34 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=886.20 +/- 728.53
Episode length: 34.76 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=878.68 +/- 691.31
Episode length: 35.76 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 249      |
|    iterations      | 155      |
|    time_elapsed    | 1271     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=853.76 +/- 694.15
Episode length: 35.44 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 854       |
| time/                   |           |
|    total_timesteps      | 317500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.57e-45 |
|    explained_variance   | 0.00638   |
|    learning_rate        | 0.001     |
|    loss                 | 6.98e+03  |
|    n_updates            | 710       |
|    policy_gradient_loss | 9.9e-11   |
|    value_loss           | 2.43e+04  |
---------------------------------------
Eval num_timesteps=318000, episode_reward=765.01 +/- 661.97
Episode length: 34.38 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=822.40 +/- 661.29
Episode length: 35.26 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=765.26 +/- 614.42
Episode length: 35.32 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 737      |
| time/              |          |
|    fps             | 249      |
|    iterations      | 156      |
|    time_elapsed    | 1279     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=839.16 +/- 652.09
Episode length: 35.80 +/- 5.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 319500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.88e-41 |
|    explained_variance   | 0.00748   |
|    learning_rate        | 0.001     |
|    loss                 | 2e+04     |
|    n_updates            | 720       |
|    policy_gradient_loss | 3.74e-09  |
|    value_loss           | 4.46e+04  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=644.02 +/- 541.24
Episode length: 33.92 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 644      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=951.56 +/- 706.51
Episode length: 36.44 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=828.61 +/- 685.38
Episode length: 35.64 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=859.80 +/- 706.13
Episode length: 35.08 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 789      |
| time/              |          |
|    fps             | 249      |
|    iterations      | 157      |
|    time_elapsed    | 1287     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=824.65 +/- 678.00
Episode length: 35.34 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 825       |
| time/                   |           |
|    total_timesteps      | 322000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.92e-43 |
|    explained_variance   | 0.00908   |
|    learning_rate        | 0.001     |
|    loss                 | 2.53e+04  |
|    n_updates            | 730       |
|    policy_gradient_loss | 4e-10     |
|    value_loss           | 2.61e+04  |
---------------------------------------
Eval num_timesteps=322500, episode_reward=725.45 +/- 628.33
Episode length: 34.16 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=725.07 +/- 590.16
Episode length: 34.44 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=782.24 +/- 691.48
Episode length: 34.82 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 862      |
| time/              |          |
|    fps             | 249      |
|    iterations      | 158      |
|    time_elapsed    | 1294     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=716.26 +/- 634.95
Episode length: 34.42 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 716       |
| time/                   |           |
|    total_timesteps      | 324000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.55e-39 |
|    explained_variance   | 0.0132    |
|    learning_rate        | 0.001     |
|    loss                 | 2.05e+04  |
|    n_updates            | 740       |
|    policy_gradient_loss | -8.56e-10 |
|    value_loss           | 4.28e+04  |
---------------------------------------
Eval num_timesteps=324500, episode_reward=858.51 +/- 688.66
Episode length: 35.06 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=915.50 +/- 693.05
Episode length: 36.56 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=931.13 +/- 720.28
Episode length: 36.22 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 877      |
| time/              |          |
|    fps             | 250      |
|    iterations      | 159      |
|    time_elapsed    | 1302     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=691.50 +/- 640.04
Episode length: 33.96 +/- 6.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 692       |
| time/                   |           |
|    total_timesteps      | 326000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1e-41    |
|    explained_variance   | 0.00571   |
|    learning_rate        | 0.001     |
|    loss                 | 1.15e+04  |
|    n_updates            | 750       |
|    policy_gradient_loss | -1.44e-10 |
|    value_loss           | 2.82e+04  |
---------------------------------------
Eval num_timesteps=326500, episode_reward=707.57 +/- 677.48
Episode length: 33.00 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=727.09 +/- 650.67
Episode length: 34.28 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=881.42 +/- 714.74
Episode length: 35.44 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 835      |
| time/              |          |
|    fps             | 250      |
|    iterations      | 160      |
|    time_elapsed    | 1309     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=801.61 +/- 637.32
Episode length: 35.66 +/- 5.68
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.7     |
|    mean_reward          | 802      |
| time/                   |          |
|    total_timesteps      | 328000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -8.6e-38 |
|    explained_variance   | 0.0101   |
|    learning_rate        | 0.001    |
|    loss                 | 2.02e+04 |
|    n_updates            | 760      |
|    policy_gradient_loss | 4.48e-09 |
|    value_loss           | 4.31e+04 |
--------------------------------------
Eval num_timesteps=328500, episode_reward=997.74 +/- 685.51
Episode length: 37.50 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=852.25 +/- 712.61
Episode length: 34.96 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=841.12 +/- 671.89
Episode length: 35.76 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 826      |
| time/              |          |
|    fps             | 250      |
|    iterations      | 161      |
|    time_elapsed    | 1316     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=849.79 +/- 635.17
Episode length: 36.12 +/- 5.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 850       |
| time/                   |           |
|    total_timesteps      | 330000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.78e-40 |
|    explained_variance   | 0.00639   |
|    learning_rate        | 0.001     |
|    loss                 | 1.89e+04  |
|    n_updates            | 770       |
|    policy_gradient_loss | 3.49e-11  |
|    value_loss           | 2.62e+04  |
---------------------------------------
Eval num_timesteps=330500, episode_reward=873.56 +/- 721.98
Episode length: 34.70 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=806.68 +/- 698.83
Episode length: 34.70 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=835.73 +/- 648.96
Episode length: 35.40 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 747      |
| time/              |          |
|    fps             | 250      |
|    iterations      | 162      |
|    time_elapsed    | 1324     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=939.02 +/- 737.54
Episode length: 35.18 +/- 6.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 939       |
| time/                   |           |
|    total_timesteps      | 332000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.62e-36 |
|    explained_variance   | 0.00173   |
|    learning_rate        | 0.001     |
|    loss                 | 2.13e+04  |
|    n_updates            | 780       |
|    policy_gradient_loss | -2.47e-11 |
|    value_loss           | 3.88e+04  |
---------------------------------------
Eval num_timesteps=332500, episode_reward=754.13 +/- 657.03
Episode length: 34.20 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=795.67 +/- 715.90
Episode length: 34.16 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=757.88 +/- 669.29
Episode length: 34.16 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 770      |
| time/              |          |
|    fps             | 250      |
|    iterations      | 163      |
|    time_elapsed    | 1331     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=710.97 +/- 638.30
Episode length: 34.26 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 711       |
| time/                   |           |
|    total_timesteps      | 334000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.61e-13 |
|    explained_variance   | 0.00581   |
|    learning_rate        | 0.001     |
|    loss                 | 1e+04     |
|    n_updates            | 790       |
|    policy_gradient_loss | 4.95e-11  |
|    value_loss           | 2.41e+04  |
---------------------------------------
Eval num_timesteps=334500, episode_reward=788.02 +/- 694.24
Episode length: 34.86 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=725.54 +/- 613.94
Episode length: 34.96 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=1054.14 +/- 753.68
Episode length: 36.82 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 250      |
|    iterations      | 164      |
|    time_elapsed    | 1338     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=877.70 +/- 725.99
Episode length: 35.22 +/- 7.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 878       |
| time/                   |           |
|    total_timesteps      | 336000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.19e-16 |
|    explained_variance   | 0.00257   |
|    learning_rate        | 0.001     |
|    loss                 | 1.5e+04   |
|    n_updates            | 800       |
|    policy_gradient_loss | -1.28e-10 |
|    value_loss           | 3e+04     |
---------------------------------------
Eval num_timesteps=336500, episode_reward=841.35 +/- 674.33
Episode length: 35.42 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=743.85 +/- 640.61
Episode length: 34.28 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=938.39 +/- 683.57
Episode length: 36.68 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 825      |
| time/              |          |
|    fps             | 251      |
|    iterations      | 165      |
|    time_elapsed    | 1346     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=801.19 +/- 725.95
Episode length: 33.94 +/- 7.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 801       |
| time/                   |           |
|    total_timesteps      | 338000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.51e-38 |
|    explained_variance   | 0.00643   |
|    learning_rate        | 0.001     |
|    loss                 | 5.3e+03   |
|    n_updates            | 810       |
|    policy_gradient_loss | 4.39e-10  |
|    value_loss           | 2.54e+04  |
---------------------------------------
Eval num_timesteps=338500, episode_reward=835.41 +/- 720.05
Episode length: 34.60 +/- 7.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=855.46 +/- 648.55
Episode length: 35.92 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=737.04 +/- 599.20
Episode length: 35.18 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 251      |
|    iterations      | 166      |
|    time_elapsed    | 1353     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=724.16 +/- 579.46
Episode length: 35.70 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 724       |
| time/                   |           |
|    total_timesteps      | 340000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.93e-34 |
|    explained_variance   | 0.0196    |
|    learning_rate        | 0.001     |
|    loss                 | 2.01e+04  |
|    n_updates            | 820       |
|    policy_gradient_loss | -2.65e-10 |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=340500, episode_reward=855.60 +/- 639.34
Episode length: 36.00 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=700.81 +/- 606.72
Episode length: 34.44 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=847.21 +/- 694.08
Episode length: 35.14 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=791.31 +/- 619.84
Episode length: 35.46 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 835      |
| time/              |          |
|    fps             | 251      |
|    iterations      | 167      |
|    time_elapsed    | 1361     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=762.64 +/- 592.83
Episode length: 35.26 +/- 6.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 763       |
| time/                   |           |
|    total_timesteps      | 342500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.89e-37 |
|    explained_variance   | 0.00617   |
|    learning_rate        | 0.001     |
|    loss                 | 1.13e+04  |
|    n_updates            | 830       |
|    policy_gradient_loss | -1.42e-09 |
|    value_loss           | 2.45e+04  |
---------------------------------------
Eval num_timesteps=343000, episode_reward=942.00 +/- 717.09
Episode length: 36.48 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=751.34 +/- 578.91
Episode length: 35.48 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=688.67 +/- 612.40
Episode length: 34.48 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 251      |
|    iterations      | 168      |
|    time_elapsed    | 1368     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=812.18 +/- 711.10
Episode length: 34.58 +/- 7.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 344500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.79e-33 |
|    explained_variance   | 0.0117    |
|    learning_rate        | 0.001     |
|    loss                 | 1.32e+04  |
|    n_updates            | 840       |
|    policy_gradient_loss | 2.31e-09  |
|    value_loss           | 3.14e+04  |
---------------------------------------
Eval num_timesteps=345000, episode_reward=940.31 +/- 705.31
Episode length: 36.20 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 940      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=875.32 +/- 683.18
Episode length: 36.08 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=773.23 +/- 707.40
Episode length: 33.86 +/- 7.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 251      |
|    iterations      | 169      |
|    time_elapsed    | 1376     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=1002.57 +/- 728.39
Episode length: 36.92 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 346500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.17e-36 |
|    explained_variance   | 0.00692   |
|    learning_rate        | 0.001     |
|    loss                 | 5.28e+03  |
|    n_updates            | 850       |
|    policy_gradient_loss | 1.52e-09  |
|    value_loss           | 2.01e+04  |
---------------------------------------
Eval num_timesteps=347000, episode_reward=896.62 +/- 754.90
Episode length: 34.58 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=760.18 +/- 693.57
Episode length: 33.74 +/- 8.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=759.41 +/- 726.54
Episode length: 33.24 +/- 8.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 251      |
|    iterations      | 170      |
|    time_elapsed    | 1383     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=672.15 +/- 579.36
Episode length: 34.68 +/- 5.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 672       |
| time/                   |           |
|    total_timesteps      | 348500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.16e-32 |
|    explained_variance   | 0.0129    |
|    learning_rate        | 0.001     |
|    loss                 | 1.46e+04  |
|    n_updates            | 860       |
|    policy_gradient_loss | -3.61e-10 |
|    value_loss           | 2.9e+04   |
---------------------------------------
Eval num_timesteps=349000, episode_reward=810.34 +/- 682.91
Episode length: 35.02 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=748.99 +/- 616.56
Episode length: 34.84 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=793.84 +/- 695.16
Episode length: 34.72 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 251      |
|    iterations      | 171      |
|    time_elapsed    | 1390     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=889.53 +/- 727.64
Episode length: 35.22 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 890       |
| time/                   |           |
|    total_timesteps      | 350500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.2e-35  |
|    explained_variance   | 0.00457   |
|    learning_rate        | 0.001     |
|    loss                 | 7.78e+03  |
|    n_updates            | 870       |
|    policy_gradient_loss | -6.08e-10 |
|    value_loss           | 2.24e+04  |
---------------------------------------
Eval num_timesteps=351000, episode_reward=915.52 +/- 719.42
Episode length: 35.66 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=882.02 +/- 652.57
Episode length: 36.38 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=768.61 +/- 696.67
Episode length: 33.82 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 713      |
| time/              |          |
|    fps             | 251      |
|    iterations      | 172      |
|    time_elapsed    | 1398     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=721.77 +/- 580.67
Episode length: 34.68 +/- 5.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 722       |
| time/                   |           |
|    total_timesteps      | 352500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.53e-32 |
|    explained_variance   | 0.008     |
|    learning_rate        | 0.001     |
|    loss                 | 1.46e+04  |
|    n_updates            | 880       |
|    policy_gradient_loss | 5.62e-10  |
|    value_loss           | 3.06e+04  |
---------------------------------------
Eval num_timesteps=353000, episode_reward=873.04 +/- 752.96
Episode length: 34.52 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=796.21 +/- 653.42
Episode length: 35.26 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=812.47 +/- 642.26
Episode length: 35.92 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 173      |
|    time_elapsed    | 1405     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=764.57 +/- 597.14
Episode length: 35.30 +/- 6.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 765       |
| time/                   |           |
|    total_timesteps      | 354500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.26e-34 |
|    explained_variance   | 0.00759   |
|    learning_rate        | 0.001     |
|    loss                 | 1.36e+04  |
|    n_updates            | 890       |
|    policy_gradient_loss | 1.42e-09  |
|    value_loss           | 2.28e+04  |
---------------------------------------
Eval num_timesteps=355000, episode_reward=814.49 +/- 698.86
Episode length: 35.00 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=1049.09 +/- 718.68
Episode length: 37.22 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=727.12 +/- 691.13
Episode length: 33.52 +/- 8.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 765      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 174      |
|    time_elapsed    | 1412     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=834.64 +/- 615.65
Episode length: 36.64 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 835       |
| time/                   |           |
|    total_timesteps      | 356500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.73e-31 |
|    explained_variance   | 0.00683   |
|    learning_rate        | 0.001     |
|    loss                 | 1.62e+04  |
|    n_updates            | 900       |
|    policy_gradient_loss | -1.43e-09 |
|    value_loss           | 3.05e+04  |
---------------------------------------
Eval num_timesteps=357000, episode_reward=788.12 +/- 604.36
Episode length: 36.38 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=829.81 +/- 674.06
Episode length: 35.34 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=761.44 +/- 666.65
Episode length: 34.76 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 175      |
|    time_elapsed    | 1420     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=841.22 +/- 640.48
Episode length: 36.32 +/- 5.89
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.3     |
|    mean_reward          | 841      |
| time/                   |          |
|    total_timesteps      | 358500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -8.7e-34 |
|    explained_variance   | 0.00512  |
|    learning_rate        | 0.001    |
|    loss                 | 1.29e+04 |
|    n_updates            | 910      |
|    policy_gradient_loss | 1.56e-10 |
|    value_loss           | 2.49e+04 |
--------------------------------------
Eval num_timesteps=359000, episode_reward=986.30 +/- 755.63
Episode length: 35.88 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=765.17 +/- 628.11
Episode length: 34.92 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=756.20 +/- 647.31
Episode length: 34.60 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 812      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 176      |
|    time_elapsed    | 1428     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=792.09 +/- 693.28
Episode length: 34.74 +/- 7.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 792       |
| time/                   |           |
|    total_timesteps      | 360500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.46e-30 |
|    explained_variance   | 0.0149    |
|    learning_rate        | 0.001     |
|    loss                 | 1.38e+04  |
|    n_updates            | 920       |
|    policy_gradient_loss | 6.52e-10  |
|    value_loss           | 2.84e+04  |
---------------------------------------
Eval num_timesteps=361000, episode_reward=839.15 +/- 645.19
Episode length: 35.94 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=958.28 +/- 712.45
Episode length: 36.54 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=885.43 +/- 671.28
Episode length: 36.18 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 838      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 177      |
|    time_elapsed    | 1435     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=916.57 +/- 671.82
Episode length: 36.58 +/- 5.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 917       |
| time/                   |           |
|    total_timesteps      | 362500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-32 |
|    explained_variance   | 0.00726   |
|    learning_rate        | 0.001     |
|    loss                 | 4.75e+03  |
|    n_updates            | 930       |
|    policy_gradient_loss | 1.73e-10  |
|    value_loss           | 3.17e+04  |
---------------------------------------
Eval num_timesteps=363000, episode_reward=956.25 +/- 737.31
Episode length: 35.94 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=925.36 +/- 732.99
Episode length: 35.64 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=949.52 +/- 696.81
Episode length: 36.14 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=813.59 +/- 687.69
Episode length: 34.66 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 178      |
|    time_elapsed    | 1444     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=775.90 +/- 594.01
Episode length: 35.64 +/- 5.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 776       |
| time/                   |           |
|    total_timesteps      | 365000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.96e-30 |
|    explained_variance   | 0.00776   |
|    learning_rate        | 0.001     |
|    loss                 | 7.99e+03  |
|    n_updates            | 940       |
|    policy_gradient_loss | 8.12e-10  |
|    value_loss           | 2.59e+04  |
---------------------------------------
Eval num_timesteps=365500, episode_reward=840.69 +/- 633.45
Episode length: 36.04 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=823.12 +/- 625.79
Episode length: 35.94 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=841.30 +/- 650.85
Episode length: 35.78 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 691      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 179      |
|    time_elapsed    | 1452     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=870.97 +/- 695.44
Episode length: 35.22 +/- 6.81
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.2     |
|    mean_reward          | 871      |
| time/                   |          |
|    total_timesteps      | 367000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -8.7e-33 |
|    explained_variance   | 0.00554  |
|    learning_rate        | 0.001    |
|    loss                 | 1.78e+04 |
|    n_updates            | 950      |
|    policy_gradient_loss | 2.18e-09 |
|    value_loss           | 2.26e+04 |
--------------------------------------
Eval num_timesteps=367500, episode_reward=1209.23 +/- 719.77
Episode length: 38.54 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.5     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
New best mean reward!
Eval num_timesteps=368000, episode_reward=909.78 +/- 710.21
Episode length: 36.16 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=1038.68 +/- 728.04
Episode length: 36.88 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 180      |
|    time_elapsed    | 1459     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=834.14 +/- 648.60
Episode length: 36.16 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 369000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.97e-29 |
|    explained_variance   | 0.00951   |
|    learning_rate        | 0.001     |
|    loss                 | 1.35e+04  |
|    n_updates            | 960       |
|    policy_gradient_loss | -1.79e-09 |
|    value_loss           | 2.96e+04  |
---------------------------------------
Eval num_timesteps=369500, episode_reward=887.54 +/- 691.44
Episode length: 35.38 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=898.83 +/- 735.19
Episode length: 34.98 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=728.27 +/- 633.62
Episode length: 34.36 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 181      |
|    time_elapsed    | 1467     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=798.06 +/- 630.69
Episode length: 35.70 +/- 5.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 798       |
| time/                   |           |
|    total_timesteps      | 371000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-31 |
|    explained_variance   | 0.0054    |
|    learning_rate        | 0.001     |
|    loss                 | 2.2e+04   |
|    n_updates            | 970       |
|    policy_gradient_loss | 8.5e-10   |
|    value_loss           | 2.68e+04  |
---------------------------------------
Eval num_timesteps=371500, episode_reward=1090.30 +/- 756.61
Episode length: 37.06 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=883.18 +/- 698.20
Episode length: 35.64 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=838.96 +/- 651.88
Episode length: 35.90 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 182      |
|    time_elapsed    | 1474     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=988.55 +/- 716.42
Episode length: 36.56 +/- 5.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 989       |
| time/                   |           |
|    total_timesteps      | 373000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-28 |
|    explained_variance   | 0.00951   |
|    learning_rate        | 0.001     |
|    loss                 | 1.22e+04  |
|    n_updates            | 980       |
|    policy_gradient_loss | -4.39e-10 |
|    value_loss           | 2.73e+04  |
---------------------------------------
Eval num_timesteps=373500, episode_reward=739.40 +/- 652.87
Episode length: 34.26 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=887.80 +/- 704.23
Episode length: 35.44 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=610.32 +/- 461.54
Episode length: 34.52 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 610      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 889      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 183      |
|    time_elapsed    | 1481     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=771.23 +/- 643.43
Episode length: 35.40 +/- 5.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 771       |
| time/                   |           |
|    total_timesteps      | 375000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.72e-31 |
|    explained_variance   | 0.00578   |
|    learning_rate        | 0.001     |
|    loss                 | 1.53e+04  |
|    n_updates            | 990       |
|    policy_gradient_loss | -1.13e-09 |
|    value_loss           | 2.87e+04  |
---------------------------------------
Eval num_timesteps=375500, episode_reward=1023.00 +/- 736.20
Episode length: 36.22 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=790.32 +/- 707.87
Episode length: 34.48 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=854.23 +/- 716.36
Episode length: 35.12 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 798      |
| time/              |          |
|    fps             | 253      |
|    iterations      | 184      |
|    time_elapsed    | 1489     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=773.45 +/- 678.11
Episode length: 34.70 +/- 7.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 773       |
| time/                   |           |
|    total_timesteps      | 377000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.48e-28 |
|    explained_variance   | 0.00671   |
|    learning_rate        | 0.001     |
|    loss                 | 1.1e+04   |
|    n_updates            | 1000      |
|    policy_gradient_loss | -6.05e-10 |
|    value_loss           | 2.48e+04  |
---------------------------------------
Eval num_timesteps=377500, episode_reward=1058.02 +/- 730.88
Episode length: 37.10 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=824.54 +/- 725.36
Episode length: 34.16 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=950.63 +/- 728.58
Episode length: 36.40 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 810      |
| time/              |          |
|    fps             | 253      |
|    iterations      | 185      |
|    time_elapsed    | 1496     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=945.22 +/- 690.61
Episode length: 36.76 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 945       |
| time/                   |           |
|    total_timesteps      | 379000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.19e-30 |
|    explained_variance   | 0.00569   |
|    learning_rate        | 0.001     |
|    loss                 | 1.26e+04  |
|    n_updates            | 1010      |
|    policy_gradient_loss | -1.42e-09 |
|    value_loss           | 2.85e+04  |
---------------------------------------
Eval num_timesteps=379500, episode_reward=936.27 +/- 742.34
Episode length: 35.68 +/- 7.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=802.07 +/- 640.48
Episode length: 35.28 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=857.97 +/- 712.28
Episode length: 35.38 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 855      |
| time/              |          |
|    fps             | 253      |
|    iterations      | 186      |
|    time_elapsed    | 1503     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=850.09 +/- 633.38
Episode length: 36.24 +/- 5.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 850       |
| time/                   |           |
|    total_timesteps      | 381000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.00812   |
|    learning_rate        | 0.001     |
|    loss                 | 1.39e+04  |
|    n_updates            | 1020      |
|    policy_gradient_loss | -1.37e-09 |
|    value_loss           | 2.61e+04  |
---------------------------------------
Eval num_timesteps=381500, episode_reward=831.62 +/- 647.46
Episode length: 36.18 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=898.91 +/- 699.91
Episode length: 36.20 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=888.73 +/- 685.22
Episode length: 36.56 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 253      |
|    iterations      | 187      |
|    time_elapsed    | 1511     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=961.20 +/- 744.61
Episode length: 35.88 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 961       |
| time/                   |           |
|    total_timesteps      | 383000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.88e-32 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.001     |
|    loss                 | 1.56e+04  |
|    n_updates            | 1030      |
|    policy_gradient_loss | 3.52e-10  |
|    value_loss           | 2.43e+04  |
---------------------------------------
Eval num_timesteps=383500, episode_reward=749.09 +/- 629.81
Episode length: 34.96 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=965.80 +/- 720.67
Episode length: 36.14 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 966      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=694.46 +/- 567.32
Episode length: 34.62 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=945.53 +/- 718.40
Episode length: 36.14 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 253      |
|    iterations      | 188      |
|    time_elapsed    | 1519     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=722.97 +/- 568.15
Episode length: 35.66 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 723       |
| time/                   |           |
|    total_timesteps      | 385500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-34 |
|    explained_variance   | 0.0103    |
|    learning_rate        | 0.001     |
|    loss                 | 1.03e+04  |
|    n_updates            | 1040      |
|    policy_gradient_loss | 1.31e-09  |
|    value_loss           | 2.65e+04  |
---------------------------------------
Eval num_timesteps=386000, episode_reward=821.99 +/- 616.69
Episode length: 36.16 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=788.34 +/- 686.66
Episode length: 34.34 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=816.08 +/- 642.84
Episode length: 35.66 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 810      |
| time/              |          |
|    fps             | 253      |
|    iterations      | 189      |
|    time_elapsed    | 1527     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=871.26 +/- 734.54
Episode length: 34.58 +/- 7.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 871       |
| time/                   |           |
|    total_timesteps      | 387500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18e-30 |
|    explained_variance   | 0.017     |
|    learning_rate        | 0.001     |
|    loss                 | 1.92e+04  |
|    n_updates            | 1050      |
|    policy_gradient_loss | 3e-10     |
|    value_loss           | 2.41e+04  |
---------------------------------------
Eval num_timesteps=388000, episode_reward=919.19 +/- 722.34
Episode length: 35.84 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=819.93 +/- 694.43
Episode length: 35.28 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=722.13 +/- 631.99
Episode length: 34.40 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 953      |
| time/              |          |
|    fps             | 253      |
|    iterations      | 190      |
|    time_elapsed    | 1534     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=949.43 +/- 747.13
Episode length: 35.88 +/- 6.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 949       |
| time/                   |           |
|    total_timesteps      | 389500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.84e-33 |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.001     |
|    loss                 | 1.63e+04  |
|    n_updates            | 1060      |
|    policy_gradient_loss | 4.06e-10  |
|    value_loss           | 3.22e+04  |
---------------------------------------
Eval num_timesteps=390000, episode_reward=903.94 +/- 661.07
Episode length: 37.06 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=795.89 +/- 628.65
Episode length: 35.12 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=940.73 +/- 739.82
Episode length: 35.82 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 880      |
| time/              |          |
|    fps             | 253      |
|    iterations      | 191      |
|    time_elapsed    | 1542     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=577.32 +/- 471.98
Episode length: 33.82 +/- 5.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 577       |
| time/                   |           |
|    total_timesteps      | 391500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.99e-29 |
|    explained_variance   | 0.0185    |
|    learning_rate        | 0.001     |
|    loss                 | 9.72e+03  |
|    n_updates            | 1070      |
|    policy_gradient_loss | -1.88e-10 |
|    value_loss           | 2.48e+04  |
---------------------------------------
Eval num_timesteps=392000, episode_reward=873.83 +/- 679.32
Episode length: 36.06 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=876.82 +/- 668.21
Episode length: 36.44 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=812.87 +/- 691.28
Episode length: 34.58 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 868      |
| time/              |          |
|    fps             | 253      |
|    iterations      | 192      |
|    time_elapsed    | 1549     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=987.54 +/- 684.03
Episode length: 37.48 +/- 5.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.5      |
|    mean_reward          | 988       |
| time/                   |           |
|    total_timesteps      | 393500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.14e-31 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.001     |
|    loss                 | 1.24e+04  |
|    n_updates            | 1080      |
|    policy_gradient_loss | -9.6e-10  |
|    value_loss           | 3.06e+04  |
---------------------------------------
Eval num_timesteps=394000, episode_reward=924.33 +/- 633.68
Episode length: 37.78 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 924      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=636.74 +/- 530.63
Episode length: 34.12 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=703.45 +/- 629.79
Episode length: 34.12 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 926      |
| time/              |          |
|    fps             | 253      |
|    iterations      | 193      |
|    time_elapsed    | 1556     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=894.17 +/- 768.74
Episode length: 34.92 +/- 7.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 894       |
| time/                   |           |
|    total_timesteps      | 395500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.74e-28 |
|    explained_variance   | 0.0212    |
|    learning_rate        | 0.001     |
|    loss                 | 1.33e+04  |
|    n_updates            | 1090      |
|    policy_gradient_loss | -1.41e-09 |
|    value_loss           | 3.04e+04  |
---------------------------------------
Eval num_timesteps=396000, episode_reward=889.71 +/- 713.29
Episode length: 35.04 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=794.44 +/- 653.79
Episode length: 35.16 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=898.64 +/- 722.11
Episode length: 35.28 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 254      |
|    iterations      | 194      |
|    time_elapsed    | 1564     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=868.15 +/- 678.01
Episode length: 35.68 +/- 6.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 868       |
| time/                   |           |
|    total_timesteps      | 397500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-30 |
|    explained_variance   | 0.00797   |
|    learning_rate        | 0.001     |
|    loss                 | 9.01e+03  |
|    n_updates            | 1100      |
|    policy_gradient_loss | 1.91e-09  |
|    value_loss           | 2.41e+04  |
---------------------------------------
Eval num_timesteps=398000, episode_reward=985.58 +/- 730.77
Episode length: 36.40 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=772.48 +/- 642.60
Episode length: 34.96 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=712.43 +/- 641.81
Episode length: 33.46 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 835      |
| time/              |          |
|    fps             | 254      |
|    iterations      | 195      |
|    time_elapsed    | 1571     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=823.71 +/- 688.94
Episode length: 35.00 +/- 7.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 824       |
| time/                   |           |
|    total_timesteps      | 399500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.75e-27 |
|    explained_variance   | 0.0212    |
|    learning_rate        | 0.001     |
|    loss                 | 2.13e+04  |
|    n_updates            | 1110      |
|    policy_gradient_loss | -6.55e-11 |
|    value_loss           | 2.74e+04  |
---------------------------------------
Eval num_timesteps=400000, episode_reward=720.22 +/- 626.74
Episode length: 34.44 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=659.19 +/- 535.92
Episode length: 34.34 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=813.68 +/- 662.22
Episode length: 35.12 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 254      |
|    iterations      | 196      |
|    time_elapsed    | 1578     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=897.25 +/- 701.11
Episode length: 35.92 +/- 6.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 897       |
| time/                   |           |
|    total_timesteps      | 401500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.75e-29 |
|    explained_variance   | 0.00712   |
|    learning_rate        | 0.001     |
|    loss                 | 4.26e+03  |
|    n_updates            | 1120      |
|    policy_gradient_loss | -1.46e-11 |
|    value_loss           | 1.95e+04  |
---------------------------------------
Eval num_timesteps=402000, episode_reward=746.36 +/- 690.75
Episode length: 33.80 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=938.24 +/- 648.62
Episode length: 37.34 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=761.41 +/- 678.97
Episode length: 34.48 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 254      |
|    iterations      | 197      |
|    time_elapsed    | 1585     |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=795.95 +/- 715.76
Episode length: 34.16 +/- 7.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 796       |
| time/                   |           |
|    total_timesteps      | 403500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.51e-26 |
|    explained_variance   | 0.0185    |
|    learning_rate        | 0.001     |
|    loss                 | 8.59e+03  |
|    n_updates            | 1130      |
|    policy_gradient_loss | 4.66e-10  |
|    value_loss           | 2.62e+04  |
---------------------------------------
Eval num_timesteps=404000, episode_reward=965.41 +/- 722.92
Episode length: 36.38 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=768.38 +/- 721.12
Episode length: 34.00 +/- 7.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=1031.97 +/- 788.37
Episode length: 36.02 +/- 8.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=807.53 +/- 711.57
Episode length: 34.70 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 254      |
|    iterations      | 198      |
|    time_elapsed    | 1594     |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=938.76 +/- 672.64
Episode length: 37.18 +/- 5.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.2      |
|    mean_reward          | 939       |
| time/                   |           |
|    total_timesteps      | 406000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.32e-28 |
|    explained_variance   | 0.00871   |
|    learning_rate        | 0.001     |
|    loss                 | 1.23e+04  |
|    n_updates            | 1140      |
|    policy_gradient_loss | -1.35e-09 |
|    value_loss           | 2.71e+04  |
---------------------------------------
Eval num_timesteps=406500, episode_reward=800.82 +/- 632.56
Episode length: 35.54 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=630.26 +/- 595.64
Episode length: 33.10 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 630      |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=918.65 +/- 675.85
Episode length: 36.90 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 851      |
| time/              |          |
|    fps             | 254      |
|    iterations      | 199      |
|    time_elapsed    | 1601     |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=808.95 +/- 703.10
Episode length: 35.26 +/- 7.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 809       |
| time/                   |           |
|    total_timesteps      | 408000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.05e-25 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.001     |
|    loss                 | 1.55e+04  |
|    n_updates            | 1150      |
|    policy_gradient_loss | -6.96e-10 |
|    value_loss           | 2.55e+04  |
---------------------------------------
Eval num_timesteps=408500, episode_reward=716.86 +/- 610.54
Episode length: 33.80 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=943.54 +/- 744.65
Episode length: 35.38 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=682.14 +/- 582.44
Episode length: 34.48 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 873      |
| time/              |          |
|    fps             | 254      |
|    iterations      | 200      |
|    time_elapsed    | 1608     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=820.86 +/- 671.52
Episode length: 35.06 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 821       |
| time/                   |           |
|    total_timesteps      | 410000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.85e-28 |
|    explained_variance   | 0.0123    |
|    learning_rate        | 0.001     |
|    loss                 | 1.76e+04  |
|    n_updates            | 1160      |
|    policy_gradient_loss | 5.94e-10  |
|    value_loss           | 3.2e+04   |
---------------------------------------
Eval num_timesteps=410500, episode_reward=808.49 +/- 659.80
Episode length: 35.46 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=657.94 +/- 591.54
Episode length: 33.90 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=734.10 +/- 616.35
Episode length: 34.88 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 879      |
| time/              |          |
|    fps             | 254      |
|    iterations      | 201      |
|    time_elapsed    | 1615     |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=842.06 +/- 658.34
Episode length: 35.90 +/- 5.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 842       |
| time/                   |           |
|    total_timesteps      | 412000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.13e-24 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.001     |
|    loss                 | 1.07e+04  |
|    n_updates            | 1170      |
|    policy_gradient_loss | -1.01e-09 |
|    value_loss           | 2.63e+04  |
---------------------------------------
Eval num_timesteps=412500, episode_reward=899.92 +/- 690.84
Episode length: 36.68 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 900      |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=902.45 +/- 684.96
Episode length: 36.36 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=953.69 +/- 723.49
Episode length: 35.90 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 254      |
|    iterations      | 202      |
|    time_elapsed    | 1623     |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=916.50 +/- 671.81
Episode length: 37.08 +/- 5.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.1      |
|    mean_reward          | 916       |
| time/                   |           |
|    total_timesteps      | 414000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.13e-27 |
|    explained_variance   | 0.00981   |
|    learning_rate        | 0.001     |
|    loss                 | 1.59e+04  |
|    n_updates            | 1180      |
|    policy_gradient_loss | -1.52e-09 |
|    value_loss           | 2.83e+04  |
---------------------------------------
Eval num_timesteps=414500, episode_reward=908.80 +/- 723.92
Episode length: 35.50 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=791.09 +/- 675.94
Episode length: 35.16 +/- 7.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=885.91 +/- 697.45
Episode length: 36.14 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 876      |
| time/              |          |
|    fps             | 254      |
|    iterations      | 203      |
|    time_elapsed    | 1630     |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=951.97 +/- 692.15
Episode length: 37.12 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.1      |
|    mean_reward          | 952       |
| time/                   |           |
|    total_timesteps      | 416000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.64e-23 |
|    explained_variance   | 0.0141    |
|    learning_rate        | 0.001     |
|    loss                 | 9.28e+03  |
|    n_updates            | 1190      |
|    policy_gradient_loss | 1.38e-09  |
|    value_loss           | 2.51e+04  |
---------------------------------------
Eval num_timesteps=416500, episode_reward=858.58 +/- 682.01
Episode length: 35.68 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=750.91 +/- 645.94
Episode length: 34.36 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=883.61 +/- 725.03
Episode length: 35.32 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 769      |
| time/              |          |
|    fps             | 255      |
|    iterations      | 204      |
|    time_elapsed    | 1637     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=750.35 +/- 644.14
Episode length: 34.38 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 750       |
| time/                   |           |
|    total_timesteps      | 418000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.71e-26 |
|    explained_variance   | 0.00667   |
|    learning_rate        | 0.001     |
|    loss                 | 8.58e+03  |
|    n_updates            | 1200      |
|    policy_gradient_loss | -2.72e-10 |
|    value_loss           | 2.49e+04  |
---------------------------------------
Eval num_timesteps=418500, episode_reward=895.23 +/- 706.98
Episode length: 35.86 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=832.21 +/- 673.87
Episode length: 35.32 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=841.14 +/- 700.75
Episode length: 34.96 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 255      |
|    iterations      | 205      |
|    time_elapsed    | 1645     |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=770.21 +/- 674.84
Episode length: 34.40 +/- 7.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 770       |
| time/                   |           |
|    total_timesteps      | 420000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.41e-23 |
|    explained_variance   | 0.0114    |
|    learning_rate        | 0.001     |
|    loss                 | 6.95e+03  |
|    n_updates            | 1210      |
|    policy_gradient_loss | -6.9e-10  |
|    value_loss           | 2.19e+04  |
---------------------------------------
Eval num_timesteps=420500, episode_reward=857.04 +/- 687.54
Episode length: 35.08 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=965.85 +/- 724.47
Episode length: 36.66 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 966      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=825.32 +/- 702.09
Episode length: 34.58 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 812      |
| time/              |          |
|    fps             | 255      |
|    iterations      | 206      |
|    time_elapsed    | 1652     |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=784.11 +/- 693.75
Episode length: 34.68 +/- 6.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 784       |
| time/                   |           |
|    total_timesteps      | 422000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.88e-25 |
|    explained_variance   | 0.00813   |
|    learning_rate        | 0.001     |
|    loss                 | 5.3e+03   |
|    n_updates            | 1220      |
|    policy_gradient_loss | 3.78e-11  |
|    value_loss           | 2.67e+04  |
---------------------------------------
Eval num_timesteps=422500, episode_reward=827.54 +/- 692.39
Episode length: 35.00 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=743.25 +/- 633.99
Episode length: 34.38 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=711.52 +/- 641.51
Episode length: 33.80 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 892      |
| time/              |          |
|    fps             | 255      |
|    iterations      | 207      |
|    time_elapsed    | 1659     |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=787.65 +/- 643.05
Episode length: 35.70 +/- 5.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 788       |
| time/                   |           |
|    total_timesteps      | 424000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.26e-43 |
|    explained_variance   | 0.0191    |
|    learning_rate        | 0.001     |
|    loss                 | 1.03e+04  |
|    n_updates            | 1230      |
|    policy_gradient_loss | 1.4e-10   |
|    value_loss           | 2.8e+04   |
---------------------------------------
Eval num_timesteps=424500, episode_reward=919.76 +/- 731.66
Episode length: 35.60 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=826.78 +/- 660.83
Episode length: 35.74 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=953.32 +/- 733.14
Episode length: 36.06 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 953      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 945      |
| time/              |          |
|    fps             | 255      |
|    iterations      | 208      |
|    time_elapsed    | 1666     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=873.64 +/- 752.90
Episode length: 34.92 +/- 7.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 874       |
| time/                   |           |
|    total_timesteps      | 426000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.95e-39 |
|    explained_variance   | 0.0288    |
|    learning_rate        | 0.001     |
|    loss                 | 2.48e+04  |
|    n_updates            | 1240      |
|    policy_gradient_loss | 4.13e-10  |
|    value_loss           | 4.04e+04  |
---------------------------------------
Eval num_timesteps=426500, episode_reward=950.70 +/- 774.14
Episode length: 35.50 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=665.22 +/- 545.36
Episode length: 35.00 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=798.57 +/- 672.26
Episode length: 34.72 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=800.72 +/- 699.66
Episode length: 34.58 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 966      |
| time/              |          |
|    fps             | 255      |
|    iterations      | 209      |
|    time_elapsed    | 1675     |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=895.52 +/- 670.76
Episode length: 35.98 +/- 5.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 896       |
| time/                   |           |
|    total_timesteps      | 428500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.54e-21 |
|    explained_variance   | 0.019     |
|    learning_rate        | 0.001     |
|    loss                 | 6.33e+03  |
|    n_updates            | 1250      |
|    policy_gradient_loss | 8.19e-10  |
|    value_loss           | 2.69e+04  |
---------------------------------------
Eval num_timesteps=429000, episode_reward=705.33 +/- 593.24
Episode length: 34.62 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=965.55 +/- 747.47
Episode length: 35.94 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 966      |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=1021.71 +/- 727.68
Episode length: 36.68 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 770      |
| time/              |          |
|    fps             | 255      |
|    iterations      | 210      |
|    time_elapsed    | 1682     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=847.80 +/- 670.86
Episode length: 36.08 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 848       |
| time/                   |           |
|    total_timesteps      | 430500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.01e-23 |
|    explained_variance   | 0.00616   |
|    learning_rate        | 0.001     |
|    loss                 | 6.88e+03  |
|    n_updates            | 1260      |
|    policy_gradient_loss | -5.3e-10  |
|    value_loss           | 2.16e+04  |
---------------------------------------
Eval num_timesteps=431000, episode_reward=874.41 +/- 727.63
Episode length: 35.52 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=895.15 +/- 729.15
Episode length: 35.38 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=928.42 +/- 724.35
Episode length: 35.98 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 845      |
| time/              |          |
|    fps             | 255      |
|    iterations      | 211      |
|    time_elapsed    | 1689     |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=912.66 +/- 702.83
Episode length: 35.90 +/- 6.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 913       |
| time/                   |           |
|    total_timesteps      | 432500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.14e-21 |
|    explained_variance   | 0.0204    |
|    learning_rate        | 0.001     |
|    loss                 | 1.33e+04  |
|    n_updates            | 1270      |
|    policy_gradient_loss | 1.22e-09  |
|    value_loss           | 2.75e+04  |
---------------------------------------
Eval num_timesteps=433000, episode_reward=738.23 +/- 620.84
Episode length: 34.74 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=799.80 +/- 687.22
Episode length: 34.50 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=865.07 +/- 683.50
Episode length: 35.40 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 944      |
| time/              |          |
|    fps             | 255      |
|    iterations      | 212      |
|    time_elapsed    | 1697     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=865.50 +/- 737.73
Episode length: 34.78 +/- 6.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 866       |
| time/                   |           |
|    total_timesteps      | 434500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.06e-24 |
|    explained_variance   | 0.00977   |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+04  |
|    n_updates            | 1280      |
|    policy_gradient_loss | -1.31e-09 |
|    value_loss           | 3.51e+04  |
---------------------------------------
Eval num_timesteps=435000, episode_reward=898.88 +/- 661.59
Episode length: 36.52 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=802.61 +/- 634.29
Episode length: 35.76 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=838.90 +/- 692.83
Episode length: 35.24 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 255      |
|    iterations      | 213      |
|    time_elapsed    | 1704     |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=742.28 +/- 612.95
Episode length: 35.22 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 742       |
| time/                   |           |
|    total_timesteps      | 436500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.14e-21 |
|    explained_variance   | 0.0127    |
|    learning_rate        | 0.001     |
|    loss                 | 6.46e+03  |
|    n_updates            | 1290      |
|    policy_gradient_loss | 6.98e-11  |
|    value_loss           | 2.29e+04  |
---------------------------------------
Eval num_timesteps=437000, episode_reward=835.46 +/- 666.50
Episode length: 35.26 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=862.98 +/- 731.86
Episode length: 34.68 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=711.52 +/- 651.50
Episode length: 33.58 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 713      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 214      |
|    time_elapsed    | 1711     |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=958.71 +/- 693.27
Episode length: 36.16 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 959       |
| time/                   |           |
|    total_timesteps      | 438500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.43e-23 |
|    explained_variance   | 0.00506   |
|    learning_rate        | 0.001     |
|    loss                 | 1.78e+04  |
|    n_updates            | 1300      |
|    policy_gradient_loss | 5.44e-10  |
|    value_loss           | 1.85e+04  |
---------------------------------------
Eval num_timesteps=439000, episode_reward=991.37 +/- 739.51
Episode length: 36.56 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=750.23 +/- 671.51
Episode length: 33.94 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=960.19 +/- 748.09
Episode length: 35.76 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 811      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 215      |
|    time_elapsed    | 1719     |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=778.02 +/- 660.75
Episode length: 35.34 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 778       |
| time/                   |           |
|    total_timesteps      | 440500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.08e-39 |
|    explained_variance   | 0.0156    |
|    learning_rate        | 0.001     |
|    loss                 | 1.08e+04  |
|    n_updates            | 1310      |
|    policy_gradient_loss | 1.73e-09  |
|    value_loss           | 2.49e+04  |
---------------------------------------
Eval num_timesteps=441000, episode_reward=848.45 +/- 703.69
Episode length: 34.62 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=754.26 +/- 630.73
Episode length: 34.88 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=779.60 +/- 696.74
Episode length: 33.74 +/- 8.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 909      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 216      |
|    time_elapsed    | 1726     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=644.01 +/- 611.89
Episode length: 32.82 +/- 6.21
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 32.8     |
|    mean_reward          | 644      |
| time/                   |          |
|    total_timesteps      | 442500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.3e-36 |
|    explained_variance   | 0.0157   |
|    learning_rate        | 0.001    |
|    loss                 | 1.78e+04 |
|    n_updates            | 1320     |
|    policy_gradient_loss | 8.96e-10 |
|    value_loss           | 3.74e+04 |
--------------------------------------
Eval num_timesteps=443000, episode_reward=887.98 +/- 644.44
Episode length: 36.92 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=805.64 +/- 657.33
Episode length: 34.74 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=773.45 +/- 627.32
Episode length: 35.26 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 217      |
|    time_elapsed    | 1733     |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=885.37 +/- 711.84
Episode length: 35.88 +/- 6.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 885       |
| time/                   |           |
|    total_timesteps      | 444500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.05e-20 |
|    explained_variance   | 0.0114    |
|    learning_rate        | 0.001     |
|    loss                 | 1.17e+04  |
|    n_updates            | 1330      |
|    policy_gradient_loss | -1.14e-10 |
|    value_loss           | 2.07e+04  |
---------------------------------------
Eval num_timesteps=445000, episode_reward=859.76 +/- 682.72
Episode length: 35.84 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=907.45 +/- 693.61
Episode length: 35.88 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=917.92 +/- 721.36
Episode length: 35.64 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 218      |
|    time_elapsed    | 1740     |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=876.78 +/- 736.85
Episode length: 35.04 +/- 7.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 877       |
| time/                   |           |
|    total_timesteps      | 446500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.66e-22 |
|    explained_variance   | 0.01      |
|    learning_rate        | 0.001     |
|    loss                 | 7.85e+03  |
|    n_updates            | 1340      |
|    policy_gradient_loss | -2.03e-09 |
|    value_loss           | 3.02e+04  |
---------------------------------------
Eval num_timesteps=447000, episode_reward=751.25 +/- 580.54
Episode length: 35.84 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=665.40 +/- 528.13
Episode length: 35.06 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=807.91 +/- 617.68
Episode length: 36.18 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=836.52 +/- 697.42
Episode length: 34.34 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 219      |
|    time_elapsed    | 1749     |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=795.99 +/- 687.23
Episode length: 33.96 +/- 7.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 796       |
| time/                   |           |
|    total_timesteps      | 449000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-20 |
|    explained_variance   | 0.0102    |
|    learning_rate        | 0.001     |
|    loss                 | 1.42e+04  |
|    n_updates            | 1350      |
|    policy_gradient_loss | 6.4e-11   |
|    value_loss           | 2.19e+04  |
---------------------------------------
Eval num_timesteps=449500, episode_reward=881.99 +/- 670.72
Episode length: 36.50 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=681.39 +/- 621.84
Episode length: 34.12 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=955.18 +/- 703.56
Episode length: 36.32 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 220      |
|    time_elapsed    | 1756     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=840.93 +/- 671.55
Episode length: 35.16 +/- 6.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 841       |
| time/                   |           |
|    total_timesteps      | 451000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.57e-23 |
|    explained_variance   | 0.00826   |
|    learning_rate        | 0.001     |
|    loss                 | 4.37e+03  |
|    n_updates            | 1360      |
|    policy_gradient_loss | 2.04e-10  |
|    value_loss           | 2.9e+04   |
---------------------------------------
Eval num_timesteps=451500, episode_reward=935.50 +/- 729.64
Episode length: 35.94 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=906.61 +/- 695.06
Episode length: 35.82 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=929.48 +/- 708.88
Episode length: 35.64 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 996      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 221      |
|    time_elapsed    | 1763     |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=709.33 +/- 582.81
Episode length: 34.34 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 709       |
| time/                   |           |
|    total_timesteps      | 453000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.26e-37 |
|    explained_variance   | 0.0207    |
|    learning_rate        | 0.001     |
|    loss                 | 6.26e+03  |
|    n_updates            | 1370      |
|    policy_gradient_loss | -8.18e-10 |
|    value_loss           | 3.12e+04  |
---------------------------------------
Eval num_timesteps=453500, episode_reward=962.63 +/- 711.19
Episode length: 36.56 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=790.75 +/- 635.92
Episode length: 35.80 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=849.66 +/- 737.80
Episode length: 34.34 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 874      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 222      |
|    time_elapsed    | 1771     |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=712.99 +/- 639.06
Episode length: 34.28 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 713       |
| time/                   |           |
|    total_timesteps      | 455000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.87e-34 |
|    explained_variance   | 0.0206    |
|    learning_rate        | 0.001     |
|    loss                 | 1.88e+04  |
|    n_updates            | 1380      |
|    policy_gradient_loss | 2.01e-09  |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=455500, episode_reward=988.52 +/- 747.54
Episode length: 35.66 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=885.30 +/- 723.61
Episode length: 35.86 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=944.99 +/- 720.34
Episode length: 36.08 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 945      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 223      |
|    time_elapsed    | 1778     |
|    total_timesteps | 456704   |
---------------------------------
Eval num_timesteps=457000, episode_reward=927.63 +/- 780.78
Episode length: 34.66 +/- 7.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 928       |
| time/                   |           |
|    total_timesteps      | 457000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.4e-37  |
|    explained_variance   | 0.0137    |
|    learning_rate        | 0.001     |
|    loss                 | 1.58e+04  |
|    n_updates            | 1390      |
|    policy_gradient_loss | -3.48e-10 |
|    value_loss           | 2.6e+04   |
---------------------------------------
Eval num_timesteps=457500, episode_reward=795.46 +/- 671.98
Episode length: 35.06 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=859.25 +/- 712.94
Episode length: 35.04 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=683.74 +/- 584.48
Episode length: 34.20 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 886      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 224      |
|    time_elapsed    | 1785     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459000, episode_reward=741.74 +/- 688.03
Episode length: 33.68 +/- 7.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 742       |
| time/                   |           |
|    total_timesteps      | 459000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.48e-31 |
|    explained_variance   | 0.0178    |
|    learning_rate        | 0.001     |
|    loss                 | 9.44e+03  |
|    n_updates            | 1400      |
|    policy_gradient_loss | 1.16e-10  |
|    value_loss           | 3.28e+04  |
---------------------------------------
Eval num_timesteps=459500, episode_reward=878.50 +/- 689.38
Episode length: 36.08 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=929.44 +/- 661.07
Episode length: 37.04 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=677.99 +/- 576.90
Episode length: 34.14 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 817      |
| time/              |          |
|    fps             | 256      |
|    iterations      | 225      |
|    time_elapsed    | 1793     |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=461000, episode_reward=1009.95 +/- 736.19
Episode length: 36.60 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 461000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.88e-34 |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.001     |
|    loss                 | 9.3e+03   |
|    n_updates            | 1410      |
|    policy_gradient_loss | 2.01e-10  |
|    value_loss           | 2.22e+04  |
---------------------------------------
Eval num_timesteps=461500, episode_reward=867.29 +/- 726.25
Episode length: 35.24 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=798.98 +/- 652.91
Episode length: 34.96 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=899.42 +/- 688.39
Episode length: 36.26 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 226      |
|    time_elapsed    | 1800     |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=846.17 +/- 683.50
Episode length: 35.88 +/- 7.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 846       |
| time/                   |           |
|    total_timesteps      | 463000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.76e-20 |
|    explained_variance   | 0.0108    |
|    learning_rate        | 0.001     |
|    loss                 | 6.7e+03   |
|    n_updates            | 1420      |
|    policy_gradient_loss | -5.03e-10 |
|    value_loss           | 2.65e+04  |
---------------------------------------
Eval num_timesteps=463500, episode_reward=749.75 +/- 661.97
Episode length: 34.52 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=837.80 +/- 672.07
Episode length: 35.82 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=733.82 +/- 671.36
Episode length: 33.90 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 864      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 227      |
|    time_elapsed    | 1807     |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=809.40 +/- 659.84
Episode length: 35.04 +/- 6.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 809       |
| time/                   |           |
|    total_timesteps      | 465000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-22 |
|    explained_variance   | 0.0102    |
|    learning_rate        | 0.001     |
|    loss                 | 2.04e+04  |
|    n_updates            | 1430      |
|    policy_gradient_loss | 2.59e-10  |
|    value_loss           | 3.41e+04  |
---------------------------------------
Eval num_timesteps=465500, episode_reward=758.23 +/- 622.63
Episode length: 34.84 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=786.60 +/- 594.71
Episode length: 36.12 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=858.43 +/- 686.40
Episode length: 35.72 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 228      |
|    time_elapsed    | 1814     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=957.11 +/- 707.48
Episode length: 36.94 +/- 5.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 957       |
| time/                   |           |
|    total_timesteps      | 467000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.23e-22 |
|    explained_variance   | 0.0159    |
|    learning_rate        | 0.001     |
|    loss                 | 1.05e+04  |
|    n_updates            | 1440      |
|    policy_gradient_loss | 1.16e-10  |
|    value_loss           | 2.57e+04  |
---------------------------------------
Eval num_timesteps=467500, episode_reward=900.91 +/- 706.42
Episode length: 36.14 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=683.69 +/- 546.44
Episode length: 34.64 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=857.43 +/- 736.18
Episode length: 34.72 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 893      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 229      |
|    time_elapsed    | 1822     |
|    total_timesteps | 468992   |
---------------------------------
Eval num_timesteps=469000, episode_reward=877.92 +/- 717.77
Episode length: 35.40 +/- 6.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 878       |
| time/                   |           |
|    total_timesteps      | 469000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.13e-24 |
|    explained_variance   | 0.011     |
|    learning_rate        | 0.001     |
|    loss                 | 8.53e+03  |
|    n_updates            | 1450      |
|    policy_gradient_loss | 1.95e-10  |
|    value_loss           | 3.15e+04  |
---------------------------------------
Eval num_timesteps=469500, episode_reward=792.90 +/- 676.51
Episode length: 34.66 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=858.80 +/- 687.31
Episode length: 35.70 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=809.98 +/- 747.34
Episode length: 33.40 +/- 8.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=970.55 +/- 752.32
Episode length: 36.14 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 984      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 230      |
|    time_elapsed    | 1830     |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=917.50 +/- 743.07
Episode length: 35.60 +/- 6.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 917       |
| time/                   |           |
|    total_timesteps      | 471500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.92e-42 |
|    explained_variance   | 0.0169    |
|    learning_rate        | 0.001     |
|    loss                 | 1.58e+04  |
|    n_updates            | 1460      |
|    policy_gradient_loss | 3.55e-10  |
|    value_loss           | 2.53e+04  |
---------------------------------------
Eval num_timesteps=472000, episode_reward=800.12 +/- 757.39
Episode length: 33.34 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=923.50 +/- 723.46
Episode length: 35.78 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 924      |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=1038.53 +/- 732.90
Episode length: 36.92 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 871      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 231      |
|    time_elapsed    | 1838     |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=721.97 +/- 628.57
Episode length: 34.68 +/- 6.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 722       |
| time/                   |           |
|    total_timesteps      | 473500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.08e-39 |
|    explained_variance   | 0.0197    |
|    learning_rate        | 0.001     |
|    loss                 | 2.92e+04  |
|    n_updates            | 1470      |
|    policy_gradient_loss | -7.8e-10  |
|    value_loss           | 4.74e+04  |
---------------------------------------
Eval num_timesteps=474000, episode_reward=884.57 +/- 686.06
Episode length: 36.26 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=790.70 +/- 665.78
Episode length: 35.40 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=1103.63 +/- 765.82
Episode length: 37.52 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 817      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 232      |
|    time_elapsed    | 1845     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=782.14 +/- 643.47
Episode length: 35.02 +/- 6.91
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35       |
|    mean_reward          | 782      |
| time/                   |          |
|    total_timesteps      | 475500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6e-21   |
|    explained_variance   | 0.0146   |
|    learning_rate        | 0.001    |
|    loss                 | 1.99e+04 |
|    n_updates            | 1480     |
|    policy_gradient_loss | 4.57e-10 |
|    value_loss           | 2.5e+04  |
--------------------------------------
Eval num_timesteps=476000, episode_reward=1054.46 +/- 749.54
Episode length: 36.80 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=918.24 +/- 706.59
Episode length: 36.40 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=866.31 +/- 701.93
Episode length: 35.80 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 233      |
|    time_elapsed    | 1853     |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=816.90 +/- 655.91
Episode length: 35.22 +/- 6.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 477500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.34e-23 |
|    explained_variance   | 0.00899   |
|    learning_rate        | 0.001     |
|    loss                 | 6.79e+03  |
|    n_updates            | 1490      |
|    policy_gradient_loss | -1.16e-09 |
|    value_loss           | 2.57e+04  |
---------------------------------------
Eval num_timesteps=478000, episode_reward=862.73 +/- 718.03
Episode length: 35.74 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=892.13 +/- 682.81
Episode length: 36.40 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=938.67 +/- 685.51
Episode length: 36.94 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 776      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 234      |
|    time_elapsed    | 1860     |
|    total_timesteps | 479232   |
---------------------------------
Eval num_timesteps=479500, episode_reward=737.34 +/- 552.42
Episode length: 35.68 +/- 5.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 737       |
| time/                   |           |
|    total_timesteps      | 479500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.37e-21 |
|    explained_variance   | 0.0124    |
|    learning_rate        | 0.001     |
|    loss                 | 8.16e+03  |
|    n_updates            | 1500      |
|    policy_gradient_loss | 1.94e-09  |
|    value_loss           | 2.22e+04  |
---------------------------------------
Eval num_timesteps=480000, episode_reward=789.55 +/- 701.20
Episode length: 34.26 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=902.86 +/- 714.96
Episode length: 35.48 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=898.06 +/- 763.16
Episode length: 35.08 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 235      |
|    time_elapsed    | 1867     |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=845.65 +/- 657.44
Episode length: 35.74 +/- 6.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 846       |
| time/                   |           |
|    total_timesteps      | 481500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.05e-23 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.001     |
|    loss                 | 1.4e+04   |
|    n_updates            | 1510      |
|    policy_gradient_loss | 1.34e-09  |
|    value_loss           | 2.78e+04  |
---------------------------------------
Eval num_timesteps=482000, episode_reward=1055.25 +/- 742.89
Episode length: 36.92 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=921.14 +/- 738.46
Episode length: 35.40 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=979.51 +/- 746.49
Episode length: 36.52 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 737      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 236      |
|    time_elapsed    | 1875     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=787.32 +/- 660.05
Episode length: 34.66 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 787       |
| time/                   |           |
|    total_timesteps      | 483500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-20 |
|    explained_variance   | 0.016     |
|    learning_rate        | 0.001     |
|    loss                 | 1.33e+04  |
|    n_updates            | 1520      |
|    policy_gradient_loss | -4.07e-11 |
|    value_loss           | 2.34e+04  |
---------------------------------------
Eval num_timesteps=484000, episode_reward=730.57 +/- 620.34
Episode length: 34.48 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=796.84 +/- 661.44
Episode length: 34.62 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=893.76 +/- 744.82
Episode length: 35.06 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 874      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 237      |
|    time_elapsed    | 1882     |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=814.56 +/- 658.40
Episode length: 35.60 +/- 6.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 815       |
| time/                   |           |
|    total_timesteps      | 485500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.28e-23 |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.001     |
|    loss                 | 1.46e+04  |
|    n_updates            | 1530      |
|    policy_gradient_loss | 1.73e-09  |
|    value_loss           | 3.39e+04  |
---------------------------------------
Eval num_timesteps=486000, episode_reward=796.57 +/- 677.66
Episode length: 34.30 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=659.18 +/- 532.41
Episode length: 34.84 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=872.45 +/- 699.75
Episode length: 35.38 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 884      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 238      |
|    time_elapsed    | 1889     |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=701.43 +/- 647.65
Episode length: 33.80 +/- 7.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 701       |
| time/                   |           |
|    total_timesteps      | 487500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-38 |
|    explained_variance   | 0.0158    |
|    learning_rate        | 0.001     |
|    loss                 | 1.49e+04  |
|    n_updates            | 1540      |
|    policy_gradient_loss | -1.45e-10 |
|    value_loss           | 2.42e+04  |
---------------------------------------
Eval num_timesteps=488000, episode_reward=841.10 +/- 742.84
Episode length: 34.32 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=822.81 +/- 691.59
Episode length: 35.50 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=684.59 +/- 601.38
Episode length: 33.80 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 870      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 239      |
|    time_elapsed    | 1896     |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=1000.76 +/- 689.99
Episode length: 37.04 +/- 5.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 489500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-35 |
|    explained_variance   | 0.0162    |
|    learning_rate        | 0.001     |
|    loss                 | 1.17e+04  |
|    n_updates            | 1550      |
|    policy_gradient_loss | -8.44e-10 |
|    value_loss           | 4.02e+04  |
---------------------------------------
Eval num_timesteps=490000, episode_reward=858.45 +/- 734.43
Episode length: 34.72 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=809.57 +/- 699.70
Episode length: 34.52 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=773.35 +/- 640.75
Episode length: 35.18 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=858.37 +/- 676.46
Episode length: 35.64 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 257      |
|    iterations      | 240      |
|    time_elapsed    | 1905     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=682.78 +/- 525.45
Episode length: 35.08 +/- 5.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 683       |
| time/                   |           |
|    total_timesteps      | 492000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.31e-20 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.001     |
|    loss                 | 7.3e+03   |
|    n_updates            | 1560      |
|    policy_gradient_loss | -1.43e-10 |
|    value_loss           | 2.49e+04  |
---------------------------------------
Eval num_timesteps=492500, episode_reward=1143.63 +/- 764.47
Episode length: 37.50 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=749.06 +/- 646.37
Episode length: 34.66 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=785.67 +/- 617.11
Episode length: 35.66 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 241      |
|    time_elapsed    | 1912     |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=704.02 +/- 620.76
Episode length: 34.44 +/- 6.30
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.4     |
|    mean_reward          | 704      |
| time/                   |          |
|    total_timesteps      | 494000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -8.8e-23 |
|    explained_variance   | 0.00714  |
|    learning_rate        | 0.001    |
|    loss                 | 1.08e+04 |
|    n_updates            | 1570     |
|    policy_gradient_loss | 1.36e-09 |
|    value_loss           | 2.22e+04 |
--------------------------------------
Eval num_timesteps=494500, episode_reward=844.95 +/- 635.92
Episode length: 35.84 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=764.76 +/- 668.82
Episode length: 34.62 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=894.42 +/- 683.37
Episode length: 36.24 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 647      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 242      |
|    time_elapsed    | 1919     |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=834.39 +/- 668.35
Episode length: 34.94 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 496000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.93e-20 |
|    explained_variance   | 0.00699   |
|    learning_rate        | 0.001     |
|    loss                 | 1.02e+04  |
|    n_updates            | 1580      |
|    policy_gradient_loss | 2.82e-10  |
|    value_loss           | 2.14e+04  |
---------------------------------------
Eval num_timesteps=496500, episode_reward=894.06 +/- 724.03
Episode length: 35.88 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=708.82 +/- 675.64
Episode length: 33.26 +/- 7.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=904.12 +/- 676.41
Episode length: 36.74 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 243      |
|    time_elapsed    | 1927     |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=793.92 +/- 643.85
Episode length: 34.52 +/- 6.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 794       |
| time/                   |           |
|    total_timesteps      | 498000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.11e-22 |
|    explained_variance   | 0.0108    |
|    learning_rate        | 0.001     |
|    loss                 | 2.16e+04  |
|    n_updates            | 1590      |
|    policy_gradient_loss | 5.14e-10  |
|    value_loss           | 3.2e+04   |
---------------------------------------
Eval num_timesteps=498500, episode_reward=842.59 +/- 700.82
Episode length: 35.32 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=844.98 +/- 662.94
Episode length: 35.94 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=770.99 +/- 631.99
Episode length: 35.12 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 888      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 244      |
|    time_elapsed    | 1934     |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=686.80 +/- 602.85
Episode length: 34.00 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 687       |
| time/                   |           |
|    total_timesteps      | 500000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.24e-36 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.001     |
|    loss                 | 1.47e+04  |
|    n_updates            | 1600      |
|    policy_gradient_loss | -1.09e-09 |
|    value_loss           | 2.39e+04  |
---------------------------------------
Eval num_timesteps=500500, episode_reward=825.86 +/- 655.33
Episode length: 35.72 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=1002.07 +/- 736.15
Episode length: 36.48 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=756.15 +/- 619.07
Episode length: 35.38 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 799      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 245      |
|    time_elapsed    | 1941     |
|    total_timesteps | 501760   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-ppo-1-1-last-dif(3)/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
