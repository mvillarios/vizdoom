/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Eval num_timesteps=500, episode_reward=285.36 +/- 664.31
Episode length: 34.44 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=539.81 +/- 772.14
Episode length: 35.70 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 540      |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 612      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
Eval num_timesteps=1500, episode_reward=280.67 +/- 671.55
Episode length: 33.64 +/- 7.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 281           |
| time/                   |               |
|    total_timesteps      | 1500          |
| train/                  |               |
|    approx_kl            | 2.5450136e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00148      |
|    explained_variance   | 0.0496        |
|    learning_rate        | 0.001         |
|    loss                 | 3.1e+04       |
|    n_updates            | 5800          |
|    policy_gradient_loss | -0.000292     |
|    value_loss           | 8.96e+04      |
-------------------------------------------
Eval num_timesteps=2000, episode_reward=478.08 +/- 755.75
Episode length: 36.14 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 446      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=436.65 +/- 710.79
Episode length: 35.40 +/- 6.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 437           |
| time/                   |               |
|    total_timesteps      | 2500          |
| train/                  |               |
|    approx_kl            | 3.1147501e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0036       |
|    explained_variance   | -0.236        |
|    learning_rate        | 0.001         |
|    loss                 | 3.57e+04      |
|    n_updates            | 5810          |
|    policy_gradient_loss | -0.000195     |
|    value_loss           | 9.67e+04      |
-------------------------------------------
Eval num_timesteps=3000, episode_reward=461.60 +/- 736.91
Episode length: 35.58 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 462      |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 470      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 3        |
|    time_elapsed    | 10       |
|    total_timesteps | 3072     |
---------------------------------
Eval num_timesteps=3500, episode_reward=382.68 +/- 644.76
Episode length: 35.74 +/- 5.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 383           |
| time/                   |               |
|    total_timesteps      | 3500          |
| train/                  |               |
|    approx_kl            | 4.2528263e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00382      |
|    explained_variance   | 0.0594        |
|    learning_rate        | 0.001         |
|    loss                 | 3.58e+04      |
|    n_updates            | 5820          |
|    policy_gradient_loss | -0.000522     |
|    value_loss           | 8.31e+04      |
-------------------------------------------
Eval num_timesteps=4000, episode_reward=535.30 +/- 768.70
Episode length: 35.30 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 407      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 4        |
|    time_elapsed    | 14       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=493.82 +/- 728.05
Episode length: 35.60 +/- 6.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 494          |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 5.043519e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00565     |
|    explained_variance   | 0.0773       |
|    learning_rate        | 0.001        |
|    loss                 | 3.21e+04     |
|    n_updates            | 5830         |
|    policy_gradient_loss | -0.000505    |
|    value_loss           | 8.67e+04     |
------------------------------------------
Eval num_timesteps=5000, episode_reward=458.25 +/- 694.34
Episode length: 36.08 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 458      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 384      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 5        |
|    time_elapsed    | 17       |
|    total_timesteps | 5120     |
---------------------------------
Eval num_timesteps=5500, episode_reward=302.24 +/- 687.43
Episode length: 33.88 +/- 6.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.9         |
|    mean_reward          | 302          |
| time/                   |              |
|    total_timesteps      | 5500         |
| train/                  |              |
|    approx_kl            | 0.0018081085 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.009       |
|    explained_variance   | -0.0197      |
|    learning_rate        | 0.001        |
|    loss                 | 2.97e+04     |
|    n_updates            | 5840         |
|    policy_gradient_loss | 0.00119      |
|    value_loss           | 7.99e+04     |
------------------------------------------
Eval num_timesteps=6000, episode_reward=326.41 +/- 677.83
Episode length: 34.66 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 326      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 467      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 6        |
|    time_elapsed    | 21       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=425.32 +/- 647.03
Episode length: 36.24 +/- 5.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 425          |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.0004217893 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00777     |
|    explained_variance   | -0.103       |
|    learning_rate        | 0.001        |
|    loss                 | 4.19e+04     |
|    n_updates            | 5850         |
|    policy_gradient_loss | -0.000506    |
|    value_loss           | 1.01e+05     |
------------------------------------------
Eval num_timesteps=7000, episode_reward=506.99 +/- 762.95
Episode length: 36.02 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 507      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 442      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 7        |
|    time_elapsed    | 25       |
|    total_timesteps | 7168     |
---------------------------------
Eval num_timesteps=7500, episode_reward=472.77 +/- 719.00
Episode length: 36.24 +/- 5.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 473          |
| time/                   |              |
|    total_timesteps      | 7500         |
| train/                  |              |
|    approx_kl            | 0.0029755773 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00801     |
|    explained_variance   | -0.0213      |
|    learning_rate        | 0.001        |
|    loss                 | 3.13e+04     |
|    n_updates            | 5860         |
|    policy_gradient_loss | 0.000176     |
|    value_loss           | 8.74e+04     |
------------------------------------------
Eval num_timesteps=8000, episode_reward=552.14 +/- 753.06
Episode length: 36.08 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 452      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 8        |
|    time_elapsed    | 28       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=213.36 +/- 672.15
Episode length: 32.96 +/- 7.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33          |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.003941182 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0135     |
|    explained_variance   | -2.62e-05   |
|    learning_rate        | 0.001       |
|    loss                 | 2.83e+04    |
|    n_updates            | 5870        |
|    policy_gradient_loss | 0.000662    |
|    value_loss           | 7.68e+04    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=337.38 +/- 666.47
Episode length: 34.96 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 443      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 9        |
|    time_elapsed    | 32       |
|    total_timesteps | 9216     |
---------------------------------
Eval num_timesteps=9500, episode_reward=387.87 +/- 705.91
Episode length: 35.70 +/- 6.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 388           |
| time/                   |               |
|    total_timesteps      | 9500          |
| train/                  |               |
|    approx_kl            | 0.00044531963 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0141       |
|    explained_variance   | -0.0699       |
|    learning_rate        | 0.001         |
|    loss                 | 4.17e+04      |
|    n_updates            | 5880          |
|    policy_gradient_loss | -0.000597     |
|    value_loss           | 9.64e+04      |
-------------------------------------------
Eval num_timesteps=10000, episode_reward=404.37 +/- 743.28
Episode length: 35.42 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 395      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 10       |
|    time_elapsed    | 35       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=335.02 +/- 650.64
Episode length: 35.08 +/- 6.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 335           |
| time/                   |               |
|    total_timesteps      | 10500         |
| train/                  |               |
|    approx_kl            | 2.0482403e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0187       |
|    explained_variance   | -0.0491       |
|    learning_rate        | 0.001         |
|    loss                 | 3.48e+04      |
|    n_updates            | 5890          |
|    policy_gradient_loss | -0.00141      |
|    value_loss           | 8.04e+04      |
-------------------------------------------
Eval num_timesteps=11000, episode_reward=486.08 +/- 753.92
Episode length: 35.38 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 342      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 11       |
|    time_elapsed    | 39       |
|    total_timesteps | 11264    |
---------------------------------
Eval num_timesteps=11500, episode_reward=562.97 +/- 699.42
Episode length: 36.52 +/- 6.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.5         |
|    mean_reward          | 563          |
| time/                   |              |
|    total_timesteps      | 11500        |
| train/                  |              |
|    approx_kl            | 0.0014383553 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0191      |
|    explained_variance   | -0.103       |
|    learning_rate        | 0.001        |
|    loss                 | 2.33e+04     |
|    n_updates            | 5900         |
|    policy_gradient_loss | -0.00193     |
|    value_loss           | 8.57e+04     |
------------------------------------------
New best mean reward!
Eval num_timesteps=12000, episode_reward=544.68 +/- 751.64
Episode length: 35.70 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 545      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 12       |
|    time_elapsed    | 43       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=472.99 +/- 728.38
Episode length: 35.30 +/- 6.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.3        |
|    mean_reward          | 473         |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.014219867 |
|    clip_fraction        | 0.00645     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.011      |
|    explained_variance   | -0.147      |
|    learning_rate        | 0.001       |
|    loss                 | 2.24e+04    |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.00081    |
|    value_loss           | 1.02e+05    |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=372.04 +/- 640.03
Episode length: 35.94 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 13       |
|    time_elapsed    | 46       |
|    total_timesteps | 13312    |
---------------------------------
Eval num_timesteps=13500, episode_reward=478.55 +/- 691.41
Episode length: 36.88 +/- 5.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.9          |
|    mean_reward          | 479           |
| time/                   |               |
|    total_timesteps      | 13500         |
| train/                  |               |
|    approx_kl            | 1.0759628e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00998      |
|    explained_variance   | -0.117        |
|    learning_rate        | 0.001         |
|    loss                 | 2.13e+04      |
|    n_updates            | 5920          |
|    policy_gradient_loss | -0.000855     |
|    value_loss           | 8.09e+04      |
-------------------------------------------
Eval num_timesteps=14000, episode_reward=386.47 +/- 702.36
Episode length: 35.76 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 456      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 14       |
|    time_elapsed    | 50       |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=488.34 +/- 718.59
Episode length: 36.28 +/- 5.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.3        |
|    mean_reward          | 488         |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.002388625 |
|    clip_fraction        | 0.00107     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0203     |
|    explained_variance   | -0.0856     |
|    learning_rate        | 0.001       |
|    loss                 | 2.39e+04    |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.000272   |
|    value_loss           | 7.14e+04    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=434.84 +/- 731.00
Episode length: 35.02 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 416      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 15       |
|    time_elapsed    | 54       |
|    total_timesteps | 15360    |
---------------------------------
Eval num_timesteps=15500, episode_reward=455.39 +/- 738.86
Episode length: 35.64 +/- 5.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 455          |
| time/                   |              |
|    total_timesteps      | 15500        |
| train/                  |              |
|    approx_kl            | 0.0013994288 |
|    clip_fraction        | 0.00322      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0159      |
|    explained_variance   | -0.0445      |
|    learning_rate        | 0.001        |
|    loss                 | 3.56e+04     |
|    n_updates            | 5940         |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 9.38e+04     |
------------------------------------------
Eval num_timesteps=16000, episode_reward=352.46 +/- 718.78
Episode length: 34.26 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 352      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 408      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 16       |
|    time_elapsed    | 57       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=423.53 +/- 689.24
Episode length: 35.02 +/- 7.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 424          |
| time/                   |              |
|    total_timesteps      | 16500        |
| train/                  |              |
|    approx_kl            | 0.0033193168 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | -0.197       |
|    learning_rate        | 0.001        |
|    loss                 | 2.85e+04     |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.000166    |
|    value_loss           | 8.72e+04     |
------------------------------------------
Eval num_timesteps=17000, episode_reward=385.00 +/- 658.32
Episode length: 35.38 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 351      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 17       |
|    time_elapsed    | 61       |
|    total_timesteps | 17408    |
---------------------------------
Eval num_timesteps=17500, episode_reward=447.03 +/- 688.86
Episode length: 35.68 +/- 6.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 447          |
| time/                   |              |
|    total_timesteps      | 17500        |
| train/                  |              |
|    approx_kl            | 0.0016312166 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0219      |
|    explained_variance   | -0.0122      |
|    learning_rate        | 0.001        |
|    loss                 | 4.22e+04     |
|    n_updates            | 5960         |
|    policy_gradient_loss | -0.000394    |
|    value_loss           | 8.38e+04     |
------------------------------------------
Eval num_timesteps=18000, episode_reward=521.99 +/- 779.38
Episode length: 35.88 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 522      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 18       |
|    time_elapsed    | 65       |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=455.99 +/- 703.13
Episode length: 35.60 +/- 6.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 456         |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.013448024 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0127     |
|    explained_variance   | -0.0092     |
|    learning_rate        | 0.001       |
|    loss                 | 3.59e+04    |
|    n_updates            | 5970        |
|    policy_gradient_loss | 0.00632     |
|    value_loss           | 1.09e+05    |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=348.53 +/- 634.96
Episode length: 35.80 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 349      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 19       |
|    time_elapsed    | 68       |
|    total_timesteps | 19456    |
---------------------------------
Eval num_timesteps=19500, episode_reward=269.23 +/- 646.23
Episode length: 32.84 +/- 7.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.8         |
|    mean_reward          | 269          |
| time/                   |              |
|    total_timesteps      | 19500        |
| train/                  |              |
|    approx_kl            | 0.0004573727 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0214      |
|    explained_variance   | 0.0915       |
|    learning_rate        | 0.001        |
|    loss                 | 2.94e+04     |
|    n_updates            | 5980         |
|    policy_gradient_loss | -0.001       |
|    value_loss           | 7.38e+04     |
------------------------------------------
Eval num_timesteps=20000, episode_reward=529.49 +/- 831.98
Episode length: 34.74 +/- 8.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 529      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 20       |
|    time_elapsed    | 72       |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=269.62 +/- 627.91
Episode length: 33.96 +/- 6.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 270          |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0004073179 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0296      |
|    explained_variance   | -0.135       |
|    learning_rate        | 0.001        |
|    loss                 | 1.73e+04     |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 6.17e+04     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=568.93 +/- 806.33
Episode length: 36.04 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 569      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
New best mean reward!
Eval num_timesteps=21500, episode_reward=459.63 +/- 714.86
Episode length: 36.52 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 374      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 21       |
|    time_elapsed    | 77       |
|    total_timesteps | 21504    |
---------------------------------
Eval num_timesteps=22000, episode_reward=462.40 +/- 799.47
Episode length: 34.78 +/- 7.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 462          |
| time/                   |              |
|    total_timesteps      | 22000        |
| train/                  |              |
|    approx_kl            | 0.0014608484 |
|    clip_fraction        | 0.00342      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.013       |
|    explained_variance   | 0.0741       |
|    learning_rate        | 0.001        |
|    loss                 | 3.12e+04     |
|    n_updates            | 6000         |
|    policy_gradient_loss | -0.000643    |
|    value_loss           | 1.01e+05     |
------------------------------------------
Eval num_timesteps=22500, episode_reward=675.11 +/- 803.29
Episode length: 36.78 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 22       |
|    time_elapsed    | 81       |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=592.65 +/- 806.37
Episode length: 35.64 +/- 6.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 593          |
| time/                   |              |
|    total_timesteps      | 23000        |
| train/                  |              |
|    approx_kl            | 0.0006763383 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0184      |
|    explained_variance   | 0.0292       |
|    learning_rate        | 0.001        |
|    loss                 | 3.78e+04     |
|    n_updates            | 6010         |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 8.24e+04     |
------------------------------------------
Eval num_timesteps=23500, episode_reward=588.59 +/- 756.41
Episode length: 37.00 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 589      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 506      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 23       |
|    time_elapsed    | 84       |
|    total_timesteps | 23552    |
---------------------------------
Eval num_timesteps=24000, episode_reward=370.47 +/- 702.81
Episode length: 34.36 +/- 6.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | 370         |
| time/                   |             |
|    total_timesteps      | 24000       |
| train/                  |             |
|    approx_kl            | 0.024613373 |
|    clip_fraction        | 0.00479     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0133     |
|    explained_variance   | -0.0157     |
|    learning_rate        | 0.001       |
|    loss                 | 3.36e+04    |
|    n_updates            | 6020        |
|    policy_gradient_loss | 0.00173     |
|    value_loss           | 8.13e+04    |
-----------------------------------------
Eval num_timesteps=24500, episode_reward=411.34 +/- 688.05
Episode length: 35.44 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 411      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 471      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 24       |
|    time_elapsed    | 88       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=555.19 +/- 746.91
Episode length: 35.78 +/- 6.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.8        |
|    mean_reward          | 555         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.007703908 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0126     |
|    explained_variance   | 0.073       |
|    learning_rate        | 0.001       |
|    loss                 | 4.21e+04    |
|    n_updates            | 6030        |
|    policy_gradient_loss | 0.000776    |
|    value_loss           | 9.24e+04    |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=542.05 +/- 810.93
Episode length: 35.34 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 542      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 416      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 25       |
|    time_elapsed    | 92       |
|    total_timesteps | 25600    |
---------------------------------
Eval num_timesteps=26000, episode_reward=488.35 +/- 746.23
Episode length: 35.48 +/- 6.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 488           |
| time/                   |               |
|    total_timesteps      | 26000         |
| train/                  |               |
|    approx_kl            | 5.6981342e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0212       |
|    explained_variance   | -0.193        |
|    learning_rate        | 0.001         |
|    loss                 | 3.83e+04      |
|    n_updates            | 6040          |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 7.81e+04      |
-------------------------------------------
Eval num_timesteps=26500, episode_reward=456.80 +/- 720.29
Episode length: 35.66 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 326      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 26       |
|    time_elapsed    | 95       |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=419.25 +/- 729.98
Episode length: 34.86 +/- 6.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 419          |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 6.703619e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.033       |
|    explained_variance   | -0.0605      |
|    learning_rate        | 0.001        |
|    loss                 | 2.08e+04     |
|    n_updates            | 6050         |
|    policy_gradient_loss | -0.00197     |
|    value_loss           | 6.5e+04      |
------------------------------------------
Eval num_timesteps=27500, episode_reward=263.00 +/- 610.53
Episode length: 34.16 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 258      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 27       |
|    time_elapsed    | 99       |
|    total_timesteps | 27648    |
---------------------------------
Eval num_timesteps=28000, episode_reward=271.17 +/- 621.63
Episode length: 34.66 +/- 6.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 271          |
| time/                   |              |
|    total_timesteps      | 28000        |
| train/                  |              |
|    approx_kl            | 0.0028081404 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0332      |
|    explained_variance   | 0.0681       |
|    learning_rate        | 0.001        |
|    loss                 | 2.76e+04     |
|    n_updates            | 6060         |
|    policy_gradient_loss | -0.000372    |
|    value_loss           | 7.57e+04     |
------------------------------------------
Eval num_timesteps=28500, episode_reward=545.79 +/- 711.84
Episode length: 36.86 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 546      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 28       |
|    time_elapsed    | 102      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=566.94 +/- 770.61
Episode length: 36.48 +/- 5.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.5         |
|    mean_reward          | 567          |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0033993144 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00613     |
|    explained_variance   | -0.0129      |
|    learning_rate        | 0.001        |
|    loss                 | 2.07e+04     |
|    n_updates            | 6070         |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 8.58e+04     |
------------------------------------------
Eval num_timesteps=29500, episode_reward=401.42 +/- 738.12
Episode length: 34.54 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 29       |
|    time_elapsed    | 106      |
|    total_timesteps | 29696    |
---------------------------------
Eval num_timesteps=30000, episode_reward=369.05 +/- 662.46
Episode length: 36.04 +/- 6.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 369           |
| time/                   |               |
|    total_timesteps      | 30000         |
| train/                  |               |
|    approx_kl            | 0.00063033326 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0129       |
|    explained_variance   | -0.274        |
|    learning_rate        | 0.001         |
|    loss                 | 2.54e+04      |
|    n_updates            | 6080          |
|    policy_gradient_loss | -0.000751     |
|    value_loss           | 8.3e+04       |
-------------------------------------------
Eval num_timesteps=30500, episode_reward=407.53 +/- 690.79
Episode length: 36.06 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 30       |
|    time_elapsed    | 110      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=478.20 +/- 746.01
Episode length: 35.68 +/- 5.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 478          |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0001068844 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0105      |
|    explained_variance   | 0.0361       |
|    learning_rate        | 0.001        |
|    loss                 | 3.29e+04     |
|    n_updates            | 6090         |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 7.45e+04     |
------------------------------------------
Eval num_timesteps=31500, episode_reward=498.49 +/- 738.49
Episode length: 35.78 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 498      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 450      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 31       |
|    time_elapsed    | 113      |
|    total_timesteps | 31744    |
---------------------------------
Eval num_timesteps=32000, episode_reward=421.99 +/- 745.52
Episode length: 34.94 +/- 7.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 422          |
| time/                   |              |
|    total_timesteps      | 32000        |
| train/                  |              |
|    approx_kl            | 0.0023408993 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00419     |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.001        |
|    loss                 | 5.07e+04     |
|    n_updates            | 6100         |
|    policy_gradient_loss | 0.000802     |
|    value_loss           | 9.64e+04     |
------------------------------------------
Eval num_timesteps=32500, episode_reward=458.17 +/- 687.98
Episode length: 35.16 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 458      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 454      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 32       |
|    time_elapsed    | 117      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=338.22 +/- 742.72
Episode length: 33.68 +/- 7.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.7         |
|    mean_reward          | 338          |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 8.579111e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00809     |
|    explained_variance   | 0.0346       |
|    learning_rate        | 0.001        |
|    loss                 | 3.77e+04     |
|    n_updates            | 6110         |
|    policy_gradient_loss | -0.000452    |
|    value_loss           | 9.33e+04     |
------------------------------------------
Eval num_timesteps=33500, episode_reward=423.61 +/- 710.71
Episode length: 36.10 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 489      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 33       |
|    time_elapsed    | 120      |
|    total_timesteps | 33792    |
---------------------------------
Eval num_timesteps=34000, episode_reward=426.85 +/- 775.68
Episode length: 34.24 +/- 7.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 427           |
| time/                   |               |
|    total_timesteps      | 34000         |
| train/                  |               |
|    approx_kl            | 0.00016865472 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00575      |
|    explained_variance   | -0.00488      |
|    learning_rate        | 0.001         |
|    loss                 | 4.95e+04      |
|    n_updates            | 6120          |
|    policy_gradient_loss | -0.000528     |
|    value_loss           | 1.27e+05      |
-------------------------------------------
Eval num_timesteps=34500, episode_reward=345.94 +/- 658.74
Episode length: 34.72 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 34       |
|    time_elapsed    | 124      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=460.51 +/- 746.78
Episode length: 35.34 +/- 7.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 461          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 9.634707e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0109      |
|    explained_variance   | -0.0465      |
|    learning_rate        | 0.001        |
|    loss                 | 5.63e+04     |
|    n_updates            | 6130         |
|    policy_gradient_loss | -0.00062     |
|    value_loss           | 9.74e+04     |
------------------------------------------
Eval num_timesteps=35500, episode_reward=533.37 +/- 775.03
Episode length: 35.84 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 486      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 35       |
|    time_elapsed    | 128      |
|    total_timesteps | 35840    |
---------------------------------
Eval num_timesteps=36000, episode_reward=485.21 +/- 727.28
Episode length: 36.14 +/- 6.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 485          |
| time/                   |              |
|    total_timesteps      | 36000        |
| train/                  |              |
|    approx_kl            | 0.0017415097 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.017       |
|    explained_variance   | -0.0388      |
|    learning_rate        | 0.001        |
|    loss                 | 2.55e+04     |
|    n_updates            | 6140         |
|    policy_gradient_loss | 0.00039      |
|    value_loss           | 8.86e+04     |
------------------------------------------
Eval num_timesteps=36500, episode_reward=439.56 +/- 660.52
Episode length: 35.82 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 440      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 441      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 36       |
|    time_elapsed    | 131      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=503.43 +/- 779.05
Episode length: 36.00 +/- 6.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 503          |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 8.509756e-05 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0147      |
|    explained_variance   | 0.139        |
|    learning_rate        | 0.001        |
|    loss                 | 4.22e+04     |
|    n_updates            | 6150         |
|    policy_gradient_loss | -0.000846    |
|    value_loss           | 8.79e+04     |
------------------------------------------
Eval num_timesteps=37500, episode_reward=482.78 +/- 734.07
Episode length: 35.64 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 483      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 495      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 37       |
|    time_elapsed    | 135      |
|    total_timesteps | 37888    |
---------------------------------
Eval num_timesteps=38000, episode_reward=569.95 +/- 747.38
Episode length: 36.36 +/- 6.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 570          |
| time/                   |              |
|    total_timesteps      | 38000        |
| train/                  |              |
|    approx_kl            | 7.270486e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0109      |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.001        |
|    loss                 | 4.11e+04     |
|    n_updates            | 6160         |
|    policy_gradient_loss | -0.000555    |
|    value_loss           | 9.45e+04     |
------------------------------------------
Eval num_timesteps=38500, episode_reward=517.65 +/- 758.55
Episode length: 36.22 +/- 7.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 527      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 38       |
|    time_elapsed    | 139      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=481.63 +/- 685.11
Episode length: 35.86 +/- 5.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 482           |
| time/                   |               |
|    total_timesteps      | 39000         |
| train/                  |               |
|    approx_kl            | 1.5443482e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00464      |
|    explained_variance   | -0.127        |
|    learning_rate        | 0.001         |
|    loss                 | 2.82e+04      |
|    n_updates            | 6170          |
|    policy_gradient_loss | -0.00158      |
|    value_loss           | 8.98e+04      |
-------------------------------------------
Eval num_timesteps=39500, episode_reward=263.94 +/- 595.29
Episode length: 33.88 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 535      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 39       |
|    time_elapsed    | 142      |
|    total_timesteps | 39936    |
---------------------------------
Eval num_timesteps=40000, episode_reward=528.79 +/- 757.71
Episode length: 36.32 +/- 6.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 529          |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 5.253649e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00633     |
|    explained_variance   | -0.0627      |
|    learning_rate        | 0.001        |
|    loss                 | 3.82e+04     |
|    n_updates            | 6180         |
|    policy_gradient_loss | -0.000527    |
|    value_loss           | 1.1e+05      |
------------------------------------------
Eval num_timesteps=40500, episode_reward=278.69 +/- 606.47
Episode length: 34.04 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 279      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 40       |
|    time_elapsed    | 146      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=411.11 +/- 684.03
Episode length: 35.54 +/- 6.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 411          |
| time/                   |              |
|    total_timesteps      | 41000        |
| train/                  |              |
|    approx_kl            | 4.001998e-05 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00965     |
|    explained_variance   | 0.0822       |
|    learning_rate        | 0.001        |
|    loss                 | 3.9e+04      |
|    n_updates            | 6190         |
|    policy_gradient_loss | -0.000391    |
|    value_loss           | 1.06e+05     |
------------------------------------------
Eval num_timesteps=41500, episode_reward=527.94 +/- 768.72
Episode length: 36.10 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 447      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 41       |
|    time_elapsed    | 149      |
|    total_timesteps | 41984    |
---------------------------------
Eval num_timesteps=42000, episode_reward=486.45 +/- 705.58
Episode length: 35.68 +/- 6.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 486           |
| time/                   |               |
|    total_timesteps      | 42000         |
| train/                  |               |
|    approx_kl            | 0.00035622483 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00732      |
|    explained_variance   | -0.238        |
|    learning_rate        | 0.001         |
|    loss                 | 3.45e+04      |
|    n_updates            | 6200          |
|    policy_gradient_loss | 0.00064       |
|    value_loss           | 9.25e+04      |
-------------------------------------------
Eval num_timesteps=42500, episode_reward=403.44 +/- 689.69
Episode length: 35.56 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=522.49 +/- 743.92
Episode length: 36.08 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 522      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 42       |
|    time_elapsed    | 155      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=430.47 +/- 803.86
Episode length: 33.80 +/- 8.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 430           |
| time/                   |               |
|    total_timesteps      | 43500         |
| train/                  |               |
|    approx_kl            | 3.2426906e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.005        |
|    explained_variance   | -0.149        |
|    learning_rate        | 0.001         |
|    loss                 | 3.83e+04      |
|    n_updates            | 6210          |
|    policy_gradient_loss | -0.000415     |
|    value_loss           | 8.51e+04      |
-------------------------------------------
Eval num_timesteps=44000, episode_reward=361.00 +/- 673.46
Episode length: 34.80 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 420      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 43       |
|    time_elapsed    | 158      |
|    total_timesteps | 44032    |
---------------------------------
Eval num_timesteps=44500, episode_reward=191.35 +/- 500.43
Episode length: 34.36 +/- 6.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 44500        |
| train/                  |              |
|    approx_kl            | 0.0036720438 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00436     |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.001        |
|    loss                 | 4.15e+04     |
|    n_updates            | 6220         |
|    policy_gradient_loss | -0.000182    |
|    value_loss           | 8.7e+04      |
------------------------------------------
Eval num_timesteps=45000, episode_reward=219.20 +/- 535.57
Episode length: 34.26 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 440      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 44       |
|    time_elapsed    | 162      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=641.84 +/- 730.34
Episode length: 36.32 +/- 7.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 642           |
| time/                   |               |
|    total_timesteps      | 45500         |
| train/                  |               |
|    approx_kl            | 1.3644458e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0016       |
|    explained_variance   | 0.0595        |
|    learning_rate        | 0.001         |
|    loss                 | 3.32e+04      |
|    n_updates            | 6230          |
|    policy_gradient_loss | -0.000192     |
|    value_loss           | 1.04e+05      |
-------------------------------------------
Eval num_timesteps=46000, episode_reward=517.73 +/- 706.23
Episode length: 36.66 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 554      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 45       |
|    time_elapsed    | 166      |
|    total_timesteps | 46080    |
---------------------------------
Eval num_timesteps=46500, episode_reward=330.65 +/- 610.74
Episode length: 35.38 +/- 5.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 331           |
| time/                   |               |
|    total_timesteps      | 46500         |
| train/                  |               |
|    approx_kl            | 2.1666056e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00167      |
|    explained_variance   | -0.205        |
|    learning_rate        | 0.001         |
|    loss                 | 3.18e+04      |
|    n_updates            | 6240          |
|    policy_gradient_loss | -0.000177     |
|    value_loss           | 1.04e+05      |
-------------------------------------------
Eval num_timesteps=47000, episode_reward=587.39 +/- 746.63
Episode length: 36.62 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 587      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 565      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 46       |
|    time_elapsed    | 169      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=600.17 +/- 746.03
Episode length: 36.90 +/- 6.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.9          |
|    mean_reward          | 600           |
| time/                   |               |
|    total_timesteps      | 47500         |
| train/                  |               |
|    approx_kl            | 3.3521792e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00651      |
|    explained_variance   | 0.0614        |
|    learning_rate        | 0.001         |
|    loss                 | 3.58e+04      |
|    n_updates            | 6250          |
|    policy_gradient_loss | -0.000312     |
|    value_loss           | 9.15e+04      |
-------------------------------------------
Eval num_timesteps=48000, episode_reward=347.69 +/- 680.02
Episode length: 34.54 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 348      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 482      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 47       |
|    time_elapsed    | 173      |
|    total_timesteps | 48128    |
---------------------------------
Eval num_timesteps=48500, episode_reward=343.36 +/- 708.69
Episode length: 34.32 +/- 6.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.3        |
|    mean_reward          | 343         |
| time/                   |             |
|    total_timesteps      | 48500       |
| train/                  |             |
|    approx_kl            | 0.009288112 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0257     |
|    explained_variance   | 0.112       |
|    learning_rate        | 0.001       |
|    loss                 | 3.55e+04    |
|    n_updates            | 6260        |
|    policy_gradient_loss | 0.00558     |
|    value_loss           | 9.73e+04    |
-----------------------------------------
Eval num_timesteps=49000, episode_reward=407.27 +/- 632.04
Episode length: 35.76 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 48       |
|    time_elapsed    | 177      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=432.24 +/- 748.40
Episode length: 34.74 +/- 7.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 432          |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0007261225 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0176      |
|    explained_variance   | 0.0683       |
|    learning_rate        | 0.001        |
|    loss                 | 3.96e+04     |
|    n_updates            | 6270         |
|    policy_gradient_loss | 0.00023      |
|    value_loss           | 8.3e+04      |
------------------------------------------
Eval num_timesteps=50000, episode_reward=257.58 +/- 602.39
Episode length: 33.96 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 293      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 49       |
|    time_elapsed    | 180      |
|    total_timesteps | 50176    |
---------------------------------
Eval num_timesteps=50500, episode_reward=488.83 +/- 761.36
Episode length: 35.66 +/- 6.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 489          |
| time/                   |              |
|    total_timesteps      | 50500        |
| train/                  |              |
|    approx_kl            | 4.538009e-05 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0301      |
|    explained_variance   | -0.165       |
|    learning_rate        | 0.001        |
|    loss                 | 1.91e+04     |
|    n_updates            | 6280         |
|    policy_gradient_loss | -0.000509    |
|    value_loss           | 4.54e+04     |
------------------------------------------
Eval num_timesteps=51000, episode_reward=471.78 +/- 744.44
Episode length: 35.38 +/- 7.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 285      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 50       |
|    time_elapsed    | 184      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=571.96 +/- 789.84
Episode length: 34.64 +/- 8.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.6        |
|    mean_reward          | 572         |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.012327539 |
|    clip_fraction        | 0.00283     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0326     |
|    explained_variance   | 0.127       |
|    learning_rate        | 0.001       |
|    loss                 | 1.75e+04    |
|    n_updates            | 6290        |
|    policy_gradient_loss | 0.016       |
|    value_loss           | 6.1e+04     |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=377.11 +/- 714.65
Episode length: 35.04 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 234      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 51       |
|    time_elapsed    | 188      |
|    total_timesteps | 52224    |
---------------------------------
Eval num_timesteps=52500, episode_reward=471.72 +/- 735.96
Episode length: 36.12 +/- 5.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 472          |
| time/                   |              |
|    total_timesteps      | 52500        |
| train/                  |              |
|    approx_kl            | 0.0017454826 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.025       |
|    explained_variance   | 0.0514       |
|    learning_rate        | 0.001        |
|    loss                 | 2.26e+04     |
|    n_updates            | 6300         |
|    policy_gradient_loss | 0.000382     |
|    value_loss           | 8.08e+04     |
------------------------------------------
Eval num_timesteps=53000, episode_reward=456.62 +/- 686.37
Episode length: 36.44 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 312      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 52       |
|    time_elapsed    | 191      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=404.50 +/- 677.96
Episode length: 35.88 +/- 5.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 405          |
| time/                   |              |
|    total_timesteps      | 53500        |
| train/                  |              |
|    approx_kl            | 0.0015326827 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0127      |
|    explained_variance   | 0.0669       |
|    learning_rate        | 0.001        |
|    loss                 | 4.68e+04     |
|    n_updates            | 6310         |
|    policy_gradient_loss | -0.000329    |
|    value_loss           | 9.2e+04      |
------------------------------------------
Eval num_timesteps=54000, episode_reward=652.56 +/- 815.98
Episode length: 36.80 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 653      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 397      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 53       |
|    time_elapsed    | 195      |
|    total_timesteps | 54272    |
---------------------------------
Eval num_timesteps=54500, episode_reward=424.18 +/- 676.22
Episode length: 35.66 +/- 6.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 424          |
| time/                   |              |
|    total_timesteps      | 54500        |
| train/                  |              |
|    approx_kl            | 0.0012448706 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00482     |
|    explained_variance   | -0.206       |
|    learning_rate        | 0.001        |
|    loss                 | 4.68e+04     |
|    n_updates            | 6320         |
|    policy_gradient_loss | -0.000863    |
|    value_loss           | 1.04e+05     |
------------------------------------------
Eval num_timesteps=55000, episode_reward=309.95 +/- 654.86
Episode length: 33.98 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 485      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 54       |
|    time_elapsed    | 199      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=516.07 +/- 786.58
Episode length: 35.52 +/- 7.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 516          |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 8.016976e-05 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00615     |
|    explained_variance   | -0.00056     |
|    learning_rate        | 0.001        |
|    loss                 | 3.07e+04     |
|    n_updates            | 6330         |
|    policy_gradient_loss | -0.000424    |
|    value_loss           | 9.92e+04     |
------------------------------------------
Eval num_timesteps=56000, episode_reward=431.82 +/- 718.20
Episode length: 35.32 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 572      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 55       |
|    time_elapsed    | 202      |
|    total_timesteps | 56320    |
---------------------------------
Eval num_timesteps=56500, episode_reward=551.41 +/- 753.00
Episode length: 36.52 +/- 6.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 551           |
| time/                   |               |
|    total_timesteps      | 56500         |
| train/                  |               |
|    approx_kl            | 9.6548465e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00185      |
|    explained_variance   | -0.0527       |
|    learning_rate        | 0.001         |
|    loss                 | 2.43e+04      |
|    n_updates            | 6340          |
|    policy_gradient_loss | -0.000411     |
|    value_loss           | 7.71e+04      |
-------------------------------------------
Eval num_timesteps=57000, episode_reward=617.84 +/- 793.33
Episode length: 36.66 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 509      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 56       |
|    time_elapsed    | 206      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=225.04 +/- 535.48
Episode length: 34.28 +/- 5.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 225          |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 0.0008988174 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00373     |
|    explained_variance   | 0.0248       |
|    learning_rate        | 0.001        |
|    loss                 | 4.05e+04     |
|    n_updates            | 6350         |
|    policy_gradient_loss | 0.000515     |
|    value_loss           | 1.01e+05     |
------------------------------------------
Eval num_timesteps=58000, episode_reward=321.73 +/- 647.14
Episode length: 34.68 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 480      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 57       |
|    time_elapsed    | 209      |
|    total_timesteps | 58368    |
---------------------------------
Eval num_timesteps=58500, episode_reward=358.73 +/- 649.58
Episode length: 34.60 +/- 5.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 359          |
| time/                   |              |
|    total_timesteps      | 58500        |
| train/                  |              |
|    approx_kl            | 7.204304e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.016       |
|    explained_variance   | 0.0141       |
|    learning_rate        | 0.001        |
|    loss                 | 3.12e+04     |
|    n_updates            | 6360         |
|    policy_gradient_loss | -0.000854    |
|    value_loss           | 8.95e+04     |
------------------------------------------
Eval num_timesteps=59000, episode_reward=360.52 +/- 660.15
Episode length: 35.40 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 368      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 58       |
|    time_elapsed    | 213      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=527.84 +/- 788.18
Episode length: 35.70 +/- 6.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 528           |
| time/                   |               |
|    total_timesteps      | 59500         |
| train/                  |               |
|    approx_kl            | 0.00049075234 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0169       |
|    explained_variance   | 0.00193       |
|    learning_rate        | 0.001         |
|    loss                 | 4.84e+04      |
|    n_updates            | 6370          |
|    policy_gradient_loss | -0.000268     |
|    value_loss           | 1e+05         |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=346.80 +/- 609.65
Episode length: 35.72 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 347      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 59       |
|    time_elapsed    | 217      |
|    total_timesteps | 60416    |
---------------------------------
Eval num_timesteps=60500, episode_reward=423.02 +/- 723.72
Episode length: 35.70 +/- 6.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 423           |
| time/                   |               |
|    total_timesteps      | 60500         |
| train/                  |               |
|    approx_kl            | 5.2129326e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0178       |
|    explained_variance   | -0.16         |
|    learning_rate        | 0.001         |
|    loss                 | 3.58e+04      |
|    n_updates            | 6380          |
|    policy_gradient_loss | -3.38e-05     |
|    value_loss           | 9.12e+04      |
-------------------------------------------
Eval num_timesteps=61000, episode_reward=300.91 +/- 707.79
Episode length: 33.52 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 301      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 60       |
|    time_elapsed    | 220      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=334.69 +/- 641.76
Episode length: 34.94 +/- 6.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 335           |
| time/                   |               |
|    total_timesteps      | 61500         |
| train/                  |               |
|    approx_kl            | 1.1600554e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0216       |
|    explained_variance   | 0.0202        |
|    learning_rate        | 0.001         |
|    loss                 | 2.58e+04      |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.000396     |
|    value_loss           | 8.11e+04      |
-------------------------------------------
Eval num_timesteps=62000, episode_reward=429.19 +/- 656.11
Episode length: 35.88 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 61       |
|    time_elapsed    | 224      |
|    total_timesteps | 62464    |
---------------------------------
Eval num_timesteps=62500, episode_reward=411.99 +/- 741.11
Episode length: 34.10 +/- 6.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.1          |
|    mean_reward          | 412           |
| time/                   |               |
|    total_timesteps      | 62500         |
| train/                  |               |
|    approx_kl            | 4.7513167e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0248       |
|    explained_variance   | 0.0954        |
|    learning_rate        | 0.001         |
|    loss                 | 3.55e+04      |
|    n_updates            | 6400          |
|    policy_gradient_loss | -0.000368     |
|    value_loss           | 7.44e+04      |
-------------------------------------------
Eval num_timesteps=63000, episode_reward=525.19 +/- 767.23
Episode length: 35.24 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 525      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 62       |
|    time_elapsed    | 227      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=501.59 +/- 786.11
Episode length: 35.26 +/- 6.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 502          |
| time/                   |              |
|    total_timesteps      | 63500        |
| train/                  |              |
|    approx_kl            | 0.0008171442 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0118      |
|    explained_variance   | -0.223       |
|    learning_rate        | 0.001        |
|    loss                 | 2.92e+04     |
|    n_updates            | 6410         |
|    policy_gradient_loss | -0.000429    |
|    value_loss           | 6.7e+04      |
------------------------------------------
Eval num_timesteps=64000, episode_reward=425.04 +/- 711.01
Episode length: 35.14 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=467.00 +/- 823.28
Episode length: 34.64 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 467      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 461      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 63       |
|    time_elapsed    | 232      |
|    total_timesteps | 64512    |
---------------------------------
Eval num_timesteps=65000, episode_reward=553.93 +/- 748.22
Episode length: 36.58 +/- 6.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.6        |
|    mean_reward          | 554         |
| time/                   |             |
|    total_timesteps      | 65000       |
| train/                  |             |
|    approx_kl            | 2.86976e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00425    |
|    explained_variance   | -0.0503     |
|    learning_rate        | 0.001       |
|    loss                 | 3.73e+04    |
|    n_updates            | 6420        |
|    policy_gradient_loss | -0.000567   |
|    value_loss           | 8.35e+04    |
-----------------------------------------
Eval num_timesteps=65500, episode_reward=381.25 +/- 680.42
Episode length: 35.52 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 513      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 64       |
|    time_elapsed    | 236      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=441.87 +/- 762.92
Episode length: 34.72 +/- 7.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 442          |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 6.825704e-05 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00714     |
|    explained_variance   | -0.143       |
|    learning_rate        | 0.001        |
|    loss                 | 4.15e+04     |
|    n_updates            | 6430         |
|    policy_gradient_loss | -0.00027     |
|    value_loss           | 1.06e+05     |
------------------------------------------
Eval num_timesteps=66500, episode_reward=452.50 +/- 743.74
Episode length: 34.94 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 482      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 65       |
|    time_elapsed    | 239      |
|    total_timesteps | 66560    |
---------------------------------
Eval num_timesteps=67000, episode_reward=445.27 +/- 702.95
Episode length: 35.44 +/- 6.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 445          |
| time/                   |              |
|    total_timesteps      | 67000        |
| train/                  |              |
|    approx_kl            | 4.315574e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0129      |
|    explained_variance   | -0.0161      |
|    learning_rate        | 0.001        |
|    loss                 | 2.24e+04     |
|    n_updates            | 6440         |
|    policy_gradient_loss | -0.000252    |
|    value_loss           | 7.34e+04     |
------------------------------------------
Eval num_timesteps=67500, episode_reward=511.68 +/- 765.75
Episode length: 35.00 +/- 7.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 413      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 66       |
|    time_elapsed    | 243      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=336.58 +/- 648.43
Episode length: 35.22 +/- 6.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 337           |
| time/                   |               |
|    total_timesteps      | 68000         |
| train/                  |               |
|    approx_kl            | 1.8647988e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00996      |
|    explained_variance   | -0.0267       |
|    learning_rate        | 0.001         |
|    loss                 | 4.55e+04      |
|    n_updates            | 6450          |
|    policy_gradient_loss | -9.36e-05     |
|    value_loss           | 8.85e+04      |
-------------------------------------------
Eval num_timesteps=68500, episode_reward=413.00 +/- 705.15
Episode length: 35.52 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 413      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 399      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 67       |
|    time_elapsed    | 247      |
|    total_timesteps | 68608    |
---------------------------------
Eval num_timesteps=69000, episode_reward=405.51 +/- 801.65
Episode length: 34.58 +/- 7.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 406          |
| time/                   |              |
|    total_timesteps      | 69000        |
| train/                  |              |
|    approx_kl            | 0.0010436419 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00739     |
|    explained_variance   | 0.13         |
|    learning_rate        | 0.001        |
|    loss                 | 3.46e+04     |
|    n_updates            | 6460         |
|    policy_gradient_loss | -0.000416    |
|    value_loss           | 8.15e+04     |
------------------------------------------
Eval num_timesteps=69500, episode_reward=582.82 +/- 758.03
Episode length: 36.38 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 583      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 440      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 68       |
|    time_elapsed    | 250      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=462.97 +/- 649.96
Episode length: 37.06 +/- 4.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.1          |
|    mean_reward          | 463           |
| time/                   |               |
|    total_timesteps      | 70000         |
| train/                  |               |
|    approx_kl            | 0.00012211187 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00818      |
|    explained_variance   | -0.0304       |
|    learning_rate        | 0.001         |
|    loss                 | 5.23e+04      |
|    n_updates            | 6470          |
|    policy_gradient_loss | -0.00022      |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=70500, episode_reward=327.94 +/- 617.66
Episode length: 34.74 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 328      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 445      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 69       |
|    time_elapsed    | 254      |
|    total_timesteps | 70656    |
---------------------------------
Eval num_timesteps=71000, episode_reward=285.41 +/- 729.39
Episode length: 32.74 +/- 7.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.7         |
|    mean_reward          | 285          |
| time/                   |              |
|    total_timesteps      | 71000        |
| train/                  |              |
|    approx_kl            | 2.615794e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00657     |
|    explained_variance   | 0.00476      |
|    learning_rate        | 0.001        |
|    loss                 | 5.23e+04     |
|    n_updates            | 6480         |
|    policy_gradient_loss | -0.000323    |
|    value_loss           | 1.21e+05     |
------------------------------------------
Eval num_timesteps=71500, episode_reward=491.82 +/- 762.06
Episode length: 35.64 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 492      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 516      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 70       |
|    time_elapsed    | 257      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=438.06 +/- 719.86
Episode length: 35.90 +/- 6.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 438           |
| time/                   |               |
|    total_timesteps      | 72000         |
| train/                  |               |
|    approx_kl            | 1.5732367e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00898      |
|    explained_variance   | -0.0268       |
|    learning_rate        | 0.001         |
|    loss                 | 2.32e+04      |
|    n_updates            | 6490          |
|    policy_gradient_loss | -0.000207     |
|    value_loss           | 8.91e+04      |
-------------------------------------------
Eval num_timesteps=72500, episode_reward=396.42 +/- 753.60
Episode length: 34.08 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 493      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 71       |
|    time_elapsed    | 261      |
|    total_timesteps | 72704    |
---------------------------------
Eval num_timesteps=73000, episode_reward=479.08 +/- 763.49
Episode length: 35.30 +/- 6.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 479           |
| time/                   |               |
|    total_timesteps      | 73000         |
| train/                  |               |
|    approx_kl            | 2.8769718e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00962      |
|    explained_variance   | 0.105         |
|    learning_rate        | 0.001         |
|    loss                 | 3.5e+04       |
|    n_updates            | 6500          |
|    policy_gradient_loss | -0.00021      |
|    value_loss           | 9.3e+04       |
-------------------------------------------
Eval num_timesteps=73500, episode_reward=411.63 +/- 726.62
Episode length: 34.94 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 464      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 72       |
|    time_elapsed    | 265      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=189.40 +/- 582.67
Episode length: 32.92 +/- 7.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.9          |
|    mean_reward          | 189           |
| time/                   |               |
|    total_timesteps      | 74000         |
| train/                  |               |
|    approx_kl            | 2.1890155e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00733      |
|    explained_variance   | -0.1          |
|    learning_rate        | 0.001         |
|    loss                 | 3.67e+04      |
|    n_updates            | 6510          |
|    policy_gradient_loss | -0.000212     |
|    value_loss           | 1.02e+05      |
-------------------------------------------
Eval num_timesteps=74500, episode_reward=541.50 +/- 710.85
Episode length: 36.36 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 595      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 73       |
|    time_elapsed    | 268      |
|    total_timesteps | 74752    |
---------------------------------
Eval num_timesteps=75000, episode_reward=484.43 +/- 822.12
Episode length: 34.48 +/- 7.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 484           |
| time/                   |               |
|    total_timesteps      | 75000         |
| train/                  |               |
|    approx_kl            | 1.4429097e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00123      |
|    explained_variance   | -0.0266       |
|    learning_rate        | 0.001         |
|    loss                 | 3.36e+04      |
|    n_updates            | 6520          |
|    policy_gradient_loss | -0.000323     |
|    value_loss           | 8.33e+04      |
-------------------------------------------
Eval num_timesteps=75500, episode_reward=594.92 +/- 780.55
Episode length: 35.88 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 595      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 495      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 74       |
|    time_elapsed    | 272      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=477.44 +/- 699.90
Episode length: 35.86 +/- 6.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 477           |
| time/                   |               |
|    total_timesteps      | 76000         |
| train/                  |               |
|    approx_kl            | 3.7701102e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00943      |
|    explained_variance   | 0.0349        |
|    learning_rate        | 0.001         |
|    loss                 | 2.68e+04      |
|    n_updates            | 6530          |
|    policy_gradient_loss | -0.000558     |
|    value_loss           | 7.51e+04      |
-------------------------------------------
Eval num_timesteps=76500, episode_reward=371.20 +/- 751.07
Episode length: 34.12 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 483      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 75       |
|    time_elapsed    | 275      |
|    total_timesteps | 76800    |
---------------------------------
Eval num_timesteps=77000, episode_reward=511.53 +/- 738.72
Episode length: 35.92 +/- 6.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 512           |
| time/                   |               |
|    total_timesteps      | 77000         |
| train/                  |               |
|    approx_kl            | 5.0770934e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0255       |
|    explained_variance   | -0.237        |
|    learning_rate        | 0.001         |
|    loss                 | 4e+04         |
|    n_updates            | 6540          |
|    policy_gradient_loss | -0.000721     |
|    value_loss           | 7.71e+04      |
-------------------------------------------
Eval num_timesteps=77500, episode_reward=433.34 +/- 754.01
Episode length: 34.14 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 360      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 76       |
|    time_elapsed    | 279      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=497.77 +/- 746.18
Episode length: 36.06 +/- 6.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 498           |
| time/                   |               |
|    total_timesteps      | 78000         |
| train/                  |               |
|    approx_kl            | 0.00021367858 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0185       |
|    explained_variance   | 0.0933        |
|    learning_rate        | 0.001         |
|    loss                 | 4.18e+04      |
|    n_updates            | 6550          |
|    policy_gradient_loss | -0.000628     |
|    value_loss           | 8.19e+04      |
-------------------------------------------
Eval num_timesteps=78500, episode_reward=336.18 +/- 746.25
Episode length: 33.94 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 336      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 77       |
|    time_elapsed    | 283      |
|    total_timesteps | 78848    |
---------------------------------
Eval num_timesteps=79000, episode_reward=547.77 +/- 781.75
Episode length: 35.34 +/- 6.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 548          |
| time/                   |              |
|    total_timesteps      | 79000        |
| train/                  |              |
|    approx_kl            | 5.805504e-05 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0124      |
|    explained_variance   | 0.0332       |
|    learning_rate        | 0.001        |
|    loss                 | 4.62e+04     |
|    n_updates            | 6560         |
|    policy_gradient_loss | -0.000343    |
|    value_loss           | 1.08e+05     |
------------------------------------------
Eval num_timesteps=79500, episode_reward=463.13 +/- 805.30
Episode length: 34.04 +/- 7.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 423      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 78       |
|    time_elapsed    | 286      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=474.31 +/- 774.52
Episode length: 35.38 +/- 6.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 474         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.013356894 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0154     |
|    explained_variance   | 0.057       |
|    learning_rate        | 0.001       |
|    loss                 | 4.07e+04    |
|    n_updates            | 6570        |
|    policy_gradient_loss | 0.0266      |
|    value_loss           | 9.73e+04    |
-----------------------------------------
Eval num_timesteps=80500, episode_reward=366.21 +/- 643.62
Episode length: 35.04 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 381      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 79       |
|    time_elapsed    | 290      |
|    total_timesteps | 80896    |
---------------------------------
Eval num_timesteps=81000, episode_reward=451.79 +/- 713.15
Episode length: 35.96 +/- 6.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 452           |
| time/                   |               |
|    total_timesteps      | 81000         |
| train/                  |               |
|    approx_kl            | 6.2271138e-06 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00691      |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.001         |
|    loss                 | 2.96e+04      |
|    n_updates            | 6580          |
|    policy_gradient_loss | -0.000303     |
|    value_loss           | 9.38e+04      |
-------------------------------------------
Eval num_timesteps=81500, episode_reward=378.77 +/- 692.14
Episode length: 34.58 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 379      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 485      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 80       |
|    time_elapsed    | 294      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=469.06 +/- 762.58
Episode length: 35.38 +/- 7.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 469          |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0016246935 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00392     |
|    explained_variance   | 0.0102       |
|    learning_rate        | 0.001        |
|    loss                 | 3.41e+04     |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.000304    |
|    value_loss           | 1.04e+05     |
------------------------------------------
Eval num_timesteps=82500, episode_reward=485.49 +/- 778.44
Episode length: 35.72 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 485      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 508      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 81       |
|    time_elapsed    | 297      |
|    total_timesteps | 82944    |
---------------------------------
Eval num_timesteps=83000, episode_reward=442.42 +/- 720.92
Episode length: 35.42 +/- 6.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 442           |
| time/                   |               |
|    total_timesteps      | 83000         |
| train/                  |               |
|    approx_kl            | 0.00076893193 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00521      |
|    explained_variance   | 0.142         |
|    learning_rate        | 0.001         |
|    loss                 | 4.25e+04      |
|    n_updates            | 6600          |
|    policy_gradient_loss | -0.000161     |
|    value_loss           | 1.05e+05      |
-------------------------------------------
Eval num_timesteps=83500, episode_reward=441.49 +/- 692.76
Episode length: 35.66 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 468      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 82       |
|    time_elapsed    | 301      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=697.02 +/- 814.28
Episode length: 37.12 +/- 6.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.1         |
|    mean_reward          | 697          |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 7.726485e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00833     |
|    explained_variance   | 0.0925       |
|    learning_rate        | 0.001        |
|    loss                 | 4.97e+04     |
|    n_updates            | 6610         |
|    policy_gradient_loss | -0.000155    |
|    value_loss           | 1.05e+05     |
------------------------------------------
New best mean reward!
Eval num_timesteps=84500, episode_reward=372.91 +/- 666.73
Episode length: 34.52 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 373      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 437      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 83       |
|    time_elapsed    | 305      |
|    total_timesteps | 84992    |
---------------------------------
Eval num_timesteps=85000, episode_reward=351.70 +/- 688.71
Episode length: 35.10 +/- 6.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 352           |
| time/                   |               |
|    total_timesteps      | 85000         |
| train/                  |               |
|    approx_kl            | 1.5781843e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0108       |
|    explained_variance   | -0.178        |
|    learning_rate        | 0.001         |
|    loss                 | 5.06e+04      |
|    n_updates            | 6620          |
|    policy_gradient_loss | -0.000265     |
|    value_loss           | 9.73e+04      |
-------------------------------------------
Eval num_timesteps=85500, episode_reward=524.73 +/- 762.02
Episode length: 36.12 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 525      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=357.82 +/- 733.76
Episode length: 34.34 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 84       |
|    time_elapsed    | 310      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=592.15 +/- 791.87
Episode length: 36.74 +/- 5.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.7         |
|    mean_reward          | 592          |
| time/                   |              |
|    total_timesteps      | 86500        |
| train/                  |              |
|    approx_kl            | 9.480864e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0116      |
|    explained_variance   | -0.0541      |
|    learning_rate        | 0.001        |
|    loss                 | 3.27e+04     |
|    n_updates            | 6630         |
|    policy_gradient_loss | -0.000189    |
|    value_loss           | 7.59e+04     |
------------------------------------------
Eval num_timesteps=87000, episode_reward=309.98 +/- 676.01
Episode length: 33.00 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 85       |
|    time_elapsed    | 313      |
|    total_timesteps | 87040    |
---------------------------------
Eval num_timesteps=87500, episode_reward=650.01 +/- 780.63
Episode length: 36.68 +/- 6.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.7          |
|    mean_reward          | 650           |
| time/                   |               |
|    total_timesteps      | 87500         |
| train/                  |               |
|    approx_kl            | 0.00053183944 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00859      |
|    explained_variance   | 0.0163        |
|    learning_rate        | 0.001         |
|    loss                 | 3.43e+04      |
|    n_updates            | 6640          |
|    policy_gradient_loss | 0.000194      |
|    value_loss           | 9.3e+04       |
-------------------------------------------
Eval num_timesteps=88000, episode_reward=349.74 +/- 646.19
Episode length: 35.40 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 86       |
|    time_elapsed    | 317      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=512.57 +/- 763.03
Episode length: 35.58 +/- 7.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 513          |
| time/                   |              |
|    total_timesteps      | 88500        |
| train/                  |              |
|    approx_kl            | 0.0021865307 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0103      |
|    explained_variance   | -0.0456      |
|    learning_rate        | 0.001        |
|    loss                 | 4.78e+04     |
|    n_updates            | 6650         |
|    policy_gradient_loss | -0.000558    |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=89000, episode_reward=531.65 +/- 712.50
Episode length: 36.84 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 532      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 87       |
|    time_elapsed    | 321      |
|    total_timesteps | 89088    |
---------------------------------
Eval num_timesteps=89500, episode_reward=372.47 +/- 710.74
Episode length: 34.36 +/- 6.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | 372         |
| time/                   |             |
|    total_timesteps      | 89500       |
| train/                  |             |
|    approx_kl            | 2.47336e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0158     |
|    explained_variance   | -0.143      |
|    learning_rate        | 0.001       |
|    loss                 | 3.5e+04     |
|    n_updates            | 6660        |
|    policy_gradient_loss | -0.000338   |
|    value_loss           | 9.25e+04    |
-----------------------------------------
Eval num_timesteps=90000, episode_reward=529.96 +/- 741.02
Episode length: 35.80 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 432      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 88       |
|    time_elapsed    | 324      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=300.03 +/- 574.10
Episode length: 34.76 +/- 5.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 300           |
| time/                   |               |
|    total_timesteps      | 90500         |
| train/                  |               |
|    approx_kl            | 6.6427747e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0172       |
|    explained_variance   | 0.0303        |
|    learning_rate        | 0.001         |
|    loss                 | 4.3e+04       |
|    n_updates            | 6670          |
|    policy_gradient_loss | -0.000365     |
|    value_loss           | 1.04e+05      |
-------------------------------------------
Eval num_timesteps=91000, episode_reward=289.07 +/- 615.56
Episode length: 33.88 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 422      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 89       |
|    time_elapsed    | 328      |
|    total_timesteps | 91136    |
---------------------------------
Eval num_timesteps=91500, episode_reward=408.26 +/- 698.84
Episode length: 35.60 +/- 5.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 408          |
| time/                   |              |
|    total_timesteps      | 91500        |
| train/                  |              |
|    approx_kl            | 4.455447e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0147      |
|    explained_variance   | -0.187       |
|    learning_rate        | 0.001        |
|    loss                 | 2.46e+04     |
|    n_updates            | 6680         |
|    policy_gradient_loss | -0.000345    |
|    value_loss           | 9.02e+04     |
------------------------------------------
Eval num_timesteps=92000, episode_reward=349.66 +/- 677.96
Episode length: 35.08 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 389      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 90       |
|    time_elapsed    | 331      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=330.06 +/- 686.44
Episode length: 34.30 +/- 6.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 330          |
| time/                   |              |
|    total_timesteps      | 92500        |
| train/                  |              |
|    approx_kl            | 0.0036446676 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0152      |
|    explained_variance   | 0.0632       |
|    learning_rate        | 0.001        |
|    loss                 | 2.93e+04     |
|    n_updates            | 6690         |
|    policy_gradient_loss | 0.000558     |
|    value_loss           | 8.38e+04     |
------------------------------------------
Eval num_timesteps=93000, episode_reward=349.74 +/- 669.37
Episode length: 35.22 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 306      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 91       |
|    time_elapsed    | 335      |
|    total_timesteps | 93184    |
---------------------------------
Eval num_timesteps=93500, episode_reward=267.55 +/- 688.59
Episode length: 33.56 +/- 7.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 268          |
| time/                   |              |
|    total_timesteps      | 93500        |
| train/                  |              |
|    approx_kl            | 0.0010162736 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0145      |
|    explained_variance   | 0.0411       |
|    learning_rate        | 0.001        |
|    loss                 | 3.15e+04     |
|    n_updates            | 6700         |
|    policy_gradient_loss | -4.57e-05    |
|    value_loss           | 8.68e+04     |
------------------------------------------
Eval num_timesteps=94000, episode_reward=398.17 +/- 673.05
Episode length: 35.26 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 303      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 92       |
|    time_elapsed    | 338      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=545.43 +/- 777.84
Episode length: 36.22 +/- 5.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 545           |
| time/                   |               |
|    total_timesteps      | 94500         |
| train/                  |               |
|    approx_kl            | 2.3947214e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.01         |
|    explained_variance   | -0.0807       |
|    learning_rate        | 0.001         |
|    loss                 | 2.94e+04      |
|    n_updates            | 6710          |
|    policy_gradient_loss | -0.000626     |
|    value_loss           | 8.44e+04      |
-------------------------------------------
Eval num_timesteps=95000, episode_reward=555.93 +/- 751.60
Episode length: 36.58 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 556      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 93       |
|    time_elapsed    | 342      |
|    total_timesteps | 95232    |
---------------------------------
Eval num_timesteps=95500, episode_reward=190.68 +/- 557.86
Episode length: 34.16 +/- 5.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 191          |
| time/                   |              |
|    total_timesteps      | 95500        |
| train/                  |              |
|    approx_kl            | 0.0007071287 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00978     |
|    explained_variance   | -0.0102      |
|    learning_rate        | 0.001        |
|    loss                 | 2.71e+04     |
|    n_updates            | 6720         |
|    policy_gradient_loss | 0.000276     |
|    value_loss           | 1e+05        |
------------------------------------------
Eval num_timesteps=96000, episode_reward=242.23 +/- 547.39
Episode length: 34.92 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 282      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 94       |
|    time_elapsed    | 346      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=337.75 +/- 719.95
Episode length: 33.94 +/- 6.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 338           |
| time/                   |               |
|    total_timesteps      | 96500         |
| train/                  |               |
|    approx_kl            | 4.0885236e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0149       |
|    explained_variance   | -0.129        |
|    learning_rate        | 0.001         |
|    loss                 | 3.32e+04      |
|    n_updates            | 6730          |
|    policy_gradient_loss | -0.000331     |
|    value_loss           | 8.17e+04      |
-------------------------------------------
Eval num_timesteps=97000, episode_reward=332.04 +/- 685.64
Episode length: 34.08 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 95       |
|    time_elapsed    | 349      |
|    total_timesteps | 97280    |
---------------------------------
Eval num_timesteps=97500, episode_reward=446.48 +/- 682.45
Episode length: 36.08 +/- 5.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 446          |
| time/                   |              |
|    total_timesteps      | 97500        |
| train/                  |              |
|    approx_kl            | 0.0009976933 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0108      |
|    explained_variance   | 0.0653       |
|    learning_rate        | 0.001        |
|    loss                 | 2e+04        |
|    n_updates            | 6740         |
|    policy_gradient_loss | -4.16e-05    |
|    value_loss           | 7.01e+04     |
------------------------------------------
Eval num_timesteps=98000, episode_reward=496.86 +/- 784.32
Episode length: 35.62 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 253      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 96       |
|    time_elapsed    | 353      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=630.04 +/- 772.08
Episode length: 36.36 +/- 7.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 630           |
| time/                   |               |
|    total_timesteps      | 98500         |
| train/                  |               |
|    approx_kl            | 1.3135141e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | -0.192        |
|    learning_rate        | 0.001         |
|    loss                 | 2.74e+04      |
|    n_updates            | 6750          |
|    policy_gradient_loss | -0.000301     |
|    value_loss           | 6.33e+04      |
-------------------------------------------
Eval num_timesteps=99000, episode_reward=409.97 +/- 699.78
Episode length: 35.32 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 410      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 299      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 97       |
|    time_elapsed    | 357      |
|    total_timesteps | 99328    |
---------------------------------
Eval num_timesteps=99500, episode_reward=399.72 +/- 758.97
Episode length: 34.32 +/- 6.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 400          |
| time/                   |              |
|    total_timesteps      | 99500        |
| train/                  |              |
|    approx_kl            | 9.415904e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00934     |
|    explained_variance   | 0.0928       |
|    learning_rate        | 0.001        |
|    loss                 | 2.13e+04     |
|    n_updates            | 6760         |
|    policy_gradient_loss | -0.000292    |
|    value_loss           | 1.03e+05     |
------------------------------------------
Eval num_timesteps=100000, episode_reward=413.64 +/- 717.03
Episode length: 35.14 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 98       |
|    time_elapsed    | 360      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=586.34 +/- 777.39
Episode length: 36.14 +/- 6.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 586           |
| time/                   |               |
|    total_timesteps      | 100500        |
| train/                  |               |
|    approx_kl            | 0.00035494223 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00546      |
|    explained_variance   | -0.101        |
|    learning_rate        | 0.001         |
|    loss                 | 2.1e+04       |
|    n_updates            | 6770          |
|    policy_gradient_loss | -0.000211     |
|    value_loss           | 7.39e+04      |
-------------------------------------------
Eval num_timesteps=101000, episode_reward=572.89 +/- 737.32
Episode length: 36.76 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 573      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 452      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 99       |
|    time_elapsed    | 364      |
|    total_timesteps | 101376   |
---------------------------------
Eval num_timesteps=101500, episode_reward=387.80 +/- 715.93
Episode length: 35.04 +/- 5.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 388           |
| time/                   |               |
|    total_timesteps      | 101500        |
| train/                  |               |
|    approx_kl            | 1.4364487e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00848      |
|    explained_variance   | 0.0135        |
|    learning_rate        | 0.001         |
|    loss                 | 3.55e+04      |
|    n_updates            | 6780          |
|    policy_gradient_loss | -0.000186     |
|    value_loss           | 8.46e+04      |
-------------------------------------------
Eval num_timesteps=102000, episode_reward=494.62 +/- 749.76
Episode length: 36.70 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 463      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 100      |
|    time_elapsed    | 368      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=518.23 +/- 758.72
Episode length: 35.88 +/- 7.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 518           |
| time/                   |               |
|    total_timesteps      | 102500        |
| train/                  |               |
|    approx_kl            | 0.00053968123 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00775      |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.001         |
|    loss                 | 3.24e+04      |
|    n_updates            | 6790          |
|    policy_gradient_loss | -9.53e-05     |
|    value_loss           | 8.26e+04      |
-------------------------------------------
Eval num_timesteps=103000, episode_reward=595.62 +/- 749.78
Episode length: 36.56 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 596      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 430      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 101      |
|    time_elapsed    | 371      |
|    total_timesteps | 103424   |
---------------------------------
Eval num_timesteps=103500, episode_reward=423.65 +/- 638.91
Episode length: 36.00 +/- 5.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 424          |
| time/                   |              |
|    total_timesteps      | 103500       |
| train/                  |              |
|    approx_kl            | 0.0002475304 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00861     |
|    explained_variance   | -0.0747      |
|    learning_rate        | 0.001        |
|    loss                 | 3.39e+04     |
|    n_updates            | 6800         |
|    policy_gradient_loss | -0.000428    |
|    value_loss           | 8.58e+04     |
------------------------------------------
Eval num_timesteps=104000, episode_reward=438.94 +/- 723.40
Episode length: 35.74 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 447      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 102      |
|    time_elapsed    | 375      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=556.52 +/- 754.64
Episode length: 36.28 +/- 5.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 557           |
| time/                   |               |
|    total_timesteps      | 104500        |
| train/                  |               |
|    approx_kl            | 1.0550721e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00761      |
|    explained_variance   | 0.0572        |
|    learning_rate        | 0.001         |
|    loss                 | 4.46e+04      |
|    n_updates            | 6810          |
|    policy_gradient_loss | -0.000226     |
|    value_loss           | 9.96e+04      |
-------------------------------------------
Eval num_timesteps=105000, episode_reward=385.42 +/- 680.55
Episode length: 35.90 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 443      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 103      |
|    time_elapsed    | 379      |
|    total_timesteps | 105472   |
---------------------------------
Eval num_timesteps=105500, episode_reward=449.96 +/- 742.12
Episode length: 35.32 +/- 6.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 450           |
| time/                   |               |
|    total_timesteps      | 105500        |
| train/                  |               |
|    approx_kl            | 2.2049062e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00626      |
|    explained_variance   | -0.00155      |
|    learning_rate        | 0.001         |
|    loss                 | 3.08e+04      |
|    n_updates            | 6820          |
|    policy_gradient_loss | -0.000103     |
|    value_loss           | 7.49e+04      |
-------------------------------------------
Eval num_timesteps=106000, episode_reward=311.01 +/- 629.00
Episode length: 34.80 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 443      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 104      |
|    time_elapsed    | 382      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=312.89 +/- 656.36
Episode length: 35.44 +/- 5.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 313         |
| time/                   |             |
|    total_timesteps      | 106500      |
| train/                  |             |
|    approx_kl            | 0.001000212 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00374    |
|    explained_variance   | -0.0415     |
|    learning_rate        | 0.001       |
|    loss                 | 4.08e+04    |
|    n_updates            | 6830        |
|    policy_gradient_loss | 0.000365    |
|    value_loss           | 9.56e+04    |
-----------------------------------------
Eval num_timesteps=107000, episode_reward=473.42 +/- 717.00
Episode length: 35.52 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=401.47 +/- 702.06
Episode length: 35.58 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 335      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 105      |
|    time_elapsed    | 387      |
|    total_timesteps | 107520   |
---------------------------------
Eval num_timesteps=108000, episode_reward=555.80 +/- 750.63
Episode length: 35.86 +/- 8.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 556           |
| time/                   |               |
|    total_timesteps      | 108000        |
| train/                  |               |
|    approx_kl            | 3.9609207e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00996      |
|    explained_variance   | 0.0371        |
|    learning_rate        | 0.001         |
|    loss                 | 2.44e+04      |
|    n_updates            | 6840          |
|    policy_gradient_loss | 0.000574      |
|    value_loss           | 7.34e+04      |
-------------------------------------------
Eval num_timesteps=108500, episode_reward=365.65 +/- 696.21
Episode length: 35.44 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 305      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 106      |
|    time_elapsed    | 391      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=380.74 +/- 704.75
Episode length: 34.66 +/- 6.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 381           |
| time/                   |               |
|    total_timesteps      | 109000        |
| train/                  |               |
|    approx_kl            | 1.3072859e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00912      |
|    explained_variance   | 0.0622        |
|    learning_rate        | 0.001         |
|    loss                 | 3.93e+04      |
|    n_updates            | 6850          |
|    policy_gradient_loss | -0.000195     |
|    value_loss           | 8.22e+04      |
-------------------------------------------
Eval num_timesteps=109500, episode_reward=419.61 +/- 715.63
Episode length: 34.98 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 365      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 107      |
|    time_elapsed    | 394      |
|    total_timesteps | 109568   |
---------------------------------
Eval num_timesteps=110000, episode_reward=438.71 +/- 759.34
Episode length: 34.92 +/- 6.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 439           |
| time/                   |               |
|    total_timesteps      | 110000        |
| train/                  |               |
|    approx_kl            | 2.3225439e-06 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00692      |
|    explained_variance   | -0.0597       |
|    learning_rate        | 0.001         |
|    loss                 | 3.28e+04      |
|    n_updates            | 6860          |
|    policy_gradient_loss | -0.000221     |
|    value_loss           | 1.03e+05      |
-------------------------------------------
Eval num_timesteps=110500, episode_reward=357.26 +/- 678.96
Episode length: 34.90 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 357      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 403      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 108      |
|    time_elapsed    | 398      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=449.90 +/- 764.66
Episode length: 35.02 +/- 6.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 450           |
| time/                   |               |
|    total_timesteps      | 111000        |
| train/                  |               |
|    approx_kl            | 2.6125927e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00585      |
|    explained_variance   | 0.0163        |
|    learning_rate        | 0.001         |
|    loss                 | 3.6e+04       |
|    n_updates            | 6870          |
|    policy_gradient_loss | -0.000233     |
|    value_loss           | 1.07e+05      |
-------------------------------------------
Eval num_timesteps=111500, episode_reward=666.08 +/- 775.60
Episode length: 37.18 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 666      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 109      |
|    time_elapsed    | 402      |
|    total_timesteps | 111616   |
---------------------------------
Eval num_timesteps=112000, episode_reward=470.61 +/- 747.64
Episode length: 35.84 +/- 5.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 471           |
| time/                   |               |
|    total_timesteps      | 112000        |
| train/                  |               |
|    approx_kl            | 5.5067358e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00862      |
|    explained_variance   | -0.461        |
|    learning_rate        | 0.001         |
|    loss                 | 2.84e+04      |
|    n_updates            | 6880          |
|    policy_gradient_loss | -0.000233     |
|    value_loss           | 7.93e+04      |
-------------------------------------------
Eval num_timesteps=112500, episode_reward=631.93 +/- 777.62
Episode length: 36.88 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 632      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 354      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 110      |
|    time_elapsed    | 405      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=492.21 +/- 745.79
Episode length: 35.56 +/- 7.20
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.6           |
|    mean_reward          | 492            |
| time/                   |                |
|    total_timesteps      | 113000         |
| train/                  |                |
|    approx_kl            | 0.000117280346 |
|    clip_fraction        | 0.000391       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00669       |
|    explained_variance   | -0.0335        |
|    learning_rate        | 0.001          |
|    loss                 | 4.83e+04       |
|    n_updates            | 6890           |
|    policy_gradient_loss | -5.33e-05      |
|    value_loss           | 9.22e+04       |
--------------------------------------------
Eval num_timesteps=113500, episode_reward=384.82 +/- 697.60
Episode length: 34.52 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 295      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 111      |
|    time_elapsed    | 409      |
|    total_timesteps | 113664   |
---------------------------------
Eval num_timesteps=114000, episode_reward=437.86 +/- 708.40
Episode length: 35.58 +/- 6.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 438          |
| time/                   |              |
|    total_timesteps      | 114000       |
| train/                  |              |
|    approx_kl            | 0.0012715147 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00781     |
|    explained_variance   | 0.036        |
|    learning_rate        | 0.001        |
|    loss                 | 3.91e+04     |
|    n_updates            | 6900         |
|    policy_gradient_loss | 6.21e-05     |
|    value_loss           | 9.17e+04     |
------------------------------------------
Eval num_timesteps=114500, episode_reward=412.13 +/- 717.97
Episode length: 34.72 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 112      |
|    time_elapsed    | 413      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=624.72 +/- 730.03
Episode length: 37.22 +/- 5.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.2         |
|    mean_reward          | 625          |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 3.996311e-05 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00563     |
|    explained_variance   | -0.0579      |
|    learning_rate        | 0.001        |
|    loss                 | 4.52e+04     |
|    n_updates            | 6910         |
|    policy_gradient_loss | -0.000213    |
|    value_loss           | 9.46e+04     |
------------------------------------------
Eval num_timesteps=115500, episode_reward=367.62 +/- 650.01
Episode length: 35.40 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 397      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 113      |
|    time_elapsed    | 416      |
|    total_timesteps | 115712   |
---------------------------------
Eval num_timesteps=116000, episode_reward=374.93 +/- 642.58
Episode length: 35.10 +/- 6.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 375           |
| time/                   |               |
|    total_timesteps      | 116000        |
| train/                  |               |
|    approx_kl            | 2.0444859e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00544      |
|    explained_variance   | -0.00641      |
|    learning_rate        | 0.001         |
|    loss                 | 3.67e+04      |
|    n_updates            | 6920          |
|    policy_gradient_loss | -0.000387     |
|    value_loss           | 8.31e+04      |
-------------------------------------------
Eval num_timesteps=116500, episode_reward=423.39 +/- 728.91
Episode length: 35.30 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 441      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 114      |
|    time_elapsed    | 420      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=588.48 +/- 751.96
Episode length: 36.34 +/- 6.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 588          |
| time/                   |              |
|    total_timesteps      | 117000       |
| train/                  |              |
|    approx_kl            | 0.0022445973 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00387     |
|    explained_variance   | 0.11         |
|    learning_rate        | 0.001        |
|    loss                 | 4.84e+04     |
|    n_updates            | 6930         |
|    policy_gradient_loss | 0.00143      |
|    value_loss           | 9.67e+04     |
------------------------------------------
Eval num_timesteps=117500, episode_reward=507.50 +/- 716.99
Episode length: 35.78 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 508      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 115      |
|    time_elapsed    | 423      |
|    total_timesteps | 117760   |
---------------------------------
Eval num_timesteps=118000, episode_reward=532.58 +/- 809.91
Episode length: 35.76 +/- 7.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.8        |
|    mean_reward          | 533         |
| time/                   |             |
|    total_timesteps      | 118000      |
| train/                  |             |
|    approx_kl            | 4.48199e-07 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00617    |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.001       |
|    loss                 | 4.15e+04    |
|    n_updates            | 6940        |
|    policy_gradient_loss | -3.33e-05   |
|    value_loss           | 9.74e+04    |
-----------------------------------------
Eval num_timesteps=118500, episode_reward=294.37 +/- 623.23
Episode length: 34.88 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 381      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 116      |
|    time_elapsed    | 427      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=489.38 +/- 690.40
Episode length: 36.88 +/- 5.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.9          |
|    mean_reward          | 489           |
| time/                   |               |
|    total_timesteps      | 119000        |
| train/                  |               |
|    approx_kl            | 5.7101715e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00426      |
|    explained_variance   | 0.00197       |
|    learning_rate        | 0.001         |
|    loss                 | 5.57e+04      |
|    n_updates            | 6950          |
|    policy_gradient_loss | -3.71e-05     |
|    value_loss           | 1.12e+05      |
-------------------------------------------
Eval num_timesteps=119500, episode_reward=407.74 +/- 708.92
Episode length: 35.24 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 434      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 117      |
|    time_elapsed    | 431      |
|    total_timesteps | 119808   |
---------------------------------
Eval num_timesteps=120000, episode_reward=345.21 +/- 656.10
Episode length: 34.80 +/- 6.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.8        |
|    mean_reward          | 345         |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.006085608 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00251    |
|    explained_variance   | -0.0886     |
|    learning_rate        | 0.001       |
|    loss                 | 3.65e+04    |
|    n_updates            | 6960        |
|    policy_gradient_loss | 0.00138     |
|    value_loss           | 1.12e+05    |
-----------------------------------------
Eval num_timesteps=120500, episode_reward=340.91 +/- 737.78
Episode length: 33.80 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 118      |
|    time_elapsed    | 434      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=436.29 +/- 706.24
Episode length: 35.32 +/- 6.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 436           |
| time/                   |               |
|    total_timesteps      | 121000        |
| train/                  |               |
|    approx_kl            | 1.5821424e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00602      |
|    explained_variance   | -0.151        |
|    learning_rate        | 0.001         |
|    loss                 | 4.3e+04       |
|    n_updates            | 6970          |
|    policy_gradient_loss | -0.000127     |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=121500, episode_reward=531.46 +/- 764.92
Episode length: 36.12 +/- 7.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 531      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 496      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 119      |
|    time_elapsed    | 438      |
|    total_timesteps | 121856   |
---------------------------------
Eval num_timesteps=122000, episode_reward=578.56 +/- 675.91
Episode length: 37.66 +/- 5.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.7         |
|    mean_reward          | 579          |
| time/                   |              |
|    total_timesteps      | 122000       |
| train/                  |              |
|    approx_kl            | 0.0035135453 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00486     |
|    explained_variance   | 0.0337       |
|    learning_rate        | 0.001        |
|    loss                 | 3e+04        |
|    n_updates            | 6980         |
|    policy_gradient_loss | 0.000744     |
|    value_loss           | 9.08e+04     |
------------------------------------------
Eval num_timesteps=122500, episode_reward=425.68 +/- 733.86
Episode length: 35.30 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 423      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 120      |
|    time_elapsed    | 442      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=432.30 +/- 784.43
Episode length: 34.60 +/- 6.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 432          |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0015920892 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00761     |
|    explained_variance   | -0.085       |
|    learning_rate        | 0.001        |
|    loss                 | 4.48e+04     |
|    n_updates            | 6990         |
|    policy_gradient_loss | -1.4e-05     |
|    value_loss           | 8.99e+04     |
------------------------------------------
Eval num_timesteps=123500, episode_reward=441.61 +/- 732.56
Episode length: 35.46 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 467      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 121      |
|    time_elapsed    | 445      |
|    total_timesteps | 123904   |
---------------------------------
Eval num_timesteps=124000, episode_reward=501.58 +/- 744.24
Episode length: 36.28 +/- 6.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 502          |
| time/                   |              |
|    total_timesteps      | 124000       |
| train/                  |              |
|    approx_kl            | 5.949405e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00485     |
|    explained_variance   | 0.0749       |
|    learning_rate        | 0.001        |
|    loss                 | 3.33e+04     |
|    n_updates            | 7000         |
|    policy_gradient_loss | -0.00012     |
|    value_loss           | 9.34e+04     |
------------------------------------------
Eval num_timesteps=124500, episode_reward=465.07 +/- 762.44
Episode length: 35.32 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 465      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 497      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 122      |
|    time_elapsed    | 449      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=512.90 +/- 736.83
Episode length: 36.44 +/- 5.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 513           |
| time/                   |               |
|    total_timesteps      | 125000        |
| train/                  |               |
|    approx_kl            | 1.2217788e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00467      |
|    explained_variance   | -0.00643      |
|    learning_rate        | 0.001         |
|    loss                 | 4.51e+04      |
|    n_updates            | 7010          |
|    policy_gradient_loss | -9.29e-05     |
|    value_loss           | 1.07e+05      |
-------------------------------------------
Eval num_timesteps=125500, episode_reward=552.74 +/- 724.53
Episode length: 36.42 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 553      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 483      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 123      |
|    time_elapsed    | 452      |
|    total_timesteps | 125952   |
---------------------------------
Eval num_timesteps=126000, episode_reward=368.33 +/- 691.86
Episode length: 34.78 +/- 6.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 368           |
| time/                   |               |
|    total_timesteps      | 126000        |
| train/                  |               |
|    approx_kl            | 3.1240052e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0055       |
|    explained_variance   | -0.0242       |
|    learning_rate        | 0.001         |
|    loss                 | 4.1e+04       |
|    n_updates            | 7020          |
|    policy_gradient_loss | -9.62e-05     |
|    value_loss           | 9.66e+04      |
-------------------------------------------
Eval num_timesteps=126500, episode_reward=624.97 +/- 786.76
Episode length: 37.14 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 625      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 494      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 124      |
|    time_elapsed    | 456      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=345.86 +/- 682.78
Episode length: 34.52 +/- 6.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.5         |
|    mean_reward          | 346          |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 5.514594e-06 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00538     |
|    explained_variance   | 0.0238       |
|    learning_rate        | 0.001        |
|    loss                 | 4.29e+04     |
|    n_updates            | 7030         |
|    policy_gradient_loss | -3.61e-05    |
|    value_loss           | 9.17e+04     |
------------------------------------------
Eval num_timesteps=127500, episode_reward=511.99 +/- 774.63
Episode length: 35.74 +/- 7.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=567.59 +/- 726.05
Episode length: 36.22 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 568      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 439      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 125      |
|    time_elapsed    | 461      |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=354.55 +/- 740.05
Episode length: 34.04 +/- 7.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34          |
|    mean_reward          | 355         |
| time/                   |             |
|    total_timesteps      | 128500      |
| train/                  |             |
|    approx_kl            | 0.008838388 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00676    |
|    explained_variance   | -0.0287     |
|    learning_rate        | 0.001       |
|    loss                 | 3.38e+04    |
|    n_updates            | 7040        |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 8.53e+04    |
-----------------------------------------
Eval num_timesteps=129000, episode_reward=466.24 +/- 766.00
Episode length: 35.36 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 126      |
|    time_elapsed    | 465      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=506.62 +/- 761.12
Episode length: 35.84 +/- 6.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 507          |
| time/                   |              |
|    total_timesteps      | 129500       |
| train/                  |              |
|    approx_kl            | 3.753812e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00546     |
|    explained_variance   | 0.0465       |
|    learning_rate        | 0.001        |
|    loss                 | 4.96e+04     |
|    n_updates            | 7050         |
|    policy_gradient_loss | -0.00016     |
|    value_loss           | 8.93e+04     |
------------------------------------------
Eval num_timesteps=130000, episode_reward=497.29 +/- 740.07
Episode length: 35.14 +/- 8.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 127      |
|    time_elapsed    | 468      |
|    total_timesteps | 130048   |
---------------------------------
Eval num_timesteps=130500, episode_reward=359.14 +/- 700.01
Episode length: 34.40 +/- 6.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 359          |
| time/                   |              |
|    total_timesteps      | 130500       |
| train/                  |              |
|    approx_kl            | 4.452886e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00538     |
|    explained_variance   | 0.0819       |
|    learning_rate        | 0.001        |
|    loss                 | 3.83e+04     |
|    n_updates            | 7060         |
|    policy_gradient_loss | -0.000133    |
|    value_loss           | 9.91e+04     |
------------------------------------------
Eval num_timesteps=131000, episode_reward=697.13 +/- 791.02
Episode length: 36.92 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 697      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 374      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 128      |
|    time_elapsed    | 472      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=427.54 +/- 759.27
Episode length: 35.20 +/- 6.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 428           |
| time/                   |               |
|    total_timesteps      | 131500        |
| train/                  |               |
|    approx_kl            | 4.0070154e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00635      |
|    explained_variance   | -0.157        |
|    learning_rate        | 0.001         |
|    loss                 | 4.78e+04      |
|    n_updates            | 7070          |
|    policy_gradient_loss | -0.000108     |
|    value_loss           | 1.03e+05      |
-------------------------------------------
Eval num_timesteps=132000, episode_reward=405.14 +/- 689.59
Episode length: 36.08 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 391      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 129      |
|    time_elapsed    | 475      |
|    total_timesteps | 132096   |
---------------------------------
Eval num_timesteps=132500, episode_reward=528.66 +/- 758.65
Episode length: 35.60 +/- 6.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 529          |
| time/                   |              |
|    total_timesteps      | 132500       |
| train/                  |              |
|    approx_kl            | 3.608875e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00594     |
|    explained_variance   | -0.111       |
|    learning_rate        | 0.001        |
|    loss                 | 2.92e+04     |
|    n_updates            | 7080         |
|    policy_gradient_loss | -0.000124    |
|    value_loss           | 8.36e+04     |
------------------------------------------
Eval num_timesteps=133000, episode_reward=370.97 +/- 694.27
Episode length: 35.00 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 130      |
|    time_elapsed    | 479      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=570.17 +/- 766.78
Episode length: 37.06 +/- 5.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.1         |
|    mean_reward          | 570          |
| time/                   |              |
|    total_timesteps      | 133500       |
| train/                  |              |
|    approx_kl            | 5.523907e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00632     |
|    explained_variance   | 0.0369       |
|    learning_rate        | 0.001        |
|    loss                 | 4.51e+04     |
|    n_updates            | 7090         |
|    policy_gradient_loss | -5.28e-05    |
|    value_loss           | 9.22e+04     |
------------------------------------------
Eval num_timesteps=134000, episode_reward=436.50 +/- 738.72
Episode length: 35.02 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 445      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 131      |
|    time_elapsed    | 483      |
|    total_timesteps | 134144   |
---------------------------------
Eval num_timesteps=134500, episode_reward=445.75 +/- 681.28
Episode length: 35.20 +/- 6.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 446           |
| time/                   |               |
|    total_timesteps      | 134500        |
| train/                  |               |
|    approx_kl            | 3.1670788e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00485      |
|    explained_variance   | 0.0289        |
|    learning_rate        | 0.001         |
|    loss                 | 4.46e+04      |
|    n_updates            | 7100          |
|    policy_gradient_loss | -0.000185     |
|    value_loss           | 1.1e+05       |
-------------------------------------------
Eval num_timesteps=135000, episode_reward=551.34 +/- 801.95
Episode length: 35.78 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 551      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 132      |
|    time_elapsed    | 487      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=557.46 +/- 756.70
Episode length: 35.74 +/- 6.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 557           |
| time/                   |               |
|    total_timesteps      | 135500        |
| train/                  |               |
|    approx_kl            | 1.4768739e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00909      |
|    explained_variance   | -0.0642       |
|    learning_rate        | 0.001         |
|    loss                 | 3.07e+04      |
|    n_updates            | 7110          |
|    policy_gradient_loss | -0.000164     |
|    value_loss           | 9.32e+04      |
-------------------------------------------
Eval num_timesteps=136000, episode_reward=370.02 +/- 649.51
Episode length: 35.54 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 483      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 133      |
|    time_elapsed    | 490      |
|    total_timesteps | 136192   |
---------------------------------
Eval num_timesteps=136500, episode_reward=369.80 +/- 729.00
Episode length: 34.32 +/- 7.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 370          |
| time/                   |              |
|    total_timesteps      | 136500       |
| train/                  |              |
|    approx_kl            | 0.0012091667 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00463     |
|    explained_variance   | 0.0587       |
|    learning_rate        | 0.001        |
|    loss                 | 3.57e+04     |
|    n_updates            | 7120         |
|    policy_gradient_loss | -0.000402    |
|    value_loss           | 9.76e+04     |
------------------------------------------
Eval num_timesteps=137000, episode_reward=370.50 +/- 725.09
Episode length: 32.88 +/- 8.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 373      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 134      |
|    time_elapsed    | 494      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=480.91 +/- 801.60
Episode length: 35.64 +/- 7.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 481          |
| time/                   |              |
|    total_timesteps      | 137500       |
| train/                  |              |
|    approx_kl            | 6.222306e-05 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00513     |
|    explained_variance   | -0.0104      |
|    learning_rate        | 0.001        |
|    loss                 | 3.7e+04      |
|    n_updates            | 7130         |
|    policy_gradient_loss | 0.000344     |
|    value_loss           | 9.3e+04      |
------------------------------------------
Eval num_timesteps=138000, episode_reward=423.58 +/- 708.70
Episode length: 35.52 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 135      |
|    time_elapsed    | 497      |
|    total_timesteps | 138240   |
---------------------------------
Eval num_timesteps=138500, episode_reward=405.26 +/- 694.57
Episode length: 35.24 +/- 6.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 405          |
| time/                   |              |
|    total_timesteps      | 138500       |
| train/                  |              |
|    approx_kl            | 0.0001197207 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00332     |
|    explained_variance   | 0.0107       |
|    learning_rate        | 0.001        |
|    loss                 | 4.52e+04     |
|    n_updates            | 7140         |
|    policy_gradient_loss | -1.46e-05    |
|    value_loss           | 9.83e+04     |
------------------------------------------
Eval num_timesteps=139000, episode_reward=517.53 +/- 757.40
Episode length: 35.72 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 440      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 136      |
|    time_elapsed    | 501      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=352.46 +/- 710.63
Episode length: 34.34 +/- 6.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 352           |
| time/                   |               |
|    total_timesteps      | 139500        |
| train/                  |               |
|    approx_kl            | 7.6537253e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0021       |
|    explained_variance   | -0.0789       |
|    learning_rate        | 0.001         |
|    loss                 | 3.37e+04      |
|    n_updates            | 7150          |
|    policy_gradient_loss | -0.000187     |
|    value_loss           | 1.04e+05      |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=589.48 +/- 778.32
Episode length: 35.94 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 589      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 468      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 137      |
|    time_elapsed    | 505      |
|    total_timesteps | 140288   |
---------------------------------
Eval num_timesteps=140500, episode_reward=418.70 +/- 769.42
Episode length: 34.98 +/- 6.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 419          |
| time/                   |              |
|    total_timesteps      | 140500       |
| train/                  |              |
|    approx_kl            | 7.029157e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00299     |
|    explained_variance   | -0.0984      |
|    learning_rate        | 0.001        |
|    loss                 | 4.71e+04     |
|    n_updates            | 7160         |
|    policy_gradient_loss | -0.000144    |
|    value_loss           | 8.59e+04     |
------------------------------------------
Eval num_timesteps=141000, episode_reward=397.51 +/- 735.29
Episode length: 34.54 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 483      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 138      |
|    time_elapsed    | 508      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=290.83 +/- 618.34
Episode length: 34.10 +/- 6.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.1          |
|    mean_reward          | 291           |
| time/                   |               |
|    total_timesteps      | 141500        |
| train/                  |               |
|    approx_kl            | 2.8391369e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00233      |
|    explained_variance   | 0.0188        |
|    learning_rate        | 0.001         |
|    loss                 | 4.9e+04       |
|    n_updates            | 7170          |
|    policy_gradient_loss | -0.000246     |
|    value_loss           | 9.45e+04      |
-------------------------------------------
Eval num_timesteps=142000, episode_reward=580.59 +/- 731.98
Episode length: 36.70 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 581      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 490      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 139      |
|    time_elapsed    | 512      |
|    total_timesteps | 142336   |
---------------------------------
Eval num_timesteps=142500, episode_reward=370.35 +/- 678.74
Episode length: 35.00 +/- 6.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 370           |
| time/                   |               |
|    total_timesteps      | 142500        |
| train/                  |               |
|    approx_kl            | 1.9563595e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00321      |
|    explained_variance   | 0.0831        |
|    learning_rate        | 0.001         |
|    loss                 | 3.17e+04      |
|    n_updates            | 7180          |
|    policy_gradient_loss | -8.01e-05     |
|    value_loss           | 8.31e+04      |
-------------------------------------------
Eval num_timesteps=143000, episode_reward=403.52 +/- 710.70
Episode length: 35.62 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 575      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 140      |
|    time_elapsed    | 516      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=367.01 +/- 741.58
Episode length: 34.06 +/- 6.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.1         |
|    mean_reward          | 367          |
| time/                   |              |
|    total_timesteps      | 143500       |
| train/                  |              |
|    approx_kl            | 3.525056e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00196     |
|    explained_variance   | 0.0679       |
|    learning_rate        | 0.001        |
|    loss                 | 3.24e+04     |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.000133    |
|    value_loss           | 1.01e+05     |
------------------------------------------
Eval num_timesteps=144000, episode_reward=473.81 +/- 758.81
Episode length: 35.42 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 545      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 141      |
|    time_elapsed    | 519      |
|    total_timesteps | 144384   |
---------------------------------
Eval num_timesteps=144500, episode_reward=234.07 +/- 609.26
Episode length: 33.58 +/- 6.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 234          |
| time/                   |              |
|    total_timesteps      | 144500       |
| train/                  |              |
|    approx_kl            | 7.058261e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00337     |
|    explained_variance   | 0.00237      |
|    learning_rate        | 0.001        |
|    loss                 | 4.29e+04     |
|    n_updates            | 7200         |
|    policy_gradient_loss | -2.75e-05    |
|    value_loss           | 9.93e+04     |
------------------------------------------
Eval num_timesteps=145000, episode_reward=467.84 +/- 788.21
Episode length: 34.42 +/- 7.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 492      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 142      |
|    time_elapsed    | 523      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=331.41 +/- 684.04
Episode length: 34.78 +/- 7.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 331           |
| time/                   |               |
|    total_timesteps      | 145500        |
| train/                  |               |
|    approx_kl            | 7.9651363e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0035       |
|    explained_variance   | 0.0388        |
|    learning_rate        | 0.001         |
|    loss                 | 5.2e+04       |
|    n_updates            | 7210          |
|    policy_gradient_loss | -8.62e-05     |
|    value_loss           | 1.22e+05      |
-------------------------------------------
Eval num_timesteps=146000, episode_reward=483.52 +/- 710.74
Episode length: 36.50 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 451      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 143      |
|    time_elapsed    | 526      |
|    total_timesteps | 146432   |
---------------------------------
Eval num_timesteps=146500, episode_reward=505.82 +/- 809.76
Episode length: 34.56 +/- 7.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 506          |
| time/                   |              |
|    total_timesteps      | 146500       |
| train/                  |              |
|    approx_kl            | 9.306823e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00313     |
|    explained_variance   | 0.0426       |
|    learning_rate        | 0.001        |
|    loss                 | 4.59e+04     |
|    n_updates            | 7220         |
|    policy_gradient_loss | -9.13e-05    |
|    value_loss           | 9.68e+04     |
------------------------------------------
Eval num_timesteps=147000, episode_reward=384.87 +/- 654.61
Episode length: 35.20 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 144      |
|    time_elapsed    | 530      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=466.79 +/- 755.41
Episode length: 35.50 +/- 6.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 467          |
| time/                   |              |
|    total_timesteps      | 147500       |
| train/                  |              |
|    approx_kl            | 3.769528e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00364     |
|    explained_variance   | 0.133        |
|    learning_rate        | 0.001        |
|    loss                 | 4.06e+04     |
|    n_updates            | 7230         |
|    policy_gradient_loss | -9.86e-05    |
|    value_loss           | 1.04e+05     |
------------------------------------------
Eval num_timesteps=148000, episode_reward=553.58 +/- 780.78
Episode length: 36.02 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 554      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 432      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 145      |
|    time_elapsed    | 534      |
|    total_timesteps | 148480   |
---------------------------------
Eval num_timesteps=148500, episode_reward=388.29 +/- 714.39
Episode length: 34.92 +/- 6.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 388          |
| time/                   |              |
|    total_timesteps      | 148500       |
| train/                  |              |
|    approx_kl            | 6.998889e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00345     |
|    explained_variance   | -0.109       |
|    learning_rate        | 0.001        |
|    loss                 | 3.61e+04     |
|    n_updates            | 7240         |
|    policy_gradient_loss | -0.0001      |
|    value_loss           | 8.24e+04     |
------------------------------------------
Eval num_timesteps=149000, episode_reward=555.27 +/- 744.77
Episode length: 37.30 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 555      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=636.64 +/- 860.84
Episode length: 36.14 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 637      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 436      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 146      |
|    time_elapsed    | 539      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=515.97 +/- 733.14
Episode length: 35.86 +/- 6.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 516          |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 7.263734e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0025      |
|    explained_variance   | 0.013        |
|    learning_rate        | 0.001        |
|    loss                 | 4.18e+04     |
|    n_updates            | 7250         |
|    policy_gradient_loss | -0.00012     |
|    value_loss           | 1.08e+05     |
------------------------------------------
Eval num_timesteps=150500, episode_reward=368.40 +/- 661.35
Episode length: 34.90 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 431      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 147      |
|    time_elapsed    | 542      |
|    total_timesteps | 150528   |
---------------------------------
Eval num_timesteps=151000, episode_reward=554.97 +/- 785.09
Episode length: 36.04 +/- 7.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 555           |
| time/                   |               |
|    total_timesteps      | 151000        |
| train/                  |               |
|    approx_kl            | 4.6519563e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00363      |
|    explained_variance   | 0.134         |
|    learning_rate        | 0.001         |
|    loss                 | 4.25e+04      |
|    n_updates            | 7260          |
|    policy_gradient_loss | -0.000135     |
|    value_loss           | 9.09e+04      |
-------------------------------------------
Eval num_timesteps=151500, episode_reward=537.20 +/- 794.41
Episode length: 35.64 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 537      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 397      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 148      |
|    time_elapsed    | 546      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=420.50 +/- 807.19
Episode length: 34.04 +/- 8.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 420           |
| time/                   |               |
|    total_timesteps      | 152000        |
| train/                  |               |
|    approx_kl            | 2.5873305e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00387      |
|    explained_variance   | -0.17         |
|    learning_rate        | 0.001         |
|    loss                 | 2.99e+04      |
|    n_updates            | 7270          |
|    policy_gradient_loss | -0.000131     |
|    value_loss           | 7.62e+04      |
-------------------------------------------
Eval num_timesteps=152500, episode_reward=467.43 +/- 656.94
Episode length: 36.72 +/- 5.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 467      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 378      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 149      |
|    time_elapsed    | 549      |
|    total_timesteps | 152576   |
---------------------------------
Eval num_timesteps=153000, episode_reward=518.58 +/- 747.57
Episode length: 36.30 +/- 5.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 519           |
| time/                   |               |
|    total_timesteps      | 153000        |
| train/                  |               |
|    approx_kl            | 6.9878297e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00357      |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.001         |
|    loss                 | 4.19e+04      |
|    n_updates            | 7280          |
|    policy_gradient_loss | -0.00014      |
|    value_loss           | 9.89e+04      |
-------------------------------------------
Eval num_timesteps=153500, episode_reward=331.17 +/- 616.99
Episode length: 35.36 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 417      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 150      |
|    time_elapsed    | 553      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=408.35 +/- 682.80
Episode length: 35.80 +/- 6.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 408           |
| time/                   |               |
|    total_timesteps      | 154000        |
| train/                  |               |
|    approx_kl            | 1.0985532e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00448      |
|    explained_variance   | -0.0171       |
|    learning_rate        | 0.001         |
|    loss                 | 4.51e+04      |
|    n_updates            | 7290          |
|    policy_gradient_loss | -0.000145     |
|    value_loss           | 8.38e+04      |
-------------------------------------------
Eval num_timesteps=154500, episode_reward=515.70 +/- 821.19
Episode length: 34.96 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 516      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 474      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 151      |
|    time_elapsed    | 557      |
|    total_timesteps | 154624   |
---------------------------------
Eval num_timesteps=155000, episode_reward=334.21 +/- 676.18
Episode length: 33.90 +/- 6.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 334           |
| time/                   |               |
|    total_timesteps      | 155000        |
| train/                  |               |
|    approx_kl            | 1.7329585e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00305      |
|    explained_variance   | -0.0429       |
|    learning_rate        | 0.001         |
|    loss                 | 3.77e+04      |
|    n_updates            | 7300          |
|    policy_gradient_loss | -0.000269     |
|    value_loss           | 8.98e+04      |
-------------------------------------------
Eval num_timesteps=155500, episode_reward=272.99 +/- 632.39
Episode length: 34.24 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 459      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 152      |
|    time_elapsed    | 560      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=474.76 +/- 811.29
Episode length: 34.96 +/- 7.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 475         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.002329429 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0033     |
|    explained_variance   | -0.00455    |
|    learning_rate        | 0.001       |
|    loss                 | 3.14e+04    |
|    n_updates            | 7310        |
|    policy_gradient_loss | 0.00144     |
|    value_loss           | 8.35e+04    |
-----------------------------------------
Eval num_timesteps=156500, episode_reward=358.56 +/- 705.43
Episode length: 33.74 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 443      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 153      |
|    time_elapsed    | 564      |
|    total_timesteps | 156672   |
---------------------------------
Eval num_timesteps=157000, episode_reward=365.08 +/- 718.41
Episode length: 33.80 +/- 7.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 365           |
| time/                   |               |
|    total_timesteps      | 157000        |
| train/                  |               |
|    approx_kl            | 2.8149225e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00322      |
|    explained_variance   | 0.15          |
|    learning_rate        | 0.001         |
|    loss                 | 3.62e+04      |
|    n_updates            | 7320          |
|    policy_gradient_loss | -7.48e-05     |
|    value_loss           | 9.05e+04      |
-------------------------------------------
Eval num_timesteps=157500, episode_reward=331.90 +/- 659.26
Episode length: 35.04 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 423      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 154      |
|    time_elapsed    | 567      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=472.90 +/- 785.61
Episode length: 34.58 +/- 7.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 473          |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 8.364441e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00543     |
|    explained_variance   | 0.121        |
|    learning_rate        | 0.001        |
|    loss                 | 4.64e+04     |
|    n_updates            | 7330         |
|    policy_gradient_loss | -0.000108    |
|    value_loss           | 8.48e+04     |
------------------------------------------
Eval num_timesteps=158500, episode_reward=492.94 +/- 745.30
Episode length: 35.52 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 493      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 423      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 155      |
|    time_elapsed    | 571      |
|    total_timesteps | 158720   |
---------------------------------
Eval num_timesteps=159000, episode_reward=298.32 +/- 607.71
Episode length: 34.32 +/- 6.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 298           |
| time/                   |               |
|    total_timesteps      | 159000        |
| train/                  |               |
|    approx_kl            | 0.00075752515 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0055       |
|    explained_variance   | -0.122        |
|    learning_rate        | 0.001         |
|    loss                 | 4.27e+04      |
|    n_updates            | 7340          |
|    policy_gradient_loss | 0.000175      |
|    value_loss           | 9.23e+04      |
-------------------------------------------
Eval num_timesteps=159500, episode_reward=442.98 +/- 771.37
Episode length: 34.58 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 414      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 156      |
|    time_elapsed    | 574      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=425.77 +/- 783.43
Episode length: 33.72 +/- 7.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.7          |
|    mean_reward          | 426           |
| time/                   |               |
|    total_timesteps      | 160000        |
| train/                  |               |
|    approx_kl            | 6.7264773e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00344      |
|    explained_variance   | 0.0145        |
|    learning_rate        | 0.001         |
|    loss                 | 3.88e+04      |
|    n_updates            | 7350          |
|    policy_gradient_loss | -0.000201     |
|    value_loss           | 9.87e+04      |
-------------------------------------------
Eval num_timesteps=160500, episode_reward=495.37 +/- 728.41
Episode length: 36.04 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 507      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 157      |
|    time_elapsed    | 578      |
|    total_timesteps | 160768   |
---------------------------------
Eval num_timesteps=161000, episode_reward=454.52 +/- 771.29
Episode length: 34.54 +/- 6.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 455           |
| time/                   |               |
|    total_timesteps      | 161000        |
| train/                  |               |
|    approx_kl            | 1.5522237e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00407      |
|    explained_variance   | 0.0321        |
|    learning_rate        | 0.001         |
|    loss                 | 4.46e+04      |
|    n_updates            | 7360          |
|    policy_gradient_loss | -0.00016      |
|    value_loss           | 9.05e+04      |
-------------------------------------------
Eval num_timesteps=161500, episode_reward=502.97 +/- 695.82
Episode length: 36.56 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 503      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.9     |
|    ep_rew_mean     | 583      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 158      |
|    time_elapsed    | 582      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=230.97 +/- 590.23
Episode length: 34.24 +/- 5.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.2        |
|    mean_reward          | 231         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.021389117 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00215    |
|    explained_variance   | -0.0793     |
|    learning_rate        | 0.001       |
|    loss                 | 4.03e+04    |
|    n_updates            | 7370        |
|    policy_gradient_loss | 0.0012      |
|    value_loss           | 8.26e+04    |
-----------------------------------------
Eval num_timesteps=162500, episode_reward=545.82 +/- 731.15
Episode length: 36.16 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 546      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 611      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 159      |
|    time_elapsed    | 585      |
|    total_timesteps | 162816   |
---------------------------------
Eval num_timesteps=163000, episode_reward=509.85 +/- 746.50
Episode length: 36.44 +/- 6.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 510          |
| time/                   |              |
|    total_timesteps      | 163000       |
| train/                  |              |
|    approx_kl            | 3.775931e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00191     |
|    explained_variance   | 0.0611       |
|    learning_rate        | 0.001        |
|    loss                 | 4.84e+04     |
|    n_updates            | 7380         |
|    policy_gradient_loss | -3.69e-05    |
|    value_loss           | 9.11e+04     |
------------------------------------------
Eval num_timesteps=163500, episode_reward=501.96 +/- 724.45
Episode length: 35.04 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 585      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 160      |
|    time_elapsed    | 589      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=447.15 +/- 733.03
Episode length: 35.00 +/- 6.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 447           |
| time/                   |               |
|    total_timesteps      | 164000        |
| train/                  |               |
|    approx_kl            | 1.1126394e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00318      |
|    explained_variance   | -0.0522       |
|    learning_rate        | 0.001         |
|    loss                 | 4.51e+04      |
|    n_updates            | 7390          |
|    policy_gradient_loss | -0.000144     |
|    value_loss           | 1.06e+05      |
-------------------------------------------
Eval num_timesteps=164500, episode_reward=325.54 +/- 675.12
Episode length: 34.12 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 326      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 532      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 161      |
|    time_elapsed    | 592      |
|    total_timesteps | 164864   |
---------------------------------
Eval num_timesteps=165000, episode_reward=406.94 +/- 747.79
Episode length: 34.40 +/- 7.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 407           |
| time/                   |               |
|    total_timesteps      | 165000        |
| train/                  |               |
|    approx_kl            | 1.9192812e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0045       |
|    explained_variance   | -0.0238       |
|    learning_rate        | 0.001         |
|    loss                 | 2.48e+04      |
|    n_updates            | 7400          |
|    policy_gradient_loss | -0.000166     |
|    value_loss           | 8.18e+04      |
-------------------------------------------
Eval num_timesteps=165500, episode_reward=404.70 +/- 741.61
Episode length: 34.92 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 162      |
|    time_elapsed    | 596      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=225.97 +/- 621.80
Episode length: 33.70 +/- 6.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.7        |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.001966858 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00578    |
|    explained_variance   | -0.0984     |
|    learning_rate        | 0.001       |
|    loss                 | 3.59e+04    |
|    n_updates            | 7410        |
|    policy_gradient_loss | -0.000134   |
|    value_loss           | 1.08e+05    |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=542.81 +/- 704.28
Episode length: 37.10 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 543      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 417      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 163      |
|    time_elapsed    | 600      |
|    total_timesteps | 166912   |
---------------------------------
Eval num_timesteps=167000, episode_reward=346.48 +/- 704.20
Episode length: 34.94 +/- 6.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 346          |
| time/                   |              |
|    total_timesteps      | 167000       |
| train/                  |              |
|    approx_kl            | 1.974171e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00361     |
|    explained_variance   | -0.0182      |
|    learning_rate        | 0.001        |
|    loss                 | 4.74e+04     |
|    n_updates            | 7420         |
|    policy_gradient_loss | -0.000365    |
|    value_loss           | 1e+05        |
------------------------------------------
Eval num_timesteps=167500, episode_reward=375.08 +/- 649.00
Episode length: 35.70 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 335      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 164      |
|    time_elapsed    | 603      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=306.45 +/- 613.03
Episode length: 35.40 +/- 5.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 306          |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 4.719943e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00545     |
|    explained_variance   | -0.0156      |
|    learning_rate        | 0.001        |
|    loss                 | 4.15e+04     |
|    n_updates            | 7430         |
|    policy_gradient_loss | -0.000232    |
|    value_loss           | 8.07e+04     |
------------------------------------------
Eval num_timesteps=168500, episode_reward=280.44 +/- 656.59
Episode length: 33.66 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 419      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 165      |
|    time_elapsed    | 607      |
|    total_timesteps | 168960   |
---------------------------------
Eval num_timesteps=169000, episode_reward=152.56 +/- 575.06
Episode length: 33.36 +/- 6.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 153           |
| time/                   |               |
|    total_timesteps      | 169000        |
| train/                  |               |
|    approx_kl            | 2.1077809e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0043       |
|    explained_variance   | -0.0336       |
|    learning_rate        | 0.001         |
|    loss                 | 5.46e+04      |
|    n_updates            | 7440          |
|    policy_gradient_loss | -0.000227     |
|    value_loss           | 1.24e+05      |
-------------------------------------------
Eval num_timesteps=169500, episode_reward=359.08 +/- 705.65
Episode length: 34.90 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 348      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 166      |
|    time_elapsed    | 610      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=376.03 +/- 714.52
Episode length: 34.88 +/- 6.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 376           |
| time/                   |               |
|    total_timesteps      | 170000        |
| train/                  |               |
|    approx_kl            | 1.0061776e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00351      |
|    explained_variance   | 0.00134       |
|    learning_rate        | 0.001         |
|    loss                 | 4.63e+04      |
|    n_updates            | 7450          |
|    policy_gradient_loss | -0.000148     |
|    value_loss           | 9.36e+04      |
-------------------------------------------
Eval num_timesteps=170500, episode_reward=481.84 +/- 780.46
Episode length: 34.88 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 482      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=565.03 +/- 776.13
Episode length: 36.88 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 565      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 416      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 167      |
|    time_elapsed    | 615      |
|    total_timesteps | 171008   |
---------------------------------
Eval num_timesteps=171500, episode_reward=286.98 +/- 606.57
Episode length: 34.66 +/- 6.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 287           |
| time/                   |               |
|    total_timesteps      | 171500        |
| train/                  |               |
|    approx_kl            | 1.6065314e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00349      |
|    explained_variance   | -0.0179       |
|    learning_rate        | 0.001         |
|    loss                 | 4.73e+04      |
|    n_updates            | 7460          |
|    policy_gradient_loss | -6.25e-05     |
|    value_loss           | 1.02e+05      |
-------------------------------------------
Eval num_timesteps=172000, episode_reward=347.75 +/- 653.28
Episode length: 34.74 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 348      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 335      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 168      |
|    time_elapsed    | 619      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=424.85 +/- 711.48
Episode length: 35.76 +/- 6.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 425          |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0129981665 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00379     |
|    explained_variance   | 0.0304       |
|    learning_rate        | 0.001        |
|    loss                 | 4.16e+04     |
|    n_updates            | 7470         |
|    policy_gradient_loss | 0.00282      |
|    value_loss           | 8.81e+04     |
------------------------------------------
Eval num_timesteps=173000, episode_reward=439.20 +/- 781.49
Episode length: 35.10 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 304      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 169      |
|    time_elapsed    | 622      |
|    total_timesteps | 173056   |
---------------------------------
Eval num_timesteps=173500, episode_reward=404.45 +/- 725.31
Episode length: 35.06 +/- 6.79
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.1           |
|    mean_reward          | 404            |
| time/                   |                |
|    total_timesteps      | 173500         |
| train/                  |                |
|    approx_kl            | 0.000121038465 |
|    clip_fraction        | 0.00127        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00301       |
|    explained_variance   | -0.106         |
|    learning_rate        | 0.001          |
|    loss                 | 4.63e+04       |
|    n_updates            | 7480           |
|    policy_gradient_loss | -0.00042       |
|    value_loss           | 9.02e+04       |
--------------------------------------------
Eval num_timesteps=174000, episode_reward=502.35 +/- 759.19
Episode length: 36.08 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 170      |
|    time_elapsed    | 626      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=460.12 +/- 723.93
Episode length: 35.34 +/- 6.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 460           |
| time/                   |               |
|    total_timesteps      | 174500        |
| train/                  |               |
|    approx_kl            | 4.2183092e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00236      |
|    explained_variance   | 0.0227        |
|    learning_rate        | 0.001         |
|    loss                 | 4.15e+04      |
|    n_updates            | 7490          |
|    policy_gradient_loss | -2.84e-05     |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=175000, episode_reward=482.53 +/- 708.16
Episode length: 36.04 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 483      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 405      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 171      |
|    time_elapsed    | 630      |
|    total_timesteps | 175104   |
---------------------------------
Eval num_timesteps=175500, episode_reward=350.37 +/- 679.63
Episode length: 34.82 +/- 6.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 350           |
| time/                   |               |
|    total_timesteps      | 175500        |
| train/                  |               |
|    approx_kl            | 2.0401785e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00255      |
|    explained_variance   | -0.0212       |
|    learning_rate        | 0.001         |
|    loss                 | 4.4e+04       |
|    n_updates            | 7500          |
|    policy_gradient_loss | -8.48e-05     |
|    value_loss           | 9.57e+04      |
-------------------------------------------
Eval num_timesteps=176000, episode_reward=272.88 +/- 596.95
Episode length: 34.86 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 172      |
|    time_elapsed    | 634      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=438.43 +/- 748.27
Episode length: 35.26 +/- 6.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 438          |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 7.660128e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00245     |
|    explained_variance   | 0.104        |
|    learning_rate        | 0.001        |
|    loss                 | 5.87e+04     |
|    n_updates            | 7510         |
|    policy_gradient_loss | -3.48e-05    |
|    value_loss           | 1.11e+05     |
------------------------------------------
Eval num_timesteps=177000, episode_reward=472.56 +/- 773.75
Episode length: 34.96 +/- 7.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 413      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 173      |
|    time_elapsed    | 637      |
|    total_timesteps | 177152   |
---------------------------------
Eval num_timesteps=177500, episode_reward=320.69 +/- 670.25
Episode length: 34.16 +/- 6.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 321           |
| time/                   |               |
|    total_timesteps      | 177500        |
| train/                  |               |
|    approx_kl            | 9.0978574e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00217      |
|    explained_variance   | 0.0649        |
|    learning_rate        | 0.001         |
|    loss                 | 3.59e+04      |
|    n_updates            | 7520          |
|    policy_gradient_loss | -5.34e-05     |
|    value_loss           | 9.81e+04      |
-------------------------------------------
Eval num_timesteps=178000, episode_reward=251.42 +/- 636.33
Episode length: 33.60 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 441      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 174      |
|    time_elapsed    | 641      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=435.40 +/- 671.25
Episode length: 36.06 +/- 5.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 435           |
| time/                   |               |
|    total_timesteps      | 178500        |
| train/                  |               |
|    approx_kl            | 4.7439244e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00185      |
|    explained_variance   | 0.0943        |
|    learning_rate        | 0.001         |
|    loss                 | 5.28e+04      |
|    n_updates            | 7530          |
|    policy_gradient_loss | -2.83e-05     |
|    value_loss           | 1.12e+05      |
-------------------------------------------
Eval num_timesteps=179000, episode_reward=416.74 +/- 691.36
Episode length: 35.16 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 480      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 175      |
|    time_elapsed    | 644      |
|    total_timesteps | 179200   |
---------------------------------
Eval num_timesteps=179500, episode_reward=639.95 +/- 740.61
Episode length: 37.14 +/- 6.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.1          |
|    mean_reward          | 640           |
| time/                   |               |
|    total_timesteps      | 179500        |
| train/                  |               |
|    approx_kl            | 6.1205355e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00144      |
|    explained_variance   | -0.112        |
|    learning_rate        | 0.001         |
|    loss                 | 3.43e+04      |
|    n_updates            | 7540          |
|    policy_gradient_loss | -6.92e-05     |
|    value_loss           | 1.03e+05      |
-------------------------------------------
Eval num_timesteps=180000, episode_reward=515.14 +/- 667.86
Episode length: 36.44 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 515      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 531      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 176      |
|    time_elapsed    | 648      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=318.11 +/- 714.20
Episode length: 34.26 +/- 6.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 318           |
| time/                   |               |
|    total_timesteps      | 180500        |
| train/                  |               |
|    approx_kl            | 1.4092075e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00235      |
|    explained_variance   | 0.0801        |
|    learning_rate        | 0.001         |
|    loss                 | 3.36e+04      |
|    n_updates            | 7550          |
|    policy_gradient_loss | -9.03e-05     |
|    value_loss           | 8.79e+04      |
-------------------------------------------
Eval num_timesteps=181000, episode_reward=501.33 +/- 752.36
Episode length: 35.32 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 501      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 177      |
|    time_elapsed    | 652      |
|    total_timesteps | 181248   |
---------------------------------
Eval num_timesteps=181500, episode_reward=434.93 +/- 857.28
Episode length: 33.82 +/- 7.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 435           |
| time/                   |               |
|    total_timesteps      | 181500        |
| train/                  |               |
|    approx_kl            | 1.9821455e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00271      |
|    explained_variance   | 0.0979        |
|    learning_rate        | 0.001         |
|    loss                 | 3.46e+04      |
|    n_updates            | 7560          |
|    policy_gradient_loss | -0.000133     |
|    value_loss           | 7.85e+04      |
-------------------------------------------
Eval num_timesteps=182000, episode_reward=299.60 +/- 617.06
Episode length: 34.08 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 178      |
|    time_elapsed    | 655      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=530.78 +/- 802.57
Episode length: 35.68 +/- 6.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 531          |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 5.798647e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00297     |
|    explained_variance   | -0.162       |
|    learning_rate        | 0.001        |
|    loss                 | 4.45e+04     |
|    n_updates            | 7570         |
|    policy_gradient_loss | -0.000113    |
|    value_loss           | 8.81e+04     |
------------------------------------------
Eval num_timesteps=183000, episode_reward=511.34 +/- 683.70
Episode length: 36.30 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 179      |
|    time_elapsed    | 659      |
|    total_timesteps | 183296   |
---------------------------------
Eval num_timesteps=183500, episode_reward=534.12 +/- 736.90
Episode length: 36.12 +/- 6.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 534           |
| time/                   |               |
|    total_timesteps      | 183500        |
| train/                  |               |
|    approx_kl            | 1.3741665e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0022       |
|    explained_variance   | 0.069         |
|    learning_rate        | 0.001         |
|    loss                 | 4.12e+04      |
|    n_updates            | 7580          |
|    policy_gradient_loss | -0.000119     |
|    value_loss           | 1.04e+05      |
-------------------------------------------
Eval num_timesteps=184000, episode_reward=475.78 +/- 776.91
Episode length: 34.78 +/- 7.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 457      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 180      |
|    time_elapsed    | 662      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=331.48 +/- 696.25
Episode length: 34.36 +/- 6.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 331          |
| time/                   |              |
|    total_timesteps      | 184500       |
| train/                  |              |
|    approx_kl            | 0.0055629127 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00314     |
|    explained_variance   | -0.0461      |
|    learning_rate        | 0.001        |
|    loss                 | 3.98e+04     |
|    n_updates            | 7590         |
|    policy_gradient_loss | 0.00175      |
|    value_loss           | 1.1e+05      |
------------------------------------------
Eval num_timesteps=185000, episode_reward=456.94 +/- 707.90
Episode length: 35.10 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 436      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 181      |
|    time_elapsed    | 666      |
|    total_timesteps | 185344   |
---------------------------------
Eval num_timesteps=185500, episode_reward=451.73 +/- 724.42
Episode length: 35.42 +/- 5.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 452           |
| time/                   |               |
|    total_timesteps      | 185500        |
| train/                  |               |
|    approx_kl            | 1.8845894e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00316      |
|    explained_variance   | 0.0547        |
|    learning_rate        | 0.001         |
|    loss                 | 2.83e+04      |
|    n_updates            | 7600          |
|    policy_gradient_loss | -0.000137     |
|    value_loss           | 8.53e+04      |
-------------------------------------------
Eval num_timesteps=186000, episode_reward=377.34 +/- 672.52
Episode length: 35.28 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 182      |
|    time_elapsed    | 670      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=422.07 +/- 736.69
Episode length: 35.30 +/- 6.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 422           |
| time/                   |               |
|    total_timesteps      | 186500        |
| train/                  |               |
|    approx_kl            | 2.6893686e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00422      |
|    explained_variance   | -0.0211       |
|    learning_rate        | 0.001         |
|    loss                 | 4.24e+04      |
|    n_updates            | 7610          |
|    policy_gradient_loss | -0.000155     |
|    value_loss           | 1e+05         |
-------------------------------------------
Eval num_timesteps=187000, episode_reward=596.63 +/- 770.04
Episode length: 36.76 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 183      |
|    time_elapsed    | 673      |
|    total_timesteps | 187392   |
---------------------------------
Eval num_timesteps=187500, episode_reward=311.71 +/- 617.94
Episode length: 35.64 +/- 6.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 312           |
| time/                   |               |
|    total_timesteps      | 187500        |
| train/                  |               |
|    approx_kl            | 3.4960103e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00288      |
|    explained_variance   | 0.0354        |
|    learning_rate        | 0.001         |
|    loss                 | 4.36e+04      |
|    n_updates            | 7620          |
|    policy_gradient_loss | -0.000482     |
|    value_loss           | 7.04e+04      |
-------------------------------------------
Eval num_timesteps=188000, episode_reward=414.86 +/- 591.10
Episode length: 36.94 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 360      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 184      |
|    time_elapsed    | 677      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=527.20 +/- 754.11
Episode length: 35.94 +/- 6.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 527          |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 1.519802e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00253     |
|    explained_variance   | -0.0969      |
|    learning_rate        | 0.001        |
|    loss                 | 4.13e+04     |
|    n_updates            | 7630         |
|    policy_gradient_loss | -6.71e-05    |
|    value_loss           | 9.62e+04     |
------------------------------------------
Eval num_timesteps=189000, episode_reward=177.53 +/- 544.07
Episode length: 33.96 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 185      |
|    time_elapsed    | 681      |
|    total_timesteps | 189440   |
---------------------------------
Eval num_timesteps=189500, episode_reward=324.90 +/- 752.31
Episode length: 33.10 +/- 7.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.1         |
|    mean_reward          | 325          |
| time/                   |              |
|    total_timesteps      | 189500       |
| train/                  |              |
|    approx_kl            | 8.593197e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00247     |
|    explained_variance   | 0.0392       |
|    learning_rate        | 0.001        |
|    loss                 | 4.68e+04     |
|    n_updates            | 7640         |
|    policy_gradient_loss | -8.57e-05    |
|    value_loss           | 9.65e+04     |
------------------------------------------
Eval num_timesteps=190000, episode_reward=601.60 +/- 784.87
Episode length: 36.06 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 602      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 186      |
|    time_elapsed    | 684      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=285.45 +/- 708.13
Episode length: 33.86 +/- 7.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 285           |
| time/                   |               |
|    total_timesteps      | 190500        |
| train/                  |               |
|    approx_kl            | 5.8073783e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00373      |
|    explained_variance   | -0.117        |
|    learning_rate        | 0.001         |
|    loss                 | 1.6e+04       |
|    n_updates            | 7650          |
|    policy_gradient_loss | -0.000261     |
|    value_loss           | 8.58e+04      |
-------------------------------------------
Eval num_timesteps=191000, episode_reward=393.48 +/- 734.04
Episode length: 34.92 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 393      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 187      |
|    time_elapsed    | 688      |
|    total_timesteps | 191488   |
---------------------------------
Eval num_timesteps=191500, episode_reward=560.60 +/- 713.46
Episode length: 36.52 +/- 5.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 561           |
| time/                   |               |
|    total_timesteps      | 191500        |
| train/                  |               |
|    approx_kl            | 1.2692763e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00428      |
|    explained_variance   | -0.0746       |
|    learning_rate        | 0.001         |
|    loss                 | 2.95e+04      |
|    n_updates            | 7660          |
|    policy_gradient_loss | -9.83e-05     |
|    value_loss           | 7.56e+04      |
-------------------------------------------
Eval num_timesteps=192000, episode_reward=436.03 +/- 724.98
Episode length: 35.42 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=218.46 +/- 551.08
Episode length: 34.16 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 188      |
|    time_elapsed    | 692      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=310.39 +/- 708.98
Episode length: 33.64 +/- 6.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 310           |
| time/                   |               |
|    total_timesteps      | 193000        |
| train/                  |               |
|    approx_kl            | 1.4294637e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00324      |
|    explained_variance   | -0.0798       |
|    learning_rate        | 0.001         |
|    loss                 | 3.71e+04      |
|    n_updates            | 7670          |
|    policy_gradient_loss | -0.000131     |
|    value_loss           | 9.18e+04      |
-------------------------------------------
Eval num_timesteps=193500, episode_reward=494.52 +/- 770.86
Episode length: 35.28 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 450      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 189      |
|    time_elapsed    | 696      |
|    total_timesteps | 193536   |
---------------------------------
Eval num_timesteps=194000, episode_reward=503.50 +/- 782.78
Episode length: 35.44 +/- 6.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 504           |
| time/                   |               |
|    total_timesteps      | 194000        |
| train/                  |               |
|    approx_kl            | 1.2243399e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00263      |
|    explained_variance   | 0.0499        |
|    learning_rate        | 0.001         |
|    loss                 | 4.21e+04      |
|    n_updates            | 7680          |
|    policy_gradient_loss | -0.000145     |
|    value_loss           | 8.51e+04      |
-------------------------------------------
Eval num_timesteps=194500, episode_reward=246.01 +/- 665.24
Episode length: 32.82 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 528      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 190      |
|    time_elapsed    | 700      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=310.99 +/- 681.32
Episode length: 34.70 +/- 6.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 311           |
| time/                   |               |
|    total_timesteps      | 195000        |
| train/                  |               |
|    approx_kl            | 1.3061217e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00292      |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.001         |
|    loss                 | 3.01e+04      |
|    n_updates            | 7690          |
|    policy_gradient_loss | -0.00019      |
|    value_loss           | 9.29e+04      |
-------------------------------------------
Eval num_timesteps=195500, episode_reward=245.78 +/- 674.38
Episode length: 33.62 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 555      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 191      |
|    time_elapsed    | 703      |
|    total_timesteps | 195584   |
---------------------------------
Eval num_timesteps=196000, episode_reward=433.27 +/- 693.34
Episode length: 36.40 +/- 6.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 433          |
| time/                   |              |
|    total_timesteps      | 196000       |
| train/                  |              |
|    approx_kl            | 4.088797e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00186     |
|    explained_variance   | -0.0576      |
|    learning_rate        | 0.001        |
|    loss                 | 2.78e+04     |
|    n_updates            | 7700         |
|    policy_gradient_loss | -9.75e-05    |
|    value_loss           | 7.94e+04     |
------------------------------------------
Eval num_timesteps=196500, episode_reward=459.76 +/- 769.25
Episode length: 34.92 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 550      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 192      |
|    time_elapsed    | 707      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=619.71 +/- 696.81
Episode length: 37.64 +/- 4.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.6         |
|    mean_reward          | 620          |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 3.334717e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00169     |
|    explained_variance   | -0.0601      |
|    learning_rate        | 0.001        |
|    loss                 | 5.07e+04     |
|    n_updates            | 7710         |
|    policy_gradient_loss | -3.49e-05    |
|    value_loss           | 1.03e+05     |
------------------------------------------
Eval num_timesteps=197500, episode_reward=394.19 +/- 673.04
Episode length: 35.34 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.8     |
|    ep_rew_mean     | 521      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 193      |
|    time_elapsed    | 711      |
|    total_timesteps | 197632   |
---------------------------------
Eval num_timesteps=198000, episode_reward=512.18 +/- 715.36
Episode length: 36.48 +/- 5.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.5         |
|    mean_reward          | 512          |
| time/                   |              |
|    total_timesteps      | 198000       |
| train/                  |              |
|    approx_kl            | 7.717754e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00184     |
|    explained_variance   | 0.0286       |
|    learning_rate        | 0.001        |
|    loss                 | 3.38e+04     |
|    n_updates            | 7720         |
|    policy_gradient_loss | -6.06e-05    |
|    value_loss           | 8.86e+04     |
------------------------------------------
Eval num_timesteps=198500, episode_reward=280.71 +/- 586.14
Episode length: 34.08 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 440      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 194      |
|    time_elapsed    | 714      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=470.05 +/- 715.17
Episode length: 35.86 +/- 6.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 470          |
| time/                   |              |
|    total_timesteps      | 199000       |
| train/                  |              |
|    approx_kl            | 4.208414e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00162     |
|    explained_variance   | 0.00852      |
|    learning_rate        | 0.001        |
|    loss                 | 4.96e+04     |
|    n_updates            | 7730         |
|    policy_gradient_loss | -3.21e-05    |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=199500, episode_reward=327.91 +/- 701.20
Episode length: 33.90 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 328      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 414      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 195      |
|    time_elapsed    | 718      |
|    total_timesteps | 199680   |
---------------------------------
Eval num_timesteps=200000, episode_reward=506.72 +/- 749.50
Episode length: 36.86 +/- 5.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.9          |
|    mean_reward          | 507           |
| time/                   |               |
|    total_timesteps      | 200000        |
| train/                  |               |
|    approx_kl            | 1.3492536e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00177      |
|    explained_variance   | 0.119         |
|    learning_rate        | 0.001         |
|    loss                 | 4.79e+04      |
|    n_updates            | 7740          |
|    policy_gradient_loss | -1.4e-05      |
|    value_loss           | 1.22e+05      |
-------------------------------------------
Eval num_timesteps=200500, episode_reward=334.42 +/- 637.00
Episode length: 34.90 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 334      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 196      |
|    time_elapsed    | 722      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=355.32 +/- 637.46
Episode length: 35.80 +/- 5.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 355           |
| time/                   |               |
|    total_timesteps      | 201000        |
| train/                  |               |
|    approx_kl            | 1.6973354e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00208      |
|    explained_variance   | -0.422        |
|    learning_rate        | 0.001         |
|    loss                 | 4.65e+04      |
|    n_updates            | 7750          |
|    policy_gradient_loss | -3.21e-05     |
|    value_loss           | 7.45e+04      |
-------------------------------------------
Eval num_timesteps=201500, episode_reward=535.07 +/- 744.85
Episode length: 36.20 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 197      |
|    time_elapsed    | 726      |
|    total_timesteps | 201728   |
---------------------------------
Eval num_timesteps=202000, episode_reward=312.80 +/- 710.94
Episode length: 34.64 +/- 6.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.6        |
|    mean_reward          | 313         |
| time/                   |             |
|    total_timesteps      | 202000      |
| train/                  |             |
|    approx_kl            | 8.03452e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00252    |
|    explained_variance   | -0.0228     |
|    learning_rate        | 0.001       |
|    loss                 | 4.63e+04    |
|    n_updates            | 7760        |
|    policy_gradient_loss | -9.53e-05   |
|    value_loss           | 7.61e+04    |
-----------------------------------------
Eval num_timesteps=202500, episode_reward=521.28 +/- 787.41
Episode length: 35.92 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 521      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 337      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 198      |
|    time_elapsed    | 729      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=361.12 +/- 631.19
Episode length: 35.28 +/- 6.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 361          |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 4.337635e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00202     |
|    explained_variance   | -0.0635      |
|    learning_rate        | 0.001        |
|    loss                 | 4.92e+04     |
|    n_updates            | 7770         |
|    policy_gradient_loss | -8.7e-05     |
|    value_loss           | 7.97e+04     |
------------------------------------------
Eval num_timesteps=203500, episode_reward=542.94 +/- 834.39
Episode length: 35.32 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 543      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 199      |
|    time_elapsed    | 733      |
|    total_timesteps | 203776   |
---------------------------------
Eval num_timesteps=204000, episode_reward=387.83 +/- 710.70
Episode length: 34.36 +/- 7.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 388           |
| time/                   |               |
|    total_timesteps      | 204000        |
| train/                  |               |
|    approx_kl            | 2.8463546e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00176      |
|    explained_variance   | -0.0129       |
|    learning_rate        | 0.001         |
|    loss                 | 3.29e+04      |
|    n_updates            | 7780          |
|    policy_gradient_loss | -1.4e-05      |
|    value_loss           | 1.16e+05      |
-------------------------------------------
Eval num_timesteps=204500, episode_reward=510.76 +/- 758.43
Episode length: 36.10 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 472      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 200      |
|    time_elapsed    | 736      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=429.09 +/- 698.87
Episode length: 35.28 +/- 7.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 429           |
| time/                   |               |
|    total_timesteps      | 205000        |
| train/                  |               |
|    approx_kl            | 2.5861664e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0017       |
|    explained_variance   | 0.11          |
|    learning_rate        | 0.001         |
|    loss                 | 4.41e+04      |
|    n_updates            | 7790          |
|    policy_gradient_loss | -4.59e-06     |
|    value_loss           | 1.03e+05      |
-------------------------------------------
Eval num_timesteps=205500, episode_reward=516.14 +/- 747.91
Episode length: 37.18 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 516      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 500      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 201      |
|    time_elapsed    | 740      |
|    total_timesteps | 205824   |
---------------------------------
Eval num_timesteps=206000, episode_reward=431.28 +/- 694.47
Episode length: 35.80 +/- 6.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 431          |
| time/                   |              |
|    total_timesteps      | 206000       |
| train/                  |              |
|    approx_kl            | 9.077485e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00216     |
|    explained_variance   | -0.0195      |
|    learning_rate        | 0.001        |
|    loss                 | 4.09e+04     |
|    n_updates            | 7800         |
|    policy_gradient_loss | 1.2e-05      |
|    value_loss           | 1.14e+05     |
------------------------------------------
Eval num_timesteps=206500, episode_reward=518.66 +/- 732.31
Episode length: 36.18 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 549      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 202      |
|    time_elapsed    | 744      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=400.48 +/- 715.96
Episode length: 34.26 +/- 6.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 400          |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 4.755566e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00157     |
|    explained_variance   | -0.0215      |
|    learning_rate        | 0.001        |
|    loss                 | 5.18e+04     |
|    n_updates            | 7810         |
|    policy_gradient_loss | -1.1e-05     |
|    value_loss           | 1.06e+05     |
------------------------------------------
Eval num_timesteps=207500, episode_reward=377.70 +/- 687.66
Episode length: 35.26 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 454      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 203      |
|    time_elapsed    | 747      |
|    total_timesteps | 207872   |
---------------------------------
Eval num_timesteps=208000, episode_reward=522.61 +/- 755.74
Episode length: 35.74 +/- 6.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 523           |
| time/                   |               |
|    total_timesteps      | 208000        |
| train/                  |               |
|    approx_kl            | 3.2270327e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00197      |
|    explained_variance   | 0.00835       |
|    learning_rate        | 0.001         |
|    loss                 | 3.63e+04      |
|    n_updates            | 7820          |
|    policy_gradient_loss | -4.49e-05     |
|    value_loss           | 9.92e+04      |
-------------------------------------------
Eval num_timesteps=208500, episode_reward=430.67 +/- 727.31
Episode length: 34.76 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 430      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 204      |
|    time_elapsed    | 751      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=409.63 +/- 663.77
Episode length: 35.84 +/- 6.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 410           |
| time/                   |               |
|    total_timesteps      | 209000        |
| train/                  |               |
|    approx_kl            | 2.1531014e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00162      |
|    explained_variance   | 0.0138        |
|    learning_rate        | 0.001         |
|    loss                 | 4.07e+04      |
|    n_updates            | 7830          |
|    policy_gradient_loss | 2.09e-06      |
|    value_loss           | 8.67e+04      |
-------------------------------------------
Eval num_timesteps=209500, episode_reward=608.01 +/- 834.33
Episode length: 36.38 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 608      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 205      |
|    time_elapsed    | 755      |
|    total_timesteps | 209920   |
---------------------------------
Eval num_timesteps=210000, episode_reward=282.74 +/- 646.09
Episode length: 34.26 +/- 7.63
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.3           |
|    mean_reward          | 283            |
| time/                   |                |
|    total_timesteps      | 210000         |
| train/                  |                |
|    approx_kl            | -6.9849193e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00141       |
|    explained_variance   | -0.0391        |
|    learning_rate        | 0.001          |
|    loss                 | 2.95e+04       |
|    n_updates            | 7840           |
|    policy_gradient_loss | -9.86e-06      |
|    value_loss           | 8.64e+04       |
--------------------------------------------
Eval num_timesteps=210500, episode_reward=385.24 +/- 665.20
Episode length: 35.62 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 351      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 206      |
|    time_elapsed    | 758      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=410.18 +/- 674.52
Episode length: 35.68 +/- 6.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 410          |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 1.014676e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0014      |
|    explained_variance   | 0.0682       |
|    learning_rate        | 0.001        |
|    loss                 | 5.06e+04     |
|    n_updates            | 7850         |
|    policy_gradient_loss | -2.36e-05    |
|    value_loss           | 1.04e+05     |
------------------------------------------
Eval num_timesteps=211500, episode_reward=536.20 +/- 796.19
Episode length: 34.80 +/- 7.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 536      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 366      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 207      |
|    time_elapsed    | 762      |
|    total_timesteps | 211968   |
---------------------------------
Eval num_timesteps=212000, episode_reward=466.30 +/- 738.48
Episode length: 35.52 +/- 6.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 466          |
| time/                   |              |
|    total_timesteps      | 212000       |
| train/                  |              |
|    approx_kl            | 1.631555e-05 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00159     |
|    explained_variance   | 0.116        |
|    learning_rate        | 0.001        |
|    loss                 | 4.91e+04     |
|    n_updates            | 7860         |
|    policy_gradient_loss | 0.000168     |
|    value_loss           | 9.72e+04     |
------------------------------------------
Eval num_timesteps=212500, episode_reward=237.33 +/- 575.49
Episode length: 34.36 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 208      |
|    time_elapsed    | 765      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=344.30 +/- 650.02
Episode length: 35.22 +/- 6.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 344           |
| time/                   |               |
|    total_timesteps      | 213000        |
| train/                  |               |
|    approx_kl            | 4.0052691e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00126      |
|    explained_variance   | 0.0327        |
|    learning_rate        | 0.001         |
|    loss                 | 4.01e+04      |
|    n_updates            | 7870          |
|    policy_gradient_loss | -5.05e-05     |
|    value_loss           | 1.09e+05      |
-------------------------------------------
Eval num_timesteps=213500, episode_reward=560.82 +/- 760.55
Episode length: 36.48 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 561      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=369.59 +/- 633.73
Episode length: 35.08 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 429      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 209      |
|    time_elapsed    | 770      |
|    total_timesteps | 214016   |
---------------------------------
Eval num_timesteps=214500, episode_reward=293.48 +/- 594.20
Episode length: 34.62 +/- 6.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 293           |
| time/                   |               |
|    total_timesteps      | 214500        |
| train/                  |               |
|    approx_kl            | 2.6061898e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00401      |
|    explained_variance   | -0.0548       |
|    learning_rate        | 0.001         |
|    loss                 | 5.33e+04      |
|    n_updates            | 7880          |
|    policy_gradient_loss | -7.63e-05     |
|    value_loss           | 8.73e+04      |
-------------------------------------------
Eval num_timesteps=215000, episode_reward=298.56 +/- 627.22
Episode length: 35.08 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 299      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 527      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 210      |
|    time_elapsed    | 774      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=350.38 +/- 733.86
Episode length: 34.18 +/- 7.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 350          |
| time/                   |              |
|    total_timesteps      | 215500       |
| train/                  |              |
|    approx_kl            | 0.0011875445 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00391     |
|    explained_variance   | 0.0705       |
|    learning_rate        | 0.001        |
|    loss                 | 4.85e+04     |
|    n_updates            | 7890         |
|    policy_gradient_loss | 0.000125     |
|    value_loss           | 1.15e+05     |
------------------------------------------
Eval num_timesteps=216000, episode_reward=337.39 +/- 663.50
Episode length: 35.18 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 529      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 211      |
|    time_elapsed    | 778      |
|    total_timesteps | 216064   |
---------------------------------
Eval num_timesteps=216500, episode_reward=355.92 +/- 668.14
Episode length: 34.86 +/- 6.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 356          |
| time/                   |              |
|    total_timesteps      | 216500       |
| train/                  |              |
|    approx_kl            | 8.108141e-05 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00913     |
|    explained_variance   | -0.243       |
|    learning_rate        | 0.001        |
|    loss                 | 4.34e+04     |
|    n_updates            | 7900         |
|    policy_gradient_loss | -0.000359    |
|    value_loss           | 9.92e+04     |
------------------------------------------
Eval num_timesteps=217000, episode_reward=529.25 +/- 766.73
Episode length: 35.98 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 529      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 212      |
|    time_elapsed    | 781      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=309.73 +/- 682.43
Episode length: 33.88 +/- 6.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 310           |
| time/                   |               |
|    total_timesteps      | 217500        |
| train/                  |               |
|    approx_kl            | 1.9485888e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00302      |
|    explained_variance   | -0.107        |
|    learning_rate        | 0.001         |
|    loss                 | 5.09e+04      |
|    n_updates            | 7910          |
|    policy_gradient_loss | -4.16e-05     |
|    value_loss           | 8.91e+04      |
-------------------------------------------
Eval num_timesteps=218000, episode_reward=377.79 +/- 677.08
Episode length: 35.06 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 213      |
|    time_elapsed    | 785      |
|    total_timesteps | 218112   |
---------------------------------
Eval num_timesteps=218500, episode_reward=441.34 +/- 700.39
Episode length: 35.58 +/- 6.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 441           |
| time/                   |               |
|    total_timesteps      | 218500        |
| train/                  |               |
|    approx_kl            | 2.0880834e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00233      |
|    explained_variance   | -0.0274       |
|    learning_rate        | 0.001         |
|    loss                 | 4.52e+04      |
|    n_updates            | 7920          |
|    policy_gradient_loss | -3.93e-05     |
|    value_loss           | 1.06e+05      |
-------------------------------------------
Eval num_timesteps=219000, episode_reward=422.66 +/- 782.71
Episode length: 34.10 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 373      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 214      |
|    time_elapsed    | 789      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=392.58 +/- 706.76
Episode length: 35.36 +/- 6.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 393           |
| time/                   |               |
|    total_timesteps      | 219500        |
| train/                  |               |
|    approx_kl            | 2.2680615e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00377      |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.001         |
|    loss                 | 2.17e+04      |
|    n_updates            | 7930          |
|    policy_gradient_loss | -0.000358     |
|    value_loss           | 9.8e+04       |
-------------------------------------------
Eval num_timesteps=220000, episode_reward=459.91 +/- 765.44
Episode length: 34.86 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 215      |
|    time_elapsed    | 792      |
|    total_timesteps | 220160   |
---------------------------------
Eval num_timesteps=220500, episode_reward=431.44 +/- 777.72
Episode length: 34.64 +/- 6.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 431           |
| time/                   |               |
|    total_timesteps      | 220500        |
| train/                  |               |
|    approx_kl            | 1.2130477e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00344      |
|    explained_variance   | 0.084         |
|    learning_rate        | 0.001         |
|    loss                 | 5.91e+04      |
|    n_updates            | 7940          |
|    policy_gradient_loss | -0.000131     |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=221000, episode_reward=511.88 +/- 728.39
Episode length: 36.34 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 433      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 216      |
|    time_elapsed    | 796      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=435.07 +/- 687.37
Episode length: 35.56 +/- 6.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 435          |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 7.998722e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00333     |
|    explained_variance   | -0.0886      |
|    learning_rate        | 0.001        |
|    loss                 | 3.8e+04      |
|    n_updates            | 7950         |
|    policy_gradient_loss | -0.000149    |
|    value_loss           | 1.21e+05     |
------------------------------------------
Eval num_timesteps=222000, episode_reward=386.32 +/- 665.48
Episode length: 35.38 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 399      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 217      |
|    time_elapsed    | 800      |
|    total_timesteps | 222208   |
---------------------------------
Eval num_timesteps=222500, episode_reward=546.58 +/- 712.30
Episode length: 35.76 +/- 6.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 547          |
| time/                   |              |
|    total_timesteps      | 222500       |
| train/                  |              |
|    approx_kl            | 2.724235e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00396     |
|    explained_variance   | 0.00939      |
|    learning_rate        | 0.001        |
|    loss                 | 5.32e+04     |
|    n_updates            | 7960         |
|    policy_gradient_loss | -0.00019     |
|    value_loss           | 1.06e+05     |
------------------------------------------
Eval num_timesteps=223000, episode_reward=446.76 +/- 736.06
Episode length: 35.78 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 378      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 218      |
|    time_elapsed    | 803      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=323.36 +/- 665.21
Episode length: 34.94 +/- 6.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 323          |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 3.870926e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00473     |
|    explained_variance   | 0.103        |
|    learning_rate        | 0.001        |
|    loss                 | 3.62e+04     |
|    n_updates            | 7970         |
|    policy_gradient_loss | -0.000193    |
|    value_loss           | 9.09e+04     |
------------------------------------------
Eval num_timesteps=224000, episode_reward=517.24 +/- 747.91
Episode length: 36.44 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 517      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 393      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 219      |
|    time_elapsed    | 807      |
|    total_timesteps | 224256   |
---------------------------------
Eval num_timesteps=224500, episode_reward=359.12 +/- 668.35
Episode length: 34.34 +/- 6.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 359           |
| time/                   |               |
|    total_timesteps      | 224500        |
| train/                  |               |
|    approx_kl            | 2.8927752e-05 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00341      |
|    explained_variance   | -0.00395      |
|    learning_rate        | 0.001         |
|    loss                 | 6.25e+04      |
|    n_updates            | 7980          |
|    policy_gradient_loss | 0.000179      |
|    value_loss           | 1.18e+05      |
-------------------------------------------
Eval num_timesteps=225000, episode_reward=376.64 +/- 694.27
Episode length: 34.66 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 220      |
|    time_elapsed    | 811      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=327.07 +/- 657.99
Episode length: 34.30 +/- 6.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 327           |
| time/                   |               |
|    total_timesteps      | 225500        |
| train/                  |               |
|    approx_kl            | 1.1653756e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00291      |
|    explained_variance   | -0.0937       |
|    learning_rate        | 0.001         |
|    loss                 | 3.45e+04      |
|    n_updates            | 7990          |
|    policy_gradient_loss | -7.7e-05      |
|    value_loss           | 7.4e+04       |
-------------------------------------------
Eval num_timesteps=226000, episode_reward=281.47 +/- 670.86
Episode length: 33.74 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 221      |
|    time_elapsed    | 814      |
|    total_timesteps | 226304   |
---------------------------------
Eval num_timesteps=226500, episode_reward=288.23 +/- 667.51
Episode length: 34.16 +/- 6.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 288           |
| time/                   |               |
|    total_timesteps      | 226500        |
| train/                  |               |
|    approx_kl            | 7.2554685e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00229      |
|    explained_variance   | -0.0168       |
|    learning_rate        | 0.001         |
|    loss                 | 2.75e+04      |
|    n_updates            | 8000          |
|    policy_gradient_loss | 0.000171      |
|    value_loss           | 7.3e+04       |
-------------------------------------------
Eval num_timesteps=227000, episode_reward=334.24 +/- 658.31
Episode length: 34.62 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 334      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 489      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 222      |
|    time_elapsed    | 818      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=431.62 +/- 699.44
Episode length: 35.00 +/- 6.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 432           |
| time/                   |               |
|    total_timesteps      | 227500        |
| train/                  |               |
|    approx_kl            | 1.9216677e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00297      |
|    explained_variance   | -0.141        |
|    learning_rate        | 0.001         |
|    loss                 | 4.26e+04      |
|    n_updates            | 8010          |
|    policy_gradient_loss | -0.000274     |
|    value_loss           | 9.76e+04      |
-------------------------------------------
Eval num_timesteps=228000, episode_reward=539.11 +/- 758.23
Episode length: 36.06 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 539      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 434      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 223      |
|    time_elapsed    | 821      |
|    total_timesteps | 228352   |
---------------------------------
Eval num_timesteps=228500, episode_reward=509.43 +/- 723.38
Episode length: 35.94 +/- 6.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 509          |
| time/                   |              |
|    total_timesteps      | 228500       |
| train/                  |              |
|    approx_kl            | 5.551381e-06 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00463     |
|    explained_variance   | 0.0389       |
|    learning_rate        | 0.001        |
|    loss                 | 4.47e+04     |
|    n_updates            | 8020         |
|    policy_gradient_loss | 0.000805     |
|    value_loss           | 1.06e+05     |
------------------------------------------
Eval num_timesteps=229000, episode_reward=410.61 +/- 735.21
Episode length: 34.20 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 411      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 416      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 224      |
|    time_elapsed    | 825      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=410.05 +/- 597.86
Episode length: 36.32 +/- 4.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 410          |
| time/                   |              |
|    total_timesteps      | 229500       |
| train/                  |              |
|    approx_kl            | 6.991322e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00304     |
|    explained_variance   | 0.037        |
|    learning_rate        | 0.001        |
|    loss                 | 3.97e+04     |
|    n_updates            | 8030         |
|    policy_gradient_loss | -6.56e-05    |
|    value_loss           | 9.31e+04     |
------------------------------------------
Eval num_timesteps=230000, episode_reward=342.24 +/- 665.15
Episode length: 34.22 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 496      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 225      |
|    time_elapsed    | 828      |
|    total_timesteps | 230400   |
---------------------------------
Eval num_timesteps=230500, episode_reward=420.13 +/- 703.08
Episode length: 35.76 +/- 6.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 420           |
| time/                   |               |
|    total_timesteps      | 230500        |
| train/                  |               |
|    approx_kl            | 4.7789654e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00243      |
|    explained_variance   | 0.0521        |
|    learning_rate        | 0.001         |
|    loss                 | 4.8e+04       |
|    n_updates            | 8040          |
|    policy_gradient_loss | -4.78e-05     |
|    value_loss           | 1.06e+05      |
-------------------------------------------
Eval num_timesteps=231000, episode_reward=485.69 +/- 717.80
Episode length: 36.10 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 443      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 226      |
|    time_elapsed    | 832      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=499.96 +/- 764.60
Episode length: 35.42 +/- 6.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 500           |
| time/                   |               |
|    total_timesteps      | 231500        |
| train/                  |               |
|    approx_kl            | 0.00066627533 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00337      |
|    explained_variance   | 0.131         |
|    learning_rate        | 0.001         |
|    loss                 | 4.22e+04      |
|    n_updates            | 8050          |
|    policy_gradient_loss | -0.000239     |
|    value_loss           | 9.34e+04      |
-------------------------------------------
Eval num_timesteps=232000, episode_reward=450.22 +/- 676.35
Episode length: 35.90 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 450      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 227      |
|    time_elapsed    | 836      |
|    total_timesteps | 232448   |
---------------------------------
Eval num_timesteps=232500, episode_reward=474.87 +/- 669.06
Episode length: 36.44 +/- 5.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 475           |
| time/                   |               |
|    total_timesteps      | 232500        |
| train/                  |               |
|    approx_kl            | 3.6890851e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00243      |
|    explained_variance   | 0.158         |
|    learning_rate        | 0.001         |
|    loss                 | 5.32e+04      |
|    n_updates            | 8060          |
|    policy_gradient_loss | -6.45e-05     |
|    value_loss           | 9.62e+04      |
-------------------------------------------
Eval num_timesteps=233000, episode_reward=448.83 +/- 707.61
Episode length: 35.76 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 449      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 410      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 228      |
|    time_elapsed    | 839      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=630.92 +/- 778.18
Episode length: 36.76 +/- 6.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.8          |
|    mean_reward          | 631           |
| time/                   |               |
|    total_timesteps      | 233500        |
| train/                  |               |
|    approx_kl            | 0.00066934153 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00273      |
|    explained_variance   | 0.0455        |
|    learning_rate        | 0.001         |
|    loss                 | 4.19e+04      |
|    n_updates            | 8070          |
|    policy_gradient_loss | -0.00022      |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=234000, episode_reward=376.41 +/- 665.41
Episode length: 34.94 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 334      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 229      |
|    time_elapsed    | 843      |
|    total_timesteps | 234496   |
---------------------------------
Eval num_timesteps=234500, episode_reward=466.29 +/- 710.39
Episode length: 36.06 +/- 6.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 466          |
| time/                   |              |
|    total_timesteps      | 234500       |
| train/                  |              |
|    approx_kl            | 0.0008043922 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00257     |
|    explained_variance   | -0.00334     |
|    learning_rate        | 0.001        |
|    loss                 | 3.64e+04     |
|    n_updates            | 8080         |
|    policy_gradient_loss | 7.01e-05     |
|    value_loss           | 1e+05        |
------------------------------------------
Eval num_timesteps=235000, episode_reward=609.98 +/- 775.02
Episode length: 35.32 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 610      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=215.36 +/- 535.49
Episode length: 33.96 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 377      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 230      |
|    time_elapsed    | 848      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=553.69 +/- 840.17
Episode length: 35.62 +/- 6.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 554          |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 8.554198e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00169     |
|    explained_variance   | 0.00457      |
|    learning_rate        | 0.001        |
|    loss                 | 4.43e+04     |
|    n_updates            | 8090         |
|    policy_gradient_loss | -6.88e-05    |
|    value_loss           | 1.11e+05     |
------------------------------------------
Eval num_timesteps=236500, episode_reward=492.87 +/- 712.13
Episode length: 36.54 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 493      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 417      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 231      |
|    time_elapsed    | 852      |
|    total_timesteps | 236544   |
---------------------------------
Eval num_timesteps=237000, episode_reward=584.57 +/- 828.01
Episode length: 36.20 +/- 7.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 585          |
| time/                   |              |
|    total_timesteps      | 237000       |
| train/                  |              |
|    approx_kl            | 3.426685e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00128     |
|    explained_variance   | 0.0997       |
|    learning_rate        | 0.001        |
|    loss                 | 5.28e+04     |
|    n_updates            | 8100         |
|    policy_gradient_loss | -5.48e-05    |
|    value_loss           | 1.17e+05     |
------------------------------------------
Eval num_timesteps=237500, episode_reward=412.99 +/- 748.29
Episode length: 34.48 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 413      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 447      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 232      |
|    time_elapsed    | 855      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=457.99 +/- 768.16
Episode length: 34.72 +/- 7.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 458           |
| time/                   |               |
|    total_timesteps      | 238000        |
| train/                  |               |
|    approx_kl            | 1.2586825e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00212      |
|    explained_variance   | 0.0344        |
|    learning_rate        | 0.001         |
|    loss                 | 5.17e+04      |
|    n_updates            | 8110          |
|    policy_gradient_loss | -7.37e-05     |
|    value_loss           | 1.05e+05      |
-------------------------------------------
Eval num_timesteps=238500, episode_reward=486.61 +/- 678.93
Episode length: 35.60 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 432      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 233      |
|    time_elapsed    | 859      |
|    total_timesteps | 238592   |
---------------------------------
Eval num_timesteps=239000, episode_reward=401.51 +/- 668.01
Episode length: 35.40 +/- 7.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 402          |
| time/                   |              |
|    total_timesteps      | 239000       |
| train/                  |              |
|    approx_kl            | 3.508816e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00441     |
|    explained_variance   | 0.066        |
|    learning_rate        | 0.001        |
|    loss                 | 2.77e+04     |
|    n_updates            | 8120         |
|    policy_gradient_loss | 6.75e-06     |
|    value_loss           | 1.05e+05     |
------------------------------------------
Eval num_timesteps=239500, episode_reward=387.02 +/- 645.52
Episode length: 35.26 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 234      |
|    time_elapsed    | 862      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=570.89 +/- 676.97
Episode length: 36.36 +/- 5.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 571           |
| time/                   |               |
|    total_timesteps      | 240000        |
| train/                  |               |
|    approx_kl            | 4.0734885e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00728      |
|    explained_variance   | 0.111         |
|    learning_rate        | 0.001         |
|    loss                 | 6.64e+04      |
|    n_updates            | 8130          |
|    policy_gradient_loss | -0.00012      |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=240500, episode_reward=468.09 +/- 702.27
Episode length: 35.88 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 235      |
|    time_elapsed    | 866      |
|    total_timesteps | 240640   |
---------------------------------
Eval num_timesteps=241000, episode_reward=280.57 +/- 652.87
Episode length: 33.94 +/- 6.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.9         |
|    mean_reward          | 281          |
| time/                   |              |
|    total_timesteps      | 241000       |
| train/                  |              |
|    approx_kl            | 4.407426e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00375     |
|    explained_variance   | -0.0569      |
|    learning_rate        | 0.001        |
|    loss                 | 5.57e+04     |
|    n_updates            | 8140         |
|    policy_gradient_loss | -0.000255    |
|    value_loss           | 1.18e+05     |
------------------------------------------
Eval num_timesteps=241500, episode_reward=397.56 +/- 696.80
Episode length: 35.88 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 479      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 236      |
|    time_elapsed    | 870      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=570.26 +/- 775.70
Episode length: 36.52 +/- 6.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 570           |
| time/                   |               |
|    total_timesteps      | 242000        |
| train/                  |               |
|    approx_kl            | 1.8135179e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00236      |
|    explained_variance   | 0.0484        |
|    learning_rate        | 0.001         |
|    loss                 | 4.14e+04      |
|    n_updates            | 8150          |
|    policy_gradient_loss | -9.67e-05     |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=242500, episode_reward=572.11 +/- 747.26
Episode length: 35.90 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 572      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 553      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 237      |
|    time_elapsed    | 873      |
|    total_timesteps | 242688   |
---------------------------------
Eval num_timesteps=243000, episode_reward=262.78 +/- 652.91
Episode length: 33.58 +/- 6.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 263           |
| time/                   |               |
|    total_timesteps      | 243000        |
| train/                  |               |
|    approx_kl            | 3.3623073e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00389      |
|    explained_variance   | 0.0335        |
|    learning_rate        | 0.001         |
|    loss                 | 3.81e+04      |
|    n_updates            | 8160          |
|    policy_gradient_loss | -0.000195     |
|    value_loss           | 9.19e+04      |
-------------------------------------------
Eval num_timesteps=243500, episode_reward=459.19 +/- 699.61
Episode length: 35.98 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 572      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 238      |
|    time_elapsed    | 877      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=483.27 +/- 746.62
Episode length: 35.32 +/- 6.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 483          |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0030745557 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00157     |
|    explained_variance   | -0.0301      |
|    learning_rate        | 0.001        |
|    loss                 | 3.48e+04     |
|    n_updates            | 8170         |
|    policy_gradient_loss | -0.00071     |
|    value_loss           | 9.87e+04     |
------------------------------------------
Eval num_timesteps=244500, episode_reward=347.74 +/- 699.46
Episode length: 34.30 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 348      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 490      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 239      |
|    time_elapsed    | 880      |
|    total_timesteps | 244736   |
---------------------------------
Eval num_timesteps=245000, episode_reward=568.30 +/- 776.60
Episode length: 36.36 +/- 6.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 568           |
| time/                   |               |
|    total_timesteps      | 245000        |
| train/                  |               |
|    approx_kl            | 1.9557774e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00123      |
|    explained_variance   | 0.0165        |
|    learning_rate        | 0.001         |
|    loss                 | 4.7e+04       |
|    n_updates            | 8180          |
|    policy_gradient_loss | -6.32e-05     |
|    value_loss           | 9.52e+04      |
-------------------------------------------
Eval num_timesteps=245500, episode_reward=496.78 +/- 766.12
Episode length: 34.94 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 360      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 240      |
|    time_elapsed    | 884      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=516.76 +/- 746.56
Episode length: 36.48 +/- 6.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 517           |
| time/                   |               |
|    total_timesteps      | 246000        |
| train/                  |               |
|    approx_kl            | 2.0262087e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00223      |
|    explained_variance   | 0.0273        |
|    learning_rate        | 0.001         |
|    loss                 | 3.24e+04      |
|    n_updates            | 8190          |
|    policy_gradient_loss | -4.06e-05     |
|    value_loss           | 8.89e+04      |
-------------------------------------------
Eval num_timesteps=246500, episode_reward=521.07 +/- 742.32
Episode length: 35.70 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 521      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 416      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 241      |
|    time_elapsed    | 888      |
|    total_timesteps | 246784   |
---------------------------------
Eval num_timesteps=247000, episode_reward=499.33 +/- 752.41
Episode length: 36.38 +/- 6.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 499           |
| time/                   |               |
|    total_timesteps      | 247000        |
| train/                  |               |
|    approx_kl            | 1.2450619e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00128      |
|    explained_variance   | 0.106         |
|    learning_rate        | 0.001         |
|    loss                 | 3.27e+04      |
|    n_updates            | 8200          |
|    policy_gradient_loss | -3.48e-05     |
|    value_loss           | 9.08e+04      |
-------------------------------------------
Eval num_timesteps=247500, episode_reward=461.37 +/- 690.01
Episode length: 35.96 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 461      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 446      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 242      |
|    time_elapsed    | 891      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=739.83 +/- 792.29
Episode length: 37.02 +/- 6.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37            |
|    mean_reward          | 740           |
| time/                   |               |
|    total_timesteps      | 248000        |
| train/                  |               |
|    approx_kl            | 4.3597538e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00118      |
|    explained_variance   | -0.00914      |
|    learning_rate        | 0.001         |
|    loss                 | 4.96e+04      |
|    n_updates            | 8210          |
|    policy_gradient_loss | -2.49e-05     |
|    value_loss           | 1.02e+05      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=248500, episode_reward=625.99 +/- 846.24
Episode length: 35.44 +/- 7.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 626      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 499      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 243      |
|    time_elapsed    | 895      |
|    total_timesteps | 248832   |
---------------------------------
Eval num_timesteps=249000, episode_reward=450.93 +/- 732.25
Episode length: 35.24 +/- 7.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 451           |
| time/                   |               |
|    total_timesteps      | 249000        |
| train/                  |               |
|    approx_kl            | 1.6472768e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00126      |
|    explained_variance   | 0.0217        |
|    learning_rate        | 0.001         |
|    loss                 | 4e+04         |
|    n_updates            | 8220          |
|    policy_gradient_loss | -1.01e-06     |
|    value_loss           | 1.03e+05      |
-------------------------------------------
Eval num_timesteps=249500, episode_reward=429.98 +/- 701.34
Episode length: 35.76 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 430      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 491      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 244      |
|    time_elapsed    | 899      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=490.55 +/- 677.61
Episode length: 35.40 +/- 7.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 491           |
| time/                   |               |
|    total_timesteps      | 250000        |
| train/                  |               |
|    approx_kl            | 0.00051118166 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0016       |
|    explained_variance   | -0.00102      |
|    learning_rate        | 0.001         |
|    loss                 | 4.69e+04      |
|    n_updates            | 8230          |
|    policy_gradient_loss | -0.000237     |
|    value_loss           | 9.84e+04      |
-------------------------------------------
Eval num_timesteps=250500, episode_reward=535.21 +/- 784.48
Episode length: 35.64 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 245      |
|    time_elapsed    | 903      |
|    total_timesteps | 250880   |
---------------------------------
Eval num_timesteps=251000, episode_reward=617.75 +/- 751.34
Episode length: 36.02 +/- 6.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 618           |
| time/                   |               |
|    total_timesteps      | 251000        |
| train/                  |               |
|    approx_kl            | 1.2869714e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00138      |
|    explained_variance   | 0.0573        |
|    learning_rate        | 0.001         |
|    loss                 | 5.28e+04      |
|    n_updates            | 8240          |
|    policy_gradient_loss | -2.4e-05      |
|    value_loss           | 1.11e+05      |
-------------------------------------------
Eval num_timesteps=251500, episode_reward=354.42 +/- 602.62
Episode length: 35.12 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 354      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 430      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 246      |
|    time_elapsed    | 906      |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=492.15 +/- 662.27
Episode length: 36.62 +/- 5.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 492           |
| time/                   |               |
|    total_timesteps      | 252000        |
| train/                  |               |
|    approx_kl            | 2.0372681e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00127      |
|    explained_variance   | 0.0494        |
|    learning_rate        | 0.001         |
|    loss                 | 3.28e+04      |
|    n_updates            | 8250          |
|    policy_gradient_loss | -2.09e-05     |
|    value_loss           | 9.3e+04       |
-------------------------------------------
Eval num_timesteps=252500, episode_reward=361.13 +/- 679.60
Episode length: 35.78 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 421      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 247      |
|    time_elapsed    | 910      |
|    total_timesteps | 252928   |
---------------------------------
Eval num_timesteps=253000, episode_reward=373.62 +/- 710.14
Episode length: 35.12 +/- 6.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 374           |
| time/                   |               |
|    total_timesteps      | 253000        |
| train/                  |               |
|    approx_kl            | 2.0995503e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00148      |
|    explained_variance   | -0.065        |
|    learning_rate        | 0.001         |
|    loss                 | 3.13e+04      |
|    n_updates            | 8260          |
|    policy_gradient_loss | -1.97e-05     |
|    value_loss           | 1.07e+05      |
-------------------------------------------
Eval num_timesteps=253500, episode_reward=316.69 +/- 667.36
Episode length: 34.28 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 466      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 248      |
|    time_elapsed    | 913      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=416.92 +/- 729.09
Episode length: 35.30 +/- 5.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 417           |
| time/                   |               |
|    total_timesteps      | 254000        |
| train/                  |               |
|    approx_kl            | 3.8644066e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00161      |
|    explained_variance   | 0.0841        |
|    learning_rate        | 0.001         |
|    loss                 | 3.84e+04      |
|    n_updates            | 8270          |
|    policy_gradient_loss | -1.45e-05     |
|    value_loss           | 9.76e+04      |
-------------------------------------------
Eval num_timesteps=254500, episode_reward=383.58 +/- 739.50
Episode length: 34.50 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 249      |
|    time_elapsed    | 917      |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=255000, episode_reward=313.79 +/- 697.71
Episode length: 34.16 +/- 7.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 314          |
| time/                   |              |
|    total_timesteps      | 255000       |
| train/                  |              |
|    approx_kl            | 8.021016e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00175     |
|    explained_variance   | -0.000767    |
|    learning_rate        | 0.001        |
|    loss                 | 3.39e+04     |
|    n_updates            | 8280         |
|    policy_gradient_loss | -3.67e-05    |
|    value_loss           | 7.68e+04     |
------------------------------------------
Eval num_timesteps=255500, episode_reward=387.28 +/- 729.78
Episode length: 34.54 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=212.36 +/- 517.97
Episode length: 33.52 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 315      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 250      |
|    time_elapsed    | 922      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=481.62 +/- 796.56
Episode length: 35.18 +/- 6.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 482           |
| time/                   |               |
|    total_timesteps      | 256500        |
| train/                  |               |
|    approx_kl            | 5.5879354e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0023       |
|    explained_variance   | -0.0214       |
|    learning_rate        | 0.001         |
|    loss                 | 2.86e+04      |
|    n_updates            | 8290          |
|    policy_gradient_loss | -4.11e-05     |
|    value_loss           | 7.3e+04       |
-------------------------------------------
Eval num_timesteps=257000, episode_reward=333.09 +/- 658.93
Episode length: 34.60 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 333      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 251      |
|    time_elapsed    | 925      |
|    total_timesteps | 257024   |
---------------------------------
Eval num_timesteps=257500, episode_reward=395.67 +/- 702.73
Episode length: 35.06 +/- 6.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 396          |
| time/                   |              |
|    total_timesteps      | 257500       |
| train/                  |              |
|    approx_kl            | 0.0010855726 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00308     |
|    explained_variance   | -0.0091      |
|    learning_rate        | 0.001        |
|    loss                 | 5.17e+04     |
|    n_updates            | 8300         |
|    policy_gradient_loss | -0.000372    |
|    value_loss           | 9.37e+04     |
------------------------------------------
Eval num_timesteps=258000, episode_reward=368.64 +/- 686.54
Episode length: 34.96 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 369      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 304      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 252      |
|    time_elapsed    | 929      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=539.85 +/- 695.51
Episode length: 37.30 +/- 6.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.3         |
|    mean_reward          | 540          |
| time/                   |              |
|    total_timesteps      | 258500       |
| train/                  |              |
|    approx_kl            | 4.506961e-06 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00342     |
|    explained_variance   | 0.0316       |
|    learning_rate        | 0.001        |
|    loss                 | 3.5e+04      |
|    n_updates            | 8310         |
|    policy_gradient_loss | -0.00015     |
|    value_loss           | 8.3e+04      |
------------------------------------------
Eval num_timesteps=259000, episode_reward=360.05 +/- 743.47
Episode length: 34.76 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 253      |
|    time_elapsed    | 933      |
|    total_timesteps | 259072   |
---------------------------------
Eval num_timesteps=259500, episode_reward=761.13 +/- 830.36
Episode length: 36.64 +/- 7.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.6         |
|    mean_reward          | 761          |
| time/                   |              |
|    total_timesteps      | 259500       |
| train/                  |              |
|    approx_kl            | 0.0016442804 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00333     |
|    explained_variance   | 0.06         |
|    learning_rate        | 0.001        |
|    loss                 | 4.14e+04     |
|    n_updates            | 8320         |
|    policy_gradient_loss | -7e-05       |
|    value_loss           | 9.37e+04     |
------------------------------------------
New best mean reward!
Eval num_timesteps=260000, episode_reward=585.60 +/- 804.26
Episode length: 35.66 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 586      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 444      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 254      |
|    time_elapsed    | 936      |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=452.34 +/- 710.38
Episode length: 36.00 +/- 5.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 452           |
| time/                   |               |
|    total_timesteps      | 260500        |
| train/                  |               |
|    approx_kl            | 1.5714322e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00293      |
|    explained_variance   | 0.066         |
|    learning_rate        | 0.001         |
|    loss                 | 4.23e+04      |
|    n_updates            | 8330          |
|    policy_gradient_loss | -0.000137     |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=261000, episode_reward=365.70 +/- 750.00
Episode length: 34.52 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 460      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 255      |
|    time_elapsed    | 940      |
|    total_timesteps | 261120   |
---------------------------------
Eval num_timesteps=261500, episode_reward=423.27 +/- 739.85
Episode length: 35.28 +/- 6.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 423           |
| time/                   |               |
|    total_timesteps      | 261500        |
| train/                  |               |
|    approx_kl            | 1.4775433e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00275      |
|    explained_variance   | -0.263        |
|    learning_rate        | 0.001         |
|    loss                 | 3.09e+04      |
|    n_updates            | 8340          |
|    policy_gradient_loss | -0.000144     |
|    value_loss           | 8.25e+04      |
-------------------------------------------
Eval num_timesteps=262000, episode_reward=445.27 +/- 709.77
Episode length: 35.26 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 256      |
|    time_elapsed    | 944      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=328.64 +/- 564.04
Episode length: 36.32 +/- 5.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 329          |
| time/                   |              |
|    total_timesteps      | 262500       |
| train/                  |              |
|    approx_kl            | 2.932502e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00298     |
|    explained_variance   | 0.0692       |
|    learning_rate        | 0.001        |
|    loss                 | 3.73e+04     |
|    n_updates            | 8350         |
|    policy_gradient_loss | -4.15e-05    |
|    value_loss           | 8.75e+04     |
------------------------------------------
Eval num_timesteps=263000, episode_reward=471.39 +/- 740.57
Episode length: 35.68 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 471      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 257      |
|    time_elapsed    | 947      |
|    total_timesteps | 263168   |
---------------------------------
Eval num_timesteps=263500, episode_reward=339.89 +/- 654.68
Episode length: 34.60 +/- 6.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 340           |
| time/                   |               |
|    total_timesteps      | 263500        |
| train/                  |               |
|    approx_kl            | 5.9144804e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0028       |
|    explained_variance   | 0.0223        |
|    learning_rate        | 0.001         |
|    loss                 | 4.5e+04       |
|    n_updates            | 8360          |
|    policy_gradient_loss | -6.02e-05     |
|    value_loss           | 8.34e+04      |
-------------------------------------------
Eval num_timesteps=264000, episode_reward=268.25 +/- 568.41
Episode length: 34.48 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 268      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 274      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 258      |
|    time_elapsed    | 951      |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=293.91 +/- 660.89
Episode length: 33.50 +/- 7.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.5         |
|    mean_reward          | 294          |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 7.933704e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00345     |
|    explained_variance   | 0.145        |
|    learning_rate        | 0.001        |
|    loss                 | 4.57e+04     |
|    n_updates            | 8370         |
|    policy_gradient_loss | -0.00011     |
|    value_loss           | 9.77e+04     |
------------------------------------------
Eval num_timesteps=265000, episode_reward=587.39 +/- 721.66
Episode length: 37.02 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 587      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 407      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 259      |
|    time_elapsed    | 954      |
|    total_timesteps | 265216   |
---------------------------------
Eval num_timesteps=265500, episode_reward=384.10 +/- 689.33
Episode length: 35.12 +/- 6.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 384          |
| time/                   |              |
|    total_timesteps      | 265500       |
| train/                  |              |
|    approx_kl            | 0.0026821378 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00305     |
|    explained_variance   | -0.0353      |
|    learning_rate        | 0.001        |
|    loss                 | 4.7e+04      |
|    n_updates            | 8380         |
|    policy_gradient_loss | 0.000358     |
|    value_loss           | 1.08e+05     |
------------------------------------------
Eval num_timesteps=266000, episode_reward=412.87 +/- 711.44
Episode length: 34.54 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 413      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 384      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 260      |
|    time_elapsed    | 958      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=362.87 +/- 657.43
Episode length: 35.04 +/- 6.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 363           |
| time/                   |               |
|    total_timesteps      | 266500        |
| train/                  |               |
|    approx_kl            | 2.7375063e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00349      |
|    explained_variance   | 0.00714       |
|    learning_rate        | 0.001         |
|    loss                 | 3.54e+04      |
|    n_updates            | 8390          |
|    policy_gradient_loss | -0.000197     |
|    value_loss           | 9.18e+04      |
-------------------------------------------
Eval num_timesteps=267000, episode_reward=501.63 +/- 780.31
Episode length: 35.50 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 261      |
|    time_elapsed    | 962      |
|    total_timesteps | 267264   |
---------------------------------
Eval num_timesteps=267500, episode_reward=342.41 +/- 695.98
Episode length: 34.78 +/- 6.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 342           |
| time/                   |               |
|    total_timesteps      | 267500        |
| train/                  |               |
|    approx_kl            | 2.9531657e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00359      |
|    explained_variance   | -0.015        |
|    learning_rate        | 0.001         |
|    loss                 | 5.14e+04      |
|    n_updates            | 8400          |
|    policy_gradient_loss | -0.000194     |
|    value_loss           | 9.28e+04      |
-------------------------------------------
Eval num_timesteps=268000, episode_reward=339.87 +/- 675.86
Episode length: 34.92 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 373      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 262      |
|    time_elapsed    | 965      |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=385.62 +/- 705.45
Episode length: 35.02 +/- 6.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 386          |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 6.339862e-05 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00304     |
|    explained_variance   | -0.102       |
|    learning_rate        | 0.001        |
|    loss                 | 3.41e+04     |
|    n_updates            | 8410         |
|    policy_gradient_loss | -0.000144    |
|    value_loss           | 8.94e+04     |
------------------------------------------
Eval num_timesteps=269000, episode_reward=616.36 +/- 843.09
Episode length: 35.16 +/- 9.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 616      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 263      |
|    time_elapsed    | 969      |
|    total_timesteps | 269312   |
---------------------------------
Eval num_timesteps=269500, episode_reward=517.88 +/- 728.23
Episode length: 36.36 +/- 6.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 518          |
| time/                   |              |
|    total_timesteps      | 269500       |
| train/                  |              |
|    approx_kl            | 2.689776e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00199     |
|    explained_variance   | 0.0949       |
|    learning_rate        | 0.001        |
|    loss                 | 4.98e+04     |
|    n_updates            | 8420         |
|    policy_gradient_loss | -5.45e-05    |
|    value_loss           | 1.13e+05     |
------------------------------------------
Eval num_timesteps=270000, episode_reward=436.82 +/- 720.88
Episode length: 35.38 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 442      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 264      |
|    time_elapsed    | 972      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=390.34 +/- 778.59
Episode length: 33.58 +/- 7.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 390          |
| time/                   |              |
|    total_timesteps      | 270500       |
| train/                  |              |
|    approx_kl            | 7.748022e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00164     |
|    explained_variance   | -0.108       |
|    learning_rate        | 0.001        |
|    loss                 | 3.56e+04     |
|    n_updates            | 8430         |
|    policy_gradient_loss | -7.99e-05    |
|    value_loss           | 8.35e+04     |
------------------------------------------
Eval num_timesteps=271000, episode_reward=549.46 +/- 808.63
Episode length: 35.76 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 527      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 265      |
|    time_elapsed    | 976      |
|    total_timesteps | 271360   |
---------------------------------
Eval num_timesteps=271500, episode_reward=522.05 +/- 803.54
Episode length: 35.36 +/- 6.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 522           |
| time/                   |               |
|    total_timesteps      | 271500        |
| train/                  |               |
|    approx_kl            | 0.00018513104 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00205      |
|    explained_variance   | 0.091         |
|    learning_rate        | 0.001         |
|    loss                 | 4.15e+04      |
|    n_updates            | 8440          |
|    policy_gradient_loss | -1.6e-05      |
|    value_loss           | 8.39e+04      |
-------------------------------------------
Eval num_timesteps=272000, episode_reward=539.43 +/- 806.78
Episode length: 35.20 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 539      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 512      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 266      |
|    time_elapsed    | 980      |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=459.64 +/- 714.42
Episode length: 35.80 +/- 6.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 460           |
| time/                   |               |
|    total_timesteps      | 272500        |
| train/                  |               |
|    approx_kl            | 1.4667981e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00475      |
|    explained_variance   | 0.0496        |
|    learning_rate        | 0.001         |
|    loss                 | 4.26e+04      |
|    n_updates            | 8450          |
|    policy_gradient_loss | -0.000136     |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=273000, episode_reward=547.92 +/- 736.37
Episode length: 36.00 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 548      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 524      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 267      |
|    time_elapsed    | 983      |
|    total_timesteps | 273408   |
---------------------------------
Eval num_timesteps=273500, episode_reward=290.74 +/- 726.33
Episode length: 33.56 +/- 7.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 291          |
| time/                   |              |
|    total_timesteps      | 273500       |
| train/                  |              |
|    approx_kl            | 0.0030443408 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00174     |
|    explained_variance   | -0.051       |
|    learning_rate        | 0.001        |
|    loss                 | 6.62e+04     |
|    n_updates            | 8460         |
|    policy_gradient_loss | -0.000234    |
|    value_loss           | 1.19e+05     |
------------------------------------------
Eval num_timesteps=274000, episode_reward=410.59 +/- 663.17
Episode length: 35.02 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 411      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 268      |
|    time_elapsed    | 987      |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=537.06 +/- 675.93
Episode length: 36.74 +/- 5.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.7         |
|    mean_reward          | 537          |
| time/                   |              |
|    total_timesteps      | 274500       |
| train/                  |              |
|    approx_kl            | 7.359311e-06 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00316     |
|    explained_variance   | -0.176       |
|    learning_rate        | 0.001        |
|    loss                 | 3.91e+04     |
|    n_updates            | 8470         |
|    policy_gradient_loss | 0.000209     |
|    value_loss           | 9.15e+04     |
------------------------------------------
Eval num_timesteps=275000, episode_reward=401.70 +/- 639.20
Episode length: 35.56 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 402      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 410      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 269      |
|    time_elapsed    | 991      |
|    total_timesteps | 275456   |
---------------------------------
Eval num_timesteps=275500, episode_reward=339.73 +/- 639.63
Episode length: 35.46 +/- 5.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 340           |
| time/                   |               |
|    total_timesteps      | 275500        |
| train/                  |               |
|    approx_kl            | 1.2092642e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00205      |
|    explained_variance   | 0.0386        |
|    learning_rate        | 0.001         |
|    loss                 | 3.9e+04       |
|    n_updates            | 8480          |
|    policy_gradient_loss | -0.000134     |
|    value_loss           | 9.88e+04      |
-------------------------------------------
Eval num_timesteps=276000, episode_reward=409.28 +/- 676.26
Episode length: 35.38 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 383      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 270      |
|    time_elapsed    | 994      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=470.69 +/- 682.64
Episode length: 35.30 +/- 5.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 471           |
| time/                   |               |
|    total_timesteps      | 276500        |
| train/                  |               |
|    approx_kl            | 2.8801733e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00181      |
|    explained_variance   | 0.195         |
|    learning_rate        | 0.001         |
|    loss                 | 3.33e+04      |
|    n_updates            | 8490          |
|    policy_gradient_loss | -0.000121     |
|    value_loss           | 8.9e+04       |
-------------------------------------------
Eval num_timesteps=277000, episode_reward=383.11 +/- 801.05
Episode length: 33.80 +/- 7.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=340.62 +/- 645.17
Episode length: 34.62 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 517      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 271      |
|    time_elapsed    | 999      |
|    total_timesteps | 277504   |
---------------------------------
Eval num_timesteps=278000, episode_reward=488.67 +/- 755.03
Episode length: 35.44 +/- 6.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 489           |
| time/                   |               |
|    total_timesteps      | 278000        |
| train/                  |               |
|    approx_kl            | 1.2743403e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00249      |
|    explained_variance   | 0.0254        |
|    learning_rate        | 0.001         |
|    loss                 | 5.17e+04      |
|    n_updates            | 8500          |
|    policy_gradient_loss | -0.000144     |
|    value_loss           | 1.19e+05      |
-------------------------------------------
Eval num_timesteps=278500, episode_reward=109.06 +/- 498.27
Episode length: 32.86 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 109      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 577      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 272      |
|    time_elapsed    | 1003     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=494.44 +/- 681.04
Episode length: 36.72 +/- 5.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.7          |
|    mean_reward          | 494           |
| time/                   |               |
|    total_timesteps      | 279000        |
| train/                  |               |
|    approx_kl            | 8.0837635e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0014       |
|    explained_variance   | 0.0908        |
|    learning_rate        | 0.001         |
|    loss                 | 3.83e+04      |
|    n_updates            | 8510          |
|    policy_gradient_loss | -0.000247     |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=279500, episode_reward=544.31 +/- 804.72
Episode length: 35.58 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 544      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 489      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 273      |
|    time_elapsed    | 1007     |
|    total_timesteps | 279552   |
---------------------------------
Eval num_timesteps=280000, episode_reward=249.26 +/- 606.82
Episode length: 34.26 +/- 6.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 249           |
| time/                   |               |
|    total_timesteps      | 280000        |
| train/                  |               |
|    approx_kl            | 1.5338883e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00128      |
|    explained_variance   | -0.128        |
|    learning_rate        | 0.001         |
|    loss                 | 3.42e+04      |
|    n_updates            | 8520          |
|    policy_gradient_loss | -0.000111     |
|    value_loss           | 9.04e+04      |
-------------------------------------------
Eval num_timesteps=280500, episode_reward=417.97 +/- 675.90
Episode length: 35.20 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 522      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 274      |
|    time_elapsed    | 1010     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=419.48 +/- 760.24
Episode length: 34.14 +/- 7.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.1          |
|    mean_reward          | 419           |
| time/                   |               |
|    total_timesteps      | 281000        |
| train/                  |               |
|    approx_kl            | 0.00043118477 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0022       |
|    explained_variance   | 0.033         |
|    learning_rate        | 0.001         |
|    loss                 | 4.03e+04      |
|    n_updates            | 8530          |
|    policy_gradient_loss | 8.65e-06      |
|    value_loss           | 1.04e+05      |
-------------------------------------------
Eval num_timesteps=281500, episode_reward=520.05 +/- 777.95
Episode length: 35.76 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 488      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 275      |
|    time_elapsed    | 1014     |
|    total_timesteps | 281600   |
---------------------------------
Eval num_timesteps=282000, episode_reward=589.39 +/- 740.07
Episode length: 37.26 +/- 6.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.3        |
|    mean_reward          | 589         |
| time/                   |             |
|    total_timesteps      | 282000      |
| train/                  |             |
|    approx_kl            | 3.06539e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00259    |
|    explained_variance   | 0.151       |
|    learning_rate        | 0.001       |
|    loss                 | 3.95e+04    |
|    n_updates            | 8540        |
|    policy_gradient_loss | -0.000192   |
|    value_loss           | 1.1e+05     |
-----------------------------------------
Eval num_timesteps=282500, episode_reward=266.01 +/- 560.09
Episode length: 34.48 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 419      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 276      |
|    time_elapsed    | 1018     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=555.10 +/- 684.50
Episode length: 36.64 +/- 6.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 555           |
| time/                   |               |
|    total_timesteps      | 283000        |
| train/                  |               |
|    approx_kl            | 1.3327808e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00171      |
|    explained_variance   | 0.00394       |
|    learning_rate        | 0.001         |
|    loss                 | 2.99e+04      |
|    n_updates            | 8550          |
|    policy_gradient_loss | -4.44e-05     |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=283500, episode_reward=429.50 +/- 716.37
Episode length: 34.74 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 430      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 409      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 277      |
|    time_elapsed    | 1022     |
|    total_timesteps | 283648   |
---------------------------------
Eval num_timesteps=284000, episode_reward=451.08 +/- 680.69
Episode length: 35.78 +/- 5.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 451           |
| time/                   |               |
|    total_timesteps      | 284000        |
| train/                  |               |
|    approx_kl            | 7.5928983e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0018       |
|    explained_variance   | 0.141         |
|    learning_rate        | 0.001         |
|    loss                 | 3.1e+04       |
|    n_updates            | 8560          |
|    policy_gradient_loss | 0.00012       |
|    value_loss           | 9.71e+04      |
-------------------------------------------
Eval num_timesteps=284500, episode_reward=404.93 +/- 657.44
Episode length: 35.50 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 397      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 278      |
|    time_elapsed    | 1026     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=603.48 +/- 757.56
Episode length: 36.96 +/- 6.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37          |
|    mean_reward          | 603         |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 5.54719e-07 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0019     |
|    explained_variance   | 0.0917      |
|    learning_rate        | 0.001       |
|    loss                 | 4.86e+04    |
|    n_updates            | 8570        |
|    policy_gradient_loss | -2.99e-05   |
|    value_loss           | 1.17e+05    |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=332.60 +/- 649.28
Episode length: 34.38 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 333      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 279      |
|    time_elapsed    | 1029     |
|    total_timesteps | 285696   |
---------------------------------
Eval num_timesteps=286000, episode_reward=436.18 +/- 656.09
Episode length: 35.86 +/- 6.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 436           |
| time/                   |               |
|    total_timesteps      | 286000        |
| train/                  |               |
|    approx_kl            | 1.3369136e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00161      |
|    explained_variance   | -0.0137       |
|    learning_rate        | 0.001         |
|    loss                 | 3.93e+04      |
|    n_updates            | 8580          |
|    policy_gradient_loss | -0.000101     |
|    value_loss           | 9.59e+04      |
-------------------------------------------
Eval num_timesteps=286500, episode_reward=538.96 +/- 791.28
Episode length: 35.32 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 539      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 280      |
|    time_elapsed    | 1033     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=522.53 +/- 790.36
Episode length: 35.54 +/- 6.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 523           |
| time/                   |               |
|    total_timesteps      | 287000        |
| train/                  |               |
|    approx_kl            | 9.3021663e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00131      |
|    explained_variance   | 0.0321        |
|    learning_rate        | 0.001         |
|    loss                 | 4.95e+04      |
|    n_updates            | 8590          |
|    policy_gradient_loss | -5.59e-05     |
|    value_loss           | 9.47e+04      |
-------------------------------------------
Eval num_timesteps=287500, episode_reward=511.30 +/- 731.88
Episode length: 36.40 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 426      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 281      |
|    time_elapsed    | 1037     |
|    total_timesteps | 287744   |
---------------------------------
Eval num_timesteps=288000, episode_reward=422.64 +/- 741.11
Episode length: 34.02 +/- 7.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 423          |
| time/                   |              |
|    total_timesteps      | 288000       |
| train/                  |              |
|    approx_kl            | 4.101661e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00149     |
|    explained_variance   | -0.0568      |
|    learning_rate        | 0.001        |
|    loss                 | 3.56e+04     |
|    n_updates            | 8600         |
|    policy_gradient_loss | -0.000267    |
|    value_loss           | 9.63e+04     |
------------------------------------------
Eval num_timesteps=288500, episode_reward=430.73 +/- 756.51
Episode length: 34.98 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 408      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 282      |
|    time_elapsed    | 1040     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=433.33 +/- 683.22
Episode length: 35.38 +/- 6.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 433           |
| time/                   |               |
|    total_timesteps      | 289000        |
| train/                  |               |
|    approx_kl            | 0.00023425889 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00143      |
|    explained_variance   | 0.0287        |
|    learning_rate        | 0.001         |
|    loss                 | 3.72e+04      |
|    n_updates            | 8610          |
|    policy_gradient_loss | -0.000106     |
|    value_loss           | 8.48e+04      |
-------------------------------------------
Eval num_timesteps=289500, episode_reward=363.84 +/- 677.00
Episode length: 35.02 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 364      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 321      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 283      |
|    time_elapsed    | 1044     |
|    total_timesteps | 289792   |
---------------------------------
Eval num_timesteps=290000, episode_reward=292.84 +/- 704.64
Episode length: 33.84 +/- 6.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.8        |
|    mean_reward          | 293         |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 1.15484e-07 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00115    |
|    explained_variance   | 0.00836     |
|    learning_rate        | 0.001       |
|    loss                 | 4.17e+04    |
|    n_updates            | 8620        |
|    policy_gradient_loss | -1.44e-05   |
|    value_loss           | 9.31e+04    |
-----------------------------------------
Eval num_timesteps=290500, episode_reward=518.87 +/- 775.04
Episode length: 34.82 +/- 7.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 284      |
|    time_elapsed    | 1048     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=403.21 +/- 738.07
Episode length: 34.36 +/- 6.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 403           |
| time/                   |               |
|    total_timesteps      | 291000        |
| train/                  |               |
|    approx_kl            | 4.0058512e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00183      |
|    explained_variance   | 0.0577        |
|    learning_rate        | 0.001         |
|    loss                 | 2.86e+04      |
|    n_updates            | 8630          |
|    policy_gradient_loss | -0.000122     |
|    value_loss           | 8.38e+04      |
-------------------------------------------
Eval num_timesteps=291500, episode_reward=338.87 +/- 642.14
Episode length: 34.90 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 285      |
|    time_elapsed    | 1052     |
|    total_timesteps | 291840   |
---------------------------------
Eval num_timesteps=292000, episode_reward=416.49 +/- 705.40
Episode length: 34.88 +/- 6.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.9        |
|    mean_reward          | 416         |
| time/                   |             |
|    total_timesteps      | 292000      |
| train/                  |             |
|    approx_kl            | 8.27841e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00304    |
|    explained_variance   | -0.00152    |
|    learning_rate        | 0.001       |
|    loss                 | 3.43e+04    |
|    n_updates            | 8640        |
|    policy_gradient_loss | -0.000168   |
|    value_loss           | 9.05e+04    |
-----------------------------------------
Eval num_timesteps=292500, episode_reward=561.70 +/- 823.01
Episode length: 35.66 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 562      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 286      |
|    time_elapsed    | 1055     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=403.30 +/- 705.94
Episode length: 34.52 +/- 7.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.5         |
|    mean_reward          | 403          |
| time/                   |              |
|    total_timesteps      | 293000       |
| train/                  |              |
|    approx_kl            | 0.0078764055 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00269     |
|    explained_variance   | 0.0965       |
|    learning_rate        | 0.001        |
|    loss                 | 2.85e+04     |
|    n_updates            | 8650         |
|    policy_gradient_loss | 0.00134      |
|    value_loss           | 7.29e+04     |
------------------------------------------
Eval num_timesteps=293500, episode_reward=385.00 +/- 739.41
Episode length: 35.18 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 287      |
|    time_elapsed    | 1059     |
|    total_timesteps | 293888   |
---------------------------------
Eval num_timesteps=294000, episode_reward=455.19 +/- 729.57
Episode length: 35.54 +/- 6.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 455           |
| time/                   |               |
|    total_timesteps      | 294000        |
| train/                  |               |
|    approx_kl            | 2.5273766e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00257      |
|    explained_variance   | 0.141         |
|    learning_rate        | 0.001         |
|    loss                 | 3.19e+04      |
|    n_updates            | 8660          |
|    policy_gradient_loss | -3.18e-05     |
|    value_loss           | 7.24e+04      |
-------------------------------------------
Eval num_timesteps=294500, episode_reward=395.20 +/- 717.94
Episode length: 35.08 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 288      |
|    time_elapsed    | 1063     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=543.60 +/- 784.78
Episode length: 36.26 +/- 5.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 544          |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 4.286063e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00436     |
|    explained_variance   | -0.0227      |
|    learning_rate        | 0.001        |
|    loss                 | 5.86e+04     |
|    n_updates            | 8670         |
|    policy_gradient_loss | -0.000232    |
|    value_loss           | 1.03e+05     |
------------------------------------------
Eval num_timesteps=295500, episode_reward=285.29 +/- 658.13
Episode length: 34.36 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 340      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 289      |
|    time_elapsed    | 1067     |
|    total_timesteps | 295936   |
---------------------------------
Eval num_timesteps=296000, episode_reward=486.91 +/- 691.12
Episode length: 35.96 +/- 6.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 487          |
| time/                   |              |
|    total_timesteps      | 296000       |
| train/                  |              |
|    approx_kl            | 2.439256e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00378     |
|    explained_variance   | 0.121        |
|    learning_rate        | 0.001        |
|    loss                 | 4.2e+04      |
|    n_updates            | 8680         |
|    policy_gradient_loss | -1.66e-05    |
|    value_loss           | 7.38e+04     |
------------------------------------------
Eval num_timesteps=296500, episode_reward=299.47 +/- 649.35
Episode length: 34.30 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 299      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 397      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 290      |
|    time_elapsed    | 1070     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=253.11 +/- 674.37
Episode length: 33.76 +/- 7.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 33.8       |
|    mean_reward          | 253        |
| time/                   |            |
|    total_timesteps      | 297000     |
| train/                  |            |
|    approx_kl            | 0.00255204 |
|    clip_fraction        | 0.00166    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00252   |
|    explained_variance   | -0.0191    |
|    learning_rate        | 0.001      |
|    loss                 | 4.1e+04    |
|    n_updates            | 8690       |
|    policy_gradient_loss | -0.000604  |
|    value_loss           | 1.07e+05   |
----------------------------------------
Eval num_timesteps=297500, episode_reward=394.06 +/- 737.13
Episode length: 34.56 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 476      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 291      |
|    time_elapsed    | 1074     |
|    total_timesteps | 297984   |
---------------------------------
Eval num_timesteps=298000, episode_reward=630.03 +/- 805.37
Episode length: 35.88 +/- 7.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 630           |
| time/                   |               |
|    total_timesteps      | 298000        |
| train/                  |               |
|    approx_kl            | 2.5873887e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00346      |
|    explained_variance   | -0.125        |
|    learning_rate        | 0.001         |
|    loss                 | 4.19e+04      |
|    n_updates            | 8700          |
|    policy_gradient_loss | -8.51e-05     |
|    value_loss           | 9.05e+04      |
-------------------------------------------
Eval num_timesteps=298500, episode_reward=564.23 +/- 813.99
Episode length: 36.20 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 564      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=509.91 +/- 732.57
Episode length: 36.30 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 510      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 548      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 292      |
|    time_elapsed    | 1079     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=504.69 +/- 670.79
Episode length: 36.10 +/- 5.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 505           |
| time/                   |               |
|    total_timesteps      | 299500        |
| train/                  |               |
|    approx_kl            | 3.3294316e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00502      |
|    explained_variance   | 0.047         |
|    learning_rate        | 0.001         |
|    loss                 | 5.09e+04      |
|    n_updates            | 8710          |
|    policy_gradient_loss | -2.69e-05     |
|    value_loss           | 1.09e+05      |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=323.49 +/- 675.85
Episode length: 34.70 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 323      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.8     |
|    ep_rew_mean     | 584      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 293      |
|    time_elapsed    | 1082     |
|    total_timesteps | 300032   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-5/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 1024, 'batch_size': 64, 'learning_rate': 0.001, 'gamma': 0.9635, 'gae_lambda': 0.879}
Training steps: 300000
Frame skip: 4
