/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-516.82 +/- 70.96
Episode length: 53.06 +/- 17.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-508.98 +/- 66.88
Episode length: 53.26 +/- 19.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=-517.98 +/- 72.00
Episode length: 51.72 +/- 18.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-504.18 +/- 78.09
Episode length: 53.10 +/- 20.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.6     |
|    ep_rew_mean     | -388     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=-294.16 +/- 105.62
Episode length: 32.02 +/- 9.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32          |
|    mean_reward          | -294        |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.013448927 |
|    clip_fraction        | 0.0929      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000159   |
|    learning_rate        | 0.0001      |
|    loss                 | 2.1e+03     |
|    n_updates            | 1542        |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 4.83e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=-296.13 +/- 94.97
Episode length: 31.42 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.4     |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-295.00 +/- 97.12
Episode length: 33.16 +/- 9.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-313.47 +/- 110.04
Episode length: 32.06 +/- 10.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.1     |
|    mean_reward     | -313     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.7     |
|    ep_rew_mean     | -357     |
| time/              |          |
|    fps             | 261      |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=-422.69 +/- 74.21
Episode length: 38.96 +/- 12.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 39           |
|    mean_reward          | -423         |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 0.0045361575 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.000131     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.49e+03     |
|    n_updates            | 1543         |
|    policy_gradient_loss | -0.00745     |
|    value_loss           | 5.06e+03     |
------------------------------------------
Eval num_timesteps=5000, episode_reward=-443.82 +/- 78.35
Episode length: 39.06 +/- 10.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.1     |
|    mean_reward     | -444     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-425.78 +/- 98.29
Episode length: 36.20 +/- 12.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-453.49 +/- 76.76
Episode length: 41.36 +/- 14.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | -453     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.3     |
|    ep_rew_mean     | -343     |
| time/              |          |
|    fps             | 268      |
|    iterations      | 3        |
|    time_elapsed    | 22       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=809.33 +/- 711.31
Episode length: 34.66 +/- 6.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 809          |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.0067885085 |
|    clip_fraction        | 0.0824       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.98        |
|    explained_variance   | 0.000296     |
|    learning_rate        | 0.0001       |
|    loss                 | 4.87e+03     |
|    n_updates            | 1544         |
|    policy_gradient_loss | -0.00647     |
|    value_loss           | 6.82e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=7000, episode_reward=764.48 +/- 708.33
Episode length: 33.62 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=703.78 +/- 551.71
Episode length: 35.14 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=840.84 +/- 712.26
Episode length: 34.22 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 58.9     |
|    ep_rew_mean     | -305     |
| time/              |          |
|    fps             | 277      |
|    iterations      | 4        |
|    time_elapsed    | 29       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=8500, episode_reward=997.14 +/- 722.26
Episode length: 36.80 +/- 6.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.8        |
|    mean_reward          | 997         |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.005437842 |
|    clip_fraction        | 0.0538      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.9        |
|    explained_variance   | 0.000913    |
|    learning_rate        | 0.0001      |
|    loss                 | 2.47e+03    |
|    n_updates            | 1545        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 5.76e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=9000, episode_reward=729.78 +/- 594.45
Episode length: 35.14 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=858.51 +/- 713.74
Episode length: 35.00 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=1024.00 +/- 781.46
Episode length: 35.84 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.9     |
|    ep_rew_mean     | -272     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 5        |
|    time_elapsed    | 36       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=10500, episode_reward=-534.01 +/- 57.68
Episode length: 50.54 +/- 16.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 50.5         |
|    mean_reward          | -534         |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0054463805 |
|    clip_fraction        | 0.0647       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.00147      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.22e+03     |
|    n_updates            | 1546         |
|    policy_gradient_loss | -0.00592     |
|    value_loss           | 8.83e+03     |
------------------------------------------
Eval num_timesteps=11000, episode_reward=-504.97 +/- 88.70
Episode length: 49.58 +/- 19.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-524.34 +/- 66.02
Episode length: 47.56 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-539.38 +/- 68.75
Episode length: 50.14 +/- 20.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -539     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.2     |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 6        |
|    time_elapsed    | 44       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=12500, episode_reward=-537.57 +/- 71.46
Episode length: 51.14 +/- 16.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.1        |
|    mean_reward          | -538        |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.005490758 |
|    clip_fraction        | 0.0536      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.000694    |
|    learning_rate        | 0.0001      |
|    loss                 | 6.35e+03    |
|    n_updates            | 1547        |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 1.04e+04    |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=-519.55 +/- 84.58
Episode length: 52.44 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-537.67 +/- 66.85
Episode length: 51.58 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-538.11 +/- 53.49
Episode length: 53.64 +/- 13.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.9     |
|    ep_rew_mean     | -243     |
| time/              |          |
|    fps             | 267      |
|    iterations      | 7        |
|    time_elapsed    | 53       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=14500, episode_reward=740.22 +/- 775.29
Episode length: 35.06 +/- 7.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.1        |
|    mean_reward          | 740         |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.009036657 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.000825    |
|    learning_rate        | 0.0001      |
|    loss                 | 5.98e+03    |
|    n_updates            | 1548        |
|    policy_gradient_loss | 0.00295     |
|    value_loss           | 1.06e+04    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=785.95 +/- 749.01
Episode length: 37.30 +/- 11.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=581.58 +/- 729.01
Episode length: 33.14 +/- 8.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 582      |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=480.73 +/- 527.35
Episode length: 33.98 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 481      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.8     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 8        |
|    time_elapsed    | 60       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=16500, episode_reward=799.73 +/- 708.74
Episode length: 34.66 +/- 6.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 800          |
| time/                   |              |
|    total_timesteps      | 16500        |
| train/                  |              |
|    approx_kl            | 0.0068617873 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.4         |
|    explained_variance   | 0.00227      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.23e+03     |
|    n_updates            | 1549         |
|    policy_gradient_loss | -0.0126      |
|    value_loss           | 8.46e+03     |
------------------------------------------
Eval num_timesteps=17000, episode_reward=986.23 +/- 743.05
Episode length: 36.02 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=882.63 +/- 680.10
Episode length: 35.60 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=694.91 +/- 599.86
Episode length: 34.02 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 9        |
|    time_elapsed    | 66       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=18500, episode_reward=703.43 +/- 532.06
Episode length: 35.16 +/- 5.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.2       |
|    mean_reward          | 703        |
| time/                   |            |
|    total_timesteps      | 18500      |
| train/                  |            |
|    approx_kl            | 0.00921605 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | 0.00299    |
|    learning_rate        | 0.0001     |
|    loss                 | 4.01e+03   |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 7.21e+03   |
----------------------------------------
Eval num_timesteps=19000, episode_reward=804.14 +/- 704.14
Episode length: 33.94 +/- 8.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=805.24 +/- 709.07
Episode length: 34.38 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=849.77 +/- 654.43
Episode length: 36.16 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33       |
|    ep_rew_mean     | -96.1    |
| time/              |          |
|    fps             | 278      |
|    iterations      | 10       |
|    time_elapsed    | 73       |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=860.89 +/- 706.38
Episode length: 35.02 +/- 6.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 861         |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.006489416 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.00337     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.64e+03    |
|    n_updates            | 1551        |
|    policy_gradient_loss | 0.00176     |
|    value_loss           | 8.71e+03    |
-----------------------------------------
Eval num_timesteps=21000, episode_reward=938.86 +/- 700.89
Episode length: 36.16 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=857.93 +/- 647.62
Episode length: 36.76 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=764.24 +/- 645.45
Episode length: 35.04 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=757.14 +/- 688.50
Episode length: 34.14 +/- 7.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.3     |
|    ep_rew_mean     | -9.49    |
| time/              |          |
|    fps             | 277      |
|    iterations      | 11       |
|    time_elapsed    | 81       |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=23000, episode_reward=854.28 +/- 633.82
Episode length: 36.62 +/- 5.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.6        |
|    mean_reward          | 854         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.006225377 |
|    clip_fraction        | 0.0881      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.00419     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.18e+03    |
|    n_updates            | 1552        |
|    policy_gradient_loss | -0.0071     |
|    value_loss           | 1.3e+04     |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=806.12 +/- 669.21
Episode length: 35.54 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=955.54 +/- 752.75
Episode length: 35.58 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=955.59 +/- 663.13
Episode length: 37.42 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.4     |
|    ep_rew_mean     | 9.31     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 12       |
|    time_elapsed    | 88       |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=25000, episode_reward=881.12 +/- 692.40
Episode length: 35.94 +/- 6.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.9       |
|    mean_reward          | 881        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01410694 |
|    clip_fraction        | 0.0794     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.974     |
|    explained_variance   | 0.00611    |
|    learning_rate        | 0.0001     |
|    loss                 | 5.69e+03   |
|    n_updates            | 1554       |
|    policy_gradient_loss | -0.00344   |
|    value_loss           | 9.26e+03   |
----------------------------------------
Eval num_timesteps=25500, episode_reward=874.25 +/- 737.71
Episode length: 34.94 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=739.28 +/- 721.14
Episode length: 33.66 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=783.01 +/- 653.03
Episode length: 35.14 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.8     |
|    ep_rew_mean     | 88.6     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 13       |
|    time_elapsed    | 94       |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=27000, episode_reward=750.37 +/- 680.78
Episode length: 34.16 +/- 7.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 750          |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0063638426 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.769       |
|    explained_variance   | 0.00497      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.06e+03     |
|    n_updates            | 1555         |
|    policy_gradient_loss | -0.00782     |
|    value_loss           | 1.03e+04     |
------------------------------------------
Eval num_timesteps=27500, episode_reward=789.09 +/- 641.21
Episode length: 34.68 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=808.12 +/- 640.93
Episode length: 35.56 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=984.39 +/- 674.42
Episode length: 36.92 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 984      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.6     |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 14       |
|    time_elapsed    | 101      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=29000, episode_reward=758.67 +/- 685.05
Episode length: 33.58 +/- 7.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.6        |
|    mean_reward          | 759         |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.005729949 |
|    clip_fraction        | 0.0492      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.631      |
|    explained_variance   | 0.00225     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.73e+03    |
|    n_updates            | 1556        |
|    policy_gradient_loss | -0.000142   |
|    value_loss           | 1.1e+04     |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=894.25 +/- 693.13
Episode length: 35.80 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=868.25 +/- 774.66
Episode length: 34.08 +/- 7.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=754.68 +/- 575.83
Episode length: 35.62 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.5     |
|    ep_rew_mean     | 210      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 15       |
|    time_elapsed    | 108      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=31000, episode_reward=723.47 +/- 659.74
Episode length: 34.30 +/- 6.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.3        |
|    mean_reward          | 723         |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.009850002 |
|    clip_fraction        | 0.0561      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.496      |
|    explained_variance   | 0.00158     |
|    learning_rate        | 0.0001      |
|    loss                 | 6.28e+03    |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.0026     |
|    value_loss           | 1.6e+04     |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=805.49 +/- 646.80
Episode length: 35.22 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=806.18 +/- 692.65
Episode length: 34.84 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=976.35 +/- 716.00
Episode length: 36.10 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 976      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.5     |
|    ep_rew_mean     | 268      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 16       |
|    time_elapsed    | 115      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=33000, episode_reward=703.27 +/- 617.30
Episode length: 34.12 +/- 7.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.1        |
|    mean_reward          | 703         |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.022822376 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.000849    |
|    learning_rate        | 0.0001      |
|    loss                 | 6.07e+03    |
|    n_updates            | 1563        |
|    policy_gradient_loss | 0.00124     |
|    value_loss           | 1.97e+04    |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=906.80 +/- 715.01
Episode length: 35.52 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=804.88 +/- 682.97
Episode length: 35.32 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=948.13 +/- 745.90
Episode length: 35.38 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33       |
|    ep_rew_mean     | 440      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 17       |
|    time_elapsed    | 122      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=35000, episode_reward=885.72 +/- 690.44
Episode length: 36.26 +/- 6.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 886          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0033104087 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.204       |
|    explained_variance   | 0.000174     |
|    learning_rate        | 0.0001       |
|    loss                 | 1.2e+04      |
|    n_updates            | 1565         |
|    policy_gradient_loss | -0.00152     |
|    value_loss           | 3.78e+04     |
------------------------------------------
Eval num_timesteps=35500, episode_reward=849.91 +/- 675.86
Episode length: 35.52 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=850.15 +/- 711.35
Episode length: 34.56 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=872.70 +/- 749.64
Episode length: 34.88 +/- 7.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 593      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 18       |
|    time_elapsed    | 129      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=37000, episode_reward=812.61 +/- 677.03
Episode length: 34.94 +/- 6.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 813          |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0019220754 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.000129     |
|    learning_rate        | 0.0001       |
|    loss                 | 4.12e+04     |
|    n_updates            | 1571         |
|    policy_gradient_loss | -0.000737    |
|    value_loss           | 5.12e+04     |
------------------------------------------
Eval num_timesteps=37500, episode_reward=828.17 +/- 714.74
Episode length: 34.32 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=905.31 +/- 720.25
Episode length: 36.32 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=875.97 +/- 717.43
Episode length: 35.90 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 704      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 19       |
|    time_elapsed    | 136      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=39000, episode_reward=965.48 +/- 719.31
Episode length: 36.10 +/- 6.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 965          |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0032990167 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0756      |
|    explained_variance   | 4.65e-06     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.42e+04     |
|    n_updates            | 1578         |
|    policy_gradient_loss | -0.000245    |
|    value_loss           | 6.1e+04      |
------------------------------------------
Eval num_timesteps=39500, episode_reward=847.46 +/- 758.65
Episode length: 34.52 +/- 7.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=987.12 +/- 788.28
Episode length: 35.44 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=887.53 +/- 701.39
Episode length: 35.46 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 689      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 20       |
|    time_elapsed    | 143      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=911.98 +/- 679.92
Episode length: 36.84 +/- 6.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.8          |
|    mean_reward          | 912           |
| time/                   |               |
|    total_timesteps      | 41000         |
| train/                  |               |
|    approx_kl            | 0.00033498576 |
|    clip_fraction        | 0.00322       |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.0354       |
|    explained_variance   | 1.25e-06      |
|    learning_rate        | 0.0001        |
|    loss                 | 3.61e+04      |
|    n_updates            | 1588          |
|    policy_gradient_loss | -0.000293     |
|    value_loss           | 5.31e+04      |
-------------------------------------------
Eval num_timesteps=41500, episode_reward=777.71 +/- 676.27
Episode length: 34.24 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=983.50 +/- 770.22
Episode length: 36.10 +/- 7.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 983      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=900.69 +/- 700.67
Episode length: 35.44 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=924.92 +/- 747.99
Episode length: 35.34 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 21       |
|    time_elapsed    | 152      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=43500, episode_reward=642.83 +/- 579.52
Episode length: 33.34 +/- 7.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.3         |
|    mean_reward          | 643          |
| time/                   |              |
|    total_timesteps      | 43500        |
| train/                  |              |
|    approx_kl            | 0.0026582729 |
|    clip_fraction        | 0.00403      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0208      |
|    explained_variance   | 6.97e-06     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.29e+04     |
|    n_updates            | 1589         |
|    policy_gradient_loss | -2.54e-05    |
|    value_loss           | 4.97e+04     |
------------------------------------------
Eval num_timesteps=44000, episode_reward=633.57 +/- 546.80
Episode length: 33.84 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 634      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=980.19 +/- 793.62
Episode length: 35.20 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=988.16 +/- 722.06
Episode length: 36.80 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 22       |
|    time_elapsed    | 158      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=997.50 +/- 712.63
Episode length: 37.00 +/- 6.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37            |
|    mean_reward          | 997           |
| time/                   |               |
|    total_timesteps      | 45500         |
| train/                  |               |
|    approx_kl            | 0.00023199653 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00557      |
|    explained_variance   | 0.000233      |
|    learning_rate        | 0.0001        |
|    loss                 | 2.2e+04       |
|    n_updates            | 1599          |
|    policy_gradient_loss | -1.23e-05     |
|    value_loss           | 5.58e+04      |
-------------------------------------------
Eval num_timesteps=46000, episode_reward=1078.14 +/- 748.35
Episode length: 36.98 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
New best mean reward!
Eval num_timesteps=46500, episode_reward=902.69 +/- 676.79
Episode length: 36.00 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=907.02 +/- 683.73
Episode length: 35.92 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 831      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 23       |
|    time_elapsed    | 166      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=839.12 +/- 687.76
Episode length: 35.24 +/- 6.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 839          |
| time/                   |              |
|    total_timesteps      | 47500        |
| train/                  |              |
|    approx_kl            | 6.236951e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00306     |
|    explained_variance   | 0.000469     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.12e+04     |
|    n_updates            | 1609         |
|    policy_gradient_loss | -6.95e-07    |
|    value_loss           | 7.36e+04     |
------------------------------------------
Eval num_timesteps=48000, episode_reward=920.29 +/- 669.42
Episode length: 36.98 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=726.62 +/- 662.09
Episode length: 33.80 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=1000.62 +/- 699.94
Episode length: 37.38 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 24       |
|    time_elapsed    | 173      |
|    total_timesteps | 49152    |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.03
Eval num_timesteps=49500, episode_reward=833.78 +/- 660.54
Episode length: 35.86 +/- 6.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 834          |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0016403685 |
|    clip_fraction        | 0.000451     |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00381     |
|    explained_variance   | 0.000552     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.43e+04     |
|    n_updates            | 1618         |
|    policy_gradient_loss | -0.000165    |
|    value_loss           | 5.32e+04     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=809.31 +/- 702.62
Episode length: 34.72 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=771.53 +/- 682.43
Episode length: 34.40 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=758.22 +/- 719.53
Episode length: 33.58 +/- 7.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 25       |
|    time_elapsed    | 180      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=961.74 +/- 748.63
Episode length: 35.84 +/- 6.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 962           |
| time/                   |               |
|    total_timesteps      | 51500         |
| train/                  |               |
|    approx_kl            | 2.4447218e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000258     |
|    explained_variance   | 0.000794      |
|    learning_rate        | 0.0001        |
|    loss                 | 2.63e+04      |
|    n_updates            | 1628          |
|    policy_gradient_loss | 4.01e-06      |
|    value_loss           | 5.73e+04      |
-------------------------------------------
Eval num_timesteps=52000, episode_reward=826.99 +/- 683.17
Episode length: 35.34 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=872.40 +/- 694.63
Episode length: 35.58 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=863.59 +/- 722.98
Episode length: 34.76 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 945      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 26       |
|    time_elapsed    | 187      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=878.14 +/- 693.11
Episode length: 35.36 +/- 7.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 878       |
| time/                   |           |
|    total_timesteps      | 53500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.24e-05 |
|    explained_variance   | 0.00178   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.11e+04  |
|    n_updates            | 1638      |
|    policy_gradient_loss | 1.17e-06  |
|    value_loss           | 7.15e+04  |
---------------------------------------
Eval num_timesteps=54000, episode_reward=663.78 +/- 594.06
Episode length: 33.88 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 664      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=862.46 +/- 705.33
Episode length: 35.54 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=706.76 +/- 582.08
Episode length: 34.86 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 923      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 27       |
|    time_elapsed    | 194      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=869.77 +/- 698.84
Episode length: 35.54 +/- 6.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 870       |
| time/                   |           |
|    total_timesteps      | 55500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.75e-07 |
|    explained_variance   | 0.00269   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.92e+04  |
|    n_updates            | 1648      |
|    policy_gradient_loss | 1.17e-07  |
|    value_loss           | 5.99e+04  |
---------------------------------------
Eval num_timesteps=56000, episode_reward=811.40 +/- 681.64
Episode length: 34.56 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=723.33 +/- 654.41
Episode length: 34.40 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=935.89 +/- 760.34
Episode length: 34.90 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 28       |
|    time_elapsed    | 202      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=927.90 +/- 697.53
Episode length: 36.50 +/- 6.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 928       |
| time/                   |           |
|    total_timesteps      | 57500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.24e-09 |
|    explained_variance   | 0.00298   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.21e+04  |
|    n_updates            | 1658      |
|    policy_gradient_loss | 3.78e-10  |
|    value_loss           | 5.27e+04  |
---------------------------------------
Eval num_timesteps=58000, episode_reward=977.31 +/- 735.33
Episode length: 36.30 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 977      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=864.94 +/- 730.52
Episode length: 34.12 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=1055.00 +/- 705.25
Episode length: 37.62 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.6     |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 805      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 29       |
|    time_elapsed    | 209      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=804.72 +/- 694.71
Episode length: 34.98 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 805       |
| time/                   |           |
|    total_timesteps      | 59500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.54e-12 |
|    explained_variance   | 0.00224   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.34e+04  |
|    n_updates            | 1668      |
|    policy_gradient_loss | 8.18e-10  |
|    value_loss           | 3.37e+04  |
---------------------------------------
Eval num_timesteps=60000, episode_reward=812.96 +/- 684.03
Episode length: 35.38 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=750.21 +/- 694.30
Episode length: 34.02 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=719.80 +/- 698.41
Episode length: 33.08 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 759      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 30       |
|    time_elapsed    | 216      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=931.50 +/- 740.35
Episode length: 35.84 +/- 6.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 931       |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.43e-15 |
|    explained_variance   | 0.00413   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.59e+04  |
|    n_updates            | 1678      |
|    policy_gradient_loss | -4.57e-10 |
|    value_loss           | 3.88e+04  |
---------------------------------------
Eval num_timesteps=62000, episode_reward=1002.49 +/- 734.70
Episode length: 37.02 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=838.09 +/- 670.56
Episode length: 35.30 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=659.89 +/- 564.70
Episode length: 34.20 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 660      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 947      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 31       |
|    time_elapsed    | 223      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=785.95 +/- 681.50
Episode length: 34.52 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 786       |
| time/                   |           |
|    total_timesteps      | 63500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.75e-20 |
|    explained_variance   | 0.0116    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.22e+04  |
|    n_updates            | 1688      |
|    policy_gradient_loss | 1.5e-09   |
|    value_loss           | 4.61e+04  |
---------------------------------------
Eval num_timesteps=64000, episode_reward=782.34 +/- 660.34
Episode length: 35.24 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=736.23 +/- 636.97
Episode length: 34.30 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=852.15 +/- 691.96
Episode length: 35.76 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=788.67 +/- 659.75
Episode length: 34.88 +/- 7.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 32       |
|    time_elapsed    | 232      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=833.84 +/- 632.98
Episode length: 35.86 +/- 5.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 66000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.65e-17 |
|    explained_variance   | 0.00504   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.72e+04  |
|    n_updates            | 1698      |
|    policy_gradient_loss | 3.55e-10  |
|    value_loss           | 3.77e+04  |
---------------------------------------
Eval num_timesteps=66500, episode_reward=874.21 +/- 713.90
Episode length: 35.68 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=898.64 +/- 713.08
Episode length: 35.62 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=688.75 +/- 559.73
Episode length: 35.00 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 868      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 33       |
|    time_elapsed    | 239      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=810.79 +/- 672.26
Episode length: 34.96 +/- 5.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 811       |
| time/                   |           |
|    total_timesteps      | 68000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.97e-20 |
|    explained_variance   | 0.00982   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.39e+04  |
|    n_updates            | 1708      |
|    policy_gradient_loss | -4.37e-12 |
|    value_loss           | 4.26e+04  |
---------------------------------------
Eval num_timesteps=68500, episode_reward=770.24 +/- 647.73
Episode length: 34.92 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=848.89 +/- 666.14
Episode length: 35.62 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=939.07 +/- 733.67
Episode length: 36.20 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 34       |
|    time_elapsed    | 246      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=992.04 +/- 711.21
Episode length: 36.50 +/- 6.24
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.5     |
|    mean_reward          | 992      |
| time/                   |          |
|    total_timesteps      | 70000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.3e-17 |
|    explained_variance   | 0.0045   |
|    learning_rate        | 0.0001   |
|    loss                 | 2.19e+04 |
|    n_updates            | 1718     |
|    policy_gradient_loss | 8.44e-10 |
|    value_loss           | 4.07e+04 |
--------------------------------------
Eval num_timesteps=70500, episode_reward=818.55 +/- 674.52
Episode length: 34.62 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=945.30 +/- 741.02
Episode length: 36.00 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 945      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=952.74 +/- 670.24
Episode length: 37.42 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 953      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 35       |
|    time_elapsed    | 254      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=758.54 +/- 665.87
Episode length: 34.08 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 759       |
| time/                   |           |
|    total_timesteps      | 72000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-19 |
|    explained_variance   | 0.0103    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.32e+04  |
|    n_updates            | 1728      |
|    policy_gradient_loss | 6.18e-10  |
|    value_loss           | 4.24e+04  |
---------------------------------------
Eval num_timesteps=72500, episode_reward=927.50 +/- 711.14
Episode length: 35.90 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=1081.47 +/- 738.98
Episode length: 36.96 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
New best mean reward!
Eval num_timesteps=73500, episode_reward=781.06 +/- 629.96
Episode length: 35.48 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 898      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 36       |
|    time_elapsed    | 261      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=678.09 +/- 603.89
Episode length: 33.78 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 678       |
| time/                   |           |
|    total_timesteps      | 74000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.75e-17 |
|    explained_variance   | 0.00778   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.38e+04  |
|    n_updates            | 1738      |
|    policy_gradient_loss | -9.14e-10 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=74500, episode_reward=976.28 +/- 747.14
Episode length: 36.14 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 976      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=831.76 +/- 643.54
Episode length: 35.84 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=826.03 +/- 703.44
Episode length: 34.74 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 37       |
|    time_elapsed    | 268      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=852.45 +/- 656.15
Episode length: 35.90 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 852       |
| time/                   |           |
|    total_timesteps      | 76000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-19 |
|    explained_variance   | 0.00947   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.77e+04  |
|    n_updates            | 1748      |
|    policy_gradient_loss | -9.28e-10 |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=76500, episode_reward=798.35 +/- 640.47
Episode length: 35.50 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=706.27 +/- 625.94
Episode length: 33.64 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=890.52 +/- 739.74
Episode length: 35.00 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 38       |
|    time_elapsed    | 275      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=981.64 +/- 726.91
Episode length: 36.86 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 982       |
| time/                   |           |
|    total_timesteps      | 78000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.49e-17 |
|    explained_variance   | 0.00603   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.59e+04  |
|    n_updates            | 1758      |
|    policy_gradient_loss | 8.27e-10  |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=78500, episode_reward=794.51 +/- 680.66
Episode length: 34.78 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=673.34 +/- 603.27
Episode length: 33.94 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=1024.89 +/- 746.91
Episode length: 36.74 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 815      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 39       |
|    time_elapsed    | 283      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=725.46 +/- 635.15
Episode length: 34.28 +/- 7.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 725       |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.39e-19 |
|    explained_variance   | 0.0081    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.49e+04  |
|    n_updates            | 1768      |
|    policy_gradient_loss | 4.42e-10  |
|    value_loss           | 4.11e+04  |
---------------------------------------
Eval num_timesteps=80500, episode_reward=700.39 +/- 556.41
Episode length: 34.94 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=635.23 +/- 553.18
Episode length: 34.24 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=898.85 +/- 669.98
Episode length: 36.76 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 897      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 40       |
|    time_elapsed    | 290      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=834.05 +/- 714.07
Episode length: 34.26 +/- 6.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 82000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.53e-17 |
|    explained_variance   | 0.00601   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.62e+04  |
|    n_updates            | 1778      |
|    policy_gradient_loss | -6.94e-10 |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=82500, episode_reward=892.97 +/- 724.69
Episode length: 36.18 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=997.36 +/- 768.13
Episode length: 36.32 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 997      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=793.36 +/- 605.90
Episode length: 36.26 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 847      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 41       |
|    time_elapsed    | 297      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=866.63 +/- 718.17
Episode length: 34.82 +/- 7.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 867       |
| time/                   |           |
|    total_timesteps      | 84000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.64e-19 |
|    explained_variance   | 0.00697   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.4e+04   |
|    n_updates            | 1788      |
|    policy_gradient_loss | 8.88e-11  |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=84500, episode_reward=714.77 +/- 570.39
Episode length: 35.12 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=713.00 +/- 686.94
Episode length: 33.44 +/- 7.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=703.07 +/- 627.69
Episode length: 33.86 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=760.20 +/- 629.40
Episode length: 34.52 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 898      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 42       |
|    time_elapsed    | 306      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=901.77 +/- 718.78
Episode length: 35.84 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 902       |
| time/                   |           |
|    total_timesteps      | 86500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.68e-17 |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.19e+04  |
|    n_updates            | 1798      |
|    policy_gradient_loss | 7.73e-10  |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=87000, episode_reward=909.71 +/- 685.62
Episode length: 36.46 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=873.79 +/- 697.57
Episode length: 36.06 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=919.07 +/- 756.05
Episode length: 35.30 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 43       |
|    time_elapsed    | 313      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=759.24 +/- 708.79
Episode length: 33.96 +/- 7.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 759       |
| time/                   |           |
|    total_timesteps      | 88500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.66e-19 |
|    explained_variance   | 0.0102    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.92e+04  |
|    n_updates            | 1808      |
|    policy_gradient_loss | -9.43e-10 |
|    value_loss           | 3.35e+04  |
---------------------------------------
Eval num_timesteps=89000, episode_reward=689.06 +/- 573.78
Episode length: 34.26 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=839.34 +/- 595.22
Episode length: 36.04 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=717.84 +/- 636.93
Episode length: 34.52 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 857      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 44       |
|    time_elapsed    | 320      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=816.42 +/- 653.83
Episode length: 35.48 +/- 6.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 816       |
| time/                   |           |
|    total_timesteps      | 90500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.48e-17 |
|    explained_variance   | 0.00829   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.9e+04   |
|    n_updates            | 1818      |
|    policy_gradient_loss | 8.38e-10  |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=91000, episode_reward=903.28 +/- 756.75
Episode length: 35.44 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=678.15 +/- 484.15
Episode length: 35.02 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=859.91 +/- 723.49
Episode length: 35.00 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 923      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 45       |
|    time_elapsed    | 327      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=754.05 +/- 634.23
Episode length: 35.12 +/- 6.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 754       |
| time/                   |           |
|    total_timesteps      | 92500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.5e-19  |
|    explained_variance   | 0.00898   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.84e+04  |
|    n_updates            | 1828      |
|    policy_gradient_loss | -5.27e-10 |
|    value_loss           | 4.02e+04  |
---------------------------------------
Eval num_timesteps=93000, episode_reward=835.66 +/- 688.16
Episode length: 35.20 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=725.17 +/- 584.01
Episode length: 35.28 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=858.07 +/- 687.62
Episode length: 35.54 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 861      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 46       |
|    time_elapsed    | 334      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=729.75 +/- 592.89
Episode length: 35.08 +/- 5.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 730       |
| time/                   |           |
|    total_timesteps      | 94500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.76e-18 |
|    explained_variance   | 0.00551   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.28e+04  |
|    n_updates            | 1838      |
|    policy_gradient_loss | -3.61e-10 |
|    value_loss           | 3.76e+04  |
---------------------------------------
Eval num_timesteps=95000, episode_reward=736.66 +/- 590.22
Episode length: 35.02 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=612.57 +/- 463.23
Episode length: 34.36 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=901.50 +/- 635.39
Episode length: 36.70 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.8     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 47       |
|    time_elapsed    | 341      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=1097.11 +/- 728.79
Episode length: 37.56 +/- 6.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.6      |
|    mean_reward          | 1.1e+03   |
| time/                   |           |
|    total_timesteps      | 96500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.42e-21 |
|    explained_variance   | 0.0105    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.41e+04  |
|    n_updates            | 1848      |
|    policy_gradient_loss | 1.67e-09  |
|    value_loss           | 3.89e+04  |
---------------------------------------
New best mean reward!
Eval num_timesteps=97000, episode_reward=821.12 +/- 674.75
Episode length: 35.30 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=860.16 +/- 722.71
Episode length: 35.06 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=790.58 +/- 648.33
Episode length: 35.46 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 48       |
|    time_elapsed    | 349      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=990.04 +/- 708.19
Episode length: 36.58 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 990       |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.77e-18 |
|    explained_variance   | 0.00268   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.4e+04   |
|    n_updates            | 1858      |
|    policy_gradient_loss | 6.32e-10  |
|    value_loss           | 3.98e+04  |
---------------------------------------
Eval num_timesteps=99000, episode_reward=885.98 +/- 669.02
Episode length: 36.28 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=906.95 +/- 684.79
Episode length: 36.40 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=740.86 +/- 645.04
Episode length: 34.36 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 49       |
|    time_elapsed    | 356      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=804.09 +/- 682.35
Episode length: 34.52 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 804       |
| time/                   |           |
|    total_timesteps      | 100500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.66e-13 |
|    explained_variance   | 0.0073    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.4e+04   |
|    n_updates            | 1868      |
|    policy_gradient_loss | 3.71e-10  |
|    value_loss           | 3.41e+04  |
---------------------------------------
Eval num_timesteps=101000, episode_reward=881.08 +/- 709.12
Episode length: 35.58 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=841.53 +/- 672.38
Episode length: 35.46 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=804.37 +/- 663.95
Episode length: 35.00 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 698      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 50       |
|    time_elapsed    | 363      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=916.11 +/- 683.92
Episode length: 35.98 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 916       |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.63e-15 |
|    explained_variance   | 0.00463   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.39e+04  |
|    n_updates            | 1878      |
|    policy_gradient_loss | 1.01e-09  |
|    value_loss           | 3.05e+04  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=764.33 +/- 638.11
Episode length: 35.10 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=759.48 +/- 649.33
Episode length: 35.12 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=839.63 +/- 680.95
Episode length: 35.68 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 51       |
|    time_elapsed    | 371      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=1021.29 +/- 743.29
Episode length: 36.70 +/- 6.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.7     |
|    mean_reward          | 1.02e+03 |
| time/                   |          |
|    total_timesteps      | 104500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -8.7e-21 |
|    explained_variance   | 0.0111   |
|    learning_rate        | 0.0001   |
|    loss                 | 2.27e+04 |
|    n_updates            | 1888     |
|    policy_gradient_loss | 6.61e-10 |
|    value_loss           | 3.94e+04 |
--------------------------------------
Eval num_timesteps=105000, episode_reward=703.43 +/- 642.99
Episode length: 33.68 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=673.03 +/- 611.85
Episode length: 33.74 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=848.18 +/- 689.34
Episode length: 35.36 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 52       |
|    time_elapsed    | 378      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=730.85 +/- 616.55
Episode length: 34.64 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 731       |
| time/                   |           |
|    total_timesteps      | 106500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.85e-18 |
|    explained_variance   | 0.00695   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.25e+04  |
|    n_updates            | 1898      |
|    policy_gradient_loss | 7.68e-10  |
|    value_loss           | 4.19e+04  |
---------------------------------------
Eval num_timesteps=107000, episode_reward=846.46 +/- 694.86
Episode length: 35.14 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=781.55 +/- 661.44
Episode length: 35.32 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=848.20 +/- 651.96
Episode length: 35.88 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=787.15 +/- 657.23
Episode length: 34.86 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 784      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 53       |
|    time_elapsed    | 386      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=798.39 +/- 711.46
Episode length: 34.32 +/- 7.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 798       |
| time/                   |           |
|    total_timesteps      | 109000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.99e-21 |
|    explained_variance   | 0.00781   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.66e+04  |
|    n_updates            | 1908      |
|    policy_gradient_loss | -5.7e-10  |
|    value_loss           | 3.65e+04  |
---------------------------------------
Eval num_timesteps=109500, episode_reward=849.88 +/- 667.87
Episode length: 35.74 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=802.00 +/- 660.07
Episode length: 35.22 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=807.39 +/- 681.33
Episode length: 34.78 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 801      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 54       |
|    time_elapsed    | 393      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=883.84 +/- 725.89
Episode length: 35.24 +/- 7.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 884       |
| time/                   |           |
|    total_timesteps      | 111000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.89e-18 |
|    explained_variance   | 0.00726   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.27e+04  |
|    n_updates            | 1918      |
|    policy_gradient_loss | 9.04e-10  |
|    value_loss           | 4.51e+04  |
---------------------------------------
Eval num_timesteps=111500, episode_reward=941.67 +/- 751.50
Episode length: 35.82 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=1142.62 +/- 769.08
Episode length: 37.28 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
New best mean reward!
Eval num_timesteps=112500, episode_reward=724.62 +/- 605.24
Episode length: 34.76 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 55       |
|    time_elapsed    | 401      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=853.41 +/- 695.32
Episode length: 35.26 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 853       |
| time/                   |           |
|    total_timesteps      | 113000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.18e-21 |
|    explained_variance   | 0.00873   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.81e+04  |
|    n_updates            | 1928      |
|    policy_gradient_loss | -8.79e-10 |
|    value_loss           | 3.86e+04  |
---------------------------------------
Eval num_timesteps=113500, episode_reward=796.35 +/- 650.28
Episode length: 35.04 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=748.24 +/- 620.05
Episode length: 34.68 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=833.99 +/- 662.61
Episode length: 35.14 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 676      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 56       |
|    time_elapsed    | 408      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=842.96 +/- 653.92
Episode length: 35.42 +/- 5.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 843       |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.86e-18 |
|    explained_variance   | 0.00557   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.56e+04  |
|    n_updates            | 1938      |
|    policy_gradient_loss | -5.82e-11 |
|    value_loss           | 4.49e+04  |
---------------------------------------
Eval num_timesteps=115500, episode_reward=883.41 +/- 660.86
Episode length: 37.04 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=801.35 +/- 651.40
Episode length: 35.60 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=904.12 +/- 727.06
Episode length: 35.98 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 815      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 57       |
|    time_elapsed    | 415      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=793.83 +/- 701.11
Episode length: 34.50 +/- 7.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 794       |
| time/                   |           |
|    total_timesteps      | 117000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.05e-21 |
|    explained_variance   | 0.00963   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.23e+04  |
|    n_updates            | 1948      |
|    policy_gradient_loss | -5.24e-11 |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=117500, episode_reward=852.08 +/- 734.47
Episode length: 34.84 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=764.84 +/- 724.96
Episode length: 34.26 +/- 7.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=896.14 +/- 678.80
Episode length: 36.34 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 951      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 58       |
|    time_elapsed    | 422      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=930.00 +/- 724.60
Episode length: 36.10 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 930       |
| time/                   |           |
|    total_timesteps      | 119000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.43e-28 |
|    explained_variance   | 0.019     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.12e+04  |
|    n_updates            | 1958      |
|    policy_gradient_loss | -8.12e-10 |
|    value_loss           | 4.52e+04  |
---------------------------------------
Eval num_timesteps=119500, episode_reward=860.52 +/- 696.87
Episode length: 35.34 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=777.71 +/- 667.85
Episode length: 34.38 +/- 7.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=855.12 +/- 700.53
Episode length: 34.90 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 992      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 59       |
|    time_elapsed    | 430      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=725.34 +/- 613.08
Episode length: 34.36 +/- 6.39
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.4     |
|    mean_reward          | 725      |
| time/                   |          |
|    total_timesteps      | 121000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.8e-25 |
|    explained_variance   | 0.0129   |
|    learning_rate        | 0.0001   |
|    loss                 | 3.96e+04 |
|    n_updates            | 1968     |
|    policy_gradient_loss | 2.12e-09 |
|    value_loss           | 6.29e+04 |
--------------------------------------
Eval num_timesteps=121500, episode_reward=858.96 +/- 673.82
Episode length: 35.58 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=838.19 +/- 685.73
Episode length: 34.84 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=705.39 +/- 655.31
Episode length: 33.82 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 917      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 60       |
|    time_elapsed    | 437      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=756.72 +/- 667.52
Episode length: 34.36 +/- 6.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 757       |
| time/                   |           |
|    total_timesteps      | 123000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.53e-18 |
|    explained_variance   | 0.0125    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.01e+04  |
|    n_updates            | 1978      |
|    policy_gradient_loss | -1.49e-09 |
|    value_loss           | 4.27e+04  |
---------------------------------------
Eval num_timesteps=123500, episode_reward=879.37 +/- 716.27
Episode length: 35.00 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=700.39 +/- 637.75
Episode length: 34.42 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=724.95 +/- 594.45
Episode length: 35.16 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 827      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 61       |
|    time_elapsed    | 444      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=660.57 +/- 594.51
Episode length: 33.60 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 661       |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.33e-14 |
|    explained_variance   | 0.0102    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.06e+04  |
|    n_updates            | 1988      |
|    policy_gradient_loss | 6.93e-10  |
|    value_loss           | 3.78e+04  |
---------------------------------------
Eval num_timesteps=125500, episode_reward=700.81 +/- 539.70
Episode length: 35.20 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=954.62 +/- 751.99
Episode length: 36.08 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=1007.15 +/- 730.96
Episode length: 36.50 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 62       |
|    time_elapsed    | 451      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=789.09 +/- 642.06
Episode length: 34.58 +/- 6.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 127000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.71e-16 |
|    explained_variance   | 0.00751   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.52e+04  |
|    n_updates            | 1998      |
|    policy_gradient_loss | 1.03e-09  |
|    value_loss           | 4.05e+04  |
---------------------------------------
Eval num_timesteps=127500, episode_reward=812.28 +/- 711.32
Episode length: 34.34 +/- 7.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=756.90 +/- 645.24
Episode length: 34.64 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=720.29 +/- 602.73
Episode length: 34.42 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=732.40 +/- 611.22
Episode length: 34.88 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 63       |
|    time_elapsed    | 459      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=905.65 +/- 651.00
Episode length: 36.90 +/- 5.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 906       |
| time/                   |           |
|    total_timesteps      | 129500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.71e-14 |
|    explained_variance   | 0.00691   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.23e+04  |
|    n_updates            | 2008      |
|    policy_gradient_loss | -3.49e-11 |
|    value_loss           | 3.65e+04  |
---------------------------------------
Eval num_timesteps=130000, episode_reward=1029.39 +/- 774.82
Episode length: 36.28 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=850.82 +/- 669.74
Episode length: 36.12 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=896.95 +/- 712.01
Episode length: 36.20 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 64       |
|    time_elapsed    | 467      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=763.61 +/- 646.32
Episode length: 34.60 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 764       |
| time/                   |           |
|    total_timesteps      | 131500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.03e-16 |
|    explained_variance   | 0.00717   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.17e+04  |
|    n_updates            | 2018      |
|    policy_gradient_loss | -1.41e-09 |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=132000, episode_reward=741.68 +/- 675.84
Episode length: 33.84 +/- 7.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=758.89 +/- 696.28
Episode length: 33.56 +/- 8.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=782.31 +/- 695.18
Episode length: 34.10 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 720      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 65       |
|    time_elapsed    | 474      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=750.74 +/- 640.23
Episode length: 34.76 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 751       |
| time/                   |           |
|    total_timesteps      | 133500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.97e-14 |
|    explained_variance   | 0.00794   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.52e+04  |
|    n_updates            | 2028      |
|    policy_gradient_loss | -1.5e-09  |
|    value_loss           | 3.79e+04  |
---------------------------------------
Eval num_timesteps=134000, episode_reward=845.15 +/- 628.01
Episode length: 36.34 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=708.89 +/- 650.02
Episode length: 34.18 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=946.31 +/- 729.97
Episode length: 36.22 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 66       |
|    time_elapsed    | 481      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=1201.96 +/- 747.77
Episode length: 37.44 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.4      |
|    mean_reward          | 1.2e+03   |
| time/                   |           |
|    total_timesteps      | 135500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.03e-16 |
|    explained_variance   | 0.00301   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.19e+04  |
|    n_updates            | 2038      |
|    policy_gradient_loss | -2.97e-10 |
|    value_loss           | 3.34e+04  |
---------------------------------------
New best mean reward!
Eval num_timesteps=136000, episode_reward=1009.55 +/- 719.90
Episode length: 36.86 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=1051.94 +/- 776.87
Episode length: 36.34 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=785.00 +/- 601.97
Episode length: 35.32 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 688      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 67       |
|    time_elapsed    | 488      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=962.46 +/- 708.66
Episode length: 36.28 +/- 5.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 962       |
| time/                   |           |
|    total_timesteps      | 137500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.13e-14 |
|    explained_variance   | -0.000797 |
|    learning_rate        | 0.0001    |
|    loss                 | 1.3e+04   |
|    n_updates            | 2048      |
|    policy_gradient_loss | -6.46e-10 |
|    value_loss           | 3.08e+04  |
---------------------------------------
Eval num_timesteps=138000, episode_reward=784.94 +/- 639.20
Episode length: 35.28 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=854.18 +/- 666.41
Episode length: 35.68 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=910.28 +/- 701.05
Episode length: 35.70 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 68       |
|    time_elapsed    | 496      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=721.22 +/- 604.75
Episode length: 34.66 +/- 5.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 721       |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.15e-16 |
|    explained_variance   | 0.00611   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.83e+04  |
|    n_updates            | 2058      |
|    policy_gradient_loss | -7.42e-11 |
|    value_loss           | 3.76e+04  |
---------------------------------------
Eval num_timesteps=140000, episode_reward=802.98 +/- 624.79
Episode length: 35.64 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=996.15 +/- 708.30
Episode length: 36.74 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=879.61 +/- 659.09
Episode length: 36.18 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 853      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 69       |
|    time_elapsed    | 503      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=810.64 +/- 658.98
Episode length: 35.22 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 811       |
| time/                   |           |
|    total_timesteps      | 141500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.04e-22 |
|    explained_variance   | 0.0125    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.06e+04  |
|    n_updates            | 2068      |
|    policy_gradient_loss | 1.12e-09  |
|    value_loss           | 4.21e+04  |
---------------------------------------
Eval num_timesteps=142000, episode_reward=706.00 +/- 573.83
Episode length: 34.36 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=704.50 +/- 630.58
Episode length: 33.64 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=939.47 +/- 726.37
Episode length: 35.86 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 846      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 70       |
|    time_elapsed    | 510      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=883.53 +/- 735.51
Episode length: 35.50 +/- 7.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 884       |
| time/                   |           |
|    total_timesteps      | 143500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.81e-19 |
|    explained_variance   | 0.0094    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.75e+04  |
|    n_updates            | 2078      |
|    policy_gradient_loss | -1.16e-09 |
|    value_loss           | 4.28e+04  |
---------------------------------------
Eval num_timesteps=144000, episode_reward=802.26 +/- 664.99
Episode length: 35.16 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=945.72 +/- 685.89
Episode length: 36.78 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=747.72 +/- 684.04
Episode length: 33.90 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 916      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 71       |
|    time_elapsed    | 517      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=968.68 +/- 699.40
Episode length: 36.76 +/- 5.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 969       |
| time/                   |           |
|    total_timesteps      | 145500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.96e-22 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.76e+04  |
|    n_updates            | 2088      |
|    policy_gradient_loss | -1e-10    |
|    value_loss           | 4.15e+04  |
---------------------------------------
Eval num_timesteps=146000, episode_reward=933.93 +/- 714.08
Episode length: 36.00 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=690.24 +/- 579.62
Episode length: 34.46 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=778.98 +/- 697.64
Episode length: 34.64 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 72       |
|    time_elapsed    | 525      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=830.86 +/- 723.46
Episode length: 34.88 +/- 7.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 831       |
| time/                   |           |
|    total_timesteps      | 147500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.08e-19 |
|    explained_variance   | 0.00314   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.01e+04  |
|    n_updates            | 2098      |
|    policy_gradient_loss | -6.64e-10 |
|    value_loss           | 4.69e+04  |
---------------------------------------
Eval num_timesteps=148000, episode_reward=743.66 +/- 640.89
Episode length: 34.26 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=835.04 +/- 687.36
Episode length: 34.88 +/- 7.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=773.28 +/- 647.41
Episode length: 34.98 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=816.31 +/- 700.47
Episode length: 34.76 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 73       |
|    time_elapsed    | 533      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=749.55 +/- 619.31
Episode length: 35.16 +/- 5.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 750       |
| time/                   |           |
|    total_timesteps      | 150000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.45e-22 |
|    explained_variance   | 0.00917   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.08e+04  |
|    n_updates            | 2108      |
|    policy_gradient_loss | -1.64e-10 |
|    value_loss           | 3.81e+04  |
---------------------------------------
Eval num_timesteps=150500, episode_reward=850.93 +/- 676.59
Episode length: 35.50 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=687.54 +/- 599.18
Episode length: 34.06 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 688      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=670.24 +/- 594.87
Episode length: 33.58 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 797      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 74       |
|    time_elapsed    | 540      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=916.84 +/- 709.94
Episode length: 35.58 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 917       |
| time/                   |           |
|    total_timesteps      | 152000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.24e-19 |
|    explained_variance   | 0.00952   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.85e+04  |
|    n_updates            | 2118      |
|    policy_gradient_loss | 5.53e-11  |
|    value_loss           | 4.43e+04  |
---------------------------------------
Eval num_timesteps=152500, episode_reward=796.43 +/- 706.25
Episode length: 34.32 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=829.70 +/- 699.21
Episode length: 35.08 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=824.30 +/- 658.71
Episode length: 34.94 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 75       |
|    time_elapsed    | 547      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=814.29 +/- 694.54
Episode length: 35.02 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 814       |
| time/                   |           |
|    total_timesteps      | 154000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-13 |
|    explained_variance   | 0.00626   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.12e+04  |
|    n_updates            | 2128      |
|    policy_gradient_loss | 5.05e-10  |
|    value_loss           | 3.54e+04  |
---------------------------------------
Eval num_timesteps=154500, episode_reward=729.57 +/- 618.18
Episode length: 35.02 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=771.09 +/- 609.92
Episode length: 35.54 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=615.31 +/- 520.79
Episode length: 33.78 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 615      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 811      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 76       |
|    time_elapsed    | 554      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=672.56 +/- 591.92
Episode length: 34.06 +/- 5.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 673       |
| time/                   |           |
|    total_timesteps      | 156000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.94e-16 |
|    explained_variance   | 0.00906   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.34e+04  |
|    n_updates            | 2138      |
|    policy_gradient_loss | 2.34e-09  |
|    value_loss           | 3.73e+04  |
---------------------------------------
Eval num_timesteps=156500, episode_reward=990.75 +/- 730.50
Episode length: 36.48 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=849.90 +/- 661.95
Episode length: 36.00 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=708.91 +/- 640.89
Episode length: 33.74 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 899      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 77       |
|    time_elapsed    | 562      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=956.00 +/- 724.09
Episode length: 36.54 +/- 5.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 956       |
| time/                   |           |
|    total_timesteps      | 158000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.55e-22 |
|    explained_variance   | 0.0115    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.2e+04   |
|    n_updates            | 2148      |
|    policy_gradient_loss | -7.01e-10 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=158500, episode_reward=860.16 +/- 681.20
Episode length: 36.10 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=885.91 +/- 729.87
Episode length: 35.24 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=832.36 +/- 675.52
Episode length: 34.96 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 860      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 78       |
|    time_elapsed    | 569      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=903.78 +/- 691.85
Episode length: 35.96 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 904       |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.23e-19 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.86e+04  |
|    n_updates            | 2158      |
|    policy_gradient_loss | -1.21e-09 |
|    value_loss           | 4.66e+04  |
---------------------------------------
Eval num_timesteps=160500, episode_reward=798.31 +/- 617.60
Episode length: 36.00 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=841.24 +/- 655.74
Episode length: 36.16 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=847.49 +/- 672.38
Episode length: 35.66 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 79       |
|    time_elapsed    | 576      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=928.63 +/- 741.09
Episode length: 35.84 +/- 7.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 929       |
| time/                   |           |
|    total_timesteps      | 162000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-21 |
|    explained_variance   | 0.00962   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.51e+04  |
|    n_updates            | 2168      |
|    policy_gradient_loss | -9.74e-10 |
|    value_loss           | 3.69e+04  |
---------------------------------------
Eval num_timesteps=162500, episode_reward=597.33 +/- 537.82
Episode length: 32.96 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=757.11 +/- 688.99
Episode length: 34.44 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=749.01 +/- 616.78
Episode length: 35.34 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 765      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 80       |
|    time_elapsed    | 584      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=839.30 +/- 674.79
Episode length: 35.46 +/- 6.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 164000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.18e-19 |
|    explained_variance   | 0.000938  |
|    learning_rate        | 0.0001    |
|    loss                 | 2.35e+04  |
|    n_updates            | 2178      |
|    policy_gradient_loss | 3.35e-11  |
|    value_loss           | 4.69e+04  |
---------------------------------------
Eval num_timesteps=164500, episode_reward=882.92 +/- 674.28
Episode length: 35.90 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=1021.94 +/- 731.67
Episode length: 36.62 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=774.93 +/- 666.40
Episode length: 34.60 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 728      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 81       |
|    time_elapsed    | 591      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=905.99 +/- 707.25
Episode length: 36.12 +/- 7.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 906       |
| time/                   |           |
|    total_timesteps      | 166000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-13 |
|    explained_variance   | 0.0091    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.62e+04  |
|    n_updates            | 2188      |
|    policy_gradient_loss | 7.2e-10   |
|    value_loss           | 3.38e+04  |
---------------------------------------
Eval num_timesteps=166500, episode_reward=808.65 +/- 665.33
Episode length: 35.14 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=816.54 +/- 705.90
Episode length: 34.56 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=948.94 +/- 675.13
Episode length: 36.92 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 726      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 82       |
|    time_elapsed    | 598      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=917.69 +/- 730.71
Episode length: 35.94 +/- 6.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 918       |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.09e-16 |
|    explained_variance   | 0.00712   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.83e+04  |
|    n_updates            | 2198      |
|    policy_gradient_loss | 3.58e-10  |
|    value_loss           | 3.24e+04  |
---------------------------------------
Eval num_timesteps=168500, episode_reward=926.85 +/- 695.13
Episode length: 36.48 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=731.22 +/- 663.69
Episode length: 33.98 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=941.70 +/- 713.58
Episode length: 36.18 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 795      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 83       |
|    time_elapsed    | 606      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=879.02 +/- 704.54
Episode length: 35.10 +/- 7.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 879       |
| time/                   |           |
|    total_timesteps      | 170000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.08e-13 |
|    explained_variance   | 0.0107    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.34e+04  |
|    n_updates            | 2208      |
|    policy_gradient_loss | -8.8e-10  |
|    value_loss           | 3.82e+04  |
---------------------------------------
Eval num_timesteps=170500, episode_reward=835.15 +/- 652.69
Episode length: 35.46 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=739.32 +/- 644.37
Episode length: 33.92 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=768.92 +/- 722.27
Episode length: 33.66 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=888.20 +/- 666.90
Episode length: 35.76 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 802      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 84       |
|    time_elapsed    | 614      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=906.03 +/- 703.79
Episode length: 35.70 +/- 5.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 906       |
| time/                   |           |
|    total_timesteps      | 172500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.77e-16 |
|    explained_variance   | 0.00744   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.51e+04  |
|    n_updates            | 2218      |
|    policy_gradient_loss | 2.39e-10  |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=173000, episode_reward=904.88 +/- 700.33
Episode length: 35.76 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=962.40 +/- 739.17
Episode length: 36.20 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=679.22 +/- 586.19
Episode length: 34.06 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 717      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 85       |
|    time_elapsed    | 621      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=856.62 +/- 723.75
Episode length: 35.08 +/- 6.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 857       |
| time/                   |           |
|    total_timesteps      | 174500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.73e-14 |
|    explained_variance   | 0.00436   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.49e+04  |
|    n_updates            | 2228      |
|    policy_gradient_loss | 3.78e-11  |
|    value_loss           | 3.4e+04   |
---------------------------------------
Eval num_timesteps=175000, episode_reward=774.55 +/- 642.47
Episode length: 34.86 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=919.45 +/- 695.64
Episode length: 36.34 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=761.38 +/- 649.58
Episode length: 34.76 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 86       |
|    time_elapsed    | 629      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=761.59 +/- 628.46
Episode length: 35.44 +/- 5.95
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.4     |
|    mean_reward          | 762      |
| time/                   |          |
|    total_timesteps      | 176500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5.9e-16 |
|    explained_variance   | 0.0113   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.56e+04 |
|    n_updates            | 2238     |
|    policy_gradient_loss | 2.04e-09 |
|    value_loss           | 4.1e+04  |
--------------------------------------
Eval num_timesteps=177000, episode_reward=915.07 +/- 715.00
Episode length: 35.26 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=878.62 +/- 720.53
Episode length: 35.10 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=847.03 +/- 636.27
Episode length: 36.08 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 904      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 87       |
|    time_elapsed    | 636      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=812.67 +/- 627.09
Episode length: 35.60 +/- 6.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 813       |
| time/                   |           |
|    total_timesteps      | 178500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.16e-14 |
|    explained_variance   | 0.0126    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.82e+04  |
|    n_updates            | 2248      |
|    policy_gradient_loss | 2.62e-10  |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=179000, episode_reward=752.19 +/- 662.19
Episode length: 34.64 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=864.21 +/- 691.43
Episode length: 35.68 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=911.61 +/- 752.18
Episode length: 35.40 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 88       |
|    time_elapsed    | 643      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=827.84 +/- 657.47
Episode length: 35.78 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 828       |
| time/                   |           |
|    total_timesteps      | 180500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.65e-16 |
|    explained_variance   | 0.00483   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.48e+04  |
|    n_updates            | 2258      |
|    policy_gradient_loss | -3.49e-11 |
|    value_loss           | 3.12e+04  |
---------------------------------------
Eval num_timesteps=181000, episode_reward=900.03 +/- 686.63
Episode length: 36.36 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 900      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=715.20 +/- 595.41
Episode length: 34.78 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=826.29 +/- 644.62
Episode length: 35.88 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 89       |
|    time_elapsed    | 650      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=695.02 +/- 610.62
Episode length: 34.26 +/- 6.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 695       |
| time/                   |           |
|    total_timesteps      | 182500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.05e-14 |
|    explained_variance   | 0.00684   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.81e+04  |
|    n_updates            | 2268      |
|    policy_gradient_loss | 7.45e-10  |
|    value_loss           | 3.44e+04  |
---------------------------------------
Eval num_timesteps=183000, episode_reward=895.96 +/- 713.70
Episode length: 35.88 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=840.30 +/- 678.16
Episode length: 35.16 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=965.37 +/- 747.49
Episode length: 35.88 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 90       |
|    time_elapsed    | 658      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=816.80 +/- 756.19
Episode length: 34.20 +/- 8.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 817       |
| time/                   |           |
|    total_timesteps      | 184500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.54e-16 |
|    explained_variance   | 0.00841   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.52e+04  |
|    n_updates            | 2278      |
|    policy_gradient_loss | -8.88e-11 |
|    value_loss           | 3.51e+04  |
---------------------------------------
Eval num_timesteps=185000, episode_reward=802.25 +/- 657.02
Episode length: 34.84 +/- 7.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=957.14 +/- 713.36
Episode length: 36.78 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=708.92 +/- 616.17
Episode length: 34.32 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 91       |
|    time_elapsed    | 665      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=883.54 +/- 709.46
Episode length: 35.02 +/- 7.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 884       |
| time/                   |           |
|    total_timesteps      | 186500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.94e-14 |
|    explained_variance   | 0.00428   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.56e+04  |
|    n_updates            | 2288      |
|    policy_gradient_loss | -2.91e-10 |
|    value_loss           | 3.72e+04  |
---------------------------------------
Eval num_timesteps=187000, episode_reward=797.57 +/- 663.71
Episode length: 34.76 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=880.24 +/- 704.42
Episode length: 35.48 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=899.48 +/- 676.62
Episode length: 36.04 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 92       |
|    time_elapsed    | 672      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=778.86 +/- 681.01
Episode length: 34.70 +/- 7.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 779       |
| time/                   |           |
|    total_timesteps      | 188500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.52e-16 |
|    explained_variance   | 0.0104    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.85e+04  |
|    n_updates            | 2298      |
|    policy_gradient_loss | -2.52e-10 |
|    value_loss           | 3.62e+04  |
---------------------------------------
Eval num_timesteps=189000, episode_reward=962.03 +/- 691.73
Episode length: 36.30 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=822.34 +/- 688.52
Episode length: 35.12 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=979.87 +/- 752.73
Episode length: 35.88 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 862      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 93       |
|    time_elapsed    | 679      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=822.22 +/- 687.34
Episode length: 34.92 +/- 6.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 822       |
| time/                   |           |
|    total_timesteps      | 190500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.81e-14 |
|    explained_variance   | 0.0119    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.67e+04  |
|    n_updates            | 2308      |
|    policy_gradient_loss | 6.37e-10  |
|    value_loss           | 3.71e+04  |
---------------------------------------
Eval num_timesteps=191000, episode_reward=994.04 +/- 685.24
Episode length: 37.34 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=859.02 +/- 690.65
Episode length: 35.32 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=805.19 +/- 655.89
Episode length: 35.24 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=660.94 +/- 565.63
Episode length: 34.14 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 94       |
|    time_elapsed    | 688      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=838.58 +/- 678.50
Episode length: 35.48 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 193000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.32e-16 |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.34e+04  |
|    n_updates            | 2318      |
|    policy_gradient_loss | -5.24e-11 |
|    value_loss           | 4.55e+04  |
---------------------------------------
Eval num_timesteps=193500, episode_reward=680.99 +/- 578.89
Episode length: 34.30 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=928.86 +/- 732.99
Episode length: 35.52 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=598.39 +/- 427.62
Episode length: 34.32 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 598      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 95       |
|    time_elapsed    | 695      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=761.33 +/- 657.91
Episode length: 34.74 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 761       |
| time/                   |           |
|    total_timesteps      | 195000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.75e-14 |
|    explained_variance   | 0.00433   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.27e+04  |
|    n_updates            | 2328      |
|    policy_gradient_loss | 8.03e-10  |
|    value_loss           | 3.48e+04  |
---------------------------------------
Eval num_timesteps=195500, episode_reward=730.66 +/- 614.46
Episode length: 34.76 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=840.28 +/- 668.27
Episode length: 35.44 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=938.55 +/- 709.34
Episode length: 36.28 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 96       |
|    time_elapsed    | 702      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=804.56 +/- 738.57
Episode length: 34.00 +/- 8.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 805       |
| time/                   |           |
|    total_timesteps      | 197000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-16 |
|    explained_variance   | 0.0087    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.11e+04  |
|    n_updates            | 2338      |
|    policy_gradient_loss | -1.78e-10 |
|    value_loss           | 3.61e+04  |
---------------------------------------
Eval num_timesteps=197500, episode_reward=700.31 +/- 639.60
Episode length: 33.72 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=711.96 +/- 617.25
Episode length: 34.30 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=607.49 +/- 559.44
Episode length: 33.08 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 607      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 97       |
|    time_elapsed    | 709      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=726.50 +/- 605.39
Episode length: 34.10 +/- 6.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 726       |
| time/                   |           |
|    total_timesteps      | 199000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.69e-14 |
|    explained_variance   | 0.00462   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.8e+04   |
|    n_updates            | 2348      |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 3.68e+04  |
---------------------------------------
Eval num_timesteps=199500, episode_reward=805.76 +/- 645.70
Episode length: 34.92 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=993.62 +/- 725.77
Episode length: 36.04 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=790.30 +/- 640.93
Episode length: 34.78 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 948      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 98       |
|    time_elapsed    | 716      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=928.46 +/- 737.23
Episode length: 35.94 +/- 7.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 928       |
| time/                   |           |
|    total_timesteps      | 201000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.24e-16 |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.51e+04  |
|    n_updates            | 2358      |
|    policy_gradient_loss | 7.79e-10  |
|    value_loss           | 4.97e+04  |
---------------------------------------
Eval num_timesteps=201500, episode_reward=828.53 +/- 678.18
Episode length: 35.56 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=795.99 +/- 649.01
Episode length: 35.54 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=1036.86 +/- 730.44
Episode length: 36.50 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.9     |
|    ep_rew_mean     | 1e+03    |
| time/              |          |
|    fps             | 279      |
|    iterations      | 99       |
|    time_elapsed    | 724      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=976.65 +/- 709.24
Episode length: 36.62 +/- 5.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 977       |
| time/                   |           |
|    total_timesteps      | 203000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.06e-22 |
|    explained_variance   | 0.0156    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.43e+04  |
|    n_updates            | 2368      |
|    policy_gradient_loss | 1.86e-10  |
|    value_loss           | 3.86e+04  |
---------------------------------------
Eval num_timesteps=203500, episode_reward=861.90 +/- 633.56
Episode length: 35.84 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=568.74 +/- 513.82
Episode length: 32.88 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 569      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=743.68 +/- 625.16
Episode length: 34.72 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 909      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 100      |
|    time_elapsed    | 731      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=900.75 +/- 684.37
Episode length: 36.12 +/- 6.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 901       |
| time/                   |           |
|    total_timesteps      | 205000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.39e-20 |
|    explained_variance   | 0.00685   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.25e+04  |
|    n_updates            | 2378      |
|    policy_gradient_loss | -6.66e-10 |
|    value_loss           | 4.72e+04  |
---------------------------------------
Eval num_timesteps=205500, episode_reward=779.68 +/- 714.00
Episode length: 34.04 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=865.33 +/- 674.07
Episode length: 35.82 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=800.01 +/- 661.07
Episode length: 34.82 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 798      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 101      |
|    time_elapsed    | 738      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=963.49 +/- 723.49
Episode length: 36.32 +/- 6.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 963       |
| time/                   |           |
|    total_timesteps      | 207000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.91e-14 |
|    explained_variance   | 0.00518   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.79e+04  |
|    n_updates            | 2388      |
|    policy_gradient_loss | 2.74e-10  |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=207500, episode_reward=691.70 +/- 600.21
Episode length: 33.86 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=844.97 +/- 667.70
Episode length: 35.64 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=855.56 +/- 730.59
Episode length: 34.74 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 102      |
|    time_elapsed    | 746      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=997.33 +/- 740.96
Episode length: 36.44 +/- 6.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 997       |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.56e-16 |
|    explained_variance   | 0.00661   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.39e+04  |
|    n_updates            | 2398      |
|    policy_gradient_loss | -5.37e-10 |
|    value_loss           | 3.44e+04  |
---------------------------------------
Eval num_timesteps=209500, episode_reward=955.03 +/- 715.91
Episode length: 36.30 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=947.26 +/- 729.91
Episode length: 36.04 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 947      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=953.05 +/- 725.07
Episode length: 36.72 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 953      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 875      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 103      |
|    time_elapsed    | 753      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=871.36 +/- 757.45
Episode length: 34.64 +/- 6.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 871       |
| time/                   |           |
|    total_timesteps      | 211000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.01e-14 |
|    explained_variance   | 0.0127    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.13e+04  |
|    n_updates            | 2408      |
|    policy_gradient_loss | 7.65e-10  |
|    value_loss           | 3.77e+04  |
---------------------------------------
Eval num_timesteps=211500, episode_reward=842.12 +/- 626.65
Episode length: 36.12 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=757.15 +/- 608.97
Episode length: 35.04 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=816.37 +/- 706.63
Episode length: 34.98 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 868      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 104      |
|    time_elapsed    | 760      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=898.47 +/- 704.74
Episode length: 35.22 +/- 6.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 898       |
| time/                   |           |
|    total_timesteps      | 213000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.02e-16 |
|    explained_variance   | 0.00986   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.82e+04  |
|    n_updates            | 2418      |
|    policy_gradient_loss | 1.62e-10  |
|    value_loss           | 3.49e+04  |
---------------------------------------
Eval num_timesteps=213500, episode_reward=943.98 +/- 710.05
Episode length: 36.46 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=885.18 +/- 732.04
Episode length: 35.68 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=687.86 +/- 657.05
Episode length: 33.08 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 688      |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=826.54 +/- 681.53
Episode length: 34.82 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 105      |
|    time_elapsed    | 769      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=841.11 +/- 666.06
Episode length: 35.76 +/- 6.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 841       |
| time/                   |           |
|    total_timesteps      | 215500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.33e-14 |
|    explained_variance   | 0.00831   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.82e+04  |
|    n_updates            | 2428      |
|    policy_gradient_loss | -7.2e-10  |
|    value_loss           | 3.42e+04  |
---------------------------------------
Eval num_timesteps=216000, episode_reward=785.52 +/- 677.37
Episode length: 34.92 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=949.85 +/- 714.13
Episode length: 36.46 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=916.58 +/- 709.24
Episode length: 35.78 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 717      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 106      |
|    time_elapsed    | 776      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=786.49 +/- 650.48
Episode length: 34.40 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 786       |
| time/                   |           |
|    total_timesteps      | 217500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.66e-16 |
|    explained_variance   | 0.00891   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.09e+04  |
|    n_updates            | 2438      |
|    policy_gradient_loss | 1.11e-10  |
|    value_loss           | 3.53e+04  |
---------------------------------------
Eval num_timesteps=218000, episode_reward=798.02 +/- 625.96
Episode length: 35.40 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=707.62 +/- 556.84
Episode length: 35.34 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=905.47 +/- 690.24
Episode length: 35.84 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 107      |
|    time_elapsed    | 783      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=802.62 +/- 656.25
Episode length: 35.40 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 803       |
| time/                   |           |
|    total_timesteps      | 219500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.21e-14 |
|    explained_variance   | 0.00971   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.92e+04  |
|    n_updates            | 2448      |
|    policy_gradient_loss | 1.37e-09  |
|    value_loss           | 3.61e+04  |
---------------------------------------
Eval num_timesteps=220000, episode_reward=697.39 +/- 664.21
Episode length: 33.66 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 697      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=728.52 +/- 631.38
Episode length: 34.52 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=769.68 +/- 668.84
Episode length: 33.90 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 952      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 108      |
|    time_elapsed    | 790      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=778.10 +/- 698.49
Episode length: 34.00 +/- 7.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 778       |
| time/                   |           |
|    total_timesteps      | 221500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.01e-16 |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.44e+04  |
|    n_updates            | 2458      |
|    policy_gradient_loss | -4.42e-10 |
|    value_loss           | 4.52e+04  |
---------------------------------------
Eval num_timesteps=222000, episode_reward=820.06 +/- 718.79
Episode length: 34.70 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=824.47 +/- 657.27
Episode length: 35.84 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=659.96 +/- 568.31
Episode length: 33.96 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 660      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 109      |
|    time_elapsed    | 797      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=789.44 +/- 667.75
Episode length: 35.18 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 223500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.12e-14 |
|    explained_variance   | 0.00322   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.02e+04  |
|    n_updates            | 2468      |
|    policy_gradient_loss | 2.15e-10  |
|    value_loss           | 3.55e+04  |
---------------------------------------
Eval num_timesteps=224000, episode_reward=1069.44 +/- 779.58
Episode length: 36.26 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=765.35 +/- 667.82
Episode length: 34.38 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=827.73 +/- 692.39
Episode length: 34.96 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 110      |
|    time_elapsed    | 805      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=661.95 +/- 648.21
Episode length: 32.98 +/- 7.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33        |
|    mean_reward          | 662       |
| time/                   |           |
|    total_timesteps      | 225500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.15e-16 |
|    explained_variance   | 0.00824   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.83e+04  |
|    n_updates            | 2478      |
|    policy_gradient_loss | 1.57e-10  |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=226000, episode_reward=857.03 +/- 662.32
Episode length: 35.88 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=785.31 +/- 629.97
Episode length: 35.42 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=715.87 +/- 654.33
Episode length: 33.96 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 808      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 111      |
|    time_elapsed    | 812      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=783.31 +/- 611.05
Episode length: 35.26 +/- 5.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 783       |
| time/                   |           |
|    total_timesteps      | 227500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.17e-14 |
|    explained_variance   | 0.0105    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.48e+04  |
|    n_updates            | 2488      |
|    policy_gradient_loss | 1.89e-11  |
|    value_loss           | 3.86e+04  |
---------------------------------------
Eval num_timesteps=228000, episode_reward=1035.96 +/- 747.56
Episode length: 36.14 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=856.78 +/- 679.08
Episode length: 35.74 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=831.64 +/- 743.11
Episode length: 34.58 +/- 7.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 722      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 112      |
|    time_elapsed    | 819      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=723.83 +/- 586.95
Episode length: 35.12 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 724       |
| time/                   |           |
|    total_timesteps      | 229500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-16 |
|    explained_variance   | 0.00712   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.13e+04  |
|    n_updates            | 2498      |
|    policy_gradient_loss | -1.46e-10 |
|    value_loss           | 3.14e+04  |
---------------------------------------
Eval num_timesteps=230000, episode_reward=890.43 +/- 740.90
Episode length: 35.22 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=927.14 +/- 685.59
Episode length: 36.10 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=712.09 +/- 623.62
Episode length: 33.56 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 113      |
|    time_elapsed    | 826      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=768.36 +/- 653.08
Episode length: 34.74 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 768       |
| time/                   |           |
|    total_timesteps      | 231500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.1e-14  |
|    explained_variance   | 0.0123    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.76e+04  |
|    n_updates            | 2508      |
|    policy_gradient_loss | -4.37e-12 |
|    value_loss           | 3.58e+04  |
---------------------------------------
Eval num_timesteps=232000, episode_reward=966.68 +/- 680.83
Episode length: 37.12 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=855.32 +/- 670.88
Episode length: 35.56 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=974.37 +/- 702.49
Episode length: 36.70 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 114      |
|    time_elapsed    | 834      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=833.91 +/- 725.02
Episode length: 35.08 +/- 6.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12e-16 |
|    explained_variance   | 0.00976   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.25e+04  |
|    n_updates            | 2518      |
|    policy_gradient_loss | -1.4e-10  |
|    value_loss           | 3.68e+04  |
---------------------------------------
Eval num_timesteps=234000, episode_reward=694.13 +/- 592.57
Episode length: 34.48 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=790.16 +/- 646.89
Episode length: 35.10 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=971.42 +/- 722.74
Episode length: 36.20 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=803.32 +/- 658.81
Episode length: 35.52 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 115      |
|    time_elapsed    | 842      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=1038.14 +/- 738.92
Episode length: 37.04 +/- 5.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 1.04e+03  |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.46e-14 |
|    explained_variance   | 0.0103    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.04e+04  |
|    n_updates            | 2528      |
|    policy_gradient_loss | -3.22e-10 |
|    value_loss           | 3.51e+04  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=889.69 +/- 683.16
Episode length: 36.28 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=813.48 +/- 636.17
Episode length: 35.86 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=1062.90 +/- 749.13
Episode length: 37.10 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 701      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 116      |
|    time_elapsed    | 849      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=995.91 +/- 748.48
Episode length: 36.38 +/- 6.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 996       |
| time/                   |           |
|    total_timesteps      | 238000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.2e-17  |
|    explained_variance   | 0.00492   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.34e+04  |
|    n_updates            | 2538      |
|    policy_gradient_loss | -4.07e-10 |
|    value_loss           | 2.79e+04  |
---------------------------------------
Eval num_timesteps=238500, episode_reward=789.22 +/- 648.90
Episode length: 35.62 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=970.86 +/- 735.86
Episode length: 36.40 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=927.77 +/- 691.58
Episode length: 36.86 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 117      |
|    time_elapsed    | 857      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=944.91 +/- 721.42
Episode length: 36.20 +/- 6.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 945       |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.31e-14 |
|    explained_variance   | 0.00959   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.16e+04  |
|    n_updates            | 2548      |
|    policy_gradient_loss | -2.65e-10 |
|    value_loss           | 3.6e+04   |
---------------------------------------
Eval num_timesteps=240500, episode_reward=865.86 +/- 725.02
Episode length: 34.66 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=758.06 +/- 646.89
Episode length: 35.22 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=999.14 +/- 702.12
Episode length: 37.40 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 118      |
|    time_elapsed    | 864      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=743.53 +/- 613.71
Episode length: 35.48 +/- 6.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 744       |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.5e-17  |
|    explained_variance   | 0.0101    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.64e+04  |
|    n_updates            | 2558      |
|    policy_gradient_loss | -1.46e-11 |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=242500, episode_reward=902.32 +/- 701.71
Episode length: 36.36 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=846.49 +/- 697.12
Episode length: 36.02 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=704.16 +/- 639.66
Episode length: 34.36 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 754      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 119      |
|    time_elapsed    | 871      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=812.06 +/- 668.51
Episode length: 35.24 +/- 6.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 812       |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.26e-14 |
|    explained_variance   | -0.00147  |
|    learning_rate        | 0.0001    |
|    loss                 | 2.22e+04  |
|    n_updates            | 2568      |
|    policy_gradient_loss | 2.44e-09  |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=244500, episode_reward=714.51 +/- 608.89
Episode length: 34.94 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=705.38 +/- 633.86
Episode length: 33.68 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=880.32 +/- 701.15
Episode length: 35.92 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 120      |
|    time_elapsed    | 878      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=948.99 +/- 771.97
Episode length: 35.70 +/- 7.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 949       |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.3e-17  |
|    explained_variance   | 0.00903   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.56e+04  |
|    n_updates            | 2578      |
|    policy_gradient_loss | -8.19e-10 |
|    value_loss           | 3.55e+04  |
---------------------------------------
Eval num_timesteps=246500, episode_reward=882.54 +/- 726.24
Episode length: 35.30 +/- 7.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=896.69 +/- 703.41
Episode length: 35.78 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=922.18 +/- 715.38
Episode length: 36.10 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 751      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 121      |
|    time_elapsed    | 886      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=630.03 +/- 510.93
Episode length: 34.48 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 630       |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.41e-14 |
|    explained_variance   | 0.00639   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.36e+04  |
|    n_updates            | 2588      |
|    policy_gradient_loss | -5.59e-10 |
|    value_loss           | 3.41e+04  |
---------------------------------------
Eval num_timesteps=248500, episode_reward=720.62 +/- 603.53
Episode length: 34.54 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=793.02 +/- 630.68
Episode length: 35.84 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=758.33 +/- 667.73
Episode length: 34.12 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 122      |
|    time_elapsed    | 893      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=873.99 +/- 666.72
Episode length: 36.48 +/- 5.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 874       |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.08e-17 |
|    explained_variance   | 0.00882   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.23e+04  |
|    n_updates            | 2598      |
|    policy_gradient_loss | -5.37e-10 |
|    value_loss           | 3.61e+04  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=812.46 +/- 660.50
Episode length: 35.40 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=777.69 +/- 709.03
Episode length: 33.88 +/- 7.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=880.12 +/- 683.70
Episode length: 35.70 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 887      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 123      |
|    time_elapsed    | 900      |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=750.25 +/- 627.76
Episode length: 35.08 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 750       |
| time/                   |           |
|    total_timesteps      | 252000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.65e-22 |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.03e+04  |
|    n_updates            | 2608      |
|    policy_gradient_loss | -4.07e-11 |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=252500, episode_reward=786.85 +/- 669.90
Episode length: 35.02 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=794.86 +/- 713.16
Episode length: 34.08 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=891.51 +/- 734.51
Episode length: 35.12 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 847      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 124      |
|    time_elapsed    | 907      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=988.40 +/- 704.94
Episode length: 37.00 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 988       |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.12e-19 |
|    explained_variance   | 0.00896   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.44e+04  |
|    n_updates            | 2618      |
|    policy_gradient_loss | -1.62e-09 |
|    value_loss           | 4.6e+04   |
---------------------------------------
Eval num_timesteps=254500, episode_reward=916.24 +/- 735.86
Episode length: 35.06 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=985.87 +/- 718.14
Episode length: 36.82 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=768.25 +/- 679.73
Episode length: 34.28 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=899.58 +/- 746.00
Episode length: 35.36 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 900      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 125      |
|    time_elapsed    | 916      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=1022.71 +/- 708.59
Episode length: 37.06 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.1      |
|    mean_reward          | 1.02e+03  |
| time/                   |           |
|    total_timesteps      | 256500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-13 |
|    explained_variance   | 0.00408   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.94e+04  |
|    n_updates            | 2628      |
|    policy_gradient_loss | 1.11e-10  |
|    value_loss           | 3.43e+04  |
---------------------------------------
Eval num_timesteps=257000, episode_reward=840.63 +/- 704.00
Episode length: 34.88 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=893.48 +/- 674.15
Episode length: 35.84 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=937.90 +/- 725.50
Episode length: 35.76 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 126      |
|    time_elapsed    | 923      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=873.66 +/- 668.51
Episode length: 35.58 +/- 5.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 874       |
| time/                   |           |
|    total_timesteps      | 258500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.01e-16 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.79e+04  |
|    n_updates            | 2638      |
|    policy_gradient_loss | -5.7e-10  |
|    value_loss           | 3.79e+04  |
---------------------------------------
Eval num_timesteps=259000, episode_reward=719.52 +/- 608.78
Episode length: 34.68 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=908.18 +/- 726.93
Episode length: 35.94 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=783.51 +/- 663.99
Episode length: 34.48 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 864      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 127      |
|    time_elapsed    | 930      |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=752.76 +/- 694.17
Episode length: 33.90 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 753       |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.29e-21 |
|    explained_variance   | 0.0155    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.4e+04   |
|    n_updates            | 2648      |
|    policy_gradient_loss | -1.09e-10 |
|    value_loss           | 3.54e+04  |
---------------------------------------
Eval num_timesteps=261000, episode_reward=828.32 +/- 636.93
Episode length: 35.92 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=672.60 +/- 645.91
Episode length: 33.18 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=754.52 +/- 669.43
Episode length: 34.30 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 856      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 128      |
|    time_elapsed    | 937      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=837.69 +/- 734.40
Episode length: 34.84 +/- 6.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 262500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.45e-19 |
|    explained_variance   | 0.0103    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.46e+04  |
|    n_updates            | 2658      |
|    policy_gradient_loss | -8.98e-10 |
|    value_loss           | 4.43e+04  |
---------------------------------------
Eval num_timesteps=263000, episode_reward=915.61 +/- 723.02
Episode length: 35.80 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=893.03 +/- 718.10
Episode length: 35.68 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=859.40 +/- 748.74
Episode length: 34.86 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 801      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 129      |
|    time_elapsed    | 945      |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=861.25 +/- 686.69
Episode length: 35.46 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 861       |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-13 |
|    explained_variance   | 0.0116    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.04e+04  |
|    n_updates            | 2668      |
|    policy_gradient_loss | 3.49e-10  |
|    value_loss           | 3.67e+04  |
---------------------------------------
Eval num_timesteps=265000, episode_reward=850.46 +/- 722.20
Episode length: 35.06 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=862.15 +/- 695.43
Episode length: 35.62 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=962.32 +/- 698.15
Episode length: 36.50 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 913      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 130      |
|    time_elapsed    | 952      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=746.48 +/- 651.86
Episode length: 34.22 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 746       |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.17e-16 |
|    explained_variance   | 0.00979   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.6e+04   |
|    n_updates            | 2678      |
|    policy_gradient_loss | -1.16e-11 |
|    value_loss           | 4.22e+04  |
---------------------------------------
Eval num_timesteps=267000, episode_reward=632.33 +/- 505.97
Episode length: 34.08 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 632      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=1077.15 +/- 708.61
Episode length: 38.04 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38       |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=980.47 +/- 719.62
Episode length: 36.62 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 131      |
|    time_elapsed    | 959      |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=885.52 +/- 614.88
Episode length: 37.40 +/- 4.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.4      |
|    mean_reward          | 886       |
| time/                   |           |
|    total_timesteps      | 268500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.02e-21 |
|    explained_variance   | 0.00477   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.74e+04  |
|    n_updates            | 2688      |
|    policy_gradient_loss | -1.1e-09  |
|    value_loss           | 3.59e+04  |
---------------------------------------
Eval num_timesteps=269000, episode_reward=873.31 +/- 708.65
Episode length: 35.18 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=858.12 +/- 690.16
Episode length: 35.38 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=685.94 +/- 556.78
Episode length: 34.80 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 713      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 132      |
|    time_elapsed    | 967      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=848.88 +/- 748.62
Episode length: 34.46 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 849       |
| time/                   |           |
|    total_timesteps      | 270500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.09e-17 |
|    explained_variance   | -0.00205  |
|    learning_rate        | 0.0001    |
|    loss                 | 1.71e+04  |
|    n_updates            | 2698      |
|    policy_gradient_loss | -1.35e-09 |
|    value_loss           | 4.28e+04  |
---------------------------------------
Eval num_timesteps=271000, episode_reward=837.77 +/- 699.67
Episode length: 35.08 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=791.41 +/- 646.33
Episode length: 34.70 +/- 8.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=819.05 +/- 682.38
Episode length: 34.96 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 815      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 133      |
|    time_elapsed    | 974      |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=918.87 +/- 708.24
Episode length: 36.26 +/- 6.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 919       |
| time/                   |           |
|    total_timesteps      | 272500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.35e-19 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.32e+04  |
|    n_updates            | 2708      |
|    policy_gradient_loss | -6.37e-10 |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=273000, episode_reward=923.18 +/- 701.29
Episode length: 36.42 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=882.73 +/- 663.49
Episode length: 36.40 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=651.81 +/- 523.26
Episode length: 35.02 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 825      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 134      |
|    time_elapsed    | 981      |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=669.34 +/- 555.26
Episode length: 34.52 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 669       |
| time/                   |           |
|    total_timesteps      | 274500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.84e-17 |
|    explained_variance   | 0.00837   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.78e+04  |
|    n_updates            | 2718      |
|    policy_gradient_loss | 5.72e-10  |
|    value_loss           | 4.31e+04  |
---------------------------------------
Eval num_timesteps=275000, episode_reward=838.54 +/- 666.99
Episode length: 35.36 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=655.77 +/- 550.46
Episode length: 34.48 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=903.47 +/- 691.69
Episode length: 35.76 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 135      |
|    time_elapsed    | 988      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=936.56 +/- 779.37
Episode length: 35.24 +/- 7.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 937       |
| time/                   |           |
|    total_timesteps      | 276500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.37e-19 |
|    explained_variance   | 0.0128    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.43e+04  |
|    n_updates            | 2728      |
|    policy_gradient_loss | 5.5e-10   |
|    value_loss           | 4.05e+04  |
---------------------------------------
Eval num_timesteps=277000, episode_reward=940.74 +/- 714.86
Episode length: 36.08 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=689.99 +/- 600.76
Episode length: 33.82 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=892.60 +/- 743.91
Episode length: 34.86 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=998.53 +/- 699.40
Episode length: 36.88 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 136      |
|    time_elapsed    | 997      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=844.04 +/- 666.39
Episode length: 34.92 +/- 6.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 844       |
| time/                   |           |
|    total_timesteps      | 279000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.86e-16 |
|    explained_variance   | 0.00443   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.37e+04  |
|    n_updates            | 2738      |
|    policy_gradient_loss | 1.55e-09  |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=279500, episode_reward=984.91 +/- 741.46
Episode length: 35.90 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 985      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=842.91 +/- 672.43
Episode length: 35.60 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=726.84 +/- 581.07
Episode length: 35.08 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 728      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 137      |
|    time_elapsed    | 1004     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=765.17 +/- 638.15
Episode length: 34.80 +/- 6.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 765       |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.9e-18  |
|    explained_variance   | 0.00416   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.01e+04  |
|    n_updates            | 2748      |
|    policy_gradient_loss | -4.45e-10 |
|    value_loss           | 3.33e+04  |
---------------------------------------
Eval num_timesteps=281500, episode_reward=853.67 +/- 702.45
Episode length: 35.24 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=758.38 +/- 642.19
Episode length: 34.28 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=906.72 +/- 717.71
Episode length: 36.14 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 717      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 138      |
|    time_elapsed    | 1011     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=877.91 +/- 704.16
Episode length: 35.16 +/- 6.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 878       |
| time/                   |           |
|    total_timesteps      | 283000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-15 |
|    explained_variance   | 0.0091    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.43e+04  |
|    n_updates            | 2758      |
|    policy_gradient_loss | -3.78e-11 |
|    value_loss           | 4.06e+04  |
---------------------------------------
Eval num_timesteps=283500, episode_reward=803.28 +/- 647.52
Episode length: 35.70 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=834.51 +/- 735.88
Episode length: 34.10 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=670.37 +/- 561.66
Episode length: 34.10 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 898      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 139      |
|    time_elapsed    | 1019     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=738.88 +/- 693.15
Episode length: 33.74 +/- 7.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 739       |
| time/                   |           |
|    total_timesteps      | 285000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.45e-18 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.64e+04  |
|    n_updates            | 2768      |
|    policy_gradient_loss | -5.03e-10 |
|    value_loss           | 4.43e+04  |
---------------------------------------
Eval num_timesteps=285500, episode_reward=834.52 +/- 665.42
Episode length: 35.32 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=771.94 +/- 660.84
Episode length: 34.84 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=811.33 +/- 662.56
Episode length: 35.32 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 140      |
|    time_elapsed    | 1026     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=851.61 +/- 709.50
Episode length: 35.36 +/- 7.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 852       |
| time/                   |           |
|    total_timesteps      | 287000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.94e-16 |
|    explained_variance   | -0.00213  |
|    learning_rate        | 0.0001    |
|    loss                 | 1.41e+04  |
|    n_updates            | 2778      |
|    policy_gradient_loss | 4.87e-10  |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=287500, episode_reward=993.68 +/- 745.71
Episode length: 36.50 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=786.32 +/- 681.94
Episode length: 34.28 +/- 7.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=814.57 +/- 712.47
Episode length: 34.70 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 141      |
|    time_elapsed    | 1033     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=828.51 +/- 668.14
Episode length: 35.32 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 829       |
| time/                   |           |
|    total_timesteps      | 289000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.26e-18 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.56e+04  |
|    n_updates            | 2788      |
|    policy_gradient_loss | 1.07e-09  |
|    value_loss           | 3.73e+04  |
---------------------------------------
Eval num_timesteps=289500, episode_reward=617.89 +/- 594.36
Episode length: 32.84 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=847.13 +/- 686.31
Episode length: 35.58 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=832.30 +/- 683.56
Episode length: 35.56 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 142      |
|    time_elapsed    | 1040     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=663.97 +/- 609.30
Episode length: 33.10 +/- 6.66
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 33.1     |
|    mean_reward          | 664      |
| time/                   |          |
|    total_timesteps      | 291000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6.9e-24 |
|    explained_variance   | 0.0118   |
|    learning_rate        | 0.0001   |
|    loss                 | 2.73e+04 |
|    n_updates            | 2798     |
|    policy_gradient_loss | 5.2e-10  |
|    value_loss           | 4.14e+04 |
--------------------------------------
Eval num_timesteps=291500, episode_reward=715.66 +/- 581.84
Episode length: 35.06 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=788.57 +/- 659.63
Episode length: 35.22 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=804.56 +/- 649.46
Episode length: 35.72 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 908      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 143      |
|    time_elapsed    | 1048     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=776.98 +/- 636.66
Episode length: 35.06 +/- 6.20
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.1     |
|    mean_reward          | 777      |
| time/                   |          |
|    total_timesteps      | 293000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.8e-21 |
|    explained_variance   | 0.014    |
|    learning_rate        | 0.0001   |
|    loss                 | 2.91e+04 |
|    n_updates            | 2808     |
|    policy_gradient_loss | 2.03e-09 |
|    value_loss           | 5.2e+04  |
--------------------------------------
Eval num_timesteps=293500, episode_reward=726.43 +/- 644.14
Episode length: 34.46 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=802.67 +/- 676.43
Episode length: 35.20 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=957.56 +/- 666.15
Episode length: 37.12 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 958      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 892      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 144      |
|    time_elapsed    | 1055     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=876.81 +/- 679.26
Episode length: 35.90 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 877       |
| time/                   |           |
|    total_timesteps      | 295000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-15 |
|    explained_variance   | 0.0145    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.82e+04  |
|    n_updates            | 2818      |
|    policy_gradient_loss | -2.91e-12 |
|    value_loss           | 4.12e+04  |
---------------------------------------
Eval num_timesteps=295500, episode_reward=764.98 +/- 622.90
Episode length: 34.96 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=881.64 +/- 664.48
Episode length: 36.34 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=964.59 +/- 731.77
Episode length: 35.66 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 737      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 145      |
|    time_elapsed    | 1062     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=808.78 +/- 628.22
Episode length: 35.18 +/- 5.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 809       |
| time/                   |           |
|    total_timesteps      | 297000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.93e-11 |
|    explained_variance   | 0.00476   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.24e+04  |
|    n_updates            | 2828      |
|    policy_gradient_loss | 2.21e-10  |
|    value_loss           | 2.89e+04  |
---------------------------------------
Eval num_timesteps=297500, episode_reward=929.25 +/- 682.77
Episode length: 36.86 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=787.09 +/- 627.28
Episode length: 35.40 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=967.61 +/- 746.64
Episode length: 35.92 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=854.86 +/- 669.92
Episode length: 35.80 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 742      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 146      |
|    time_elapsed    | 1071     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=928.73 +/- 681.82
Episode length: 36.78 +/- 5.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 929       |
| time/                   |           |
|    total_timesteps      | 299500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.57e-13 |
|    explained_variance   | 0.00898   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.35e+04  |
|    n_updates            | 2838      |
|    policy_gradient_loss | -1.47e-09 |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=300000, episode_reward=925.86 +/- 754.81
Episode length: 35.66 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=834.71 +/- 715.95
Episode length: 34.92 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=848.03 +/- 703.35
Episode length: 35.28 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 921      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 147      |
|    time_elapsed    | 1078     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=887.58 +/- 686.20
Episode length: 35.84 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 888       |
| time/                   |           |
|    total_timesteps      | 301500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.31e-18 |
|    explained_variance   | 0.0146    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.73e+04  |
|    n_updates            | 2848      |
|    policy_gradient_loss | -6.69e-11 |
|    value_loss           | 4.09e+04  |
---------------------------------------
Eval num_timesteps=302000, episode_reward=581.32 +/- 506.72
Episode length: 33.44 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 581      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=854.75 +/- 691.53
Episode length: 35.70 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=711.31 +/- 603.86
Episode length: 34.34 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 906      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 148      |
|    time_elapsed    | 1085     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=844.15 +/- 708.11
Episode length: 34.90 +/- 7.61
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.9     |
|    mean_reward          | 844      |
| time/                   |          |
|    total_timesteps      | 303500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.4e-15 |
|    explained_variance   | 0.0111   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.56e+04 |
|    n_updates            | 2858     |
|    policy_gradient_loss | 6.69e-11 |
|    value_loss           | 3.93e+04 |
--------------------------------------
Eval num_timesteps=304000, episode_reward=828.13 +/- 639.31
Episode length: 36.16 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=715.10 +/- 608.13
Episode length: 34.10 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=864.47 +/- 640.51
Episode length: 36.00 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 890      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 149      |
|    time_elapsed    | 1092     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=761.23 +/- 652.89
Episode length: 34.58 +/- 7.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 761       |
| time/                   |           |
|    total_timesteps      | 305500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.34e-18 |
|    explained_variance   | 0.0138    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.5e+04   |
|    n_updates            | 2868      |
|    policy_gradient_loss | 2.12e-10  |
|    value_loss           | 4.13e+04  |
---------------------------------------
Eval num_timesteps=306000, episode_reward=995.42 +/- 763.51
Episode length: 36.02 +/- 7.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 995      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=743.92 +/- 685.56
Episode length: 33.96 +/- 8.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=813.76 +/- 635.40
Episode length: 36.02 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 150      |
|    time_elapsed    | 1100     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=696.76 +/- 603.03
Episode length: 34.58 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 697       |
| time/                   |           |
|    total_timesteps      | 307500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.38e-15 |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.54e+04  |
|    n_updates            | 2878      |
|    policy_gradient_loss | -7.63e-10 |
|    value_loss           | 4.2e+04   |
---------------------------------------
Eval num_timesteps=308000, episode_reward=735.51 +/- 620.06
Episode length: 34.70 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=805.36 +/- 673.94
Episode length: 34.62 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=929.70 +/- 695.60
Episode length: 36.48 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 930      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 880      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 151      |
|    time_elapsed    | 1107     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=945.05 +/- 713.21
Episode length: 36.24 +/- 6.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 945       |
| time/                   |           |
|    total_timesteps      | 309500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.42e-18 |
|    explained_variance   | 0.0134    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.04e+04  |
|    n_updates            | 2888      |
|    policy_gradient_loss | -2.36e-10 |
|    value_loss           | 4.05e+04  |
---------------------------------------
Eval num_timesteps=310000, episode_reward=735.38 +/- 636.39
Episode length: 34.06 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=789.87 +/- 683.51
Episode length: 34.48 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=777.30 +/- 563.89
Episode length: 36.74 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 152      |
|    time_elapsed    | 1114     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=741.22 +/- 663.50
Episode length: 34.08 +/- 7.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 741       |
| time/                   |           |
|    total_timesteps      | 311500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.48e-15 |
|    explained_variance   | 0.00695   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.52e+04  |
|    n_updates            | 2898      |
|    policy_gradient_loss | -2.21e-10 |
|    value_loss           | 3.65e+04  |
---------------------------------------
Eval num_timesteps=312000, episode_reward=788.27 +/- 713.04
Episode length: 34.40 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=757.64 +/- 618.35
Episode length: 35.16 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=723.05 +/- 618.74
Episode length: 34.26 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 153      |
|    time_elapsed    | 1121     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=725.64 +/- 614.68
Episode length: 35.02 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 726       |
| time/                   |           |
|    total_timesteps      | 313500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.19e-11 |
|    explained_variance   | 0.00904   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.73e+04  |
|    n_updates            | 2908      |
|    policy_gradient_loss | -3.32e-10 |
|    value_loss           | 3.63e+04  |
---------------------------------------
Eval num_timesteps=314000, episode_reward=1022.34 +/- 702.94
Episode length: 37.14 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=773.00 +/- 675.53
Episode length: 34.24 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=706.00 +/- 592.92
Episode length: 34.66 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 154      |
|    time_elapsed    | 1129     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=1003.63 +/- 746.92
Episode length: 36.30 +/- 6.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 315500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.79e-13 |
|    explained_variance   | 0.00774   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.38e+04  |
|    n_updates            | 2918      |
|    policy_gradient_loss | 4.92e-10  |
|    value_loss           | 3.75e+04  |
---------------------------------------
Eval num_timesteps=316000, episode_reward=865.47 +/- 668.32
Episode length: 35.92 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=1037.70 +/- 747.52
Episode length: 36.30 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=980.55 +/- 685.09
Episode length: 36.86 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 981      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 861      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 155      |
|    time_elapsed    | 1136     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=794.69 +/- 622.31
Episode length: 35.16 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 795       |
| time/                   |           |
|    total_timesteps      | 317500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.57e-18 |
|    explained_variance   | 0.015     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.78e+04  |
|    n_updates            | 2928      |
|    policy_gradient_loss | -6.89e-10 |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=318000, episode_reward=1048.41 +/- 748.72
Episode length: 37.08 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=752.54 +/- 601.40
Episode length: 35.02 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=977.86 +/- 722.27
Episode length: 36.62 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 978      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 726      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 156      |
|    time_elapsed    | 1143     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=962.71 +/- 749.75
Episode length: 36.48 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 963       |
| time/                   |           |
|    total_timesteps      | 319500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.59e-15 |
|    explained_variance   | 0.00805   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.47e+04  |
|    n_updates            | 2938      |
|    policy_gradient_loss | -4.54e-10 |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=835.64 +/- 655.66
Episode length: 35.36 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=754.10 +/- 609.32
Episode length: 35.40 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=944.19 +/- 713.16
Episode length: 36.48 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=816.61 +/- 716.65
Episode length: 34.42 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 157      |
|    time_elapsed    | 1152     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=814.45 +/- 613.60
Episode length: 36.00 +/- 5.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 814       |
| time/                   |           |
|    total_timesteps      | 322000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.13e-11 |
|    explained_variance   | 0.00878   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.77e+04  |
|    n_updates            | 2948      |
|    policy_gradient_loss | 1.46e-12  |
|    value_loss           | 3.24e+04  |
---------------------------------------
Eval num_timesteps=322500, episode_reward=742.34 +/- 619.60
Episode length: 34.66 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=931.49 +/- 685.16
Episode length: 36.48 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=967.13 +/- 714.83
Episode length: 36.96 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 835      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 158      |
|    time_elapsed    | 1159     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=658.18 +/- 562.74
Episode length: 34.30 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 658       |
| time/                   |           |
|    total_timesteps      | 324000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.83e-13 |
|    explained_variance   | 0.00753   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.98e+04  |
|    n_updates            | 2958      |
|    policy_gradient_loss | -1.63e-10 |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=324500, episode_reward=900.53 +/- 699.00
Episode length: 35.80 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=885.21 +/- 764.33
Episode length: 35.42 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=803.83 +/- 643.18
Episode length: 35.92 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 159      |
|    time_elapsed    | 1167     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=867.62 +/- 715.89
Episode length: 35.36 +/- 7.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 868       |
| time/                   |           |
|    total_timesteps      | 326000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.89e-11 |
|    explained_variance   | 0.00373   |
|    learning_rate        | 0.0001    |
|    loss                 | 8.76e+03  |
|    n_updates            | 2968      |
|    policy_gradient_loss | -3.06e-10 |
|    value_loss           | 3.06e+04  |
---------------------------------------
Eval num_timesteps=326500, episode_reward=835.71 +/- 694.78
Episode length: 35.34 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=810.87 +/- 689.60
Episode length: 34.34 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=808.27 +/- 711.62
Episode length: 34.18 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 731      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 160      |
|    time_elapsed    | 1174     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=979.46 +/- 703.33
Episode length: 36.74 +/- 5.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 979       |
| time/                   |           |
|    total_timesteps      | 328000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.68e-13 |
|    explained_variance   | 0.0127    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.7e+04   |
|    n_updates            | 2978      |
|    policy_gradient_loss | 1.78e-10  |
|    value_loss           | 4.16e+04  |
---------------------------------------
Eval num_timesteps=328500, episode_reward=868.43 +/- 649.55
Episode length: 36.42 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=666.97 +/- 561.53
Episode length: 34.48 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 667      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=837.28 +/- 692.21
Episode length: 35.36 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 161      |
|    time_elapsed    | 1181     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=779.50 +/- 745.64
Episode length: 34.18 +/- 8.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 780       |
| time/                   |           |
|    total_timesteps      | 330000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.84e-11 |
|    explained_variance   | 0.00597   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.73e+04  |
|    n_updates            | 2988      |
|    policy_gradient_loss | 4.47e-10  |
|    value_loss           | 3.39e+04  |
---------------------------------------
Eval num_timesteps=330500, episode_reward=1006.33 +/- 720.18
Episode length: 37.18 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=889.41 +/- 702.49
Episode length: 35.66 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=920.49 +/- 694.16
Episode length: 36.54 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 870      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 162      |
|    time_elapsed    | 1189     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=874.35 +/- 693.13
Episode length: 35.92 +/- 6.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 874       |
| time/                   |           |
|    total_timesteps      | 332000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.57e-13 |
|    explained_variance   | 0.0132    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.56e+04  |
|    n_updates            | 2998      |
|    policy_gradient_loss | -4.28e-10 |
|    value_loss           | 4.47e+04  |
---------------------------------------
Eval num_timesteps=332500, episode_reward=973.08 +/- 713.32
Episode length: 36.12 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=916.39 +/- 751.10
Episode length: 35.62 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=899.61 +/- 743.54
Episode length: 35.52 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 900      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 163      |
|    time_elapsed    | 1196     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=936.01 +/- 705.24
Episode length: 36.84 +/- 6.03
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.8     |
|    mean_reward          | 936      |
| time/                   |          |
|    total_timesteps      | 334000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.4e-11 |
|    explained_variance   | 0.00739  |
|    learning_rate        | 0.0001   |
|    loss                 | 2.02e+04 |
|    n_updates            | 3008     |
|    policy_gradient_loss | -1.5e-09 |
|    value_loss           | 3.47e+04 |
--------------------------------------
Eval num_timesteps=334500, episode_reward=724.53 +/- 622.06
Episode length: 34.74 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=875.92 +/- 678.01
Episode length: 36.02 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=952.22 +/- 764.17
Episode length: 35.92 +/- 7.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 887      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 164      |
|    time_elapsed    | 1203     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=797.68 +/- 642.62
Episode length: 35.54 +/- 6.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 798       |
| time/                   |           |
|    total_timesteps      | 336000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.53e-13 |
|    explained_variance   | 0.01      |
|    learning_rate        | 0.0001    |
|    loss                 | 2.61e+04  |
|    n_updates            | 3018      |
|    policy_gradient_loss | -2.76e-11 |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=336500, episode_reward=951.14 +/- 743.31
Episode length: 36.22 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=819.86 +/- 591.95
Episode length: 36.82 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=879.69 +/- 715.94
Episode length: 35.22 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 165      |
|    time_elapsed    | 1211     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=767.44 +/- 592.98
Episode length: 35.26 +/- 5.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 767       |
| time/                   |           |
|    total_timesteps      | 338000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.94e-11 |
|    explained_variance   | 0.0072    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.52e+04  |
|    n_updates            | 3028      |
|    policy_gradient_loss | 2.11e-10  |
|    value_loss           | 3.36e+04  |
---------------------------------------
Eval num_timesteps=338500, episode_reward=854.81 +/- 653.26
Episode length: 35.96 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=799.06 +/- 662.96
Episode length: 35.02 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=746.64 +/- 680.70
Episode length: 33.92 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 166      |
|    time_elapsed    | 1218     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=1004.24 +/- 733.51
Episode length: 36.68 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 340000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-13 |
|    explained_variance   | 0.0113    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.71e+04  |
|    n_updates            | 3038      |
|    policy_gradient_loss | 8.93e-10  |
|    value_loss           | 4.16e+04  |
---------------------------------------
Eval num_timesteps=340500, episode_reward=863.20 +/- 714.98
Episode length: 35.42 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=747.81 +/- 613.29
Episode length: 34.86 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=820.71 +/- 680.79
Episode length: 35.16 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=796.57 +/- 714.89
Episode length: 34.52 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 880      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 167      |
|    time_elapsed    | 1226     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=862.51 +/- 690.96
Episode length: 35.16 +/- 7.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 863       |
| time/                   |           |
|    total_timesteps      | 342500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.46e-18 |
|    explained_variance   | 0.0121    |
|    learning_rate        | 0.0001    |
|    loss                 | 9.74e+03  |
|    n_updates            | 3048      |
|    policy_gradient_loss | 1.18e-09  |
|    value_loss           | 3.63e+04  |
---------------------------------------
Eval num_timesteps=343000, episode_reward=807.53 +/- 681.41
Episode length: 34.76 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=769.03 +/- 684.97
Episode length: 34.46 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=795.85 +/- 580.09
Episode length: 36.28 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 168      |
|    time_elapsed    | 1234     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=765.48 +/- 662.31
Episode length: 34.62 +/- 6.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 765       |
| time/                   |           |
|    total_timesteps      | 344500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.46e-16 |
|    explained_variance   | 0.0098    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.74e+04  |
|    n_updates            | 3058      |
|    policy_gradient_loss | -6.64e-10 |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=345000, episode_reward=925.54 +/- 751.21
Episode length: 35.48 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=810.79 +/- 672.11
Episode length: 35.48 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=838.63 +/- 647.33
Episode length: 36.12 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 743      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 169      |
|    time_elapsed    | 1241     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=730.62 +/- 623.26
Episode length: 34.88 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 731       |
| time/                   |           |
|    total_timesteps      | 346500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.98e-11 |
|    explained_variance   | 0.00211   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.02e+04  |
|    n_updates            | 3068      |
|    policy_gradient_loss | -1.5e-10  |
|    value_loss           | 2.91e+04  |
---------------------------------------
Eval num_timesteps=347000, episode_reward=1020.85 +/- 779.29
Episode length: 36.14 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=653.25 +/- 542.76
Episode length: 34.28 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 653      |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=715.49 +/- 590.75
Episode length: 35.02 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 750      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 170      |
|    time_elapsed    | 1248     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=821.56 +/- 641.30
Episode length: 35.70 +/- 6.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 822       |
| time/                   |           |
|    total_timesteps      | 348500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12e-13 |
|    explained_variance   | 0.00947   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.43e+04  |
|    n_updates            | 3078      |
|    policy_gradient_loss | 1.18e-09  |
|    value_loss           | 3.69e+04  |
---------------------------------------
Eval num_timesteps=349000, episode_reward=928.10 +/- 697.65
Episode length: 35.84 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=753.55 +/- 660.06
Episode length: 34.46 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=722.39 +/- 671.16
Episode length: 34.04 +/- 7.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 838      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 171      |
|    time_elapsed    | 1255     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=685.15 +/- 573.93
Episode length: 34.46 +/- 5.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 685       |
| time/                   |           |
|    total_timesteps      | 350500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.5e-18  |
|    explained_variance   | 0.0121    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.19e+04  |
|    n_updates            | 3088      |
|    policy_gradient_loss | -5.18e-10 |
|    value_loss           | 3.84e+04  |
---------------------------------------
Eval num_timesteps=351000, episode_reward=894.34 +/- 676.02
Episode length: 36.66 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=692.54 +/- 579.56
Episode length: 34.42 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=760.87 +/- 687.16
Episode length: 34.38 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 915      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 172      |
|    time_elapsed    | 1263     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=884.38 +/- 731.44
Episode length: 35.58 +/- 6.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 884       |
| time/                   |           |
|    total_timesteps      | 352500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.55e-16 |
|    explained_variance   | 0.0151    |
|    learning_rate        | 0.0001    |
|    loss                 | 2e+04     |
|    n_updates            | 3098      |
|    policy_gradient_loss | 2.44e-10  |
|    value_loss           | 4.4e+04   |
---------------------------------------
Eval num_timesteps=353000, episode_reward=568.99 +/- 532.76
Episode length: 32.64 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 569      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=829.53 +/- 744.33
Episode length: 33.90 +/- 8.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=853.07 +/- 653.33
Episode length: 36.20 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 896      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 173      |
|    time_elapsed    | 1270     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=991.81 +/- 736.92
Episode length: 35.88 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 992       |
| time/                   |           |
|    total_timesteps      | 354500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.68e-18 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.8e+04   |
|    n_updates            | 3108      |
|    policy_gradient_loss | 5.65e-10  |
|    value_loss           | 4.04e+04  |
---------------------------------------
Eval num_timesteps=355000, episode_reward=802.44 +/- 702.45
Episode length: 34.40 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=999.01 +/- 706.22
Episode length: 36.70 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=868.16 +/- 706.31
Episode length: 35.02 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 174      |
|    time_elapsed    | 1277     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=825.87 +/- 649.74
Episode length: 35.44 +/- 6.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 356500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.08e-16 |
|    explained_variance   | 0.00998   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.25e+04  |
|    n_updates            | 3118      |
|    policy_gradient_loss | 5.09e-11  |
|    value_loss           | 3.77e+04  |
---------------------------------------
Eval num_timesteps=357000, episode_reward=856.18 +/- 658.84
Episode length: 36.06 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=882.73 +/- 685.57
Episode length: 35.16 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=901.80 +/- 706.93
Episode length: 35.68 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 175      |
|    time_elapsed    | 1284     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=958.25 +/- 753.59
Episode length: 35.80 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 958       |
| time/                   |           |
|    total_timesteps      | 358500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.42e-18 |
|    explained_variance   | 0.0132    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.36e+04  |
|    n_updates            | 3128      |
|    policy_gradient_loss | 7.67e-10  |
|    value_loss           | 3.52e+04  |
---------------------------------------
Eval num_timesteps=359000, episode_reward=721.29 +/- 604.84
Episode length: 35.14 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=777.96 +/- 697.05
Episode length: 34.48 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=684.03 +/- 607.13
Episode length: 33.66 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 851      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 176      |
|    time_elapsed    | 1292     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=943.50 +/- 719.11
Episode length: 36.22 +/- 5.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 944       |
| time/                   |           |
|    total_timesteps      | 360500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.13e-16 |
|    explained_variance   | 0.0163    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.22e+04  |
|    n_updates            | 3138      |
|    policy_gradient_loss | 9.07e-10  |
|    value_loss           | 4.45e+04  |
---------------------------------------
Eval num_timesteps=361000, episode_reward=850.32 +/- 688.68
Episode length: 35.96 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=725.50 +/- 644.79
Episode length: 34.08 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=802.91 +/- 635.92
Episode length: 35.74 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 177      |
|    time_elapsed    | 1299     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=672.69 +/- 612.18
Episode length: 33.90 +/- 6.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 673       |
| time/                   |           |
|    total_timesteps      | 362500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.7e-18  |
|    explained_variance   | 0.0154    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.69e+04  |
|    n_updates            | 3148      |
|    policy_gradient_loss | -7.71e-10 |
|    value_loss           | 4.02e+04  |
---------------------------------------
Eval num_timesteps=363000, episode_reward=936.09 +/- 662.21
Episode length: 36.84 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=770.98 +/- 677.54
Episode length: 34.44 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=887.61 +/- 685.54
Episode length: 35.62 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=555.88 +/- 522.70
Episode length: 32.48 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.5     |
|    mean_reward     | 556      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 940      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 178      |
|    time_elapsed    | 1307     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=830.86 +/- 620.39
Episode length: 36.60 +/- 5.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 831       |
| time/                   |           |
|    total_timesteps      | 365000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.24e-16 |
|    explained_variance   | 0.0212    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.67e+04  |
|    n_updates            | 3158      |
|    policy_gradient_loss | -1.22e-09 |
|    value_loss           | 4.18e+04  |
---------------------------------------
Eval num_timesteps=365500, episode_reward=803.89 +/- 643.00
Episode length: 35.76 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=737.60 +/- 702.19
Episode length: 33.88 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=869.89 +/- 679.41
Episode length: 36.04 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 179      |
|    time_elapsed    | 1315     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=768.36 +/- 665.05
Episode length: 34.86 +/- 6.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 768       |
| time/                   |           |
|    total_timesteps      | 367000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.95e-11 |
|    explained_variance   | 0.012     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.87e+04  |
|    n_updates            | 3168      |
|    policy_gradient_loss | 1.08e-09  |
|    value_loss           | 3.68e+04  |
---------------------------------------
Eval num_timesteps=367500, episode_reward=900.07 +/- 713.64
Episode length: 36.08 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 900      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=973.92 +/- 708.47
Episode length: 36.86 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=742.71 +/- 704.94
Episode length: 33.74 +/- 7.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 757      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 180      |
|    time_elapsed    | 1322     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=726.74 +/- 663.39
Episode length: 34.02 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 727       |
| time/                   |           |
|    total_timesteps      | 369000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.55e-14 |
|    explained_variance   | 0.00881   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.38e+04  |
|    n_updates            | 3178      |
|    policy_gradient_loss | 5.5e-10   |
|    value_loss           | 3.77e+04  |
---------------------------------------
Eval num_timesteps=369500, episode_reward=814.08 +/- 694.25
Episode length: 34.98 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=820.92 +/- 728.38
Episode length: 34.00 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=886.17 +/- 718.61
Episode length: 35.76 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 181      |
|    time_elapsed    | 1329     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=933.58 +/- 685.83
Episode length: 36.82 +/- 5.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 934       |
| time/                   |           |
|    total_timesteps      | 371000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2e-18    |
|    explained_variance   | 0.0167    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.81e+04  |
|    n_updates            | 3188      |
|    policy_gradient_loss | -2.01e-10 |
|    value_loss           | 3.71e+04  |
---------------------------------------
Eval num_timesteps=371500, episode_reward=904.00 +/- 723.07
Episode length: 35.84 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=776.68 +/- 607.33
Episode length: 35.06 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=851.39 +/- 716.40
Episode length: 34.96 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 182      |
|    time_elapsed    | 1337     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=825.78 +/- 646.83
Episode length: 35.26 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 373000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.01e-16 |
|    explained_variance   | 0.0149    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.65e+04  |
|    n_updates            | 3198      |
|    policy_gradient_loss | 2.81e-10  |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=373500, episode_reward=871.68 +/- 628.48
Episode length: 36.40 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=639.84 +/- 535.69
Episode length: 34.18 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=883.70 +/- 680.94
Episode length: 35.84 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 183      |
|    time_elapsed    | 1344     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=807.13 +/- 667.17
Episode length: 35.06 +/- 6.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 807       |
| time/                   |           |
|    total_timesteps      | 375000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.8e-18  |
|    explained_variance   | 0.00874   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.33e+04  |
|    n_updates            | 3208      |
|    policy_gradient_loss | -6.52e-10 |
|    value_loss           | 3.27e+04  |
---------------------------------------
Eval num_timesteps=375500, episode_reward=820.53 +/- 693.28
Episode length: 34.92 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=718.44 +/- 644.93
Episode length: 33.90 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=878.87 +/- 717.99
Episode length: 35.20 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 862      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 184      |
|    time_elapsed    | 1351     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=863.96 +/- 680.31
Episode length: 35.50 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 864       |
| time/                   |           |
|    total_timesteps      | 377000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.93e-16 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.67e+04  |
|    n_updates            | 3218      |
|    policy_gradient_loss | -6.66e-10 |
|    value_loss           | 3.77e+04  |
---------------------------------------
Eval num_timesteps=377500, episode_reward=788.55 +/- 654.93
Episode length: 35.52 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=904.37 +/- 728.30
Episode length: 35.50 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=747.29 +/- 622.38
Episode length: 34.66 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 912      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 185      |
|    time_elapsed    | 1358     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=784.04 +/- 675.88
Episode length: 35.00 +/- 6.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 784       |
| time/                   |           |
|    total_timesteps      | 379000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.67e-18 |
|    explained_variance   | 0.0162    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.29e+04  |
|    n_updates            | 3228      |
|    policy_gradient_loss | -6.82e-10 |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=379500, episode_reward=715.44 +/- 553.78
Episode length: 35.20 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=810.85 +/- 685.33
Episode length: 34.56 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=959.33 +/- 699.43
Episode length: 36.94 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 922      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 186      |
|    time_elapsed    | 1366     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=855.40 +/- 710.00
Episode length: 35.20 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 855       |
| time/                   |           |
|    total_timesteps      | 381000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.67e-16 |
|    explained_variance   | 0.019     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.65e+04  |
|    n_updates            | 3238      |
|    policy_gradient_loss | -1.78e-10 |
|    value_loss           | 4.08e+04  |
---------------------------------------
Eval num_timesteps=381500, episode_reward=645.06 +/- 607.53
Episode length: 33.94 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 645      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=742.31 +/- 609.57
Episode length: 35.02 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=911.25 +/- 682.68
Episode length: 36.38 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 908      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 187      |
|    time_elapsed    | 1373     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=851.79 +/- 695.39
Episode length: 35.72 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 852       |
| time/                   |           |
|    total_timesteps      | 383000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.47e-18 |
|    explained_variance   | 0.0178    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.09e+04  |
|    n_updates            | 3248      |
|    policy_gradient_loss | -8.59e-11 |
|    value_loss           | 4.39e+04  |
---------------------------------------
Eval num_timesteps=383500, episode_reward=897.65 +/- 676.97
Episode length: 36.40 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=1068.83 +/- 693.47
Episode length: 38.22 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.2     |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=664.79 +/- 610.91
Episode length: 33.74 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=767.57 +/- 652.14
Episode length: 35.08 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 864      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 188      |
|    time_elapsed    | 1381     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=820.70 +/- 666.16
Episode length: 35.64 +/- 6.17
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.6     |
|    mean_reward          | 821      |
| time/                   |          |
|    total_timesteps      | 385500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -4.8e-16 |
|    explained_variance   | 0.00892  |
|    learning_rate        | 0.0001   |
|    loss                 | 2.5e+04  |
|    n_updates            | 3258     |
|    policy_gradient_loss | 7.07e-10 |
|    value_loss           | 3.85e+04 |
--------------------------------------
Eval num_timesteps=386000, episode_reward=794.66 +/- 714.56
Episode length: 34.42 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=792.56 +/- 741.06
Episode length: 34.10 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=887.96 +/- 689.68
Episode length: 36.08 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 810      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 189      |
|    time_elapsed    | 1389     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=891.62 +/- 701.25
Episode length: 35.72 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 892       |
| time/                   |           |
|    total_timesteps      | 387500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.24e-18 |
|    explained_variance   | 0.0129    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.96e+04  |
|    n_updates            | 3268      |
|    policy_gradient_loss | 8.43e-10  |
|    value_loss           | 3.68e+04  |
---------------------------------------
Eval num_timesteps=388000, episode_reward=1147.41 +/- 725.55
Episode length: 37.82 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=894.45 +/- 745.24
Episode length: 35.50 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=1020.10 +/- 728.31
Episode length: 36.44 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 797      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 190      |
|    time_elapsed    | 1396     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=793.75 +/- 666.04
Episode length: 34.06 +/- 7.47
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.1     |
|    mean_reward          | 794      |
| time/                   |          |
|    total_timesteps      | 389500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.8e-16 |
|    explained_variance   | 0.0104   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.74e+04 |
|    n_updates            | 3278     |
|    policy_gradient_loss | 1.24e-09 |
|    value_loss           | 4.02e+04 |
--------------------------------------
Eval num_timesteps=390000, episode_reward=825.75 +/- 700.53
Episode length: 34.28 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=783.65 +/- 705.79
Episode length: 33.86 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=820.73 +/- 660.02
Episode length: 35.86 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 799      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 191      |
|    time_elapsed    | 1403     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=898.85 +/- 675.53
Episode length: 36.50 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 899       |
| time/                   |           |
|    total_timesteps      | 391500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1e-11    |
|    explained_variance   | 0.0167    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.04e+04  |
|    n_updates            | 3288      |
|    policy_gradient_loss | -7.57e-11 |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=392000, episode_reward=853.31 +/- 687.77
Episode length: 35.42 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=784.76 +/- 680.20
Episode length: 34.34 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=734.44 +/- 652.99
Episode length: 34.42 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 192      |
|    time_elapsed    | 1411     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=1064.93 +/- 710.73
Episode length: 37.12 +/- 6.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.1      |
|    mean_reward          | 1.06e+03  |
| time/                   |           |
|    total_timesteps      | 393500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.93e-14 |
|    explained_variance   | 0.0101    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.78e+04  |
|    n_updates            | 3298      |
|    policy_gradient_loss | -3.78e-11 |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=394000, episode_reward=710.50 +/- 635.33
Episode length: 33.96 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 710      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=652.21 +/- 575.66
Episode length: 33.72 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=703.39 +/- 663.85
Episode length: 33.34 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 193      |
|    time_elapsed    | 1418     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=763.88 +/- 607.19
Episode length: 35.40 +/- 5.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 764       |
| time/                   |           |
|    total_timesteps      | 395500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.58e-18 |
|    explained_variance   | 0.0127    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.67e+04  |
|    n_updates            | 3308      |
|    policy_gradient_loss | -5.01e-10 |
|    value_loss           | 3.61e+04  |
---------------------------------------
Eval num_timesteps=396000, episode_reward=911.44 +/- 702.28
Episode length: 35.74 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=918.85 +/- 677.13
Episode length: 36.50 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=794.19 +/- 687.72
Episode length: 34.74 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 194      |
|    time_elapsed    | 1425     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=884.98 +/- 713.23
Episode length: 35.74 +/- 6.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 885       |
| time/                   |           |
|    total_timesteps      | 397500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.77e-16 |
|    explained_variance   | 0.0091    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.37e+04  |
|    n_updates            | 3318      |
|    policy_gradient_loss | 1.02e-09  |
|    value_loss           | 4.03e+04  |
---------------------------------------
Eval num_timesteps=398000, episode_reward=808.13 +/- 709.15
Episode length: 34.22 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=864.85 +/- 679.24
Episode length: 35.54 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=864.99 +/- 690.31
Episode length: 35.90 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 894      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 195      |
|    time_elapsed    | 1432     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=652.98 +/- 542.49
Episode length: 34.42 +/- 5.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 653       |
| time/                   |           |
|    total_timesteps      | 399500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.43e-18 |
|    explained_variance   | 0.0173    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.48e+04  |
|    n_updates            | 3328      |
|    policy_gradient_loss | 6.72e-10  |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=400000, episode_reward=952.32 +/- 754.44
Episode length: 35.64 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=815.74 +/- 696.04
Episode length: 34.28 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=696.70 +/- 598.53
Episode length: 34.48 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 697      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 196      |
|    time_elapsed    | 1440     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=883.14 +/- 696.40
Episode length: 35.98 +/- 5.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 883       |
| time/                   |           |
|    total_timesteps      | 401500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.82e-16 |
|    explained_variance   | 0.00777   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.17e+04  |
|    n_updates            | 3338      |
|    policy_gradient_loss | -5.88e-10 |
|    value_loss           | 4.08e+04  |
---------------------------------------
Eval num_timesteps=402000, episode_reward=919.47 +/- 694.45
Episode length: 36.72 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=847.46 +/- 684.67
Episode length: 35.60 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=788.22 +/- 664.04
Episode length: 34.82 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 856      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 197      |
|    time_elapsed    | 1447     |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=823.45 +/- 664.28
Episode length: 35.42 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 823       |
| time/                   |           |
|    total_timesteps      | 403500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.88e-12 |
|    explained_variance   | 0.015     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.85e+04  |
|    n_updates            | 3348      |
|    policy_gradient_loss | -1.67e-10 |
|    value_loss           | 4.11e+04  |
---------------------------------------
Eval num_timesteps=404000, episode_reward=835.55 +/- 659.48
Episode length: 35.86 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=1039.65 +/- 795.94
Episode length: 36.40 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=770.70 +/- 674.49
Episode length: 34.06 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=890.46 +/- 699.53
Episode length: 35.60 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 198      |
|    time_elapsed    | 1455     |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=749.57 +/- 588.98
Episode length: 35.64 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 750       |
| time/                   |           |
|    total_timesteps      | 406000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.26e-14 |
|    explained_variance   | 0.00473   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.5e+04   |
|    n_updates            | 3358      |
|    policy_gradient_loss | 1.43e-10  |
|    value_loss           | 3.12e+04  |
---------------------------------------
Eval num_timesteps=406500, episode_reward=761.10 +/- 621.55
Episode length: 35.54 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=735.95 +/- 637.86
Episode length: 34.96 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=757.08 +/- 680.74
Episode length: 34.62 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 199      |
|    time_elapsed    | 1463     |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=872.80 +/- 694.56
Episode length: 35.90 +/- 6.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 873       |
| time/                   |           |
|    total_timesteps      | 408000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.86e-19 |
|    explained_variance   | 0.0149    |
|    learning_rate        | 0.0001    |
|    loss                 | 2e+04     |
|    n_updates            | 3368      |
|    policy_gradient_loss | 6.26e-11  |
|    value_loss           | 4e+04     |
---------------------------------------
Eval num_timesteps=408500, episode_reward=840.18 +/- 675.48
Episode length: 35.54 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=880.62 +/- 712.62
Episode length: 35.34 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=834.40 +/- 698.88
Episode length: 34.44 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 200      |
|    time_elapsed    | 1470     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=741.97 +/- 605.29
Episode length: 35.08 +/- 6.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 742       |
| time/                   |           |
|    total_timesteps      | 410000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.87e-16 |
|    explained_variance   | 0.0128    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.87e+04  |
|    n_updates            | 3378      |
|    policy_gradient_loss | -2.3e-09  |
|    value_loss           | 4.42e+04  |
---------------------------------------
Eval num_timesteps=410500, episode_reward=775.60 +/- 631.68
Episode length: 35.32 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=913.78 +/- 726.90
Episode length: 35.74 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=828.78 +/- 715.30
Episode length: 35.00 +/- 7.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 201      |
|    time_elapsed    | 1478     |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=651.38 +/- 609.12
Episode length: 33.34 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.3      |
|    mean_reward          | 651       |
| time/                   |           |
|    total_timesteps      | 412000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.58e-12 |
|    explained_variance   | 0.012     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.67e+04  |
|    n_updates            | 3388      |
|    policy_gradient_loss | -7.71e-11 |
|    value_loss           | 3.48e+04  |
---------------------------------------
Eval num_timesteps=412500, episode_reward=863.50 +/- 693.14
Episode length: 35.40 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=751.16 +/- 677.46
Episode length: 34.04 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=791.66 +/- 655.34
Episode length: 35.38 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 202      |
|    time_elapsed    | 1485     |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=958.03 +/- 699.83
Episode length: 36.90 +/- 5.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 958       |
| time/                   |           |
|    total_timesteps      | 414000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.61e-14 |
|    explained_variance   | 0.012     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.77e+04  |
|    n_updates            | 3398      |
|    policy_gradient_loss | 1.43e-09  |
|    value_loss           | 3.26e+04  |
---------------------------------------
Eval num_timesteps=414500, episode_reward=838.44 +/- 678.44
Episode length: 35.36 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=830.79 +/- 687.39
Episode length: 34.98 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=920.72 +/- 730.14
Episode length: 35.52 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 203      |
|    time_elapsed    | 1493     |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=992.32 +/- 711.79
Episode length: 36.86 +/- 5.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 992       |
| time/                   |           |
|    total_timesteps      | 416000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.23e-12 |
|    explained_variance   | 0.0105    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.69e+04  |
|    n_updates            | 3408      |
|    policy_gradient_loss | 1.22e-10  |
|    value_loss           | 3.35e+04  |
---------------------------------------
Eval num_timesteps=416500, episode_reward=758.65 +/- 626.63
Episode length: 34.62 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=825.15 +/- 662.77
Episode length: 36.00 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=875.66 +/- 709.88
Episode length: 34.82 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 204      |
|    time_elapsed    | 1500     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=813.81 +/- 764.01
Episode length: 33.50 +/- 7.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 814       |
| time/                   |           |
|    total_timesteps      | 418000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.77e-14 |
|    explained_variance   | 0.0119    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.23e+04  |
|    n_updates            | 3418      |
|    policy_gradient_loss | 1.26e-09  |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=418500, episode_reward=914.05 +/- 736.68
Episode length: 35.28 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=980.69 +/- 706.38
Episode length: 36.42 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 981      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=880.97 +/- 685.23
Episode length: 35.88 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 890      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 205      |
|    time_elapsed    | 1508     |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=676.25 +/- 553.61
Episode length: 34.24 +/- 5.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 676       |
| time/                   |           |
|    total_timesteps      | 420000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.73e-12 |
|    explained_variance   | 0.0186    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.62e+04  |
|    n_updates            | 3428      |
|    policy_gradient_loss | -8.59e-10 |
|    value_loss           | 4.07e+04  |
---------------------------------------
Eval num_timesteps=420500, episode_reward=774.10 +/- 676.81
Episode length: 34.12 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=845.19 +/- 703.41
Episode length: 35.32 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=822.99 +/- 716.59
Episode length: 34.76 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 945      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 206      |
|    time_elapsed    | 1515     |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=917.80 +/- 738.21
Episode length: 35.02 +/- 7.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 918       |
| time/                   |           |
|    total_timesteps      | 422000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.39e-14 |
|    explained_variance   | 0.017     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.69e+04  |
|    n_updates            | 3438      |
|    policy_gradient_loss | -1.66e-09 |
|    value_loss           | 4.68e+04  |
---------------------------------------
Eval num_timesteps=422500, episode_reward=756.78 +/- 675.56
Episode length: 33.94 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=943.20 +/- 714.47
Episode length: 36.26 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=764.63 +/- 655.73
Episode length: 34.48 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 935      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 207      |
|    time_elapsed    | 1522     |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=814.33 +/- 718.11
Episode length: 34.62 +/- 7.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 814       |
| time/                   |           |
|    total_timesteps      | 424000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.46e-19 |
|    explained_variance   | 0.0209    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.57e+04  |
|    n_updates            | 3448      |
|    policy_gradient_loss | 6.72e-10  |
|    value_loss           | 3.73e+04  |
---------------------------------------
Eval num_timesteps=424500, episode_reward=770.02 +/- 679.88
Episode length: 33.94 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=783.42 +/- 658.00
Episode length: 34.72 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=760.92 +/- 672.19
Episode length: 34.46 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 899      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 208      |
|    time_elapsed    | 1529     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=909.48 +/- 676.05
Episode length: 36.28 +/- 6.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 909       |
| time/                   |           |
|    total_timesteps      | 426000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.53e-16 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.5e+04   |
|    n_updates            | 3458      |
|    policy_gradient_loss | 1.36e-09  |
|    value_loss           | 4.47e+04  |
---------------------------------------
Eval num_timesteps=426500, episode_reward=974.87 +/- 714.45
Episode length: 35.94 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=934.15 +/- 756.56
Episode length: 35.68 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=956.57 +/- 753.39
Episode length: 36.06 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=972.83 +/- 727.40
Episode length: 36.30 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 881      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 209      |
|    time_elapsed    | 1538     |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=775.81 +/- 641.72
Episode length: 35.52 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 776       |
| time/                   |           |
|    total_timesteps      | 428500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.69e-12 |
|    explained_variance   | 0.0164    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.73e+04  |
|    n_updates            | 3468      |
|    policy_gradient_loss | 1.04e-09  |
|    value_loss           | 3.47e+04  |
---------------------------------------
Eval num_timesteps=429000, episode_reward=839.27 +/- 638.96
Episode length: 35.64 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=901.37 +/- 661.32
Episode length: 37.04 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=816.51 +/- 702.07
Episode length: 34.62 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.2     |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    fps             | 278      |
|    iterations      | 210      |
|    time_elapsed    | 1545     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=723.34 +/- 622.19
Episode length: 34.50 +/- 6.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 723       |
| time/                   |           |
|    total_timesteps      | 430500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-14 |
|    explained_variance   | 0.0178    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.74e+04  |
|    n_updates            | 3478      |
|    policy_gradient_loss | -5.63e-10 |
|    value_loss           | 4.94e+04  |
---------------------------------------
Eval num_timesteps=431000, episode_reward=804.86 +/- 672.68
Episode length: 35.44 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=824.16 +/- 640.50
Episode length: 35.64 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=901.39 +/- 642.16
Episode length: 36.46 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 946      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 211      |
|    time_elapsed    | 1552     |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=908.82 +/- 749.94
Episode length: 35.12 +/- 7.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 909       |
| time/                   |           |
|    total_timesteps      | 432500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.87e-19 |
|    explained_variance   | 0.013     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.56e+04  |
|    n_updates            | 3488      |
|    policy_gradient_loss | -1.75e-11 |
|    value_loss           | 3.65e+04  |
---------------------------------------
Eval num_timesteps=433000, episode_reward=862.77 +/- 704.24
Episode length: 35.72 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=947.74 +/- 730.50
Episode length: 36.38 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=790.92 +/- 678.22
Episode length: 35.16 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 212      |
|    time_elapsed    | 1560     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=724.07 +/- 633.27
Episode length: 34.32 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 724       |
| time/                   |           |
|    total_timesteps      | 434500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.16e-17 |
|    explained_variance   | 0.00526   |
|    learning_rate        | 0.0001    |
|    loss                 | 4.05e+04  |
|    n_updates            | 3498      |
|    policy_gradient_loss | 1.8e-09   |
|    value_loss           | 4.29e+04  |
---------------------------------------
Eval num_timesteps=435000, episode_reward=748.74 +/- 639.78
Episode length: 34.50 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=1118.00 +/- 728.87
Episode length: 37.74 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.7     |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=812.11 +/- 623.81
Episode length: 35.80 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 728      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 213      |
|    time_elapsed    | 1567     |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=860.24 +/- 653.48
Episode length: 36.22 +/- 5.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 860       |
| time/                   |           |
|    total_timesteps      | 436500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.21e-12 |
|    explained_variance   | 0.00605   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.96e+04  |
|    n_updates            | 3508      |
|    policy_gradient_loss | 2.33e-11  |
|    value_loss           | 3.4e+04   |
---------------------------------------
Eval num_timesteps=437000, episode_reward=888.31 +/- 660.81
Episode length: 36.52 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=782.32 +/- 610.15
Episode length: 35.70 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=688.69 +/- 604.27
Episode length: 34.32 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 214      |
|    time_elapsed    | 1574     |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=927.19 +/- 712.42
Episode length: 36.40 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 927       |
| time/                   |           |
|    total_timesteps      | 438500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-14 |
|    explained_variance   | 0.0173    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.54e+04  |
|    n_updates            | 3518      |
|    policy_gradient_loss | -9.6e-10  |
|    value_loss           | 4.13e+04  |
---------------------------------------
Eval num_timesteps=439000, episode_reward=853.23 +/- 660.90
Episode length: 35.60 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=904.73 +/- 708.04
Episode length: 35.78 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=832.00 +/- 689.45
Episode length: 35.08 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 915      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 215      |
|    time_elapsed    | 1581     |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=721.16 +/- 624.75
Episode length: 34.50 +/- 7.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 721       |
| time/                   |           |
|    total_timesteps      | 440500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.98e-12 |
|    explained_variance   | 0.0136    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.64e+04  |
|    n_updates            | 3528      |
|    policy_gradient_loss | 4.19e-10  |
|    value_loss           | 3.92e+04  |
---------------------------------------
Eval num_timesteps=441000, episode_reward=993.64 +/- 751.55
Episode length: 35.90 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=897.32 +/- 716.00
Episode length: 35.36 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=832.04 +/- 726.91
Episode length: 34.96 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 216      |
|    time_elapsed    | 1588     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=923.78 +/- 702.84
Episode length: 36.06 +/- 6.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 924       |
| time/                   |           |
|    total_timesteps      | 442500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.26e-14 |
|    explained_variance   | 0.0162    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.04e+04  |
|    n_updates            | 3538      |
|    policy_gradient_loss | -5.12e-10 |
|    value_loss           | 4.42e+04  |
---------------------------------------
Eval num_timesteps=443000, episode_reward=779.98 +/- 658.78
Episode length: 34.76 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=739.46 +/- 630.12
Episode length: 35.12 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=860.53 +/- 669.12
Episode length: 35.54 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 908      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 217      |
|    time_elapsed    | 1596     |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=902.73 +/- 660.73
Episode length: 37.24 +/- 4.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.2      |
|    mean_reward          | 903       |
| time/                   |           |
|    total_timesteps      | 444500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.97e-12 |
|    explained_variance   | 0.0171    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.35e+04  |
|    n_updates            | 3548      |
|    policy_gradient_loss | -2.07e-09 |
|    value_loss           | 3.72e+04  |
---------------------------------------
Eval num_timesteps=445000, episode_reward=838.69 +/- 654.57
Episode length: 36.04 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=753.73 +/- 694.28
Episode length: 33.98 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=852.92 +/- 745.89
Episode length: 34.40 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 218      |
|    time_elapsed    | 1603     |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=896.03 +/- 712.19
Episode length: 35.46 +/- 5.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 896       |
| time/                   |           |
|    total_timesteps      | 446500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-14 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.0001    |
|    loss                 | 7.96e+03  |
|    n_updates            | 3558      |
|    policy_gradient_loss | 5.47e-10  |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=447000, episode_reward=864.59 +/- 737.83
Episode length: 35.20 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=855.31 +/- 682.67
Episode length: 35.32 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=766.71 +/- 629.38
Episode length: 34.94 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=967.61 +/- 691.05
Episode length: 36.66 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 810      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 219      |
|    time_elapsed    | 1611     |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=861.91 +/- 683.31
Episode length: 36.24 +/- 5.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 862       |
| time/                   |           |
|    total_timesteps      | 449000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.93e-12 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.41e+04  |
|    n_updates            | 3568      |
|    policy_gradient_loss | 9.72e-10  |
|    value_loss           | 3.62e+04  |
---------------------------------------
Eval num_timesteps=449500, episode_reward=621.37 +/- 539.84
Episode length: 33.84 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 621      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=905.99 +/- 713.54
Episode length: 35.30 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=743.65 +/- 656.46
Episode length: 34.60 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 698      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 220      |
|    time_elapsed    | 1619     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=880.68 +/- 713.02
Episode length: 35.78 +/- 7.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 881       |
| time/                   |           |
|    total_timesteps      | 451000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-14 |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.43e+04  |
|    n_updates            | 3578      |
|    policy_gradient_loss | -3.09e-10 |
|    value_loss           | 3.08e+04  |
---------------------------------------
Eval num_timesteps=451500, episode_reward=739.01 +/- 625.92
Episode length: 34.48 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=808.23 +/- 630.48
Episode length: 35.42 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=736.72 +/- 687.38
Episode length: 33.36 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 760      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 221      |
|    time_elapsed    | 1626     |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=849.57 +/- 703.95
Episode length: 35.14 +/- 6.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 850       |
| time/                   |           |
|    total_timesteps      | 453000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.44e-12 |
|    explained_variance   | 0.0174    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.34e+04  |
|    n_updates            | 3588      |
|    policy_gradient_loss | -7.57e-11 |
|    value_loss           | 3.84e+04  |
---------------------------------------
Eval num_timesteps=453500, episode_reward=804.37 +/- 727.56
Episode length: 33.86 +/- 7.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=1019.84 +/- 734.67
Episode length: 36.56 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=929.08 +/- 666.31
Episode length: 36.40 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 222      |
|    time_elapsed    | 1633     |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=912.91 +/- 706.04
Episode length: 35.88 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 913       |
| time/                   |           |
|    total_timesteps      | 455000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.5e-14  |
|    explained_variance   | 0.0129    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.53e+04  |
|    n_updates            | 3598      |
|    policy_gradient_loss | -8.98e-10 |
|    value_loss           | 3.88e+04  |
---------------------------------------
Eval num_timesteps=455500, episode_reward=916.47 +/- 647.68
Episode length: 37.10 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=833.46 +/- 677.05
Episode length: 35.16 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=833.65 +/- 650.74
Episode length: 35.68 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 223      |
|    time_elapsed    | 1640     |
|    total_timesteps | 456704   |
---------------------------------
Eval num_timesteps=457000, episode_reward=580.64 +/- 510.75
Episode length: 32.88 +/- 5.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.9      |
|    mean_reward          | 581       |
| time/                   |           |
|    total_timesteps      | 457000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.3e-12  |
|    explained_variance   | 0.0189    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.46e+04  |
|    n_updates            | 3608      |
|    policy_gradient_loss | -9.47e-10 |
|    value_loss           | 3.67e+04  |
---------------------------------------
Eval num_timesteps=457500, episode_reward=835.79 +/- 712.82
Episode length: 35.42 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=724.66 +/- 635.84
Episode length: 34.42 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=868.52 +/- 652.23
Episode length: 36.20 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 835      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 224      |
|    time_elapsed    | 1647     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459000, episode_reward=807.03 +/- 637.00
Episode length: 35.92 +/- 5.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 807       |
| time/                   |           |
|    total_timesteps      | 459000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.41e-14 |
|    explained_variance   | 0.0139    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.94e+04  |
|    n_updates            | 3618      |
|    policy_gradient_loss | -7.68e-10 |
|    value_loss           | 3.93e+04  |
---------------------------------------
Eval num_timesteps=459500, episode_reward=892.06 +/- 696.36
Episode length: 35.88 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=770.48 +/- 671.37
Episode length: 34.36 +/- 8.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=833.53 +/- 686.59
Episode length: 35.20 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 847      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 225      |
|    time_elapsed    | 1654     |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=461000, episode_reward=788.51 +/- 619.54
Episode length: 35.44 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 461000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.34e-12 |
|    explained_variance   | 0.0193    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.89e+04  |
|    n_updates            | 3628      |
|    policy_gradient_loss | 1.64e-10  |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=461500, episode_reward=878.16 +/- 675.25
Episode length: 35.78 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=823.91 +/- 684.47
Episode length: 35.16 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=728.78 +/- 645.86
Episode length: 34.26 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 226      |
|    time_elapsed    | 1662     |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=742.16 +/- 645.11
Episode length: 34.02 +/- 7.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 742       |
| time/                   |           |
|    total_timesteps      | 463000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.35e-14 |
|    explained_variance   | 0.0125    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.59e+04  |
|    n_updates            | 3638      |
|    policy_gradient_loss | 7.74e-10  |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=463500, episode_reward=878.02 +/- 705.30
Episode length: 35.42 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=774.76 +/- 589.76
Episode length: 35.96 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=836.12 +/- 686.19
Episode length: 35.26 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 227      |
|    time_elapsed    | 1669     |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=652.00 +/- 587.31
Episode length: 33.96 +/- 6.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 652       |
| time/                   |           |
|    total_timesteps      | 465000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.26e-12 |
|    explained_variance   | 0.0171    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.63e+04  |
|    n_updates            | 3648      |
|    policy_gradient_loss | 4.28e-10  |
|    value_loss           | 3.51e+04  |
---------------------------------------
Eval num_timesteps=465500, episode_reward=855.57 +/- 621.55
Episode length: 36.24 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=725.31 +/- 662.84
Episode length: 34.02 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=913.05 +/- 722.94
Episode length: 36.04 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 228      |
|    time_elapsed    | 1676     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=821.27 +/- 640.79
Episode length: 36.14 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 821       |
| time/                   |           |
|    total_timesteps      | 467000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.16e-14 |
|    explained_variance   | 0.0143    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.89e+04  |
|    n_updates            | 3658      |
|    policy_gradient_loss | 2.91e-10  |
|    value_loss           | 3.59e+04  |
---------------------------------------
Eval num_timesteps=467500, episode_reward=827.68 +/- 661.56
Episode length: 35.84 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=879.76 +/- 748.57
Episode length: 34.98 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=949.04 +/- 729.03
Episode length: 35.74 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 851      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 229      |
|    time_elapsed    | 1683     |
|    total_timesteps | 468992   |
---------------------------------
Eval num_timesteps=469000, episode_reward=908.35 +/- 692.99
Episode length: 36.04 +/- 5.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 908       |
| time/                   |           |
|    total_timesteps      | 469000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.26e-12 |
|    explained_variance   | 0.0166    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.35e+04  |
|    n_updates            | 3668      |
|    policy_gradient_loss | -6.26e-10 |
|    value_loss           | 3.54e+04  |
---------------------------------------
Eval num_timesteps=469500, episode_reward=795.62 +/- 670.86
Episode length: 34.74 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=962.78 +/- 746.40
Episode length: 35.96 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=740.10 +/- 621.92
Episode length: 34.08 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=807.98 +/- 663.83
Episode length: 35.10 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 808      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 230      |
|    time_elapsed    | 1692     |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=792.35 +/- 704.51
Episode length: 34.12 +/- 6.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 792       |
| time/                   |           |
|    total_timesteps      | 471500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.36e-14 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.51e+04  |
|    n_updates            | 3678      |
|    policy_gradient_loss | -7.01e-10 |
|    value_loss           | 3.33e+04  |
---------------------------------------
Eval num_timesteps=472000, episode_reward=742.15 +/- 619.91
Episode length: 35.14 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=963.39 +/- 742.04
Episode length: 35.70 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=969.83 +/- 702.42
Episode length: 36.72 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 231      |
|    time_elapsed    | 1699     |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=736.29 +/- 655.56
Episode length: 34.14 +/- 7.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 736       |
| time/                   |           |
|    total_timesteps      | 473500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.99e-19 |
|    explained_variance   | 0.0189    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.47e+04  |
|    n_updates            | 3688      |
|    policy_gradient_loss | -8.77e-10 |
|    value_loss           | 4.1e+04   |
---------------------------------------
Eval num_timesteps=474000, episode_reward=721.52 +/- 611.78
Episode length: 34.52 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=778.31 +/- 663.31
Episode length: 34.54 +/- 7.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=804.29 +/- 693.38
Episode length: 34.64 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 232      |
|    time_elapsed    | 1706     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=909.35 +/- 693.37
Episode length: 35.86 +/- 6.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 909       |
| time/                   |           |
|    total_timesteps      | 475500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.88e-17 |
|    explained_variance   | 0.0192    |
|    learning_rate        | 0.0001    |
|    loss                 | 2e+04     |
|    n_updates            | 3698      |
|    policy_gradient_loss | -7.67e-10 |
|    value_loss           | 4.43e+04  |
---------------------------------------
Eval num_timesteps=476000, episode_reward=816.58 +/- 699.49
Episode length: 34.58 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=913.77 +/- 753.28
Episode length: 35.24 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=774.84 +/- 651.07
Episode length: 35.34 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 883      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 233      |
|    time_elapsed    | 1713     |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=1007.49 +/- 760.33
Episode length: 36.06 +/- 7.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 477500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.46e-19 |
|    explained_variance   | 0.0219    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.6e+04   |
|    n_updates            | 3708      |
|    policy_gradient_loss | -3.26e-10 |
|    value_loss           | 4.16e+04  |
---------------------------------------
Eval num_timesteps=478000, episode_reward=822.30 +/- 643.48
Episode length: 35.96 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=830.70 +/- 655.16
Episode length: 35.52 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=914.99 +/- 701.03
Episode length: 36.12 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 958      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 234      |
|    time_elapsed    | 1721     |
|    total_timesteps | 479232   |
---------------------------------
Eval num_timesteps=479500, episode_reward=868.17 +/- 724.70
Episode length: 34.90 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 868       |
| time/                   |           |
|    total_timesteps      | 479500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.37e-18 |
|    explained_variance   | 0.0201    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.06e+04  |
|    n_updates            | 3718      |
|    policy_gradient_loss | -1.42e-09 |
|    value_loss           | 4.53e+04  |
---------------------------------------
Eval num_timesteps=480000, episode_reward=807.96 +/- 687.22
Episode length: 34.74 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=851.20 +/- 692.71
Episode length: 35.64 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=983.50 +/- 732.72
Episode length: 36.68 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 984      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 831      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 235      |
|    time_elapsed    | 1728     |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=734.93 +/- 696.15
Episode length: 33.74 +/- 7.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 735       |
| time/                   |           |
|    total_timesteps      | 481500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.32e-13 |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.46e+04  |
|    n_updates            | 3728      |
|    policy_gradient_loss | 1.82e-10  |
|    value_loss           | 3.86e+04  |
---------------------------------------
Eval num_timesteps=482000, episode_reward=784.67 +/- 646.98
Episode length: 35.52 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=842.68 +/- 699.62
Episode length: 34.98 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=705.41 +/- 632.27
Episode length: 34.02 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 236      |
|    time_elapsed    | 1735     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=934.13 +/- 653.79
Episode length: 37.32 +/- 5.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.3      |
|    mean_reward          | 934       |
| time/                   |           |
|    total_timesteps      | 483500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.77e-15 |
|    explained_variance   | 0.0187    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.88e+04  |
|    n_updates            | 3738      |
|    policy_gradient_loss | 4.13e-10  |
|    value_loss           | 3.6e+04   |
---------------------------------------
Eval num_timesteps=484000, episode_reward=892.86 +/- 716.12
Episode length: 35.42 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=928.36 +/- 691.79
Episode length: 36.22 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=809.21 +/- 608.43
Episode length: 35.74 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 237      |
|    time_elapsed    | 1742     |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=796.65 +/- 716.26
Episode length: 34.12 +/- 7.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 797       |
| time/                   |           |
|    total_timesteps      | 485500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.41e-13 |
|    explained_variance   | 0.0167    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.83e+04  |
|    n_updates            | 3748      |
|    policy_gradient_loss | 6.64e-10  |
|    value_loss           | 3.76e+04  |
---------------------------------------
Eval num_timesteps=486000, episode_reward=868.21 +/- 689.38
Episode length: 35.80 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 868      |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=907.49 +/- 723.36
Episode length: 35.64 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=809.45 +/- 697.60
Episode length: 34.72 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 726      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 238      |
|    time_elapsed    | 1750     |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=754.57 +/- 693.70
Episode length: 33.90 +/- 7.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 755       |
| time/                   |           |
|    total_timesteps      | 487500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.77e-15 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.44e+04  |
|    n_updates            | 3758      |
|    policy_gradient_loss | -3.9e-10  |
|    value_loss           | 3.24e+04  |
---------------------------------------
Eval num_timesteps=488000, episode_reward=859.73 +/- 711.63
Episode length: 34.92 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=886.78 +/- 658.51
Episode length: 36.40 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=808.54 +/- 674.09
Episode length: 35.22 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 239      |
|    time_elapsed    | 1757     |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=686.21 +/- 610.22
Episode length: 33.62 +/- 6.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 686       |
| time/                   |           |
|    total_timesteps      | 489500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.45e-20 |
|    explained_variance   | 0.014     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.01e+04  |
|    n_updates            | 3768      |
|    policy_gradient_loss | 6.61e-10  |
|    value_loss           | 3.71e+04  |
---------------------------------------
Eval num_timesteps=490000, episode_reward=871.97 +/- 698.12
Episode length: 35.66 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=727.61 +/- 624.16
Episode length: 34.64 +/- 7.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=843.30 +/- 754.37
Episode length: 34.32 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=819.27 +/- 637.84
Episode length: 35.72 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 240      |
|    time_elapsed    | 1765     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=730.18 +/- 629.31
Episode length: 34.28 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 730       |
| time/                   |           |
|    total_timesteps      | 492000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.87e-18 |
|    explained_variance   | 0.0102    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.42e+04  |
|    n_updates            | 3778      |
|    policy_gradient_loss | 1.18e-09  |
|    value_loss           | 4.94e+04  |
---------------------------------------
Eval num_timesteps=492500, episode_reward=821.80 +/- 714.07
Episode length: 34.48 +/- 7.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=996.20 +/- 726.61
Episode length: 36.32 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=869.00 +/- 681.20
Episode length: 36.18 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 241      |
|    time_elapsed    | 1772     |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=893.84 +/- 676.81
Episode length: 36.44 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 894       |
| time/                   |           |
|    total_timesteps      | 494000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.46e-13 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.08e+04  |
|    n_updates            | 3788      |
|    policy_gradient_loss | -1.47e-10 |
|    value_loss           | 3.76e+04  |
---------------------------------------
Eval num_timesteps=494500, episode_reward=756.10 +/- 626.79
Episode length: 35.08 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=853.91 +/- 665.76
Episode length: 35.74 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=692.61 +/- 600.69
Episode length: 34.94 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 673      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 242      |
|    time_elapsed    | 1780     |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=839.68 +/- 672.42
Episode length: 35.32 +/- 6.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 840       |
| time/                   |           |
|    total_timesteps      | 496000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.94e-09 |
|    explained_variance   | 0.00893   |
|    learning_rate        | 0.0001    |
|    loss                 | 9.54e+03  |
|    n_updates            | 3798      |
|    policy_gradient_loss | -5.53e-11 |
|    value_loss           | 2.89e+04  |
---------------------------------------
Eval num_timesteps=496500, episode_reward=855.53 +/- 713.11
Episode length: 34.54 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=830.53 +/- 675.91
Episode length: 35.38 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=883.49 +/- 693.24
Episode length: 36.18 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 243      |
|    time_elapsed    | 1787     |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=760.81 +/- 687.18
Episode length: 34.14 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 761       |
| time/                   |           |
|    total_timesteps      | 498000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.09e-11 |
|    explained_variance   | 0.0108    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.99e+04  |
|    n_updates            | 3808      |
|    policy_gradient_loss | -2.65e-09 |
|    value_loss           | 3.72e+04  |
---------------------------------------
Eval num_timesteps=498500, episode_reward=876.33 +/- 670.25
Episode length: 36.44 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=753.51 +/- 629.07
Episode length: 34.90 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=806.79 +/- 682.10
Episode length: 34.98 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 845      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 244      |
|    time_elapsed    | 1794     |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=695.48 +/- 618.88
Episode length: 34.06 +/- 7.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 695       |
| time/                   |           |
|    total_timesteps      | 500000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.75e-15 |
|    explained_variance   | 0.016     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.98e+04  |
|    n_updates            | 3818      |
|    policy_gradient_loss | 1.33e-09  |
|    value_loss           | 3.75e+04  |
---------------------------------------
Eval num_timesteps=500500, episode_reward=844.73 +/- 696.43
Episode length: 35.30 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=683.59 +/- 578.53
Episode length: 34.38 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=920.13 +/- 713.15
Episode length: 35.82 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 935      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 245      |
|    time_elapsed    | 1801     |
|    total_timesteps | 501760   |
---------------------------------
