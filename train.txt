Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 1073     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 84.2         |
|    ep_rew_mean          | 0.292        |
| time/                   |              |
|    fps                  | 935          |
|    iterations           | 2            |
|    time_elapsed         | 4            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0059339125 |
|    clip_fraction        | 0.0731       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.0373      |
|    learning_rate        | 0.00074      |
|    loss                 | -0.0053      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0102      |
|    value_loss           | 0.0664       |
------------------------------------------
Eval num_timesteps=5000, episode_reward=-0.70 +/- 0.46
Episode length: 72.70 +/- 10.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 72.7       |
|    mean_reward          | -0.7       |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01691927 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.3        |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0209    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 0.0576     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.5     |
|    ep_rew_mean     | 0.315    |
| time/              |          |
|    fps             | 859      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 86.9        |
|    ep_rew_mean          | 0.624       |
| time/                   |             |
|    fps                  | 869         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.019881222 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.386       |
|    learning_rate        | 0.00074     |
|    loss                 | 0.000655    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0311     |
|    value_loss           | 0.0543      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=3.00 +/- 2.19
Episode length: 105.70 +/- 39.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 3           |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.025911894 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0374     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0382     |
|    value_loss           | 0.0663      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.1     |
|    ep_rew_mean     | 1.07     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 97         |
|    ep_rew_mean          | 1.61       |
| time/                   |            |
|    fps                  | 837        |
|    iterations           | 6          |
|    time_elapsed         | 14         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.03461919 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.448      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0216    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0416    |
|    value_loss           | 0.0745     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 102        |
|    ep_rew_mean          | 2.19       |
| time/                   |            |
|    fps                  | 844        |
|    iterations           | 7          |
|    time_elapsed         | 16         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.03172418 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.498      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0469    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0425    |
|    value_loss           | 0.0767     |
----------------------------------------
Eval num_timesteps=15000, episode_reward=4.20 +/- 1.78
Episode length: 127.20 +/- 30.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 127        |
|    mean_reward          | 4.2        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.04173664 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.502      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0479    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.0718     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.99     |
| time/              |          |
|    fps             | 815      |
|    iterations      | 8        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 3.67        |
| time/                   |             |
|    fps                  | 823         |
|    iterations           | 9           |
|    time_elapsed         | 22          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.050289646 |
|    clip_fraction        | 0.366       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0372     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0403     |
|    value_loss           | 0.0738      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=13.20 +/- 3.60
Episode length: 239.90 +/- 53.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 240         |
|    mean_reward          | 13.2        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.051310375 |
|    clip_fraction        | 0.378       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.491       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0657     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.043      |
|    value_loss           | 0.0799      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 4.19     |
| time/              |          |
|    fps             | 780      |
|    iterations      | 10       |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 130         |
|    ep_rew_mean          | 4.9         |
| time/                   |             |
|    fps                  | 789         |
|    iterations           | 11          |
|    time_elapsed         | 28          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.062323872 |
|    clip_fraction        | 0.363       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.491       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.052      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0472     |
|    value_loss           | 0.073       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 137         |
|    ep_rew_mean          | 5.5         |
| time/                   |             |
|    fps                  | 798         |
|    iterations           | 12          |
|    time_elapsed         | 30          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.051157728 |
|    clip_fraction        | 0.358       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0396     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.0833      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=12.80 +/- 3.37
Episode length: 238.20 +/- 48.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 238        |
|    mean_reward          | 12.8       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.09550306 |
|    clip_fraction        | 0.42       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.985     |
|    explained_variance   | 0.341      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0393    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0546    |
|    value_loss           | 0.0776     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 5.96     |
| time/              |          |
|    fps             | 769      |
|    iterations      | 13       |
|    time_elapsed    | 34       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 149        |
|    ep_rew_mean          | 6.42       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 14         |
|    time_elapsed         | 36         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.06008868 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.967     |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0377    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0417    |
|    value_loss           | 0.0804     |
----------------------------------------
Eval num_timesteps=30000, episode_reward=12.00 +/- 2.76
Episode length: 229.90 +/- 36.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 230        |
|    mean_reward          | 12         |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.08447303 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.901     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0598    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0382    |
|    value_loss           | 0.0735     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 6.86     |
| time/              |          |
|    fps             | 755      |
|    iterations      | 15       |
|    time_elapsed    | 40       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 159        |
|    ep_rew_mean          | 7.24       |
| time/                   |            |
|    fps                  | 762        |
|    iterations           | 16         |
|    time_elapsed         | 42         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.08823315 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.863     |
|    explained_variance   | 0.445      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0475    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0515    |
|    value_loss           | 0.0719     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 166         |
|    ep_rew_mean          | 7.75        |
| time/                   |             |
|    fps                  | 769         |
|    iterations           | 17          |
|    time_elapsed         | 45          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.092912294 |
|    clip_fraction        | 0.397       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.844      |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0655     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.045      |
|    value_loss           | 0.0778      |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=11.70 +/- 5.18
Episode length: 215.60 +/- 74.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 11.7       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.13818805 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.808     |
|    explained_variance   | 0.593      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0615    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0444    |
|    value_loss           | 0.0668     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 8.26     |
| time/              |          |
|    fps             | 753      |
|    iterations      | 18       |
|    time_elapsed    | 48       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 176         |
|    ep_rew_mean          | 8.71        |
| time/                   |             |
|    fps                  | 760         |
|    iterations           | 19          |
|    time_elapsed         | 51          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.094738364 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.78       |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0568     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0409     |
|    value_loss           | 0.0786      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=12.10 +/- 5.52
Episode length: 215.90 +/- 87.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 216         |
|    mean_reward          | 12.1        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.090213284 |
|    clip_fraction        | 0.382       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.752      |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0272     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0455     |
|    value_loss           | 0.0744      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 9.12     |
| time/              |          |
|    fps             | 746      |
|    iterations      | 20       |
|    time_elapsed    | 54       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 185        |
|    ep_rew_mean          | 9.34       |
| time/                   |            |
|    fps                  | 752        |
|    iterations           | 21         |
|    time_elapsed         | 57         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.12033726 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.755     |
|    explained_variance   | 0.5        |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0956    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.0731     |
----------------------------------------
Eval num_timesteps=45000, episode_reward=9.40 +/- 2.94
Episode length: 187.60 +/- 55.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 188         |
|    mean_reward          | 9.4         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.103404574 |
|    clip_fraction        | 0.403       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.755      |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0232     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0421     |
|    value_loss           | 0.0674      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 9.65     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 22       |
|    time_elapsed    | 60       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 188        |
|    ep_rew_mean          | 9.7        |
| time/                   |            |
|    fps                  | 747        |
|    iterations           | 23         |
|    time_elapsed         | 62         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.09495172 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.712     |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0306    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 0.0649     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 9.97        |
| time/                   |             |
|    fps                  | 753         |
|    iterations           | 24          |
|    time_elapsed         | 65          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.117283255 |
|    clip_fraction        | 0.411       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.751      |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.06       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0482     |
|    value_loss           | 0.063       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=13.90 +/- 3.78
Episode length: 245.20 +/- 48.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 245        |
|    mean_reward          | 13.9       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.08739968 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.739     |
|    explained_variance   | 0.572      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0453    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0433    |
|    value_loss           | 0.0756     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 25       |
|    time_elapsed    | 69       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 198        |
|    ep_rew_mean          | 10.6       |
| time/                   |            |
|    fps                  | 746        |
|    iterations           | 26         |
|    time_elapsed         | 71         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.15722936 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.606     |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0393    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0406    |
|    value_loss           | 0.0704     |
----------------------------------------
Eval num_timesteps=55000, episode_reward=12.50 +/- 4.22
Episode length: 228.60 +/- 72.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 229        |
|    mean_reward          | 12.5       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.12357506 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0626    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0489    |
|    value_loss           | 0.0657     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    fps             | 736      |
|    iterations      | 27       |
|    time_elapsed    | 75       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 10.7       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 28         |
|    time_elapsed         | 77         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.14514868 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.671     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0437    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0421    |
|    value_loss           | 0.0662     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 10.8       |
| time/                   |            |
|    fps                  | 745        |
|    iterations           | 29         |
|    time_elapsed         | 79         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.14797145 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.676     |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0688    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0483    |
|    value_loss           | 0.0659     |
----------------------------------------
Eval num_timesteps=60000, episode_reward=12.00 +/- 3.19
Episode length: 221.70 +/- 39.42
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 222      |
|    mean_reward          | 12       |
| time/                   |          |
|    total_timesteps      | 60000    |
| train/                  |          |
|    approx_kl            | 0.132904 |
|    clip_fraction        | 0.377    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.619   |
|    explained_variance   | 0.583    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.032   |
|    n_updates            | 290      |
|    policy_gradient_loss | -0.0447  |
|    value_loss           | 0.0697   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 736      |
|    iterations      | 30       |
|    time_elapsed    | 83       |
|    total_timesteps | 61440    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 11.2       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 31         |
|    time_elapsed         | 85         |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.12487627 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.648     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0596    |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0482    |
|    value_loss           | 0.071      |
----------------------------------------
Eval num_timesteps=65000, episode_reward=13.80 +/- 2.14
Episode length: 246.10 +/- 29.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 246        |
|    mean_reward          | 13.8       |
| time/                   |            |
|    total_timesteps      | 65000      |
| train/                  |            |
|    approx_kl            | 0.13169399 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.615     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0816    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0473    |
|    value_loss           | 0.0699     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 11.3     |
| time/              |          |
|    fps             | 729      |
|    iterations      | 32       |
|    time_elapsed    | 89       |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 208        |
|    ep_rew_mean          | 11.6       |
| time/                   |            |
|    fps                  | 732        |
|    iterations           | 33         |
|    time_elapsed         | 92         |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.13037246 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.655     |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.081     |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0539    |
|    value_loss           | 0.0602     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 209        |
|    ep_rew_mean          | 11.7       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 34         |
|    time_elapsed         | 94         |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.11736009 |
|    clip_fraction        | 0.381      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.641     |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0599    |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.052     |
|    value_loss           | 0.0639     |
----------------------------------------
Eval num_timesteps=70000, episode_reward=12.50 +/- 3.38
Episode length: 207.60 +/- 45.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 208        |
|    mean_reward          | 12.5       |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.12256755 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.627     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0672    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0403    |
|    value_loss           | 0.0712     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 11.5     |
| time/              |          |
|    fps             | 728      |
|    iterations      | 35       |
|    time_elapsed    | 98       |
|    total_timesteps | 71680    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 201        |
|    ep_rew_mean          | 11.3       |
| time/                   |            |
|    fps                  | 731        |
|    iterations           | 36         |
|    time_elapsed         | 100        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.16748676 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.647     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0783    |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.0636     |
----------------------------------------
Eval num_timesteps=75000, episode_reward=12.70 +/- 2.24
Episode length: 209.90 +/- 50.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 12.7       |
| time/                   |            |
|    total_timesteps      | 75000      |
| train/                  |            |
|    approx_kl            | 0.11767058 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.689     |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.05      |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0484    |
|    value_loss           | 0.0647     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 11.4     |
| time/              |          |
|    fps             | 723      |
|    iterations      | 37       |
|    time_elapsed    | 104      |
|    total_timesteps | 75776    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 198        |
|    ep_rew_mean          | 11.1       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 38         |
|    time_elapsed         | 107        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.16348352 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.683     |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0749    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0455    |
|    value_loss           | 0.0668     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 196        |
|    ep_rew_mean          | 11.1       |
| time/                   |            |
|    fps                  | 730        |
|    iterations           | 39         |
|    time_elapsed         | 109        |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.12408273 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.664     |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0797    |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0484    |
|    value_loss           | 0.0573     |
----------------------------------------
Eval num_timesteps=80000, episode_reward=9.40 +/- 3.20
Episode length: 171.30 +/- 34.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 171        |
|    mean_reward          | 9.4        |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.12416533 |
|    clip_fraction        | 0.402      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.711     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0716    |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0549    |
|    value_loss           | 0.0623     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 726      |
|    iterations      | 40       |
|    time_elapsed    | 112      |
|    total_timesteps | 81920    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 182       |
|    ep_rew_mean          | 10.2      |
| time/                   |           |
|    fps                  | 729       |
|    iterations           | 41        |
|    time_elapsed         | 115       |
|    total_timesteps      | 83968     |
| train/                  |           |
|    approx_kl            | 0.1359899 |
|    clip_fraction        | 0.421     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.736    |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0681   |
|    n_updates            | 400       |
|    policy_gradient_loss | -0.0529   |
|    value_loss           | 0.0567    |
---------------------------------------
Eval num_timesteps=85000, episode_reward=7.70 +/- 1.55
Episode length: 154.40 +/- 22.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 154        |
|    mean_reward          | 7.7        |
| time/                   |            |
|    total_timesteps      | 85000      |
| train/                  |            |
|    approx_kl            | 0.15099993 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0714    |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0566    |
|    value_loss           | 0.0585     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 9.8      |
| time/              |          |
|    fps             | 725      |
|    iterations      | 42       |
|    time_elapsed    | 118      |
|    total_timesteps | 86016    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 9.49       |
| time/                   |            |
|    fps                  | 728        |
|    iterations           | 43         |
|    time_elapsed         | 120        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.19169167 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.761     |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0698    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0622    |
|    value_loss           | 0.0613     |
----------------------------------------
Eval num_timesteps=90000, episode_reward=12.40 +/- 3.47
Episode length: 225.30 +/- 50.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 225        |
|    mean_reward          | 12.4       |
| time/                   |            |
|    total_timesteps      | 90000      |
| train/                  |            |
|    approx_kl            | 0.16003755 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.677     |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0696    |
|    n_updates            | 430        |
|    policy_gradient_loss | -0.0599    |
|    value_loss           | 0.0553     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 9.44     |
| time/              |          |
|    fps             | 722      |
|    iterations      | 44       |
|    time_elapsed    | 124      |
|    total_timesteps | 90112    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 173        |
|    ep_rew_mean          | 9.27       |
| time/                   |            |
|    fps                  | 724        |
|    iterations           | 45         |
|    time_elapsed         | 127        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.17785974 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.695     |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.056     |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0604    |
|    value_loss           | 0.052      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 170        |
|    ep_rew_mean          | 9.08       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 46         |
|    time_elapsed         | 129        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.16428947 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.733     |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0817    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.0611     |
----------------------------------------
Eval num_timesteps=95000, episode_reward=10.60 +/- 2.65
Episode length: 186.60 +/- 41.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 187        |
|    mean_reward          | 10.6       |
| time/                   |            |
|    total_timesteps      | 95000      |
| train/                  |            |
|    approx_kl            | 0.21620926 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.79      |
|    explained_variance   | 0.681      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0824    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0672    |
|    value_loss           | 0.0548     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 8.95     |
| time/              |          |
|    fps             | 722      |
|    iterations      | 47       |
|    time_elapsed    | 133      |
|    total_timesteps | 96256    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 8.86       |
| time/                   |            |
|    fps                  | 725        |
|    iterations           | 48         |
|    time_elapsed         | 135        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.15632202 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.743     |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0877    |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.055      |
----------------------------------------
Eval num_timesteps=100000, episode_reward=8.50 +/- 1.96
Episode length: 152.70 +/- 25.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 153        |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.16066015 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.791     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0983    |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0611    |
|    value_loss           | 0.0556     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 8.92     |
| time/              |          |
|    fps             | 722      |
|    iterations      | 49       |
|    time_elapsed    | 138      |
|    total_timesteps | 100352   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 173        |
|    ep_rew_mean          | 8.9        |
| time/                   |            |
|    fps                  | 725        |
|    iterations           | 50         |
|    time_elapsed         | 141        |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.18986814 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.771     |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0964    |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.068     |
|    value_loss           | 0.0526     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 8.93       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 51         |
|    time_elapsed         | 143        |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.20019904 |
|    clip_fraction        | 0.484      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0669    |
|    n_updates            | 500        |
|    policy_gradient_loss | -0.0677    |
|    value_loss           | 0.0629     |
----------------------------------------
Eval num_timesteps=105000, episode_reward=7.80 +/- 2.18
Episode length: 156.50 +/- 33.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 156        |
|    mean_reward          | 7.8        |
| time/                   |            |
|    total_timesteps      | 105000     |
| train/                  |            |
|    approx_kl            | 0.20759436 |
|    clip_fraction        | 0.509      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.781     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0875    |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0657    |
|    value_loss           | 0.0558     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 8.73     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 52       |
|    time_elapsed    | 146      |
|    total_timesteps | 106496   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 170        |
|    ep_rew_mean          | 8.7        |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 53         |
|    time_elapsed         | 149        |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.17037027 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.731     |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0901    |
|    n_updates            | 520        |
|    policy_gradient_loss | -0.0607    |
|    value_loss           | 0.0621     |
----------------------------------------
Eval num_timesteps=110000, episode_reward=9.70 +/- 3.03
Episode length: 172.20 +/- 40.78
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 172      |
|    mean_reward          | 9.7      |
| time/                   |          |
|    total_timesteps      | 110000   |
| train/                  |          |
|    approx_kl            | 0.1898   |
|    clip_fraction        | 0.482    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.748   |
|    explained_variance   | 0.594    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0892  |
|    n_updates            | 530      |
|    policy_gradient_loss | -0.064   |
|    value_loss           | 0.0667   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 8.78     |
| time/              |          |
|    fps             | 723      |
|    iterations      | 54       |
|    time_elapsed    | 152      |
|    total_timesteps | 110592   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 170        |
|    ep_rew_mean          | 8.76       |
| time/                   |            |
|    fps                  | 726        |
|    iterations           | 55         |
|    time_elapsed         | 155        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.16629294 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.75      |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.08      |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0614    |
|    value_loss           | 0.0651     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 8.6        |
| time/                   |            |
|    fps                  | 728        |
|    iterations           | 56         |
|    time_elapsed         | 157        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.16922241 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.805     |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0931    |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0642    |
|    value_loss           | 0.0544     |
----------------------------------------
Eval num_timesteps=115000, episode_reward=9.30 +/- 2.00
Episode length: 167.60 +/- 33.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 168        |
|    mean_reward          | 9.3        |
| time/                   |            |
|    total_timesteps      | 115000     |
| train/                  |            |
|    approx_kl            | 0.17494644 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.819     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0766    |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0685    |
|    value_loss           | 0.0589     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 8.47     |
| time/              |          |
|    fps             | 725      |
|    iterations      | 57       |
|    time_elapsed    | 160      |
|    total_timesteps | 116736   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 163        |
|    ep_rew_mean          | 8.55       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 58         |
|    time_elapsed         | 163        |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.16603711 |
|    clip_fraction        | 0.484      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.767     |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0717    |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.0561    |
|    value_loss           | 0.065      |
----------------------------------------
Eval num_timesteps=120000, episode_reward=8.60 +/- 2.62
Episode length: 168.50 +/- 34.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 168        |
|    mean_reward          | 8.6        |
| time/                   |            |
|    total_timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 0.19234934 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0655    |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0662    |
|    value_loss           | 0.0604     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 8.64     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 59       |
|    time_elapsed    | 166      |
|    total_timesteps | 120832   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 167        |
|    ep_rew_mean          | 9.02       |
| time/                   |            |
|    fps                  | 726        |
|    iterations           | 60         |
|    time_elapsed         | 169        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.19343153 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.773     |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.105     |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0681    |
|    value_loss           | 0.0628     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 167        |
|    ep_rew_mean          | 9.12       |
| time/                   |            |
|    fps                  | 728        |
|    iterations           | 61         |
|    time_elapsed         | 171        |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.19959292 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.743     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.106     |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0651    |
|    value_loss           | 0.0548     |
----------------------------------------
Eval num_timesteps=125000, episode_reward=10.50 +/- 3.17
Episode length: 175.80 +/- 33.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 176        |
|    mean_reward          | 10.5       |
| time/                   |            |
|    total_timesteps      | 125000     |
| train/                  |            |
|    approx_kl            | 0.20717709 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.702     |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0783    |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0655    |
|    value_loss           | 0.0577     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 9.18     |
| time/              |          |
|    fps             | 725      |
|    iterations      | 62       |
|    time_elapsed    | 174      |
|    total_timesteps | 126976   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 170       |
|    ep_rew_mean          | 9.33      |
| time/                   |           |
|    fps                  | 727       |
|    iterations           | 63        |
|    time_elapsed         | 177       |
|    total_timesteps      | 129024    |
| train/                  |           |
|    approx_kl            | 0.1810057 |
|    clip_fraction        | 0.455     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.649    |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.063    |
|    n_updates            | 620       |
|    policy_gradient_loss | -0.0531   |
|    value_loss           | 0.0624    |
---------------------------------------
Eval num_timesteps=130000, episode_reward=9.20 +/- 3.09
Episode length: 161.60 +/- 40.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 162        |
|    mean_reward          | 9.2        |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.20840357 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.663     |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0834    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.063     |
|    value_loss           | 0.0581     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 9.55     |
| time/              |          |
|    fps             | 725      |
|    iterations      | 64       |
|    time_elapsed    | 180      |
|    total_timesteps | 131072   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 9.86       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 65         |
|    time_elapsed         | 183        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.23474205 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.612     |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0645    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.0557     |
----------------------------------------
Eval num_timesteps=135000, episode_reward=9.60 +/- 2.91
Episode length: 170.00 +/- 41.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 170        |
|    mean_reward          | 9.6        |
| time/                   |            |
|    total_timesteps      | 135000     |
| train/                  |            |
|    approx_kl            | 0.21247862 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.079     |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0596    |
|    value_loss           | 0.0539     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 9.86     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 66       |
|    time_elapsed    | 186      |
|    total_timesteps | 135168   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 173        |
|    ep_rew_mean          | 10         |
| time/                   |            |
|    fps                  | 726        |
|    iterations           | 67         |
|    time_elapsed         | 188        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.25250104 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.623     |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0786    |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0652    |
|    value_loss           | 0.058      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 10.1       |
| time/                   |            |
|    fps                  | 728        |
|    iterations           | 68         |
|    time_elapsed         | 191        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.18280579 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0856    |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.065     |
|    value_loss           | 0.0546     |
----------------------------------------
Eval num_timesteps=140000, episode_reward=11.70 +/- 2.76
Episode length: 197.80 +/- 48.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 198        |
|    mean_reward          | 11.7       |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.19862148 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.693     |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0629    |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0597    |
|    value_loss           | 0.0625     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 10       |
| time/              |          |
|    fps             | 725      |
|    iterations      | 69       |
|    time_elapsed    | 194      |
|    total_timesteps | 141312   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | 10.2       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 70         |
|    time_elapsed         | 197        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.19412674 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0562    |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0635    |
|    value_loss           | 0.062      |
----------------------------------------
Eval num_timesteps=145000, episode_reward=13.70 +/- 2.72
Episode length: 215.20 +/- 39.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 13.7       |
| time/                   |            |
|    total_timesteps      | 145000     |
| train/                  |            |
|    approx_kl            | 0.21994042 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.641     |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0814    |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0581    |
|    value_loss           | 0.064      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 723      |
|    iterations      | 71       |
|    time_elapsed    | 200      |
|    total_timesteps | 145408   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | 10.6       |
| time/                   |            |
|    fps                  | 725        |
|    iterations           | 72         |
|    time_elapsed         | 203        |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.24180578 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.66      |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0691    |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.0643    |
|    value_loss           | 0.0562     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 10.5       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 73         |
|    time_elapsed         | 205        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.18507725 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.655     |
|    explained_variance   | 0.648      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0975    |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0657    |
|    value_loss           | 0.0573     |
----------------------------------------
Eval num_timesteps=150000, episode_reward=15.60 +/- 2.33
Episode length: 273.60 +/- 47.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 274        |
|    mean_reward          | 15.6       |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.21472651 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.631     |
|    explained_variance   | 0.524      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.108     |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0587    |
|    value_loss           | 0.0652     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 722      |
|    iterations      | 74       |
|    time_elapsed    | 209      |
|    total_timesteps | 151552   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 10.9       |
| time/                   |            |
|    fps                  | 724        |
|    iterations           | 75         |
|    time_elapsed         | 212        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.25086775 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.587     |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0381    |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0598    |
|    value_loss           | 0.0731     |
----------------------------------------
Eval num_timesteps=155000, episode_reward=12.60 +/- 3.26
Episode length: 225.70 +/- 59.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 226        |
|    mean_reward          | 12.6       |
| time/                   |            |
|    total_timesteps      | 155000     |
| train/                  |            |
|    approx_kl            | 0.23118243 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.62      |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.101     |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.0584    |
|    value_loss           | 0.0627     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 11.4     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 76       |
|    time_elapsed    | 215      |
|    total_timesteps | 155648   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 201       |
|    ep_rew_mean          | 11.6      |
| time/                   |           |
|    fps                  | 722       |
|    iterations           | 77        |
|    time_elapsed         | 218       |
|    total_timesteps      | 157696    |
| train/                  |           |
|    approx_kl            | 0.2882849 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.615    |
|    explained_variance   | 0.498     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0898   |
|    n_updates            | 760       |
|    policy_gradient_loss | -0.0545   |
|    value_loss           | 0.0706    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 200       |
|    ep_rew_mean          | 11.5      |
| time/                   |           |
|    fps                  | 724       |
|    iterations           | 78        |
|    time_elapsed         | 220       |
|    total_timesteps      | 159744    |
| train/                  |           |
|    approx_kl            | 0.2539034 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.646    |
|    explained_variance   | 0.604     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0794   |
|    n_updates            | 770       |
|    policy_gradient_loss | -0.0619   |
|    value_loss           | 0.0569    |
---------------------------------------
Eval num_timesteps=160000, episode_reward=13.30 +/- 2.79
Episode length: 223.20 +/- 44.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 223       |
|    mean_reward          | 13.3      |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.2505154 |
|    clip_fraction        | 0.429     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.573    |
|    explained_variance   | 0.532     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.093    |
|    n_updates            | 780       |
|    policy_gradient_loss | -0.0567   |
|    value_loss           | 0.0652    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 11.5     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 79       |
|    time_elapsed    | 224      |
|    total_timesteps | 161792   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 11.4       |
| time/                   |            |
|    fps                  | 722        |
|    iterations           | 80         |
|    time_elapsed         | 226        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.34993175 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.592     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0829    |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0634    |
|    value_loss           | 0.0673     |
----------------------------------------
Eval num_timesteps=165000, episode_reward=9.50 +/- 3.07
Episode length: 187.80 +/- 47.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 188       |
|    mean_reward          | 9.5       |
| time/                   |           |
|    total_timesteps      | 165000    |
| train/                  |           |
|    approx_kl            | 0.2950874 |
|    clip_fraction        | 0.462     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.598    |
|    explained_variance   | 0.623     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0632   |
|    n_updates            | 800       |
|    policy_gradient_loss | -0.0577   |
|    value_loss           | 0.0617    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | 11.4     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 81       |
|    time_elapsed    | 230      |
|    total_timesteps | 165888   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 203       |
|    ep_rew_mean          | 11.6      |
| time/                   |           |
|    fps                  | 721       |
|    iterations           | 82        |
|    time_elapsed         | 232       |
|    total_timesteps      | 167936    |
| train/                  |           |
|    approx_kl            | 0.3340149 |
|    clip_fraction        | 0.452     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.595    |
|    explained_variance   | 0.659     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0719   |
|    n_updates            | 810       |
|    policy_gradient_loss | -0.0598   |
|    value_loss           | 0.0615    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 206        |
|    ep_rew_mean          | 11.7       |
| time/                   |            |
|    fps                  | 723        |
|    iterations           | 83         |
|    time_elapsed         | 235        |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.27351803 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.553     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0817    |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0533    |
|    value_loss           | 0.0676     |
----------------------------------------
Eval num_timesteps=170000, episode_reward=12.40 +/- 2.54
Episode length: 225.30 +/- 46.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 225        |
|    mean_reward          | 12.4       |
| time/                   |            |
|    total_timesteps      | 170000     |
| train/                  |            |
|    approx_kl            | 0.24312389 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.616     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.068     |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0562    |
|    value_loss           | 0.0592     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 11.6     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 84       |
|    time_elapsed    | 238      |
|    total_timesteps | 172032   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 11.6       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 85         |
|    time_elapsed         | 241        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.27987385 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.621     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.076     |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.0578     |
----------------------------------------
Eval num_timesteps=175000, episode_reward=12.70 +/- 3.03
Episode length: 234.30 +/- 44.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 234        |
|    mean_reward          | 12.7       |
| time/                   |            |
|    total_timesteps      | 175000     |
| train/                  |            |
|    approx_kl            | 0.27209094 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.6       |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0597    |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.0598     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 207      |
|    ep_rew_mean     | 11.6     |
| time/              |          |
|    fps             | 718      |
|    iterations      | 86       |
|    time_elapsed    | 245      |
|    total_timesteps | 176128   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 206       |
|    ep_rew_mean          | 11.5      |
| time/                   |           |
|    fps                  | 719       |
|    iterations           | 87        |
|    time_elapsed         | 247       |
|    total_timesteps      | 178176    |
| train/                  |           |
|    approx_kl            | 0.5167918 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.56     |
|    explained_variance   | 0.711     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.049    |
|    n_updates            | 860       |
|    policy_gradient_loss | -0.0626   |
|    value_loss           | 0.0475    |
---------------------------------------
Eval num_timesteps=180000, episode_reward=10.30 +/- 3.49
Episode length: 198.50 +/- 52.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 198       |
|    mean_reward          | 10.3      |
| time/                   |           |
|    total_timesteps      | 180000    |
| train/                  |           |
|    approx_kl            | 0.2736429 |
|    clip_fraction        | 0.444     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.558    |
|    explained_variance   | 0.673     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0381   |
|    n_updates            | 870       |
|    policy_gradient_loss | -0.063    |
|    value_loss           | 0.0557    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 11.5     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 88       |
|    time_elapsed    | 251      |
|    total_timesteps | 180224   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 205       |
|    ep_rew_mean          | 11.3      |
| time/                   |           |
|    fps                  | 718       |
|    iterations           | 89        |
|    time_elapsed         | 253       |
|    total_timesteps      | 182272    |
| train/                  |           |
|    approx_kl            | 0.3433475 |
|    clip_fraction        | 0.438     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.558    |
|    explained_variance   | 0.617     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0672   |
|    n_updates            | 880       |
|    policy_gradient_loss | -0.0525   |
|    value_loss           | 0.0574    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 11.2       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 90         |
|    time_elapsed         | 255        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.33490485 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.579     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0906    |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0586    |
|    value_loss           | 0.0595     |
----------------------------------------
Eval num_timesteps=185000, episode_reward=10.00 +/- 2.00
Episode length: 185.40 +/- 35.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 185        |
|    mean_reward          | 10         |
| time/                   |            |
|    total_timesteps      | 185000     |
| train/                  |            |
|    approx_kl            | 0.29438442 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.6       |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0679    |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0577    |
|    value_loss           | 0.0621     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 718      |
|    iterations      | 91       |
|    time_elapsed    | 259      |
|    total_timesteps | 186368   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 10.4       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 92         |
|    time_elapsed         | 261        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.28428793 |
|    clip_fraction        | 0.503      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0766    |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0697    |
|    value_loss           | 0.0566     |
----------------------------------------
Eval num_timesteps=190000, episode_reward=11.10 +/- 2.26
Episode length: 207.20 +/- 32.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 207        |
|    mean_reward          | 11.1       |
| time/                   |            |
|    total_timesteps      | 190000     |
| train/                  |            |
|    approx_kl            | 0.34398127 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.643     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0699    |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0673    |
|    value_loss           | 0.0546     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 93       |
|    time_elapsed    | 265      |
|    total_timesteps | 190464   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 188        |
|    ep_rew_mean          | 9.94       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 94         |
|    time_elapsed         | 267        |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.43655217 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.679     |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.101     |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0727    |
|    value_loss           | 0.0497     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 183       |
|    ep_rew_mean          | 9.65      |
| time/                   |           |
|    fps                  | 720       |
|    iterations           | 95        |
|    time_elapsed         | 270       |
|    total_timesteps      | 194560    |
| train/                  |           |
|    approx_kl            | 0.2995167 |
|    clip_fraction        | 0.493     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.626    |
|    explained_variance   | 0.601     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0959   |
|    n_updates            | 940       |
|    policy_gradient_loss | -0.0678   |
|    value_loss           | 0.0588    |
---------------------------------------
Eval num_timesteps=195000, episode_reward=8.50 +/- 2.01
Episode length: 180.20 +/- 44.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 180        |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 195000     |
| train/                  |            |
|    approx_kl            | 0.28022143 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.669     |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.105     |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.069     |
|    value_loss           | 0.0539     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 9.34     |
| time/              |          |
|    fps             | 718      |
|    iterations      | 96       |
|    time_elapsed    | 273      |
|    total_timesteps | 196608   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 176       |
|    ep_rew_mean          | 9.2       |
| time/                   |           |
|    fps                  | 719       |
|    iterations           | 97        |
|    time_elapsed         | 276       |
|    total_timesteps      | 198656    |
| train/                  |           |
|    approx_kl            | 0.3337917 |
|    clip_fraction        | 0.494     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.62     |
|    explained_variance   | 0.614     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0919   |
|    n_updates            | 960       |
|    policy_gradient_loss | -0.063    |
|    value_loss           | 0.0576    |
---------------------------------------
Eval num_timesteps=200000, episode_reward=10.50 +/- 2.20
Episode length: 189.10 +/- 41.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 189       |
|    mean_reward          | 10.5      |
| time/                   |           |
|    total_timesteps      | 200000    |
| train/                  |           |
|    approx_kl            | 0.2904059 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.632    |
|    explained_variance   | 0.677     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.103    |
|    n_updates            | 970       |
|    policy_gradient_loss | -0.0706   |
|    value_loss           | 0.0534    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 9.17     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 98       |
|    time_elapsed    | 279      |
|    total_timesteps | 200704   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | 9.26       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 99         |
|    time_elapsed         | 282        |
|    total_timesteps      | 202752     |
| train/                  |            |
|    approx_kl            | 0.28864735 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.644     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0878    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0635    |
|    value_loss           | 0.0541     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 173        |
|    ep_rew_mean          | 8.97       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 100        |
|    time_elapsed         | 284        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.27081984 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.673     |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.00074    |
|    loss                 | -0.087     |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0677    |
|    value_loss           | 0.0519     |
----------------------------------------
Eval num_timesteps=205000, episode_reward=7.50 +/- 1.57
Episode length: 152.20 +/- 30.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 152        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 205000     |
| train/                  |            |
|    approx_kl            | 0.34637323 |
|    clip_fraction        | 0.508      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.643     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0962    |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.0584     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 8.7      |
| time/              |          |
|    fps             | 718      |
|    iterations      | 101      |
|    time_elapsed    | 287      |
|    total_timesteps | 206848   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 8.56       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 102        |
|    time_elapsed         | 290        |
|    total_timesteps      | 208896     |
| train/                  |            |
|    approx_kl            | 0.35733014 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.686     |
|    explained_variance   | 0.68       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.082     |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0666    |
|    value_loss           | 0.0529     |
----------------------------------------
Eval num_timesteps=210000, episode_reward=8.50 +/- 2.01
Episode length: 163.40 +/- 32.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 163        |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 210000     |
| train/                  |            |
|    approx_kl            | 0.35812426 |
|    clip_fraction        | 0.526      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.636     |
|    explained_variance   | 0.601      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0972    |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0678    |
|    value_loss           | 0.0595     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 8.42     |
| time/              |          |
|    fps             | 718      |
|    iterations      | 103      |
|    time_elapsed    | 293      |
|    total_timesteps | 210944   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 8.34       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 104        |
|    time_elapsed         | 296        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.37264493 |
|    clip_fraction        | 0.516      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.659     |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0933    |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0679    |
|    value_loss           | 0.0581     |
----------------------------------------
Eval num_timesteps=215000, episode_reward=9.50 +/- 2.06
Episode length: 176.60 +/- 31.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 177        |
|    mean_reward          | 9.5        |
| time/                   |            |
|    total_timesteps      | 215000     |
| train/                  |            |
|    approx_kl            | 0.33922237 |
|    clip_fraction        | 0.517      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0761    |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0656    |
|    value_loss           | 0.0622     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 8.42     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 105      |
|    time_elapsed    | 299      |
|    total_timesteps | 215040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 8.42       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 106        |
|    time_elapsed         | 302        |
|    total_timesteps      | 217088     |
| train/                  |            |
|    approx_kl            | 0.32666302 |
|    clip_fraction        | 0.511      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.623     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0726    |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.067     |
|    value_loss           | 0.0571     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 167       |
|    ep_rew_mean          | 8.45      |
| time/                   |           |
|    fps                  | 719       |
|    iterations           | 107       |
|    time_elapsed         | 304       |
|    total_timesteps      | 219136    |
| train/                  |           |
|    approx_kl            | 0.3601726 |
|    clip_fraction        | 0.497     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.59     |
|    explained_variance   | 0.61      |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0778   |
|    n_updates            | 1060      |
|    policy_gradient_loss | -0.0654   |
|    value_loss           | 0.0585    |
---------------------------------------
Eval num_timesteps=220000, episode_reward=10.70 +/- 1.62
Episode length: 180.70 +/- 30.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 181        |
|    mean_reward          | 10.7       |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.31068656 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.593     |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0965    |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.069     |
|    value_loss           | 0.0633     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 8.63     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 108      |
|    time_elapsed    | 308      |
|    total_timesteps | 221184   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | 8.96       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 109        |
|    time_elapsed         | 310        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.32329258 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.566     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0863    |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.067     |
|    value_loss           | 0.0563     |
----------------------------------------
Eval num_timesteps=225000, episode_reward=9.90 +/- 1.64
Episode length: 184.70 +/- 32.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 185        |
|    mean_reward          | 9.9        |
| time/                   |            |
|    total_timesteps      | 225000     |
| train/                  |            |
|    approx_kl            | 0.33830625 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.558     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.104     |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0646    |
|    value_loss           | 0.0588     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 9.14     |
| time/              |          |
|    fps             | 716      |
|    iterations      | 110      |
|    time_elapsed    | 314      |
|    total_timesteps | 225280   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 9.28       |
| time/                   |            |
|    fps                  | 717        |
|    iterations           | 111        |
|    time_elapsed         | 316        |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.31864172 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.569     |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.102     |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0685    |
|    value_loss           | 0.0511     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 9.42       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 112        |
|    time_elapsed         | 318        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.31020796 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.59      |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0729    |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0616    |
|    value_loss           | 0.0547     |
----------------------------------------
Eval num_timesteps=230000, episode_reward=9.70 +/- 2.69
Episode length: 195.50 +/- 49.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 196       |
|    mean_reward          | 9.7       |
| time/                   |           |
|    total_timesteps      | 230000    |
| train/                  |           |
|    approx_kl            | 0.4069578 |
|    clip_fraction        | 0.483     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.624    |
|    explained_variance   | 0.671     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0672   |
|    n_updates            | 1120      |
|    policy_gradient_loss | -0.064    |
|    value_loss           | 0.0585    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 9.49     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 113      |
|    time_elapsed    | 322      |
|    total_timesteps | 231424   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 179        |
|    ep_rew_mean          | 9.53       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 114        |
|    time_elapsed         | 325        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.43547904 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.608     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.127     |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0688    |
|    value_loss           | 0.0539     |
----------------------------------------
Eval num_timesteps=235000, episode_reward=10.90 +/- 2.43
Episode length: 211.60 +/- 29.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 10.9       |
| time/                   |            |
|    total_timesteps      | 235000     |
| train/                  |            |
|    approx_kl            | 0.39252388 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.619     |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0763    |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.0672    |
|    value_loss           | 0.0493     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 9.53     |
| time/              |          |
|    fps             | 716      |
|    iterations      | 115      |
|    time_elapsed    | 328      |
|    total_timesteps | 235520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 9.47       |
| time/                   |            |
|    fps                  | 717        |
|    iterations           | 116        |
|    time_elapsed         | 331        |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.35468012 |
|    clip_fraction        | 0.518      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.638     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0768    |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0714    |
|    value_loss           | 0.0556     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 181        |
|    ep_rew_mean          | 9.46       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 117        |
|    time_elapsed         | 333        |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.30242586 |
|    clip_fraction        | 0.53       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0856    |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.069     |
|    value_loss           | 0.0527     |
----------------------------------------
Eval num_timesteps=240000, episode_reward=9.30 +/- 1.95
Episode length: 181.10 +/- 39.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 181       |
|    mean_reward          | 9.3       |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.3145235 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.629    |
|    explained_variance   | 0.676     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0887   |
|    n_updates            | 1170      |
|    policy_gradient_loss | -0.0715   |
|    value_loss           | 0.0571    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 9.28     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 118      |
|    time_elapsed    | 336      |
|    total_timesteps | 241664   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 183       |
|    ep_rew_mean          | 9.45      |
| time/                   |           |
|    fps                  | 718       |
|    iterations           | 119       |
|    time_elapsed         | 339       |
|    total_timesteps      | 243712    |
| train/                  |           |
|    approx_kl            | 0.3347134 |
|    clip_fraction        | 0.495     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.586    |
|    explained_variance   | 0.713     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0964   |
|    n_updates            | 1180      |
|    policy_gradient_loss | -0.0652   |
|    value_loss           | 0.0505    |
---------------------------------------
Eval num_timesteps=245000, episode_reward=9.30 +/- 2.15
Episode length: 181.00 +/- 29.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 181        |
|    mean_reward          | 9.3        |
| time/                   |            |
|    total_timesteps      | 245000     |
| train/                  |            |
|    approx_kl            | 0.40306878 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.566     |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0767    |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0632    |
|    value_loss           | 0.0541     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 9.54     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 120      |
|    time_elapsed    | 342      |
|    total_timesteps | 245760   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 185       |
|    ep_rew_mean          | 9.63      |
| time/                   |           |
|    fps                  | 718       |
|    iterations           | 121       |
|    time_elapsed         | 344       |
|    total_timesteps      | 247808    |
| train/                  |           |
|    approx_kl            | 0.3665422 |
|    clip_fraction        | 0.477     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.578    |
|    explained_variance   | 0.653     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0553   |
|    n_updates            | 1200      |
|    policy_gradient_loss | -0.0602   |
|    value_loss           | 0.0606    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 184       |
|    ep_rew_mean          | 9.62      |
| time/                   |           |
|    fps                  | 719       |
|    iterations           | 122       |
|    time_elapsed         | 347       |
|    total_timesteps      | 249856    |
| train/                  |           |
|    approx_kl            | 0.3197071 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.548    |
|    explained_variance   | 0.661     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0955   |
|    n_updates            | 1210      |
|    policy_gradient_loss | -0.0702   |
|    value_loss           | 0.0565    |
---------------------------------------
Eval num_timesteps=250000, episode_reward=11.40 +/- 3.50
Episode length: 202.10 +/- 55.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 202       |
|    mean_reward          | 11.4      |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.2845108 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.623    |
|    explained_variance   | 0.687     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0757   |
|    n_updates            | 1220      |
|    policy_gradient_loss | -0.0655   |
|    value_loss           | 0.0547    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 9.56     |
| time/              |          |
|    fps             | 718      |
|    iterations      | 123      |
|    time_elapsed    | 350      |
|    total_timesteps | 251904   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 9.47       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 124        |
|    time_elapsed         | 353        |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.29799843 |
|    clip_fraction        | 0.479      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.608     |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0833    |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0683    |
|    value_loss           | 0.0551     |
----------------------------------------
Eval num_timesteps=255000, episode_reward=8.50 +/- 3.88
Episode length: 173.40 +/- 60.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 173        |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 255000     |
| train/                  |            |
|    approx_kl            | 0.34541363 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.62      |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0531    |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.0636    |
|    value_loss           | 0.0574     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 9.46     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 125      |
|    time_elapsed    | 356      |
|    total_timesteps | 256000   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 178        |
|    ep_rew_mean          | 9.36       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 126        |
|    time_elapsed         | 358        |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.36738762 |
|    clip_fraction        | 0.491      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.628     |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0844    |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.0654    |
|    value_loss           | 0.0545     |
----------------------------------------
Eval num_timesteps=260000, episode_reward=9.00 +/- 3.29
Episode length: 173.70 +/- 52.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 174       |
|    mean_reward          | 9         |
| time/                   |           |
|    total_timesteps      | 260000    |
| train/                  |           |
|    approx_kl            | 0.3042346 |
|    clip_fraction        | 0.504     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.618    |
|    explained_variance   | 0.658     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.1      |
|    n_updates            | 1260      |
|    policy_gradient_loss | -0.0653   |
|    value_loss           | 0.0579    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 9.28     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 127      |
|    time_elapsed    | 362      |
|    total_timesteps | 260096   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 9.08       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 128        |
|    time_elapsed         | 364        |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.39387003 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.591     |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0816    |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.0715    |
|    value_loss           | 0.0595     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 173       |
|    ep_rew_mean          | 8.99      |
| time/                   |           |
|    fps                  | 719       |
|    iterations           | 129       |
|    time_elapsed         | 367       |
|    total_timesteps      | 264192    |
| train/                  |           |
|    approx_kl            | 0.3995062 |
|    clip_fraction        | 0.501     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.577    |
|    explained_variance   | 0.622     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.097    |
|    n_updates            | 1280      |
|    policy_gradient_loss | -0.067    |
|    value_loss           | 0.0592    |
---------------------------------------
Eval num_timesteps=265000, episode_reward=12.30 +/- 1.55
Episode length: 218.60 +/- 28.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 219        |
|    mean_reward          | 12.3       |
| time/                   |            |
|    total_timesteps      | 265000     |
| train/                  |            |
|    approx_kl            | 0.34248197 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.536     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0739    |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0577    |
|    value_loss           | 0.0624     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 8.98     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 130      |
|    time_elapsed    | 370      |
|    total_timesteps | 266240   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 173       |
|    ep_rew_mean          | 9.06      |
| time/                   |           |
|    fps                  | 719       |
|    iterations           | 131       |
|    time_elapsed         | 373       |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.3877503 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.547    |
|    explained_variance   | 0.551     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0611   |
|    n_updates            | 1300      |
|    policy_gradient_loss | -0.0615   |
|    value_loss           | 0.0682    |
---------------------------------------
Eval num_timesteps=270000, episode_reward=8.20 +/- 3.54
Episode length: 179.70 +/- 57.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 180       |
|    mean_reward          | 8.2       |
| time/                   |           |
|    total_timesteps      | 270000    |
| train/                  |           |
|    approx_kl            | 0.3734335 |
|    clip_fraction        | 0.449     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.563    |
|    explained_variance   | 0.582     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0648   |
|    n_updates            | 1310      |
|    policy_gradient_loss | -0.0612   |
|    value_loss           | 0.0652    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 9.18     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 132      |
|    time_elapsed    | 376      |
|    total_timesteps | 270336   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 9.25       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 133        |
|    time_elapsed         | 378        |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.36593565 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.576     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.075     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0596    |
|    value_loss           | 0.0575     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | 9.36       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 134        |
|    time_elapsed         | 381        |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.33500853 |
|    clip_fraction        | 0.449      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.544     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0718    |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0562    |
|    value_loss           | 0.0589     |
----------------------------------------
Eval num_timesteps=275000, episode_reward=7.40 +/- 4.13
Episode length: 144.80 +/- 49.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 145        |
|    mean_reward          | 7.4        |
| time/                   |            |
|    total_timesteps      | 275000     |
| train/                  |            |
|    approx_kl            | 0.38932413 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.549     |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.109     |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0628    |
|    value_loss           | 0.0527     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 9.4      |
| time/              |          |
|    fps             | 719      |
|    iterations      | 135      |
|    time_elapsed    | 384      |
|    total_timesteps | 276480   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 181        |
|    ep_rew_mean          | 9.53       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 136        |
|    time_elapsed         | 386        |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.41395265 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.572     |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.111     |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.067     |
|    value_loss           | 0.0557     |
----------------------------------------
Eval num_timesteps=280000, episode_reward=7.50 +/- 2.11
Episode length: 157.20 +/- 36.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 157        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.35495082 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.584     |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0953    |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0617    |
|    value_loss           | 0.0567     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 9.41     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 137      |
|    time_elapsed    | 390      |
|    total_timesteps | 280576   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 181        |
|    ep_rew_mean          | 9.46       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 138        |
|    time_elapsed         | 392        |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.42238927 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.558     |
|    explained_variance   | 0.648      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0997    |
|    n_updates            | 1370       |
|    policy_gradient_loss | -0.0597    |
|    value_loss           | 0.055      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | 9.17       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 139        |
|    time_elapsed         | 394        |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.43408453 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.536     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0888    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0633    |
|    value_loss           | 0.058      |
----------------------------------------
Eval num_timesteps=285000, episode_reward=5.90 +/- 2.26
Episode length: 137.20 +/- 32.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 137        |
|    mean_reward          | 5.9        |
| time/                   |            |
|    total_timesteps      | 285000     |
| train/                  |            |
|    approx_kl            | 0.34301937 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.582     |
|    explained_variance   | 0.533      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0684    |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.0537    |
|    value_loss           | 0.0665     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 9.09     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 140      |
|    time_elapsed    | 397      |
|    total_timesteps | 286720   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | 9.07       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 141        |
|    time_elapsed         | 400        |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.33281407 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.581     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0867    |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0552    |
|    value_loss           | 0.061      |
----------------------------------------
Eval num_timesteps=290000, episode_reward=9.10 +/- 3.83
Episode length: 172.90 +/- 51.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 173        |
|    mean_reward          | 9.1        |
| time/                   |            |
|    total_timesteps      | 290000     |
| train/                  |            |
|    approx_kl            | 0.75144875 |
|    clip_fraction        | 0.481      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.57      |
|    explained_variance   | 0.545      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0644    |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.056     |
|    value_loss           | 0.0688     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 8.99     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 142      |
|    time_elapsed    | 403      |
|    total_timesteps | 290816   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 8.91       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 143        |
|    time_elapsed         | 405        |
|    total_timesteps      | 292864     |
| train/                  |            |
|    approx_kl            | 0.34628487 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.591     |
|    explained_variance   | 0.543      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.1       |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.057     |
|    value_loss           | 0.0669     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 172       |
|    ep_rew_mean          | 8.73      |
| time/                   |           |
|    fps                  | 722       |
|    iterations           | 144       |
|    time_elapsed         | 407       |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.4342491 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.583    |
|    explained_variance   | 0.534     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0579   |
|    n_updates            | 1430      |
|    policy_gradient_loss | -0.0563   |
|    value_loss           | 0.0624    |
---------------------------------------
Eval num_timesteps=295000, episode_reward=9.80 +/- 3.03
Episode length: 190.30 +/- 47.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 190        |
|    mean_reward          | 9.8        |
| time/                   |            |
|    total_timesteps      | 295000     |
| train/                  |            |
|    approx_kl            | 0.42453584 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.536     |
|    explained_variance   | 0.536      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0597    |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.0544    |
|    value_loss           | 0.0647     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 8.74     |
| time/              |          |
|    fps             | 721      |
|    iterations      | 145      |
|    time_elapsed    | 411      |
|    total_timesteps | 296960   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 171       |
|    ep_rew_mean          | 8.81      |
| time/                   |           |
|    fps                  | 722       |
|    iterations           | 146       |
|    time_elapsed         | 413       |
|    total_timesteps      | 299008    |
| train/                  |           |
|    approx_kl            | 0.4168067 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.554    |
|    explained_variance   | 0.446     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0777   |
|    n_updates            | 1450      |
|    policy_gradient_loss | -0.0543   |
|    value_loss           | 0.0634    |
---------------------------------------
Eval num_timesteps=300000, episode_reward=2.40 +/- 1.50
Episode length: 92.80 +/- 23.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 92.8      |
|    mean_reward          | 2.4       |
| time/                   |           |
|    total_timesteps      | 300000    |
| train/                  |           |
|    approx_kl            | 0.4774266 |
|    clip_fraction        | 0.499     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.544    |
|    explained_variance   | 0.488     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0929   |
|    n_updates            | 1460      |
|    policy_gradient_loss | -0.0589   |
|    value_loss           | 0.0608    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 8.32     |
| time/              |          |
|    fps             | 722      |
|    iterations      | 147      |
|    time_elapsed    | 416      |
|    total_timesteps | 301056   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 153        |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 723        |
|    iterations           | 148        |
|    time_elapsed         | 418        |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.43900877 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.577     |
|    explained_variance   | 0.298      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.097     |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0283    |
|    value_loss           | 0.0686     |
----------------------------------------
Eval num_timesteps=305000, episode_reward=4.80 +/- 1.72
Episode length: 103.90 +/- 29.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 104        |
|    mean_reward          | 4.8        |
| time/                   |            |
|    total_timesteps      | 305000     |
| train/                  |            |
|    approx_kl            | 0.39925563 |
|    clip_fraction        | 0.495      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.603     |
|    explained_variance   | 0.32       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.051     |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.0583    |
|    value_loss           | 0.0775     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 7.22     |
| time/              |          |
|    fps             | 723      |
|    iterations      | 149      |
|    time_elapsed    | 421      |
|    total_timesteps | 305152   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 139        |
|    ep_rew_mean          | 6.69       |
| time/                   |            |
|    fps                  | 723        |
|    iterations           | 150        |
|    time_elapsed         | 424        |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.39524496 |
|    clip_fraction        | 0.503      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.618     |
|    explained_variance   | 0.392      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0328    |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.047     |
|    value_loss           | 0.0731     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 138        |
|    ep_rew_mean          | 6.68       |
| time/                   |            |
|    fps                  | 724        |
|    iterations           | 151        |
|    time_elapsed         | 426        |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.37593678 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.588     |
|    explained_variance   | 0.345      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0776    |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.08       |
----------------------------------------
Eval num_timesteps=310000, episode_reward=8.00 +/- 3.46
Episode length: 159.20 +/- 40.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 159        |
|    mean_reward          | 8          |
| time/                   |            |
|    total_timesteps      | 310000     |
| train/                  |            |
|    approx_kl            | 0.37093228 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.573     |
|    explained_variance   | 0.414      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0473    |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.0793     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 6.45     |
| time/              |          |
|    fps             | 723      |
|    iterations      | 152      |
|    time_elapsed    | 430      |
|    total_timesteps | 311296   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 136        |
|    ep_rew_mean          | 6.72       |
| time/                   |            |
|    fps                  | 724        |
|    iterations           | 153        |
|    time_elapsed         | 432        |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.33088905 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.596     |
|    explained_variance   | 0.464      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0899    |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0609    |
|    value_loss           | 0.0686     |
----------------------------------------
Eval num_timesteps=315000, episode_reward=11.00 +/- 2.97
Episode length: 187.70 +/- 51.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 188       |
|    mean_reward          | 11        |
| time/                   |           |
|    total_timesteps      | 315000    |
| train/                  |           |
|    approx_kl            | 0.6600022 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.549    |
|    explained_variance   | 0.412     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0694   |
|    n_updates            | 1530      |
|    policy_gradient_loss | -0.0561   |
|    value_loss           | 0.0723    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 7.18     |
| time/              |          |
|    fps             | 723      |
|    iterations      | 154      |
|    time_elapsed    | 436      |
|    total_timesteps | 315392   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 146        |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 724        |
|    iterations           | 155        |
|    time_elapsed         | 438        |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.41484648 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.561     |
|    explained_variance   | 0.521      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0479    |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.0625    |
|    value_loss           | 0.0675     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 150        |
|    ep_rew_mean          | 7.98       |
| time/                   |            |
|    fps                  | 724        |
|    iterations           | 156        |
|    time_elapsed         | 440        |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.53894037 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.589     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0602    |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.0539    |
|    value_loss           | 0.0601     |
----------------------------------------
Eval num_timesteps=320000, episode_reward=9.30 +/- 2.53
Episode length: 170.10 +/- 45.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 170       |
|    mean_reward          | 9.3       |
| time/                   |           |
|    total_timesteps      | 320000    |
| train/                  |           |
|    approx_kl            | 0.3486667 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.599    |
|    explained_variance   | 0.692     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0809   |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.0644   |
|    value_loss           | 0.0565    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 8.3      |
| time/              |          |
|    fps             | 723      |
|    iterations      | 157      |
|    time_elapsed    | 444      |
|    total_timesteps | 321536   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 159       |
|    ep_rew_mean          | 8.68      |
| time/                   |           |
|    fps                  | 724       |
|    iterations           | 158       |
|    time_elapsed         | 446       |
|    total_timesteps      | 323584    |
| train/                  |           |
|    approx_kl            | 0.3429731 |
|    clip_fraction        | 0.469     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.573    |
|    explained_variance   | 0.641     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0818   |
|    n_updates            | 1570      |
|    policy_gradient_loss | -0.0628   |
|    value_loss           | 0.0621    |
---------------------------------------
Eval num_timesteps=325000, episode_reward=10.70 +/- 3.38
Episode length: 186.50 +/- 35.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 186       |
|    mean_reward          | 10.7      |
| time/                   |           |
|    total_timesteps      | 325000    |
| train/                  |           |
|    approx_kl            | 0.4259438 |
|    clip_fraction        | 0.453     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.551    |
|    explained_variance   | 0.536     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0794   |
|    n_updates            | 1580      |
|    policy_gradient_loss | -0.0586   |
|    value_loss           | 0.069     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 8.9      |
| time/              |          |
|    fps             | 723      |
|    iterations      | 159      |
|    time_elapsed    | 450      |
|    total_timesteps | 325632   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 9.38       |
| time/                   |            |
|    fps                  | 724        |
|    iterations           | 160        |
|    time_elapsed         | 452        |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.42423305 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.514     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0831    |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0591    |
|    value_loss           | 0.0628     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 170       |
|    ep_rew_mean          | 9.52      |
| time/                   |           |
|    fps                  | 724       |
|    iterations           | 161       |
|    time_elapsed         | 454       |
|    total_timesteps      | 329728    |
| train/                  |           |
|    approx_kl            | 0.4071663 |
|    clip_fraction        | 0.428     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.518    |
|    explained_variance   | 0.626     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0809   |
|    n_updates            | 1600      |
|    policy_gradient_loss | -0.0569   |
|    value_loss           | 0.0603    |
---------------------------------------
Eval num_timesteps=330000, episode_reward=2.50 +/- 0.81
Episode length: 95.00 +/- 15.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 95        |
|    mean_reward          | 2.5       |
| time/                   |           |
|    total_timesteps      | 330000    |
| train/                  |           |
|    approx_kl            | 0.4252265 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.56     |
|    explained_variance   | 0.632     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0687   |
|    n_updates            | 1610      |
|    policy_gradient_loss | -0.053    |
|    value_loss           | 0.0614    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 8.92     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 162      |
|    time_elapsed    | 457      |
|    total_timesteps | 331776   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 160       |
|    ep_rew_mean          | 8.58      |
| time/                   |           |
|    fps                  | 724       |
|    iterations           | 163       |
|    time_elapsed         | 460       |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.4168742 |
|    clip_fraction        | 0.479     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.597    |
|    explained_variance   | 0.51      |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0602   |
|    n_updates            | 1620      |
|    policy_gradient_loss | -0.0583   |
|    value_loss           | 0.0793    |
---------------------------------------
Eval num_timesteps=335000, episode_reward=2.90 +/- 3.24
Episode length: 85.90 +/- 40.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 85.9       |
|    mean_reward          | 2.9        |
| time/                   |            |
|    total_timesteps      | 335000     |
| train/                  |            |
|    approx_kl            | 0.39369237 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.608     |
|    explained_variance   | 0.46       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0983    |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.0569    |
|    value_loss           | 0.0785     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 8.54     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 164      |
|    time_elapsed    | 463      |
|    total_timesteps | 335872   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 162       |
|    ep_rew_mean          | 8.58      |
| time/                   |           |
|    fps                  | 725       |
|    iterations           | 165       |
|    time_elapsed         | 466       |
|    total_timesteps      | 337920    |
| train/                  |           |
|    approx_kl            | 0.3725765 |
|    clip_fraction        | 0.488     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.597    |
|    explained_variance   | 0.569     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0882   |
|    n_updates            | 1640      |
|    policy_gradient_loss | -0.0575   |
|    value_loss           | 0.0651    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 165       |
|    ep_rew_mean          | 8.63      |
| time/                   |           |
|    fps                  | 725       |
|    iterations           | 166       |
|    time_elapsed         | 468       |
|    total_timesteps      | 339968    |
| train/                  |           |
|    approx_kl            | 0.4880023 |
|    clip_fraction        | 0.485     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.568    |
|    explained_variance   | 0.62      |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0657   |
|    n_updates            | 1650      |
|    policy_gradient_loss | -0.0588   |
|    value_loss           | 0.0639    |
---------------------------------------
Eval num_timesteps=340000, episode_reward=8.70 +/- 3.20
Episode length: 171.40 +/- 45.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 171        |
|    mean_reward          | 8.7        |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.53244776 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.547     |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0751    |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.0594     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 8.63     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 167      |
|    time_elapsed    | 472      |
|    total_timesteps | 342016   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 161       |
|    ep_rew_mean          | 8.38      |
| time/                   |           |
|    fps                  | 724       |
|    iterations           | 168       |
|    time_elapsed         | 474       |
|    total_timesteps      | 344064    |
| train/                  |           |
|    approx_kl            | 0.4464175 |
|    clip_fraction        | 0.467     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.518    |
|    explained_variance   | 0.594     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0607   |
|    n_updates            | 1670      |
|    policy_gradient_loss | -0.063    |
|    value_loss           | 0.066     |
---------------------------------------
Eval num_timesteps=345000, episode_reward=11.30 +/- 2.24
Episode length: 185.90 +/- 38.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 186       |
|    mean_reward          | 11.3      |
| time/                   |           |
|    total_timesteps      | 345000    |
| train/                  |           |
|    approx_kl            | 0.4366095 |
|    clip_fraction        | 0.448     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.494    |
|    explained_variance   | 0.612     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.078    |
|    n_updates            | 1680      |
|    policy_gradient_loss | -0.0531   |
|    value_loss           | 0.0675    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 8.5      |
| time/              |          |
|    fps             | 723      |
|    iterations      | 169      |
|    time_elapsed    | 478      |
|    total_timesteps | 346112   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 168       |
|    ep_rew_mean          | 9         |
| time/                   |           |
|    fps                  | 724       |
|    iterations           | 170       |
|    time_elapsed         | 480       |
|    total_timesteps      | 348160    |
| train/                  |           |
|    approx_kl            | 0.4406762 |
|    clip_fraction        | 0.45      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.491    |
|    explained_variance   | 0.678     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0764   |
|    n_updates            | 1690      |
|    policy_gradient_loss | -0.0545   |
|    value_loss           | 0.0601    |
---------------------------------------
Eval num_timesteps=350000, episode_reward=8.50 +/- 2.46
Episode length: 174.30 +/- 48.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 174        |
|    mean_reward          | 8.5        |
| time/                   |            |
|    total_timesteps      | 350000     |
| train/                  |            |
|    approx_kl            | 0.37844077 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.509     |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0629    |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0606    |
|    value_loss           | 0.0553     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 9.43     |
| time/              |          |
|    fps             | 722      |
|    iterations      | 171      |
|    time_elapsed    | 484      |
|    total_timesteps | 350208   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | 9.78       |
| time/                   |            |
|    fps                  | 723        |
|    iterations           | 172        |
|    time_elapsed         | 486        |
|    total_timesteps      | 352256     |
| train/                  |            |
|    approx_kl            | 0.38861606 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.515     |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0863    |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.0508    |
|    value_loss           | 0.0701     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 181       |
|    ep_rew_mean          | 10.2      |
| time/                   |           |
|    fps                  | 724       |
|    iterations           | 173       |
|    time_elapsed         | 489       |
|    total_timesteps      | 354304    |
| train/                  |           |
|    approx_kl            | 0.3247328 |
|    clip_fraction        | 0.457     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.55     |
|    explained_variance   | 0.6       |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0695   |
|    n_updates            | 1720      |
|    policy_gradient_loss | -0.056    |
|    value_loss           | 0.072     |
---------------------------------------
Eval num_timesteps=355000, episode_reward=9.30 +/- 2.05
Episode length: 179.60 +/- 32.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 180        |
|    mean_reward          | 9.3        |
| time/                   |            |
|    total_timesteps      | 355000     |
| train/                  |            |
|    approx_kl            | 0.34883517 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.549     |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0681    |
|    n_updates            | 1730       |
|    policy_gradient_loss | -0.0589    |
|    value_loss           | 0.0658     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 723      |
|    iterations      | 174      |
|    time_elapsed    | 492      |
|    total_timesteps | 356352   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 179       |
|    ep_rew_mean          | 10.4      |
| time/                   |           |
|    fps                  | 723       |
|    iterations           | 175       |
|    time_elapsed         | 495       |
|    total_timesteps      | 358400    |
| train/                  |           |
|    approx_kl            | 0.4251083 |
|    clip_fraction        | 0.458     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.543    |
|    explained_variance   | 0.587     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0509   |
|    n_updates            | 1740      |
|    policy_gradient_loss | -0.0604   |
|    value_loss           | 0.0593    |
---------------------------------------
Eval num_timesteps=360000, episode_reward=9.70 +/- 2.57
Episode length: 182.90 +/- 48.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 183        |
|    mean_reward          | 9.7        |
| time/                   |            |
|    total_timesteps      | 360000     |
| train/                  |            |
|    approx_kl            | 0.35352582 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.547     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0424    |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.0545    |
|    value_loss           | 0.0599     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 722      |
|    iterations      | 176      |
|    time_elapsed    | 498      |
|    total_timesteps | 360448   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 10.8       |
| time/                   |            |
|    fps                  | 723        |
|    iterations           | 177        |
|    time_elapsed         | 501        |
|    total_timesteps      | 362496     |
| train/                  |            |
|    approx_kl            | 0.51858544 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.529     |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0926    |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0576    |
|    value_loss           | 0.0594     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 185        |
|    ep_rew_mean          | 10.7       |
| time/                   |            |
|    fps                  | 723        |
|    iterations           | 178        |
|    time_elapsed         | 503        |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.38203442 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.56      |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.095     |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.0601    |
|    value_loss           | 0.058      |
----------------------------------------
Eval num_timesteps=365000, episode_reward=10.70 +/- 2.05
Episode length: 196.80 +/- 42.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 197        |
|    mean_reward          | 10.7       |
| time/                   |            |
|    total_timesteps      | 365000     |
| train/                  |            |
|    approx_kl            | 0.46778202 |
|    clip_fraction        | 0.486      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.578     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0857    |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0614    |
|    value_loss           | 0.0632     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 722      |
|    iterations      | 179      |
|    time_elapsed    | 507      |
|    total_timesteps | 366592   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | 10.5       |
| time/                   |            |
|    fps                  | 722        |
|    iterations           | 180        |
|    time_elapsed         | 510        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.32735616 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.609     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0511    |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.0608    |
|    value_loss           | 0.0664     |
----------------------------------------
Eval num_timesteps=370000, episode_reward=10.60 +/- 2.20
Episode length: 201.50 +/- 27.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 10.6       |
| time/                   |            |
|    total_timesteps      | 370000     |
| train/                  |            |
|    approx_kl            | 0.33633307 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.577     |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0783    |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0638    |
|    value_loss           | 0.067      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 181      |
|    time_elapsed    | 514      |
|    total_timesteps | 370688   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 180      |
|    ep_rew_mean          | 10.3     |
| time/                   |          |
|    fps                  | 721      |
|    iterations           | 182      |
|    time_elapsed         | 516      |
|    total_timesteps      | 372736   |
| train/                  |          |
|    approx_kl            | 0.364627 |
|    clip_fraction        | 0.479    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.596   |
|    explained_variance   | 0.724    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0656  |
|    n_updates            | 1810     |
|    policy_gradient_loss | -0.0626  |
|    value_loss           | 0.054    |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 10.1       |
| time/                   |            |
|    fps                  | 722        |
|    iterations           | 183        |
|    time_elapsed         | 519        |
|    total_timesteps      | 374784     |
| train/                  |            |
|    approx_kl            | 0.44625413 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.571     |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.089     |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0617    |
|    value_loss           | 0.0566     |
----------------------------------------
Eval num_timesteps=375000, episode_reward=7.50 +/- 1.91
Episode length: 153.60 +/- 26.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 154        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 375000     |
| train/                  |            |
|    approx_kl            | 0.41342854 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.582     |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0676    |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.0649    |
|    value_loss           | 0.0574     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 9.93     |
| time/              |          |
|    fps             | 721      |
|    iterations      | 184      |
|    time_elapsed    | 522      |
|    total_timesteps | 376832   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 173       |
|    ep_rew_mean          | 9.62      |
| time/                   |           |
|    fps                  | 721       |
|    iterations           | 185       |
|    time_elapsed         | 524       |
|    total_timesteps      | 378880    |
| train/                  |           |
|    approx_kl            | 0.4689415 |
|    clip_fraction        | 0.497     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.582    |
|    explained_variance   | 0.628     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0835   |
|    n_updates            | 1840      |
|    policy_gradient_loss | -0.0629   |
|    value_loss           | 0.0683    |
---------------------------------------
Eval num_timesteps=380000, episode_reward=10.80 +/- 2.52
Episode length: 184.10 +/- 40.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 184        |
|    mean_reward          | 10.8       |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.44893152 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.552     |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.11      |
|    n_updates            | 1850       |
|    policy_gradient_loss | -0.0626    |
|    value_loss           | 0.0652     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 9.51     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 186      |
|    time_elapsed    | 528      |
|    total_timesteps | 380928   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 9.19       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 187        |
|    time_elapsed         | 530        |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.53144646 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.547     |
|    explained_variance   | 0.716      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0785    |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0625    |
|    value_loss           | 0.0547     |
----------------------------------------
Eval num_timesteps=385000, episode_reward=9.70 +/- 2.41
Episode length: 183.40 +/- 32.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 183       |
|    mean_reward          | 9.7       |
| time/                   |           |
|    total_timesteps      | 385000    |
| train/                  |           |
|    approx_kl            | 0.5397215 |
|    clip_fraction        | 0.477     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.528    |
|    explained_variance   | 0.656     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0621   |
|    n_updates            | 1870      |
|    policy_gradient_loss | -0.0628   |
|    value_loss           | 0.0651    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 9.26     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 188      |
|    time_elapsed    | 534      |
|    total_timesteps | 385024   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | 8.76       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 189        |
|    time_elapsed         | 536        |
|    total_timesteps      | 387072     |
| train/                  |            |
|    approx_kl            | 0.63635373 |
|    clip_fraction        | 0.486      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.533     |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0782    |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0562    |
|    value_loss           | 0.0726     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 163        |
|    ep_rew_mean          | 8.84       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 190        |
|    time_elapsed         | 539        |
|    total_timesteps      | 389120     |
| train/                  |            |
|    approx_kl            | 0.44867682 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.557     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0895    |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0593    |
|    value_loss           | 0.066      |
----------------------------------------
Eval num_timesteps=390000, episode_reward=11.70 +/- 3.23
Episode length: 194.50 +/- 32.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 194        |
|    mean_reward          | 11.7       |
| time/                   |            |
|    total_timesteps      | 390000     |
| train/                  |            |
|    approx_kl            | 0.44336843 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.477     |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0762    |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0604    |
|    value_loss           | 0.0639     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 8.93     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 191      |
|    time_elapsed    | 542      |
|    total_timesteps | 391168   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 167        |
|    ep_rew_mean          | 9.08       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 192        |
|    time_elapsed         | 544        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.44058973 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.509     |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.086     |
|    n_updates            | 1910       |
|    policy_gradient_loss | -0.0597    |
|    value_loss           | 0.0538     |
----------------------------------------
Eval num_timesteps=395000, episode_reward=9.40 +/- 2.24
Episode length: 169.30 +/- 40.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 169       |
|    mean_reward          | 9.4       |
| time/                   |           |
|    total_timesteps      | 395000    |
| train/                  |           |
|    approx_kl            | 0.4591329 |
|    clip_fraction        | 0.445     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.494    |
|    explained_variance   | 0.706     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0659   |
|    n_updates            | 1920      |
|    policy_gradient_loss | -0.0601   |
|    value_loss           | 0.0617    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 9.18     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 193      |
|    time_elapsed    | 548      |
|    total_timesteps | 395264   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 168       |
|    ep_rew_mean          | 9.17      |
| time/                   |           |
|    fps                  | 721       |
|    iterations           | 194       |
|    time_elapsed         | 550       |
|    total_timesteps      | 397312    |
| train/                  |           |
|    approx_kl            | 0.5053436 |
|    clip_fraction        | 0.449     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.501    |
|    explained_variance   | 0.674     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0849   |
|    n_updates            | 1930      |
|    policy_gradient_loss | -0.057    |
|    value_loss           | 0.0622    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 172        |
|    ep_rew_mean          | 9.5        |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 195        |
|    time_elapsed         | 553        |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.43882984 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.526     |
|    explained_variance   | 0.718      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0835    |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.0625     |
----------------------------------------
Eval num_timesteps=400000, episode_reward=8.40 +/- 3.14
Episode length: 172.00 +/- 58.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 172        |
|    mean_reward          | 8.4        |
| time/                   |            |
|    total_timesteps      | 400000     |
| train/                  |            |
|    approx_kl            | 0.43588305 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.517     |
|    explained_variance   | 0.647      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0794    |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.0576    |
|    value_loss           | 0.0722     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 9.61     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 196      |
|    time_elapsed    | 556      |
|    total_timesteps | 401408   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 176       |
|    ep_rew_mean          | 9.69      |
| time/                   |           |
|    fps                  | 721       |
|    iterations           | 197       |
|    time_elapsed         | 559       |
|    total_timesteps      | 403456    |
| train/                  |           |
|    approx_kl            | 0.5084504 |
|    clip_fraction        | 0.479     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.514    |
|    explained_variance   | 0.616     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0614   |
|    n_updates            | 1960      |
|    policy_gradient_loss | -0.0585   |
|    value_loss           | 0.0761    |
---------------------------------------
Eval num_timesteps=405000, episode_reward=12.60 +/- 2.46
Episode length: 204.90 +/- 31.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 205        |
|    mean_reward          | 12.6       |
| time/                   |            |
|    total_timesteps      | 405000     |
| train/                  |            |
|    approx_kl            | 0.45825848 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.516     |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.078     |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.0577    |
|    value_loss           | 0.0697     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 9.81     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 198      |
|    time_elapsed    | 562      |
|    total_timesteps | 405504   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 175      |
|    ep_rew_mean          | 9.73     |
| time/                   |          |
|    fps                  | 720      |
|    iterations           | 199      |
|    time_elapsed         | 565      |
|    total_timesteps      | 407552   |
| train/                  |          |
|    approx_kl            | 0.533105 |
|    clip_fraction        | 0.476    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.505   |
|    explained_variance   | 0.672    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0951  |
|    n_updates            | 1980     |
|    policy_gradient_loss | -0.0552  |
|    value_loss           | 0.0596   |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 9.74       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 200        |
|    time_elapsed         | 567        |
|    total_timesteps      | 409600     |
| train/                  |            |
|    approx_kl            | 0.57064545 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.498     |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0609    |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.0559    |
|    value_loss           | 0.0632     |
----------------------------------------
Eval num_timesteps=410000, episode_reward=10.90 +/- 2.51
Episode length: 181.60 +/- 32.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 182        |
|    mean_reward          | 10.9       |
| time/                   |            |
|    total_timesteps      | 410000     |
| train/                  |            |
|    approx_kl            | 0.47523808 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.53      |
|    explained_variance   | 0.6        |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0898    |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.0698     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 9.49     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 201      |
|    time_elapsed    | 571      |
|    total_timesteps | 411648   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | 9.26       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 202        |
|    time_elapsed         | 573        |
|    total_timesteps      | 413696     |
| train/                  |            |
|    approx_kl            | 0.42210126 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.54      |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0567    |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.0624    |
|    value_loss           | 0.0751     |
----------------------------------------
Eval num_timesteps=415000, episode_reward=8.70 +/- 2.87
Episode length: 157.00 +/- 33.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 157       |
|    mean_reward          | 8.7       |
| time/                   |           |
|    total_timesteps      | 415000    |
| train/                  |           |
|    approx_kl            | 0.3884076 |
|    clip_fraction        | 0.492     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.58     |
|    explained_variance   | 0.626     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0711   |
|    n_updates            | 2020      |
|    policy_gradient_loss | -0.0555   |
|    value_loss           | 0.0743    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 9.4      |
| time/              |          |
|    fps             | 720      |
|    iterations      | 203      |
|    time_elapsed    | 577      |
|    total_timesteps | 415744   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 167        |
|    ep_rew_mean          | 9.09       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 204        |
|    time_elapsed         | 579        |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.35325885 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.512     |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0742    |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.0563    |
|    value_loss           | 0.0651     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 167        |
|    ep_rew_mean          | 9.09       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 205        |
|    time_elapsed         | 581        |
|    total_timesteps      | 419840     |
| train/                  |            |
|    approx_kl            | 0.42309305 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.553     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0825    |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.0578    |
|    value_loss           | 0.0785     |
----------------------------------------
Eval num_timesteps=420000, episode_reward=9.20 +/- 2.99
Episode length: 170.30 +/- 36.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 170        |
|    mean_reward          | 9.2        |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.44533595 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.55      |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0689    |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0504    |
|    value_loss           | 0.0748     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 8.87     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 206      |
|    time_elapsed    | 585      |
|    total_timesteps | 421888   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 9.1        |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 207        |
|    time_elapsed         | 587        |
|    total_timesteps      | 423936     |
| train/                  |            |
|    approx_kl            | 0.46914217 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.554     |
|    explained_variance   | 0.467      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0733    |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.0835     |
----------------------------------------
Eval num_timesteps=425000, episode_reward=12.70 +/- 4.12
Episode length: 221.60 +/- 66.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 222        |
|    mean_reward          | 12.7       |
| time/                   |            |
|    total_timesteps      | 425000     |
| train/                  |            |
|    approx_kl            | 0.45235312 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.522     |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.033     |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.0582    |
|    value_loss           | 0.0763     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 8.96     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 208      |
|    time_elapsed    | 591      |
|    total_timesteps | 425984   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 9.24       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 209        |
|    time_elapsed         | 593        |
|    total_timesteps      | 428032     |
| train/                  |            |
|    approx_kl            | 0.47037762 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.498     |
|    explained_variance   | 0.516      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0882    |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.0568    |
|    value_loss           | 0.0679     |
----------------------------------------
Eval num_timesteps=430000, episode_reward=10.40 +/- 2.42
Episode length: 191.30 +/- 42.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 191        |
|    mean_reward          | 10.4       |
| time/                   |            |
|    total_timesteps      | 430000     |
| train/                  |            |
|    approx_kl            | 0.42985517 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.502     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0804    |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.0557    |
|    value_loss           | 0.0566     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 9.44     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 210      |
|    time_elapsed    | 597      |
|    total_timesteps | 430080   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 165        |
|    ep_rew_mean          | 9.18       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 211        |
|    time_elapsed         | 599        |
|    total_timesteps      | 432128     |
| train/                  |            |
|    approx_kl            | 0.35026574 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.511     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0566    |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.042     |
|    value_loss           | 0.0583     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 9.49       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 212        |
|    time_elapsed         | 602        |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.39343062 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.475     |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0863    |
|    n_updates            | 2110       |
|    policy_gradient_loss | -0.0547    |
|    value_loss           | 0.0659     |
----------------------------------------
Eval num_timesteps=435000, episode_reward=10.00 +/- 3.07
Episode length: 184.70 +/- 44.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 185        |
|    mean_reward          | 10         |
| time/                   |            |
|    total_timesteps      | 435000     |
| train/                  |            |
|    approx_kl            | 0.34489673 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.532     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0694    |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0571    |
|    value_loss           | 0.0704     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 9.62     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 213      |
|    time_elapsed    | 605      |
|    total_timesteps | 436224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 170        |
|    ep_rew_mean          | 9.64       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 214        |
|    time_elapsed         | 608        |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.36015326 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.551     |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0789    |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.0617    |
|    value_loss           | 0.0657     |
----------------------------------------
Eval num_timesteps=440000, episode_reward=8.00 +/- 2.90
Episode length: 157.90 +/- 31.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 158        |
|    mean_reward          | 8          |
| time/                   |            |
|    total_timesteps      | 440000     |
| train/                  |            |
|    approx_kl            | 0.42442688 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.508     |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0459    |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.061     |
|    value_loss           | 0.0614     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 9.59     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 215      |
|    time_elapsed    | 611      |
|    total_timesteps | 440320   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 169        |
|    ep_rew_mean          | 9.7        |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 216        |
|    time_elapsed         | 614        |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.37924707 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.497     |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0393    |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.0584     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 166       |
|    ep_rew_mean          | 9.52      |
| time/                   |           |
|    fps                  | 720       |
|    iterations           | 217       |
|    time_elapsed         | 616       |
|    total_timesteps      | 444416    |
| train/                  |           |
|    approx_kl            | 0.5322962 |
|    clip_fraction        | 0.437     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.489    |
|    explained_variance   | 0.686     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.074    |
|    n_updates            | 2160      |
|    policy_gradient_loss | -0.0638   |
|    value_loss           | 0.0632    |
---------------------------------------
Eval num_timesteps=445000, episode_reward=10.80 +/- 3.22
Episode length: 200.40 +/- 60.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 200        |
|    mean_reward          | 10.8       |
| time/                   |            |
|    total_timesteps      | 445000     |
| train/                  |            |
|    approx_kl            | 0.45078826 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.489     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.068     |
|    n_updates            | 2170       |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.0728     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 9.51     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 218      |
|    time_elapsed    | 620      |
|    total_timesteps | 446464   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 165       |
|    ep_rew_mean          | 9.28      |
| time/                   |           |
|    fps                  | 720       |
|    iterations           | 219       |
|    time_elapsed         | 622       |
|    total_timesteps      | 448512    |
| train/                  |           |
|    approx_kl            | 0.5304034 |
|    clip_fraction        | 0.445     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.504    |
|    explained_variance   | 0.597     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0779   |
|    n_updates            | 2180      |
|    policy_gradient_loss | -0.0517   |
|    value_loss           | 0.0739    |
---------------------------------------
Eval num_timesteps=450000, episode_reward=6.10 +/- 2.91
Episode length: 141.00 +/- 31.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 141        |
|    mean_reward          | 6.1        |
| time/                   |            |
|    total_timesteps      | 450000     |
| train/                  |            |
|    approx_kl            | 0.47548383 |
|    clip_fraction        | 0.503      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.546     |
|    explained_variance   | 0.553      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0871    |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.061     |
|    value_loss           | 0.0711     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 8.71     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 220      |
|    time_elapsed    | 625      |
|    total_timesteps | 450560   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | 8.73       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 221        |
|    time_elapsed         | 628        |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.49581277 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.556     |
|    explained_variance   | 0.543      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0592    |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.0688     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 167        |
|    ep_rew_mean          | 9.01       |
| time/                   |            |
|    fps                  | 721        |
|    iterations           | 222        |
|    time_elapsed         | 630        |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.44812748 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.484     |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0528    |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.0509    |
|    value_loss           | 0.0721     |
----------------------------------------
Eval num_timesteps=455000, episode_reward=11.20 +/- 3.46
Episode length: 202.50 +/- 51.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 11.2       |
| time/                   |            |
|    total_timesteps      | 455000     |
| train/                  |            |
|    approx_kl            | 0.60982066 |
|    clip_fraction        | 0.443      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.509     |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0694    |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0573    |
|    value_loss           | 0.0734     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 9.23     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 223      |
|    time_elapsed    | 634      |
|    total_timesteps | 456704   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | 9.19       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 224        |
|    time_elapsed         | 636        |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.41128084 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.509     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0437    |
|    n_updates            | 2230       |
|    policy_gradient_loss | -0.051     |
|    value_loss           | 0.0722     |
----------------------------------------
Eval num_timesteps=460000, episode_reward=13.20 +/- 3.06
Episode length: 215.30 +/- 42.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 13.2       |
| time/                   |            |
|    total_timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.46516615 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.505     |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0435    |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.0674     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 9.32     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 225      |
|    time_elapsed    | 640      |
|    total_timesteps | 460800   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | 9.09       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 226        |
|    time_elapsed         | 642        |
|    total_timesteps      | 462848     |
| train/                  |            |
|    approx_kl            | 0.39386776 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.494     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0751    |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.0544    |
|    value_loss           | 0.073      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | 9.44       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 227        |
|    time_elapsed         | 645        |
|    total_timesteps      | 464896     |
| train/                  |            |
|    approx_kl            | 0.44039422 |
|    clip_fraction        | 0.445      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.529     |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0813    |
|    n_updates            | 2260       |
|    policy_gradient_loss | -0.0559    |
|    value_loss           | 0.0608     |
----------------------------------------
Eval num_timesteps=465000, episode_reward=10.00 +/- 2.32
Episode length: 186.80 +/- 38.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 187        |
|    mean_reward          | 10         |
| time/                   |            |
|    total_timesteps      | 465000     |
| train/                  |            |
|    approx_kl            | 0.33938834 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.563     |
|    explained_variance   | 0.714      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0974    |
|    n_updates            | 2270       |
|    policy_gradient_loss | -0.061     |
|    value_loss           | 0.0601     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 9.82     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 228      |
|    time_elapsed    | 649      |
|    total_timesteps | 466944   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 176       |
|    ep_rew_mean          | 9.8       |
| time/                   |           |
|    fps                  | 719       |
|    iterations           | 229       |
|    time_elapsed         | 651       |
|    total_timesteps      | 468992    |
| train/                  |           |
|    approx_kl            | 0.4199983 |
|    clip_fraction        | 0.436     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.54     |
|    explained_variance   | 0.708     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0738   |
|    n_updates            | 2280      |
|    policy_gradient_loss | -0.0587   |
|    value_loss           | 0.0663    |
---------------------------------------
Eval num_timesteps=470000, episode_reward=8.70 +/- 4.15
Episode length: 160.80 +/- 47.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 161        |
|    mean_reward          | 8.7        |
| time/                   |            |
|    total_timesteps      | 470000     |
| train/                  |            |
|    approx_kl            | 0.41808933 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.519     |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0296    |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0589    |
|    value_loss           | 0.074      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 9.48     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 230      |
|    time_elapsed    | 654      |
|    total_timesteps | 471040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 155        |
|    ep_rew_mean          | 8.22       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 231        |
|    time_elapsed         | 657        |
|    total_timesteps      | 473088     |
| train/                  |            |
|    approx_kl            | 0.39326307 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.562     |
|    explained_variance   | 0.549      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0593    |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.0869     |
----------------------------------------
Eval num_timesteps=475000, episode_reward=6.40 +/- 2.01
Episode length: 142.00 +/- 36.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 142       |
|    mean_reward          | 6.4       |
| time/                   |           |
|    total_timesteps      | 475000    |
| train/                  |           |
|    approx_kl            | 0.5389725 |
|    clip_fraction        | 0.502     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.575    |
|    explained_variance   | 0.452     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0688   |
|    n_updates            | 2310      |
|    policy_gradient_loss | -0.0474   |
|    value_loss           | 0.0841    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 232      |
|    time_elapsed    | 660      |
|    total_timesteps | 475136   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 143        |
|    ep_rew_mean          | 7.3        |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 233        |
|    time_elapsed         | 663        |
|    total_timesteps      | 477184     |
| train/                  |            |
|    approx_kl            | 0.45904562 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.542     |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0613    |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.0535    |
|    value_loss           | 0.0894     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 139        |
|    ep_rew_mean          | 6.93       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 234        |
|    time_elapsed         | 665        |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.54239863 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.5       |
|    explained_variance   | 0.562      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0402    |
|    n_updates            | 2330       |
|    policy_gradient_loss | -0.0511    |
|    value_loss           | 0.0826     |
----------------------------------------
Eval num_timesteps=480000, episode_reward=9.40 +/- 2.73
Episode length: 178.30 +/- 49.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 178        |
|    mean_reward          | 9.4        |
| time/                   |            |
|    total_timesteps      | 480000     |
| train/                  |            |
|    approx_kl            | 0.67856735 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.487     |
|    explained_variance   | 0.39       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0669    |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.052     |
|    value_loss           | 0.0924     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 6.69     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 235      |
|    time_elapsed    | 668      |
|    total_timesteps | 481280   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 137        |
|    ep_rew_mean          | 6.67       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 236        |
|    time_elapsed         | 671        |
|    total_timesteps      | 483328     |
| train/                  |            |
|    approx_kl            | 0.47361457 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.496     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0482    |
|    n_updates            | 2350       |
|    policy_gradient_loss | -0.0493    |
|    value_loss           | 0.0739     |
----------------------------------------
Eval num_timesteps=485000, episode_reward=7.90 +/- 2.17
Episode length: 156.50 +/- 34.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 156        |
|    mean_reward          | 7.9        |
| time/                   |            |
|    total_timesteps      | 485000     |
| train/                  |            |
|    approx_kl            | 0.52832955 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.503     |
|    explained_variance   | 0.429      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0818    |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.0533    |
|    value_loss           | 0.0811     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 6.62     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 237      |
|    time_elapsed    | 674      |
|    total_timesteps | 485376   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 145        |
|    ep_rew_mean          | 7.23       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 238        |
|    time_elapsed         | 676        |
|    total_timesteps      | 487424     |
| train/                  |            |
|    approx_kl            | 0.45711488 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.546     |
|    explained_variance   | 0.401      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0487    |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.0804     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 152        |
|    ep_rew_mean          | 7.69       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 239        |
|    time_elapsed         | 679        |
|    total_timesteps      | 489472     |
| train/                  |            |
|    approx_kl            | 0.49389577 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.503     |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0774    |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.056     |
|    value_loss           | 0.0752     |
----------------------------------------
Eval num_timesteps=490000, episode_reward=9.90 +/- 3.70
Episode length: 205.00 +/- 56.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 205        |
|    mean_reward          | 9.9        |
| time/                   |            |
|    total_timesteps      | 490000     |
| train/                  |            |
|    approx_kl            | 0.38650858 |
|    clip_fraction        | 0.416      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.479     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0637    |
|    n_updates            | 2390       |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.068      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 8.17     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 240      |
|    time_elapsed    | 683      |
|    total_timesteps | 491520   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 166       |
|    ep_rew_mean          | 8.74      |
| time/                   |           |
|    fps                  | 720       |
|    iterations           | 241       |
|    time_elapsed         | 685       |
|    total_timesteps      | 493568    |
| train/                  |           |
|    approx_kl            | 0.5157785 |
|    clip_fraction        | 0.446     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.47     |
|    explained_variance   | 0.675     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0707   |
|    n_updates            | 2400      |
|    policy_gradient_loss | -0.0552   |
|    value_loss           | 0.0674    |
---------------------------------------
Eval num_timesteps=495000, episode_reward=9.30 +/- 3.13
Episode length: 166.20 +/- 38.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 166       |
|    mean_reward          | 9.3       |
| time/                   |           |
|    total_timesteps      | 495000    |
| train/                  |           |
|    approx_kl            | 0.5991184 |
|    clip_fraction        | 0.405     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.431    |
|    explained_variance   | 0.682     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0731   |
|    n_updates            | 2410      |
|    policy_gradient_loss | -0.0499   |
|    value_loss           | 0.0733    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 8.95     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 242      |
|    time_elapsed    | 688      |
|    total_timesteps | 495616   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 9.61       |
| time/                   |            |
|    fps                  | 720        |
|    iterations           | 243        |
|    time_elapsed         | 691        |
|    total_timesteps      | 497664     |
| train/                  |            |
|    approx_kl            | 0.49648345 |
|    clip_fraction        | 0.407      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.442     |
|    explained_variance   | 0.641      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0543    |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.051     |
|    value_loss           | 0.0707     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 181       |
|    ep_rew_mean          | 10.2      |
| time/                   |           |
|    fps                  | 720       |
|    iterations           | 244       |
|    time_elapsed         | 693       |
|    total_timesteps      | 499712    |
| train/                  |           |
|    approx_kl            | 0.4516257 |
|    clip_fraction        | 0.409     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.441    |
|    explained_variance   | 0.669     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0772   |
|    n_updates            | 2430      |
|    policy_gradient_loss | -0.054    |
|    value_loss           | 0.0725    |
---------------------------------------
Eval num_timesteps=500000, episode_reward=10.70 +/- 3.93
Episode length: 191.00 +/- 44.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 191        |
|    mean_reward          | 10.7       |
| time/                   |            |
|    total_timesteps      | 500000     |
| train/                  |            |
|    approx_kl            | 0.45517915 |
|    clip_fraction        | 0.409      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.419     |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0807    |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0532    |
|    value_loss           | 0.0804     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 245      |
|    time_elapsed    | 697      |
|    total_timesteps | 501760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 198        |
|    ep_rew_mean          | 11.5       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 246        |
|    time_elapsed         | 699        |
|    total_timesteps      | 503808     |
| train/                  |            |
|    approx_kl            | 0.48811215 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.432     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0731    |
|    n_updates            | 2450       |
|    policy_gradient_loss | -0.0508    |
|    value_loss           | 0.0721     |
----------------------------------------
Eval num_timesteps=505000, episode_reward=11.20 +/- 1.17
Episode length: 187.80 +/- 17.24
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 188      |
|    mean_reward          | 11.2     |
| time/                   |          |
|    total_timesteps      | 505000   |
| train/                  |          |
|    approx_kl            | 0.557269 |
|    clip_fraction        | 0.429    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.425   |
|    explained_variance   | 0.649    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0565  |
|    n_updates            | 2460     |
|    policy_gradient_loss | -0.0547  |
|    value_loss           | 0.0694   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 12.1     |
| time/              |          |
|    fps             | 718      |
|    iterations      | 247      |
|    time_elapsed    | 703      |
|    total_timesteps | 505856   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 206        |
|    ep_rew_mean          | 12.4       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 248        |
|    time_elapsed         | 706        |
|    total_timesteps      | 507904     |
| train/                  |            |
|    approx_kl            | 0.39568183 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.449     |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0763    |
|    n_updates            | 2470       |
|    policy_gradient_loss | -0.0445    |
|    value_loss           | 0.0743     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 208        |
|    ep_rew_mean          | 12.6       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 249        |
|    time_elapsed         | 708        |
|    total_timesteps      | 509952     |
| train/                  |            |
|    approx_kl            | 0.35352272 |
|    clip_fraction        | 0.416      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.479     |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0607    |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.0526    |
|    value_loss           | 0.0672     |
----------------------------------------
Eval num_timesteps=510000, episode_reward=9.20 +/- 2.96
Episode length: 168.50 +/- 23.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 168        |
|    mean_reward          | 9.2        |
| time/                   |            |
|    total_timesteps      | 510000     |
| train/                  |            |
|    approx_kl            | 0.35645443 |
|    clip_fraction        | 0.428      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.505     |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0574    |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.07       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | 12.6     |
| time/              |          |
|    fps             | 718      |
|    iterations      | 250      |
|    time_elapsed    | 712      |
|    total_timesteps | 512000   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 12.3       |
| time/                   |            |
|    fps                  | 719        |
|    iterations           | 251        |
|    time_elapsed         | 714        |
|    total_timesteps      | 514048     |
| train/                  |            |
|    approx_kl            | 0.43931824 |
|    clip_fraction        | 0.417      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.434     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0613    |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.051     |
|    value_loss           | 0.0652     |
----------------------------------------
Eval num_timesteps=515000, episode_reward=10.30 +/- 4.24
Episode length: 187.50 +/- 48.09
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 188      |
|    mean_reward          | 10.3     |
| time/                   |          |
|    total_timesteps      | 515000   |
| train/                  |          |
|    approx_kl            | 0.46166  |
|    clip_fraction        | 0.449    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.475   |
|    explained_variance   | 0.552    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0525  |
|    n_updates            | 2510     |
|    policy_gradient_loss | -0.0514  |
|    value_loss           | 0.0841   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 210      |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 718      |
|    iterations      | 252      |
|    time_elapsed    | 718      |
|    total_timesteps | 516096   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 11.9       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 253        |
|    time_elapsed         | 720        |
|    total_timesteps      | 518144     |
| train/                  |            |
|    approx_kl            | 0.44738355 |
|    clip_fraction        | 0.441      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.504     |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.054     |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.0684     |
----------------------------------------
Eval num_timesteps=520000, episode_reward=9.40 +/- 3.88
Episode length: 180.50 +/- 48.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 180       |
|    mean_reward          | 9.4       |
| time/                   |           |
|    total_timesteps      | 520000    |
| train/                  |           |
|    approx_kl            | 0.4122954 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.502    |
|    explained_variance   | 0.582     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0725   |
|    n_updates            | 2530      |
|    policy_gradient_loss | -0.0537   |
|    value_loss           | 0.0774    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 11.3     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 254      |
|    time_elapsed    | 724      |
|    total_timesteps | 520192   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 195       |
|    ep_rew_mean          | 11.1      |
| time/                   |           |
|    fps                  | 718       |
|    iterations           | 255       |
|    time_elapsed         | 727       |
|    total_timesteps      | 522240    |
| train/                  |           |
|    approx_kl            | 0.5242053 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.497    |
|    explained_variance   | 0.58      |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0516   |
|    n_updates            | 2540      |
|    policy_gradient_loss | -0.0566   |
|    value_loss           | 0.0786    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 195       |
|    ep_rew_mean          | 11.1      |
| time/                   |           |
|    fps                  | 718       |
|    iterations           | 256       |
|    time_elapsed         | 729       |
|    total_timesteps      | 524288    |
| train/                  |           |
|    approx_kl            | 0.4742331 |
|    clip_fraction        | 0.425     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.495    |
|    explained_variance   | 0.695     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0706   |
|    n_updates            | 2550      |
|    policy_gradient_loss | -0.0592   |
|    value_loss           | 0.0658    |
---------------------------------------
Eval num_timesteps=525000, episode_reward=10.80 +/- 4.56
Episode length: 195.50 +/- 54.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 196       |
|    mean_reward          | 10.8      |
| time/                   |           |
|    total_timesteps      | 525000    |
| train/                  |           |
|    approx_kl            | 0.4009764 |
|    clip_fraction        | 0.447     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.511    |
|    explained_variance   | 0.684     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0723   |
|    n_updates            | 2560      |
|    policy_gradient_loss | -0.0567   |
|    value_loss           | 0.0654    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 257      |
|    time_elapsed    | 733      |
|    total_timesteps | 526336   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 195       |
|    ep_rew_mean          | 11        |
| time/                   |           |
|    fps                  | 717       |
|    iterations           | 258       |
|    time_elapsed         | 736       |
|    total_timesteps      | 528384    |
| train/                  |           |
|    approx_kl            | 0.4474489 |
|    clip_fraction        | 0.424     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.459    |
|    explained_variance   | 0.694     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0499   |
|    n_updates            | 2570      |
|    policy_gradient_loss | -0.0552   |
|    value_loss           | 0.0716    |
---------------------------------------
Eval num_timesteps=530000, episode_reward=8.20 +/- 2.52
Episode length: 158.40 +/- 36.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 158       |
|    mean_reward          | 8.2       |
| time/                   |           |
|    total_timesteps      | 530000    |
| train/                  |           |
|    approx_kl            | 0.6226477 |
|    clip_fraction        | 0.42      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.444    |
|    explained_variance   | 0.671     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0601   |
|    n_updates            | 2580      |
|    policy_gradient_loss | -0.0526   |
|    value_loss           | 0.0789    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 259      |
|    time_elapsed    | 739      |
|    total_timesteps | 530432   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 201       |
|    ep_rew_mean          | 11.4      |
| time/                   |           |
|    fps                  | 717       |
|    iterations           | 260       |
|    time_elapsed         | 741       |
|    total_timesteps      | 532480    |
| train/                  |           |
|    approx_kl            | 0.3812798 |
|    clip_fraction        | 0.425     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.491    |
|    explained_variance   | 0.624     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0729   |
|    n_updates            | 2590      |
|    policy_gradient_loss | -0.0508   |
|    value_loss           | 0.0719    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 202        |
|    ep_rew_mean          | 11.7       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 261        |
|    time_elapsed         | 744        |
|    total_timesteps      | 534528     |
| train/                  |            |
|    approx_kl            | 0.39217204 |
|    clip_fraction        | 0.425      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.467     |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0352    |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0469    |
|    value_loss           | 0.067      |
----------------------------------------
Eval num_timesteps=535000, episode_reward=15.50 +/- 2.80
Episode length: 257.40 +/- 41.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 257        |
|    mean_reward          | 15.5       |
| time/                   |            |
|    total_timesteps      | 535000     |
| train/                  |            |
|    approx_kl            | 0.44962156 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.453     |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0521    |
|    n_updates            | 2610       |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.08       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 11.9     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 262      |
|    time_elapsed    | 748      |
|    total_timesteps | 536576   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 209       |
|    ep_rew_mean          | 12.3      |
| time/                   |           |
|    fps                  | 717       |
|    iterations           | 263       |
|    time_elapsed         | 750       |
|    total_timesteps      | 538624    |
| train/                  |           |
|    approx_kl            | 0.3513633 |
|    clip_fraction        | 0.426     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.48     |
|    explained_variance   | 0.676     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0726   |
|    n_updates            | 2620      |
|    policy_gradient_loss | -0.0575   |
|    value_loss           | 0.0726    |
---------------------------------------
Eval num_timesteps=540000, episode_reward=13.60 +/- 5.48
Episode length: 224.80 +/- 61.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 225        |
|    mean_reward          | 13.6       |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.34935826 |
|    clip_fraction        | 0.417      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0584    |
|    n_updates            | 2630       |
|    policy_gradient_loss | -0.051     |
|    value_loss           | 0.0668     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 716      |
|    iterations      | 264      |
|    time_elapsed    | 754      |
|    total_timesteps | 540672   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 226        |
|    ep_rew_mean          | 13.7       |
| time/                   |            |
|    fps                  | 717        |
|    iterations           | 265        |
|    time_elapsed         | 756        |
|    total_timesteps      | 542720     |
| train/                  |            |
|    approx_kl            | 0.35755414 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.484     |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0616    |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.0489    |
|    value_loss           | 0.0646     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 226       |
|    ep_rew_mean          | 13.7      |
| time/                   |           |
|    fps                  | 717       |
|    iterations           | 266       |
|    time_elapsed         | 759       |
|    total_timesteps      | 544768    |
| train/                  |           |
|    approx_kl            | 0.4404595 |
|    clip_fraction        | 0.392     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.442    |
|    explained_variance   | 0.647     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0368   |
|    n_updates            | 2650      |
|    policy_gradient_loss | -0.0491   |
|    value_loss           | 0.0726    |
---------------------------------------
Eval num_timesteps=545000, episode_reward=14.50 +/- 4.08
Episode length: 248.70 +/- 57.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 249        |
|    mean_reward          | 14.5       |
| time/                   |            |
|    total_timesteps      | 545000     |
| train/                  |            |
|    approx_kl            | 0.42893514 |
|    clip_fraction        | 0.392      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.461     |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.07      |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.0435    |
|    value_loss           | 0.0649     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 14       |
| time/              |          |
|    fps             | 716      |
|    iterations      | 267      |
|    time_elapsed    | 763      |
|    total_timesteps | 546816   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 228       |
|    ep_rew_mean          | 14        |
| time/                   |           |
|    fps                  | 716       |
|    iterations           | 268       |
|    time_elapsed         | 765       |
|    total_timesteps      | 548864    |
| train/                  |           |
|    approx_kl            | 0.3848635 |
|    clip_fraction        | 0.431     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.499    |
|    explained_variance   | 0.742     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0708   |
|    n_updates            | 2670      |
|    policy_gradient_loss | -0.0578   |
|    value_loss           | 0.0587    |
---------------------------------------
Eval num_timesteps=550000, episode_reward=11.30 +/- 3.74
Episode length: 191.80 +/- 40.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 192        |
|    mean_reward          | 11.3       |
| time/                   |            |
|    total_timesteps      | 550000     |
| train/                  |            |
|    approx_kl            | 0.36156756 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.478     |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0867    |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.0783     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 14.3     |
| time/              |          |
|    fps             | 716      |
|    iterations      | 269      |
|    time_elapsed    | 769      |
|    total_timesteps | 550912   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 14.3       |
| time/                   |            |
|    fps                  | 716        |
|    iterations           | 270        |
|    time_elapsed         | 771        |
|    total_timesteps      | 552960     |
| train/                  |            |
|    approx_kl            | 0.45731634 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.487     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.00074    |
|    loss                 | 0.0142     |
|    n_updates            | 2690       |
|    policy_gradient_loss | -0.0477    |
|    value_loss           | 0.0725     |
----------------------------------------
Eval num_timesteps=555000, episode_reward=10.50 +/- 3.72
Episode length: 204.50 +/- 38.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 204       |
|    mean_reward          | 10.5      |
| time/                   |           |
|    total_timesteps      | 555000    |
| train/                  |           |
|    approx_kl            | 0.3397705 |
|    clip_fraction        | 0.445     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.515    |
|    explained_variance   | 0.741     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0549   |
|    n_updates            | 2700      |
|    policy_gradient_loss | -0.0519   |
|    value_loss           | 0.0648    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | 13.9     |
| time/              |          |
|    fps             | 715      |
|    iterations      | 271      |
|    time_elapsed    | 775      |
|    total_timesteps | 555008   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | 13.8       |
| time/                   |            |
|    fps                  | 716        |
|    iterations           | 272        |
|    time_elapsed         | 777        |
|    total_timesteps      | 557056     |
| train/                  |            |
|    approx_kl            | 0.46886188 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.497     |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.071     |
|    n_updates            | 2710       |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.0684     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 13.7       |
| time/                   |            |
|    fps                  | 716        |
|    iterations           | 273        |
|    time_elapsed         | 779        |
|    total_timesteps      | 559104     |
| train/                  |            |
|    approx_kl            | 0.37755558 |
|    clip_fraction        | 0.433      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.515     |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0721    |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.048     |
|    value_loss           | 0.0605     |
----------------------------------------
Eval num_timesteps=560000, episode_reward=13.10 +/- 4.25
Episode length: 220.50 +/- 62.25
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 220      |
|    mean_reward          | 13.1     |
| time/                   |          |
|    total_timesteps      | 560000   |
| train/                  |          |
|    approx_kl            | 0.343169 |
|    clip_fraction        | 0.406    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.455   |
|    explained_variance   | 0.7      |
|    learning_rate        | 0.00074  |
|    loss                 | -0.037   |
|    n_updates            | 2730     |
|    policy_gradient_loss | -0.0524  |
|    value_loss           | 0.0676   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 715      |
|    iterations      | 274      |
|    time_elapsed    | 784      |
|    total_timesteps | 561152   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 212        |
|    ep_rew_mean          | 12.7       |
| time/                   |            |
|    fps                  | 716        |
|    iterations           | 275        |
|    time_elapsed         | 786        |
|    total_timesteps      | 563200     |
| train/                  |            |
|    approx_kl            | 0.41189608 |
|    clip_fraction        | 0.412      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0654    |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.0499    |
|    value_loss           | 0.0581     |
----------------------------------------
Eval num_timesteps=565000, episode_reward=11.70 +/- 4.08
Episode length: 204.50 +/- 55.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 204        |
|    mean_reward          | 11.7       |
| time/                   |            |
|    total_timesteps      | 565000     |
| train/                  |            |
|    approx_kl            | 0.48076397 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.481     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.057     |
|    n_updates            | 2750       |
|    policy_gradient_loss | -0.0578    |
|    value_loss           | 0.0821     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 715      |
|    iterations      | 276      |
|    time_elapsed    | 790      |
|    total_timesteps | 565248   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 209        |
|    ep_rew_mean          | 12.3       |
| time/                   |            |
|    fps                  | 715        |
|    iterations           | 277        |
|    time_elapsed         | 792        |
|    total_timesteps      | 567296     |
| train/                  |            |
|    approx_kl            | 0.37712812 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.472     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.00074    |
|    loss                 | 0.0752     |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.0486    |
|    value_loss           | 0.0804     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 210        |
|    ep_rew_mean          | 12.3       |
| time/                   |            |
|    fps                  | 716        |
|    iterations           | 278        |
|    time_elapsed         | 795        |
|    total_timesteps      | 569344     |
| train/                  |            |
|    approx_kl            | 0.40610406 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.461     |
|    explained_variance   | 0.673      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0514    |
|    n_updates            | 2770       |
|    policy_gradient_loss | -0.0531    |
|    value_loss           | 0.0721     |
----------------------------------------
Eval num_timesteps=570000, episode_reward=10.30 +/- 2.19
Episode length: 188.70 +/- 33.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 189        |
|    mean_reward          | 10.3       |
| time/                   |            |
|    total_timesteps      | 570000     |
| train/                  |            |
|    approx_kl            | 0.47538143 |
|    clip_fraction        | 0.436      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.437     |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0743    |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.0534     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 210      |
|    ep_rew_mean     | 12.2     |
| time/              |          |
|    fps             | 715      |
|    iterations      | 279      |
|    time_elapsed    | 798      |
|    total_timesteps | 571392   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 208        |
|    ep_rew_mean          | 12.1       |
| time/                   |            |
|    fps                  | 715        |
|    iterations           | 280        |
|    time_elapsed         | 801        |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.50424796 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.485     |
|    explained_variance   | 0.716      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0663    |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.0543    |
|    value_loss           | 0.068      |
----------------------------------------
Eval num_timesteps=575000, episode_reward=8.70 +/- 3.47
Episode length: 171.50 +/- 47.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 172       |
|    mean_reward          | 8.7       |
| time/                   |           |
|    total_timesteps      | 575000    |
| train/                  |           |
|    approx_kl            | 0.4319035 |
|    clip_fraction        | 0.453     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.472    |
|    explained_variance   | 0.628     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0719   |
|    n_updates            | 2800      |
|    policy_gradient_loss | -0.0613   |
|    value_loss           | 0.0798    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    fps             | 715      |
|    iterations      | 281      |
|    time_elapsed    | 804      |
|    total_timesteps | 575488   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 197       |
|    ep_rew_mean          | 11.2      |
| time/                   |           |
|    fps                  | 715       |
|    iterations           | 282       |
|    time_elapsed         | 807       |
|    total_timesteps      | 577536    |
| train/                  |           |
|    approx_kl            | 0.4766693 |
|    clip_fraction        | 0.45      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.456    |
|    explained_variance   | 0.559     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0529   |
|    n_updates            | 2810      |
|    policy_gradient_loss | -0.0414   |
|    value_loss           | 0.0766    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 10.7       |
| time/                   |            |
|    fps                  | 716        |
|    iterations           | 283        |
|    time_elapsed         | 809        |
|    total_timesteps      | 579584     |
| train/                  |            |
|    approx_kl            | 0.42330208 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.479     |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0455    |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.0516    |
|    value_loss           | 0.0775     |
----------------------------------------
Eval num_timesteps=580000, episode_reward=7.20 +/- 1.66
Episode length: 156.20 +/- 23.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 156        |
|    mean_reward          | 7.2        |
| time/                   |            |
|    total_timesteps      | 580000     |
| train/                  |            |
|    approx_kl            | 0.66918635 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.518     |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0672    |
|    n_updates            | 2830       |
|    policy_gradient_loss | -0.057     |
|    value_loss           | 0.0777     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 715      |
|    iterations      | 284      |
|    time_elapsed    | 812      |
|    total_timesteps | 581632   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 178        |
|    ep_rew_mean          | 9.72       |
| time/                   |            |
|    fps                  | 716        |
|    iterations           | 285        |
|    time_elapsed         | 815        |
|    total_timesteps      | 583680     |
| train/                  |            |
|    approx_kl            | 0.34661245 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.539     |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0531    |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.0806     |
----------------------------------------
Eval num_timesteps=585000, episode_reward=9.40 +/- 3.20
Episode length: 175.60 +/- 46.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 176        |
|    mean_reward          | 9.4        |
| time/                   |            |
|    total_timesteps      | 585000     |
| train/                  |            |
|    approx_kl            | 0.42361856 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.494     |
|    explained_variance   | 0.562      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0586    |
|    n_updates            | 2850       |
|    policy_gradient_loss | -0.0469    |
|    value_loss           | 0.0876     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 9.33     |
| time/              |          |
|    fps             | 715      |
|    iterations      | 286      |
|    time_elapsed    | 818      |
|    total_timesteps | 585728   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 173        |
|    ep_rew_mean          | 9.33       |
| time/                   |            |
|    fps                  | 715        |
|    iterations           | 287        |
|    time_elapsed         | 820        |
|    total_timesteps      | 587776     |
| train/                  |            |
|    approx_kl            | 0.40771845 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.505     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0553    |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.0512    |
|    value_loss           | 0.0711     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | 9.23       |
| time/                   |            |
|    fps                  | 716        |
|    iterations           | 288        |
|    time_elapsed         | 823        |
|    total_timesteps      | 589824     |
| train/                  |            |
|    approx_kl            | 0.33866286 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.504     |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0687    |
|    n_updates            | 2870       |
|    policy_gradient_loss | -0.0509    |
|    value_loss           | 0.0609     |
----------------------------------------
Eval num_timesteps=590000, episode_reward=10.00 +/- 2.41
Episode length: 179.40 +/- 39.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 179        |
|    mean_reward          | 10         |
| time/                   |            |
|    total_timesteps      | 590000     |
| train/                  |            |
|    approx_kl            | 0.39344704 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.514     |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0736    |
|    n_updates            | 2880       |
|    policy_gradient_loss | -0.0571    |
|    value_loss           | 0.0628     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 9.2      |
| time/              |          |
|    fps             | 715      |
|    iterations      | 289      |
|    time_elapsed    | 826      |
|    total_timesteps | 591872   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 170      |
|    ep_rew_mean          | 9.43     |
| time/                   |          |
|    fps                  | 716      |
|    iterations           | 290      |
|    time_elapsed         | 829      |
|    total_timesteps      | 593920   |
| train/                  |          |
|    approx_kl            | 0.393781 |
|    clip_fraction        | 0.405    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.435   |
|    explained_variance   | 0.745    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0383  |
|    n_updates            | 2890     |
|    policy_gradient_loss | -0.0428  |
|    value_loss           | 0.0617   |
--------------------------------------
Eval num_timesteps=595000, episode_reward=9.70 +/- 4.65
Episode length: 172.70 +/- 54.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 173       |
|    mean_reward          | 9.7       |
| time/                   |           |
|    total_timesteps      | 595000    |
| train/                  |           |
|    approx_kl            | 0.4597059 |
|    clip_fraction        | 0.392     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.436    |
|    explained_variance   | 0.748     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0846   |
|    n_updates            | 2900      |
|    policy_gradient_loss | -0.0529   |
|    value_loss           | 0.0539    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 9.69     |
| time/              |          |
|    fps             | 715      |
|    iterations      | 291      |
|    time_elapsed    | 832      |
|    total_timesteps | 595968   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 175       |
|    ep_rew_mean          | 9.94      |
| time/                   |           |
|    fps                  | 716       |
|    iterations           | 292       |
|    time_elapsed         | 835       |
|    total_timesteps      | 598016    |
| train/                  |           |
|    approx_kl            | 0.5997316 |
|    clip_fraction        | 0.43      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.469    |
|    explained_variance   | 0.679     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0711   |
|    n_updates            | 2910      |
|    policy_gradient_loss | -0.0563   |
|    value_loss           | 0.0696    |
---------------------------------------
Eval num_timesteps=600000, episode_reward=8.10 +/- 2.59
Episode length: 151.10 +/- 35.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 151        |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.46477875 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.483     |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0697    |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.0605     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 715      |
|    iterations      | 293      |
|    time_elapsed    | 838      |
|    total_timesteps | 600064   |
---------------------------------
